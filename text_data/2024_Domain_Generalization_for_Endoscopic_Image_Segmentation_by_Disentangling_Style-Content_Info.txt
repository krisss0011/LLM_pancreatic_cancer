Domain Generalization for Endoscopic Image
Segmentation by Disentangling Style-Content
Information and SuperPixel Consistency
1st Mansoor Ali Teevno
School of Engineering and Sciences
Tecnologico de Monterrey
Monterrey, Mexico
Email: a01753093@tec.mx
2nd Rafael Martinez-Garcia-Pe˜na
School of Engineering and Sciences
Tecnologico de Monterrey
Monterrey, Mexico
Email: rafael.mgp@tec.mx
3rd Gilberto Ochoa-Ruiz
School of Engineering and Sciences
Tecnologico de Monterrey
Monterrey, Mexico
Email: gilberto.ochoa@tec.mx
4th Sharib Ali
School of Computing
University of Leeds
Leeds, UK
Email: S.S.Ali@leeds.ac.uk
Abstract—Frequent monitoring is necessary to stratify individ-
uals based on their likelihood of developing gastrointestinal (GI)
cancer precursors. In the clinical practice, white-light imaging
(WLI), and complimentary modalities such as narrow-band
imaging (NBI) and fluorescence imaging are used to assess risk
areas. However, conventional deep learning (DL) models have
depleted performance due to domain gap when a model is
trained on one modality and tested on a different one. In our
earlier approach we used superpixel based method referred to as
“SUPRA” to effectively learn domain-invariant information using
color and space distances to generate groups of pixels. One of
the main limitations of this early work is that the aggregation
does not exploit structural information, making it sub-optimal for
segmentation tasks, especially for polyps and heterogeneous color
distributions. Therefore, in this work, we propose an approach
for style-content disentanglement using instance normalization
and instance selective whitening (ISW) for an improved domain
generalization when combined with SUPRA. We evaluate our
approach on two datasets: EndoUDA Barret’s Esophagus and
EndoUDA polyps and compare its performance with previous
three state-of-the-art (SOTA) methods. Our findings demonstrate
a notable enhancement in performance compared to both baseline
and state-of-the-art methods across the target domain data.
Specifically, our approach exhibited improvements of 14%, 10%,
8%, and 18% over the baseline and three SOTA methods on the
polyp dataset. Additionally, it surpassed the second best method
(EndoUDA) on the BE dataset by nearly 2%.
Index Terms—Domain Generalization, SuperPixel Consistency,
Instance normalization, Whitening transformation, Endoscopic
imaging modalities
I. INTRODUCTION
The burden and associated costs of GI cancer is increasing
rapidly worldwide. In 2020 alone, gastric cancer was respon-
sible for 1.089 million new cases and 0.769 million deaths.
This makes this disease the fifth most common type of malig-
nancy and fourth major cause of cancer related mortality [1].
Although endoscopy is a vital tool in GI cancer screening and
Dataset 1:
Polyp
Dataset 2:
BE
Source Domain
Target Domain
Input
Mask
Input
Mask
Mask
Mask
Input
Input
Fig. 1: Sample images from EndoUDA dataset. On the
left, these show the images acquired with white light imaging
(WLI) and on the right, a narrow-band imaging frames (NBI)
for polyps and Barret’s Esophagus (BE) [6].
surveillance, this technique is still highly operator-dependent
and thus 12% of these types of cancers are often missed on a
daily basis [2].
In order to cope with such issues, AI and in particular
computer vision methods have been applied in several Com-
puter Aided Diagnosis (CADx) tools. In the endoscopic sur-
gical domain, image analysis methods have yielded promising
outcomes in various downstream tasks such as segmentation,
tracking, and detection in clinical settings [3]. More recently,
deep learning-based (DL) based techniques are being increas-
ingly deployed in endoscopic analysis [4] as data becomes
more available and procedures become more complex [5].
Despite these recent advances, there are still many limita-
tions associated with DL methods. Traditionally in DL, it is
assumed that training and test sets are sampled from the same
data distribution. However, when the training set is sampled
arXiv:2409.12450v1  [cs.CV]  19 Sep 2024
from a given source domain, whereas the test set is obtained
from a similar but different target domain, there will be a
domain shift problem. For instance, if a DL model is trained
on data from given endoscope of data from a hospital A,
it will perform well on the local testing set from the same
hospital data. However, the performance of this model will
be sub-optimal if tested on data from another acquisition
device (i.e., different vendor) or hospital center B. In general,
sources of distribution shift may occur due to multiple factors,
such as the use of different endoscope models, differing
imaging modalities, and even different camera parameters. It is
therefore essential to develop robust and generalizable models,
capable of coping with such data distribution shift problems,
while exploiting the existing data as much as possible, as
medical data for training is scarce is most situations.
II. MOTIVATION AND MEDICAL CONTEXT
In endoscopic imaging, there can be task-specific subdo-
mains where use of specific instrumentation can significantly
modify the visual properties of frames being captured, thus
producing domain shift problem. More precisely, certain en-
doscopic examinations for spotting pre-cancerous or cancerous
lesions can make use of different lighting modalities such
as WLI which is used for a general examination, while
NBI highlights more specific areas. These modalities allow
a clinician to inspect different anatomical aspects of the same
lesion [7], [8]. This is the case for a disorder called Barrett’s
Oesophagus (BE), in which the oesophagus contains columnar
epithelium rather than the squamous epithelium that is typical
for this region of the body. Columnar epithelium is the sort
of lining that typically covers the stomach and intestines [9].
The presence of epithelium can enhance the risk of esophageal
cancer.
For a BE segmentation model to work effectively in both
imaging modalities (Fig. 1 showing two sample images from
the EndoUDA dataset [6] using two different modalities), it is
essential that model learns enough discriminant features from
the source (WLI) modality to generalize well on the target
(NBI) modality as there is a strong indication of change in the
visual properties between the two modalities [6]. In order to
reduce the model complexity and decrease the cost of training,
the model should be able to cope with these varying lighting
conditions, without requiring modality-specific model training.
A number of studies have been conducted to alleviate the
domain shift problem in DL models [11], [12]. These methods
can be subdivided into two main groups: i) domain adaptation
(DA) approaches, where models have access to the target
domain during training and ii) domain generalization (DG)
techniques, where model is trained solely on source domain
and it is designed to perform well on the target domain. There
can be several ways to implement these methods such as using
regularization or augmentation in the context of training of
conventional DL methods; other approaches include exploiting
the image style and content information for disentanglement,
or using frequency spectrum information [13].
Recently, DG methods have demonstrated a promising per-
formance in semantic segmentation tasks in other areas not
related to endoscopy, such as autonomous driving. Inspired
by this previous work, we build upon our own earlier work
SUPRA [14], in which Simple Linear Iterative Clustering
(SLIC) was used to generate preliminary segmentation mask
with the help of novel SLICloss function to propose an
extension that disentangles style and content information from
the image in order to enhance the results obtained by the
superpixel consistency method. Thus, we exploit the feature
covariance of original and photometric transformed images to
effectively suppress the style information and retain the image
content for a better generalization performance of the model,
as introduced in [15]. To evaluate the efficacy of our approach,
we use two sub-sets from the EndoUDA dataset, which was
introduced to assess DG capabilities in segmentation tasks.
The dataset contains images of Barret Esophagus and Polyps
in both WLI and NBI imaging modalities. Our experimental
results indicate a significant performance improvement over
earlier methods in the literature.
The rest of this paper is organized as follows: Section III
surveys the DG methods developed in the literature. Section
IV discusses the proposed DG approach. In Section V we
discuss the details of the the experimental setup, with training
and testing details. Section VI presents the quantitative and
qualitative results of our approach in the EndoUDA dataset.
Finally, Section VII presents conclusion and future work.
III. STATE OF THE ART
DG for semantic segmentation has recently attracted sig-
nificant attention from the research community. Some earlier
approaches have tried to replace batch normalization with
instance norm (IN) for better generalization performance.
To that extent, IBN-Net [16] proposed to integrate instance
norm and batch norm together in the backbone network to
improve model’s ability to learn domain-invariant features.
Authors in [15] proposed to use IN while exploiting the feature
covariance matrix to suppress domain-specific style. To tackle
the limitations of IN, which removes some domain-invariant
information along-with the domain specific information, the
restitution module was proposed to restore the such task-
relevant features [17]. Some other approaches using memory
banks have been proposed to continuously remember the
domain-agnostic knowledge of classes across domains [18].
More recently, yet another approach that has emerged in
recent years for DG is based on the definition of soft con-
straints to improve learned feature representations, instead of
relying on more training data, which is particularly difficult
to obtain in the medical field. For example, authors have
used clustering and patch-based constraints to improve DG
in segmentation applications in other fields [19]. The main
rationale of constraint-based methods is to tackle domain shift
problem as an overfit scenario, where constraints have the
effect of a regularizer that encourages the model to learn
domain-invariant features globally across source domain data.
In this context, a super-pixel patch consistency constraint was
SUPRA
Output mask
Input image
GT mask
Output mask √
˟
Proposed Model
√
˟
√
Encoder
eF
F
Simple Linear Iterative Clustering (SLIC)
Output mask + grid
Output mask
GT mask
GT mask
: Cross-entropy loss
: Instance selective whitening loss
: SLIC loss that uses a weighted cross-entropy and super-pixel guided loss [18]
Input image
Transformed Image
Style-content Disentanglement
Weighted CE and super-pixel
guided loss
Decoder
ISW
block
Fig. 2: Block diagram of our proposed model. Input image is provided to both segmentation network and Simple Linear
Iterative Clustering (SLIC) [10] to compute superpixels. We take intermediate feature maps from the ResNet50 backbone to
apply ISW transformation to disentangle style-content information. The output prediction mask is then combined with the
superpixel grid, where two different loss objectives are computed and combined together denoted as LSLIC: 1) the superpixel
guided loss, which assesses how closely the mask follows the superpixel boundaries (a red circle indicates a segmentation
that does not follow the object edges, while a blue checkmark indicates a border that is carefully followed). 2) binary cross
entroppy loss which determines the overall performance of the network (green checkmark indicates good accuracy while the
red cross mark shows poor performance.)
proposed to reduce the domain shift between data of different
lighting modalities [14].
Superpixel based constraints generate a series of evenly-
spaced centers and group pixels based on color similarities
and distance from each other achieving very good results
in a wide variety of images. The main weakness of such
methods is that they rely on domain-speciic hyper-parameter
that are difficult to optimize To tackle this problem, herein we
propose a dual-learning strategy that the super-pixel guided
consistency of SUPRA with feature covariances to suppress
domain-specific information for endoscopic segmentation. As
our results demonstrate, our proposed approach enables us
to produce state of the art results for two imaging modal-
ities, white light imaging and narrow-band imaging in the
EndoUDA dataset.
IV. PROPOSED APPROACH
A. Preliminaries:
SUPRA [14] is a supepixel based consistency framework
that was proposed for improved generalization on the En-
doUDA BE dataset. The method addressed the DG problem by
proposing a loss function to penalize output predictions from a
base segmentation model in disagreement with color variations
present in the image. The proposed loss was computed by
combining Binary cross entropy loss (for improved classifica-
tion performance) and superpixel guided loss to constrain the
network to focus on preserving color consistency.
Superpixel based consistency can generate evenly spaced
centers and can group pixels based on color similarities, which
achieved promising outcomes in variety of images. However,
the hyperparameter selection and tuning is quite tedious when
it comes to superpixels. It is quite difficult to know how many
superpixels need to be generated, what weight should be given
to any specific color and some spatial boundaries can pose
problems and cause large variability in the results.
Therefore, to alleviate this problem and inspired by the
work in [15], we augment the SUPRA [14] architecture with
ISW to exploit the feature covariance of the image. We
assume that a feature covariance matrix contains both style
and content information of image and the variance of original
and its photometric transformed image can specifically help
us disentangle style-content of the image. The sections below
outline the details of superpixel based consistency and the ISW
block.
B. SLIC superpixel generation and SLICLoss
To alleviate the domain shift problem, SUPRA employs
superpixels that can leverage the visual properties which are
more globally relevant to the lesions in the dataset. The
SLIC superpixel generation algorithm works by using two
main parameters [10]. The first parameter is k, which is the
number of superpixels to generate; this parameter enforces the
generation of similarly sized regions with spacing S =
p
N/k,
where N is the number of pixels in the image. The second
parameter of the algorithm is m, a constant used to calculate
a distance measure used to determine which region a pixel
belongs to,
D =
s
d2c + ds
S
2
m2
(1)
where dc is euclidean distance for each color space, and ds
is the euclidean distance between pixels. A higher value of
m will encourage compactness, creating regions with a lower
area-to-perimeter ratio and more regular shapes. When m is
lower, it produces more irregular superpixels that more strictly
adhere to areas that present a change in color.
The loss function is constructed from two main elements
as can be seen in Fig. 2: The first is Cross Entroppy (CE)
represented by LCE (Fig. 2), which evaluates the overall
correctness of the prediction mask (y’) by comparing it to
the ground truth mask (y). The second is a SLIC consistency
measure denoted as Superpixel Guided Loss (represented
with LSG in Fig. 2). This constraint evaluates whether the
results produced are conforming the superpixel segmentation
generated by SLIC operating on the input frame (xi).
The LSG loss determines the extent to which the superpixel
area is occupied by a single class. This is achieved by
computing the difference between each class occupied area
within a superpixel, and then comparing it with a threshold.
Any superpixel that is occupied by less than the threshold is
said to be inconsistent with the expected boundary (red circle
in Fig. 2). Every superpixel is evaluated in this manner, with
the inconsistencies summed and averaged to produce a final
loss metric.
To combine LCE and LSG, the loss is multiplied by a
weighing factor (λ). The result is then used as the loss for
the network.
LSLIC(x, y, y′) = λ1LCE(y, y′) + λ2LSG(x, y′)
(2)
where λ1 and λ2 are the weighting factors. From the eq. 2, it
can be observed that the weighing factor (λ2) can be leveraged
to favor more accurate results, or more superpixel consistent
results. A higher value of λ will boost the effect of the
superpixels, but at the cost of decrease in the overall accuracy.
The weighting factor (λ2), number of superpixel generated
(k), compactness (m), and the threshold for superpixel guided
loss (LSLIC) are hyperparameters that must be tuned in
accordance to properties in the source domain. For the details
on hyperparameter tuning, readers are directed to the earlier
work in [14].
C. Instance Selective Whitening (ISW) block
Previous literature shows that a whitening transformation
(WT) can remove domain-specific style information and boost
the overall DG performance [20]–[22]. If we denote a feature
map as F ∈RN×C×H×W , then a WT is a linear transforma-
tion which standardizes features by keeping the variance to 1
while removing the correlations among the channels. The de-
correlation process makes the feature covariance matrix (θs)
close to an identity matrix. Earlier approaches to compute
WT was using eigen-value decomposition which is highly
computationally intensive. Alternatively, the deep whitening
transformation (DWT) was proposed in GDWCT [20] and can
be determined by:
LDWT = E[∥θµ −I∥1]
(3)
where E represents the arithmetic mean. There are two lim-
itations of WT: i) it tends to distort segmentation boundaries
[21] and ii) it reduces feature discrimination [22] because
feature covariance matrix (θs) contains both domain-specific
and domain-invariant information. Therefore, in our frame-
work work we introduce the ISW block to selectively remove
the style while retain structural information (see Fig. 3). The
ISW block takes standardized features (Instance normalization
applied to the original features) F of the original and trans-
formed images from the intermediate layers of the backbone
network. The network is initially trained for n epochs (n
was empirically set to 5) to obtain stable covariance matrices.
Afterwards, the covariance θs for both feature maps, as well
as their variance V are computed as follows:
θs =
1
h × w( eF)( eF)T
(4)
V = 1
N
N
X
i=1
1
2((θs(xi) −µθ)2 + (θs(T xi) −µθ)2)
(5)
where µθ represents the mean of both covariance matrices. It
is assumed that high variance values in V indicate the presence
of style information which must be suppressed. Therefore,
a mechanism to disentangle and separate such values was
implemented using k-means clustering. This results in two
distinct groups, Ghigh (containing domain style) and Glow
(containing content information). Based on this clustered V,
we compute the mask Mv and consequently LISW as follows,
LISW = E[|θs
J Mv|]
(6)
The overall cost function for our proposed approach is given
by,
Ltotal = Ltask +
L
X
i
λLi
ISW + LSLIC(x, y, y′)
(7)
with λ denotes the weight of ISW loss, L represents the
number of layers, Ltask is cross-entropy loss for semantic
segmentation and x, y, y’ denote input image, ground truth
and prediction respectively. Fig. 4 presents the impact of using
combination of LISW and LSLIC.
V. EXPERIMENTAL DESIGN
A. Implementation details
For the ISW implementation, we used DeepLabv3 as the
baseline model for semantic segmentation with ResNet50 as
a backbone. We used SGD as an optimizer and a starting
learning rate of 1e-2 followed by polynomial learning rate
eF
LISW
Instance selective whitening (ISW) block
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Variance, V
1
0
1
0
0
0
0
0
0
0
1
0
0
0
1
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
θs(xi)
θs(xi)
θs(T xi)
Mask, Mv
Input image
Transformed Image
Encoder
√
˟
√
Encoder
ISW
block
eF
F
LISW
Decoder
Ltask
ISW
block
F
LISW
Decoder
Ltask
Ltotal = Ltask + LISW + Cm
Linear Iterative Clustering
Superpixel Guided Loss
Cross Entroppy (CE) loss
output mask + grid
SLICLoss(Cm)
Ltotal = λ1CE + λ2Cm + LISW
(Cm)
Fig. 3: Instance selective whitening (ISW) block. Original input image and its photometric transformed image are passed
through the backbone architecture (ResNet50) where feature covariance matrices are computed at three different layers of
ResNet50. Variance is computed from both covariance to determine the the mask matrix which is then used to selectively
whiten the feature covariance to disentangle style-content information.
SUPRA
Output mask
Input image
GT mask
Output mask √
˟
Proposed Model
Fig. 4: Effect of combined loss: Evaluating the impact of using combined loss of the proposed model and superpixel guided
loss. It can be observed in the top part that SUPRA [14] fails to delineate object boundaries which could lead towards poor
segmentation output while our proposed architecture does well at correctly segmenting the object of interest. The superpixel
grids obtained in the proposed model are at k=500.
scheduling with power of 0.9 was used for loss optimization.
We Incorporated various augmentations such as color jittering,
center cropping, Gaussian blur, random cropping, random
horizontal flipping and random scaling. The batch size was
set as 2 and a center cropping size of 500.
The model was trained using Microsoft azure 2 NVIDIA
Tesla P100-SXM2-16GB GPUs. For the superpixel consis-
tency, we resized the images from 1164 x 1030 to 256
x 256 using bilinear interpolation for all models. We used
Geometric data augmentations including horizontal mirroring,
rotation, width and height shift, shearing, and zooming with a
probability of 5% except for rotation which was fixed to 20%.
B. Datasets
We used EndoUDA dataset which contains endoscopic GI
images of two datasets namely, Barrett’s esophagus (BE) and
polyps. To evaluate the efficacy of the proposed pipeline we
have used clinically acquired white light imaging (WLI)and
narrow-band imaging (NBI) modality data for BE and polyps.
BE dataset consists of 799 endoscopy images acquired from 68
unique patients of which 515 WLI images are used as source
domain data (train set: 80%, validation set:10%, and test set:
10%) and 284 NBI images as target domain data are used for
testing. Similarly, for the polyp dataset, we used 1042 images
of which 1000 in WLI modality (train set: 80%, validation
set: 10%, test set: 10%) and 42 images in NBI modality for
testing.
VI. RESULTS
In this section, we present the quantitative and qualitative
results of baseline models and our proposed approach. We
compare our results with three state of the art methods: IBN-
Net [16], RobustNet [15], EndoUDA [6] and vanilla baseline
method DeepLabv3+ [23]. The results are reported in terms
TABLE I: Table showing Intersection over Union (IoU) scores for the baseline, SOTA and our proposed method. Standard
deviations (σ) are provided except SUPRA [14] method. All the results were obtained on models trained on source domain
dataset only. The missing values indicate that results were not available for the method.
EndoUDA (polyp)
Test (WLI modality)
Target (NBI modality)
Method
IoU
Prec.
Rec.
Acc.
IoU
Prec.
Rec.
Acc.
DeepLabv3+(baseline) [23]
81.4 ± 16.3
90.0
70.0
89.1
64.2 ± 15.5
80.0
80.0
78.3
IBN-Net (CVPR’18) [16]
77.0 ± 16.3
80.0
90.0
85.4
68.0 ± 15.4
100.0
70.0
75.4
RobustNet (CVPR’21) [15]
78.1 ± 16.2
80.0
90.0
85.6
70.2 ± 14.8
100.0
80.0
76.2
EndoUDA [6]
–
–
–
–
60.5 ± 14.0
72.2
70.4
–
SUPRA [14]
–
–
–
–
–
–
–
–
Ours
80.8 ± 16.6
90.0
90.0
87.3
78.0±15.0
90.0
80.0
83.8
EndoUDA (BE)
DeepLabv3+(baseline) [23]
88.2 ± 17.2
90.0
90.0
91.8
60.7 ± 14.7
70.0
90.0
65.4
IBN-Net (CVPR’18) [16]
76.8 ± 18.6
80.0
90.0
80.5
68.3 ± 14.2
80.0
90.0
77.8
RobustNet (CVPR’21) [15]
84.2 ± 16.7
90.0
90.0
87.7
71.3 ± 13.8
80.0
90.0
77.9
EndoUDA [6]
–
–
–
–
73.3 ± 14.0
83.2
78.4
–
SUPRA [14]
67.5
–
–
–
64.5
–
–
–
Ours
84.5 ± 14.2
90.0
90.0
87.5
75.1 ± 13.7
90.0
90.0
83.2
BE Target
Image
GT
DeepLabv3+
IBN-Net
RobustNet EndoUDA
Ours
Polyp Target
Fig. 5: Qualitative comparison: We include frames from tested models on BE and polyp target domain datasets, comparing
between DeepLabv3+ [23], IBN-Net [16], RobustNet [15], EndoUDA [6] and our proposed method. Results indicate that our
proposed model performed very close to the ground truth as compared to other methods.
of four different evaluation metrics including intersection-over-
union (IoU), precision, recall and accuracy.
Quantitative results: We report quantitative results in Ta-
ble I. It can be observed from Table I that DeepLabv3+
[23] achieved better performance on source domain than
our method. However, our approach outperformed all the
state of the art methods as well as the baseline on the
target domain, showing better generalizability. This is ex-
pected due to the regularization effects provided by SUPRA:
some loss in performance is expected in the source domain,
with the advantage of enhanced results on unseen data. The
results on EndoUDA polyp source domain dataset indicate
that DeepLabv3+ [23] achieved 81.4% IoU score followed
by our approach which obtained 80.8% IoU score. On the
target domain, our approach outperformed all state-of-the-
art methods and and the Deeplabv3+ [23] baseline model
with IoU score of 78.0%. This is approximately 14%, 10%,
8% and 18% higher than baseline DeepLabv3+ [23], SOTA
IBN-Net [16], RobustNet [15], and EndoUDA [6], respec-
tively. Similar performance can be observed on EndoUDA
source Barret Esophagus sub-dataset, where DeepLabv3+ [23]
achieved the best performance with 88.2% IoU, whereas our
approach obtained second best results with 84.5% IoU score.
On the BE target domain, our approach achieved an IoU
score of 71.1%, which is substantially higher when compared
to the other methods: approximately 15%, 7%, 4%, and 2%
compared to DeepLabv3+ [23], IBN-Net [16], RobustNet [15],
and EndoUDA [6], respectively. Similar improvements can be
observed for other evaluation metrics for both datasets and
also much higher improvements compared to other state of
the art methods.
Qualitative results: We present the qualitative performance of
our proposed approach, baseline DeepLabv3+ and state-of-the-
art methods in Fig. 5 on both EndoUDA BE and polyp target
domain sub-datasets. It can be observed that our proposed
approach produces results more closer to the ground truth
Hyperparameter
Source Domain IoU
Target Domain IoU
Fixed Parameters
λ
Weight 50%
67.5
69.8
k = 100,
Weight 75%*
81.6
77.1
m = 40
Weight 100%
61.3
55.7
k
50 Superpixels
78.3
74.9
λ = 75%,
150 Superpixels
74.5
70.3
m = 40
500 Superpixels*
76.1
78.0
1000 Superpixels
70.7
65.4
m
20 Consistency
77.8
55.5
λ = 75%,
30 Consistency
79.4
61.8
k = 100
50 Consistency*
71.9
74.1
TABLE II: Effect of hyperparameters on source and target domains: Experiments performed to observe the effect of
different hyperparameters on our proposed network performance. The validation split from the source domain and target test
domain is used to determine the results. We used grid search mechanism to choose the appropriate hyperparameters for the
validation spilt from source domain while we used qualitative assessment of the generated superpixel boundaries to select the
best values of λ, m, k. λ: Weighing factor for the superpixel boundary consistency k: Number of superpixels generated. m:
Compactness. * Hyperparameter value used for the proposed model. The best results are shown with bold formatting.
than other methods in comparison. For example, in the case
of Barrett’s esophagus, the upper regions in the first row is
better segmented than the closes EndoUDA. Similarly, for
polyp target dataset in the last row our method has improved
segmentation compared to the second best methods RobustNet.
The performance mostly is degraded significantly on baseline
and IBN-Net models.
Hyperparameter Search: SLIC-based consistency methods
are highly reliant on the hyperparameter values. To that end,
we employed two ways to evaluate the effect of hyperpa-
rameters on the network performance and to select the best
values for three variables i.e., λ, m, and k: grid search (on
validation source domain data) and qualitative assessment (on
target set data). Table II presents the results for the experiments
performed for the hyperparameters search.
VII. CONCLUSIONS
In this work, we proposed a style-content disentanglement
pipeline to boost generalization performance with superpixel-
based consistency on two domain endoscopic (white light and
narrow-band) imaging datasets. Our novel approach improves
over the SOTA RobustNet model. As demonstrated by our
experiments, it can suppress the domain-specific information,
suggesting that the model can learn discriminant features
for improved generalization from source to target domain.
We evaluated our approach on two gastrointestinal tract (GI)
datasets with significant improvements in target modality over
all SOTA and the baseline with competitive results on the
same modality data. As a future work, we intend to explore
different backbone architectures such as UNet and validate the
approach on data from different centers.
REFERENCES
[1] H. Sung, J. Ferlay, R. L. Siegel, M. Laversanne, I. Soerjomataram, A. Je-
mal, and F. Bray, “Global cancer statistics 2020: Globocan estimates of
incidence and mortality worldwide for 36 cancers in 185 countries,” CA:
a cancer journal for clinicians, vol. 71, no. 3, pp. 209–249, 2021.
[2] S. Menon and N. Trudgill, “How commonly is upper gastrointestinal
cancer missed at endoscopy? a meta-analysis,” Endoscopy international
open, vol. 2, no. 02, pp. E46–E50, 2014.
[3] M. Ali, R. M. G. Pena, G. O. Ruiz, and S. Ali, “A comprehensive
survey on recent deep learning-based methods applied to surgical data,”
2022. [Online]. Available: https://arxiv.org/abs/2209.01435
[4] S. Ali, “Where do we stand in ai for endoscopic image analysis?
deciphering gaps and future directions,” npj Digital Medicine, vol. 5,
no. 1, p. 184, 2022.
[5] X. Luo, K. Mori, and T. M. Peters, “Advanced Endoscopic Navigation:
Surgical Big Data, Methodology, and Applications,” Annual Review of
Biomedical Engineering, vol. 20, pp. 221–251, Jun. 2018.
[6] N. Celik, S. Ali, S. Gupta, B. Braden, and J. Rittscher, “EndoUDA: A
Modality Independent Segmentation Approach for Endoscopy Imaging,”
in Medical Image Computing and Computer Assisted Intervention –
MICCAI 2021, ser. Lecture Notes in Computer Science, M. de Bruijne,
P. C. Cattin, S. Cotin, N. Padoy, S. Speidel, Y. Zheng, and C. Essert,
Eds.
Cham: Springer International Publishing, 2021, pp. 303–312.
[7] S. Barbeiro, D. Libˆanio, R. Castro, M. Dinis-Ribeiro, and P. Pimentel-
Nunes, “Narrow-Band Imaging: Clinical Application in Gastrointestinal
Endoscopy,” GE - Portuguese Journal of Gastroenterology, vol. 26,
no. 1, pp. 40–53, 2019, publisher: Karger Publishers. [Online].
Available: https://www.karger.com/Article/FullText/487470
[8] M. Kato, M. Kaise, J. Yonezawa, H. Toyoizumi, N. Yoshimura,
Y. Yoshida, M. Kawamura, and H. Tajiri, “Magnifying endoscopy with
narrow-band imaging achieves superior accuracy in the differential
diagnosis of superficial gastric lesions identified with white-light
endoscopy: a prospective study,” Gastrointestinal Endoscopy, vol. 72,
no. 3, pp. 523–529, Sep. 2010. [Online]. Available: https://www.
sciencedirect.com/science/article/pii/S0016510710015695
[9] S.
J.
Spechler
and
R.
K.
Goyal,
“Barrett’s
Esophagus,”
New
England
Journal
of
Medicine,
vol.
315,
no.
6,
pp.
362–371,
Aug. 1986. [Online]. Available: http://www.nejm.org/doi/abs/10.1056/
NEJM198608073150605
[10] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. S¨usstrunk,
“SLIC superpixels compared to state-of-the-art superpixel methods,”
IEEE transactions on pattern analysis and machine intelligence, vol. 34,
no. 11, pp. 2274–2282, Nov. 2012.
[11] G. Csurka, R. Volpi, and B. Chidlovskii, “Unsupervised Domain
Adaptation
for
Semantic
Image
Segmentation:
a
Comprehensive
Survey,”
Dec.
2021,
arXiv:2112.03241
[cs].
[Online].
Available:
http://arxiv.org/abs/2112.03241
[12] T. Tommasi, M. Lanzi, P. Russo, and B. Caputo, “Learning the Roots
of Visual Domain Shift,” in Computer Vision – ECCV 2016 Workshops,
ser. Lecture Notes in Computer Science, G. Hua and H. J´egou, Eds.
Cham: Springer International Publishing, 2016, pp. 475–482.
[13] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy, “Domain General-
ization: A Survey,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, pp. 1–20, 2022.
[14] R. Martinez-Garcia-Pena, M. A. Teevno, G. Ochoa-Ruiz, and S. Ali,
“Supra: Superpixel guided loss for improved multi-modal segmentation
in endoscopy,” in Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, 2023, pp. 285–294.
[15] S. Choi, S. Jung, H. Yun, J. T. Kim, S. Kim, and J. Choo, “Robustnet:
Improving domain generalization in urban-scene segmentation via in-
stance selective whitening,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2021, pp. 11 580–11 590.
[16] X. Pan, P. Luo, J. Shi, and X. Tang, “Two at once: Enhancing learning
and generalization capacities via ibn-net,” in Proceedings of the euro-
pean conference on computer vision (ECCV), 2018, pp. 464–479.
[17] X. Jin, C. Lan, W. Zeng, Z. Chen, and L. Zhang, “Style normalization
and restitution for generalizable person re-identification,” in proceedings
of the IEEE/CVF conference on computer vision and pattern recognition,
2020, pp. 3143–3152.
[18] J. Kim, J. Lee, J. Park, D. Min, and K. Sohn, “Pin the memory: Learning
to generalize semantic segmentation,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2022, pp.
4350–4360.
[19] Y. Zhang, Z. Qiu, T. Yao, C.-W. Ngo, D. Liu, and T. Mei, “Transferring
and Regularizing Prediction for Semantic Segmentation,” in 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR).
Seattle, WA, USA: IEEE, Jun. 2020, pp. 9618–9627.
[Online]. Available: https://ieeexplore.ieee.org/document/9156873/
[20] W. Cho, S. Choi, D. K. Park, I. Shin, and J. Choo, “Image-to-image
translation via group-wise deep whitening-and-coloring transformation,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2019, pp. 10 639–10 647.
[21] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Universal
style transfer via feature transforms,” Advances in neural information
processing systems, vol. 30, 2017.
[22] X. Pan, X. Zhan, J. Shi, X. Tang, and P. Luo, “Switchable whitening
for deep representation learning,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2019, pp. 1863–1871.
[23] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-
decoder with atrous separable convolution for semantic image segmen-
tation,” in Proceedings of the European conference on computer vision
(ECCV), 2018, pp. 801–818.
