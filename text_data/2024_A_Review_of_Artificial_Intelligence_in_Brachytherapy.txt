A Review of Artificial Intelligence in Brachytherapy
Jingchu Chen1,2,Richard L.J. Qiu1, Tonghe Wang3, Shadab Momin1 and Xiaofeng Yang1∗
1Department of Radiation Oncology and Winship Cancer Institute, Emory University, Atlanta, GA
30308
2School of Mechanical Engineering, Georgia Institute of Technology, GA, Atlanta, USA
3Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY 10065
*Corresponding author:
Xiaofeng Yang, PhD
Department of Radiation Oncology
Emory University School of Medicine
1365 Clifton Road NE
Atlanta, GA 30322
E-mail: xiaofeng.yang@emory.edu
Abstract
Artificial intelligence (AI) has the potential to revolutionize brachytherapy’s clinical workflow. This re-
view comprehensively examines the application of AI, focusing on machine learning and deep learning,
in facilitating various aspects of brachytherapy. We analyze AI’s role in making brachytherapy treat-
ments more personalized, efficient, and effective. The applications are systematically categorized into
seven categories: imaging, preplanning, treatment planning, applicator reconstruction, quality assur-
ance, outcome prediction, and real-time monitoring. Each major category is further subdivided based
on cancer type or specific tasks, with detailed summaries of models, data sizes, and results presented
in corresponding tables. This review offers insights into the current advancements, challenges, and the
impact of AI on treatment paradigms, encouraging further research to expand its clinical utility.
Keywords: AI, machine learning, brachytherapy, HDR, LDR
1
INTRODUCTION
Brachytherapy is a form of internal radiation therapy that is delivered with either low dose rate (LDR) or
high dose rate (HDR). It involves the direct placement of radioactive sources within or near the tumor
via applicators. Brachytherapy plays a crucial role in radiation therapy due to its ability to deliver
high and conformal radiation doses to the tumor with reduced dose to adjacent organ-at-risk (OARs),
thereby offering an advantageous therapeutic ratio. [13, 20, 75, 111, 117]
After patient consultation and consent for treatment, both LDR and HDR, forms of brachytherapy pro-
cedures, may contain several key steps: (a) Preplanning diagnostic imaging; (b) Surgical insertion of
needles, applicators, or catheters; (c) Treatment planning imaging and verification; (d) Image registra-
tion and segmentation; (e) Applicators/catheters reconstruction; (d) Treatment planning; (f) Quality
assurance (QA); (g) Treatment delivery; (h) Patient follow up.
Overall workflow for brachytherapy procedures can be labor and resource intensive for personnel in-
volved from different disciplines.
However, efficiency and efficacy of brachytherapy procedures is
1
arXiv:2409.16543v1  [physics.med-ph]  25 Sep 2024
Figure 1. Overview of the number of studies in applications of AI in brachytherapy from 2015 to
August 2024 with an approximate dotted trendline. ’Imaging’ combines the image registration, image
segmentation, and other imaging-related tasks. ’Planning’ includes both preplanning and treatment
planning. ’Applicator reconstruction’ includes both prostate and gynecologic cancer-related applicator
reconstructions including needles, catheters, and seeds. ’QA, Outcome, and Monitoring’ combined the
remaining three categories because of their relatively small number of studies.
highly dependent on the clinician’s skills and level of experience owing to their involvement at various
points in the workflow including diagnostics, implantation, and treatment planning. With the goal of
further improving the efficiency and addressing the challenges present in brachytherapy procedures,
studies have integrated artificial intelligence (AI) to facilitate these procedures. In recent years, it has
become clear that AI, which could be considered the fourth industrial revolution, is emerging as a trans-
formative force across various sectors, including healthcare. [98, 109] Radiation oncology and medical
physics, disciplines at the forefront of integrating cutting-edge scientific and technological innovations,
are increasingly exploring the potential of AI to revolutionize treatment paradigms. [29, 52, 94] Within
the realm of AI, Machine learning (ML) relies on statistical models to learn from previous data and
make predictive decisions, which can be useful in selecting brachytherapy applicators and predicting
outcomes. [1, 27, 113, 118, 125] Deep learning (DL), a subset of ML, uses neural networks such as
Convolutional Neural Network (CNN) [89] and Generative Adversarial Network (GAN) [36] to execute
complex image analysis tasks [134], which are fundamentally involved in most of the brachytherapy
workflow. [44]
Figure 1 shows the number of studies that utilized AI for different brachytherapy purposes from 2015
up to August 2024. After a significant increase in the number of studies on AI applications in brachyther-
apy up to 2020, there is a stagnation in the growth of these studies in the subsequent years (Figure 1).
Given the rapid advancements and the potentials of AI to refine and redefine brachytherapy treatment
workflow, a comprehensive review of the current and emerging applications of AI in the context of
brachytherapy is both timely and pivotal to encourage more researchers to study brachytherapy and
to provide physicians with an overview of the current state of AI in brachytherapy. Therefore, we
collected recent developments in the applications of AI in different brachytherapy procedures to pro-
vide a detailed analysis of the potentials of AI in leading to more personalized, efficient, and effective
brachytherapy treatments.
2
Figure 2. Percentage pie chart of applications of AI in different procedures in brachytherapy.
The literature search was conducted in August 2024, initially retrieving 205 papers from PubMed us-
ing the keywords ”brachytherapy”, combined with ”artificial intelligence”, ”deep learning”, or ”ma-
chine learning”. Additional papers were supplemented from The American Association of Physicists
in Medicine (AAPM) and Google Scholar by searching the same keywords. A total of 111 studies were
selected and thoroughly reviewed. The innovative approaches used to develop each unique AI model,
along with their corresponding performances, are presented and categorized by different clinical proce-
dures in brachytherapy workflow, with further sub-categorization based on specific organs or detailed
applications.
The studies are categorized into 7 sections: imaging (section 2), preplanning (section 3), treatment plan-
ning (section 4), applicator reconstruction (section 5), QA (section 6), outcome prediction (section 7),
and real-time monitoring (section 8). The detailed percentage distribution of these studies is illustrated
in Figure 2.
Below is a summary of several common evaluation methods used in the literature for evaluating the
performance of the AI models.
1. The Dice Similarity Coefficient (DSC) is the most used metric for measuring the overlap ratio of
between the automatic (A) and ground truth (B) contours.
DSC = 2 × (A ∩B)
|A| + |B|
,
(1)
2. Hausdorff Distance (HD) measures the maximum distance between points in the predicted segmen-
tation and points in the ground truth images, serving as a measure of dissimilarity. HD95, commonly
used in many studies, disregards outliers by considering only the 95th percentile of HD.
HD95 = max
k95%{sup
a∈A
inf
b∈Bd(a,B),sup
b∈B
inf
a∈Ad(A,b)},
(2)
where: a are the points belong to image set A, and b are the points belong to image set B.
3. Mean Surface Distance (MSD) compares the average difference between the surface of automatic
segmentation (A) and ground truth contours (B).
MSD = 1
AΣa∈A min
b∈B |a −b| + 1
BΣb∈B min
a∈A |b −a|,
(3)
where: a are the points belong to image set A, and b are the points belong to image set B.
3
Figure 3: Inter-fractional changes in gynecologic brachytherapy. The red circle indicates the GTV. The
images display axial (left column), sagittal (middle column), and coronal (right column) views,
demonstrating the variations observed across two treatment fractions with one week interval (upper
and lower rows).
2
Imaging
Many modern imaging models use CNN [89] or fully convolutional networks (FCN) [74], which incor-
porate spatial information to perform pixel-wise classification for computer vision tasks. Ronneberger
et al. [102] built the first U-Net structure based on FCN specifically for biomedical image segmenta-
tion. The U-Net model specializes in biomedical image segmentation because of its incorporation of
data augmentation with elastic deformation, which reduces the need for large training dataset. This
model also addresses tissue deformation variability through elastic deformation data augmentation.
Several models use U-Net as a backbone, enhancing segmentation results with additional multiple skip
connections between encoder levels and self-adapting frameworks, such as Unet++ [152] and nnU-Net
(no-new-net) [53]. Additionally, U-Net can be combined with Transformers, such as the TransUNet [15]
and UNETR (UNEt TRansformers) [41], which use local semantic and texture information while incor-
porating long-range dependencies among pixels [42]. Since imaging is a crucial element in brachyther-
apy, the neural network-based models and their variations are extensively employed to perform image
registration, image segmentation, and other applications.
2.1
Image Registration
Image registration is the process of aligning multimodality medical images or the single modality med-
ical images between different treatment fractions in brachytherapy. Table 1 provides an overview of the
methods and results from current AI-based image registration implementations.
Gynecologic (GYN) brachytherapy typically involves multiple treatment fractions, during which organ
deformation occurs due to varying bladder and rectum filling, applicator insertion, and inter-fractional
tumor changes [115]. Figure 3 illustrates the inter-fractional changes of the gross tumor volume (GTV),
which is labeled in red, in GYN brachytherapy. Organ deformations cause dosimetric uncertainty for
the target and OARs, making image registration necessary. The transformation in image registration can
be subdivided into rigid, affine, and deformable, while the registration method can be either intensity-
based or geometric-based.
To address inter-fractional changes of organs involved in GYN brachytherapy, Salehi et al. [103] devel-
oped a deep learning-based non-rigid deformable image registration algorithm (DIRNet) for aligning
4
CT images of the cervix and OARs. By fixing the bony structures and applying deformed binary masks
for the organs, the study showed that DIRNet achieved comparable results in the DSC and significantly
better Jaccard distance (JI) – the ratio of the intersected region relative to the union of the automatic
and ground truth contours - and MSD than the conventional intensity-based image registration (Sim-
pleElastix), as detailed in Table 1. Besides deformable image registration, rigid registration can be
performed based on applicator geometries. Ecker et al. [26] combined deep learning-based applicator
segmentation with existing rigid registration methods to automate the registration between MR-guided
GYN brachytherapy fractions. A 3D UNETR model was used to segment applicators with a DSC of 0.70
± 0.07 and served as the reference for rigid registration. The mean distance error (MDE) of registration
using the predicted segmentation was 2.7 ± 1.4 mm, which is relatively high compared to the error
using ground truth segmentation, which was only 0.7 ± 0.5 mm. Although the current registration
error remains above the desired registration error of 2 mm, this discrepancy is likely due to the lim-
ited segmentation accuracy of applicators. The study demonstrated that if a segmentation method that
produces similar results as the ground truth is used, the registration error can be reduced, achieving an
automated registration process significantly faster than manual methods.
In prostate brachytherapy, image registration is crucial for fusing organ information between multiple
image modalities. Transrectal ultrasound (TRUS) is often used for guiding the insertion of needles or
catheters in prostate brachytherapy, while MRI offers greater soft tissue contrasts compared to TRUS.
The incorporation of knowledge from MRI into TRUS images can thus provide additional soft tissue
guidance. However, registering MRI to TRUS images remains challenging due to the limited availability
of ground truth deformation of the prostate.
To address this challenge, several different AI-based methods are provided. Zeng et al. [145] developed
a fully automated deep learning system based on a weakly supervised method, which trains models
using only partially labeled data. The entire workflow could be summarized into three steps: initial
segmentation on TRUS and MRI using two FCNs, affine registration using a 2D CNN, and non-rigid
registration using 3D U-Net based network. The initial segmentation results produced reliable results
for the registration, where the DSC = 0.88 ± 0.05 and 0.92 ± 0.03 for the MRI and for TRUS. The affine
and non-rigid registration methods were followed using MRI-TRUS labels (SR-L) and MRI-TRUS images
(SR-I) as inputs. Overall, the model produced the best DSC, mean target registration error, mean MSD,
and HD using the SR-L input method, with DSC = 0.91 ± 0.02, target registration error = 2.53 ± 1.39
mm, MSD = 0.88 mm, and HD = 4.41 mm. Using the labels as input and deep-learning methods for
initialization, Zeng’s approach showed high accuracy in automatic registration of MRI-TRUS images of
prostate.
Besides the label-based method, Chen et al. [17] provided a segmentation-based method, which used
3D V-Net models to segment the prostate after catheter insertion on MR and US images, align the
centroids, and probability maps to predict deformable displacement fields. Despite the presence of
catheter artifacts in the images, the model achieved a DSC of 0.87 ± 0.05, a Center of Mass distance
error of 1.7 ± 0.89 mm, an HD of 7.21 ± 2.07 mm, and an MSD of 1.61 ± 0.64 mm. The segmentation-
based method provided slightly inferior results than the label-based method, likely due to the results
of initial segmentation (DSC values of 0.86 ± 0.05 and 0.90 ± 0.03 on MR and US) and utilization of
hierarchical information of anatomy rather than image intensities. While current studies have shown
positive outcomes when integrating MRI into TRUS-based workflows, using MRI-registered contours,
whether rigid, semi-rigid, or deformable, may still lead to significant dose under-coverage. [107]
2.2
Image Segmentation
Image segmentation involves defining various target volumes that require treatment and OARs that
require sparing during brachytherapy. It serves as the foundation for various tasks in the brachytherapy
workflow including treatment planning. We summarized the AI segmentation methods and results for
GYN-related tasks in Table 2 and for prostate-related tasks in Table 3.
2.2.1
GYN
The GYN-related segmentation studies focused on segmenting the GTV, the high-risk clinical target
volume (HR-CTV) which extends from the GTV to account for possible microscopic spread of cancer,
5
TABLE I. AI applications in image registration
Cancer Site
Image
Modality
Registration
Method
Number
of
Patients
Model
Result Summary
Citation
GYN
CT
Non-rigid
57
DIRNet
Mean MSD (mm): Model = 1.61 ± 0.46,
1.17 ± 0.15, 1.06 ± 0.42; SimpleElastix
= 2.94 ± 0.78, 3.26 ± 0.74, 3.04 ± 1.50
(cervix, bladder, rectum).
Mean MSD (mm): Model = 1.61 ± 0.46,
1.17 ± 0.15, 1.06 ± 0.42; SimpleElastix
= 2.94 ± 0.78, 3.26 ± 0.74, 3.04 ± 1.50
(cervix, bladder, rectum).
Mean JI: Model = 86 ± 4%, 93 ± 1%, 88 ±
4%; SimpleElastix = 71 ± 8%, 83 ± 4%, 67
± 11% (cervix, bladder, rectum).
[103]
GYN
MRI
Rigid
56
2D
U-Net
and
3D
UNETR
MDE between dwell positions = 2.7±1.4 mm.
[26]
Prostate
MRI-TRUS
Affine and non-
rigid
36
FCN,
2D
CNN,
and
3D U-Net
DSC = 0.91 ± 0.02, target registration error =
2.53 ± 1.39 mm, MSD = 0.88 mm, and HD =
4.41 mm.
[145]
Prostate
MRI-TRUS
Affine and non-
rigid
32
FCN
and
RNN
DSC = 0.90 ± 0.04, target registration error =
2.77 ± 1.40 mm.
[141]
Prostate
MRI-TRUS
Rigid
121
3D
V-Net
and
Proba-
bility Maps
DSC = 0.87 ± 0.05, Center of Mass distance
error = 1.7 ± 0.89 mm, HD = 7.21 ± 2.07 mm,
and MSD = 1.61± 0.64 mm.
[17]
Prostate
MRI-TRUS
Deformable
642
Weakly-
supervised
Volumetric
Registration
DSC = 0.873 ± 0.113, HD = 4.56 ± 1.95 mm,
and MSD = 0.053 ± 0.026 mm.
[131]
Prostate
MRI-TRUS
Rigid
662
Attention-
Reg
DSC = 0.82 ± 0.06 and Surface Registration
Error = 5.99 ± 3.52 mm.
[112]
Note: Abbreviations: RNN (recurrent neural network).
Figure 4: Image segmentation on MRI for a GYN brachytherapy patient, showing axial, sagittal, and
coronal views. The segmented structures are indicated by different colors: magenta for the cervix,
yellow for the bladder, green for the bowel, orange for the uterus, gold for the rectum, and blue for the
recto sigmoid.
and the OARs including bladder, rectum, sigmoid, and small intestine. As shown in Figure 4, various
structures need to be considered for GYN brachytherapy patients.
Incorporating MRI into image-guided brachytherapy treatment planning enhances the clarity of targets
and OARs. However, contouring organs on MRI is a labor-intensive process, especially problematic
when performed while the patient is immobilized with the applicator inserted during brachytherapy.
For this reason, several studies aim at developing an automated method for GYN organ segmentation
on MRI post applicator insertion. Yoganathan et al. [135] trained two deep CNN architectures, a resid-
ual neural network (ResNet50) and an inception residual network (InRN) to automatically segment the
GTV, HR-CTV, and the OARs on MRI scans for patients with Foleys catheter inserted. The GTV often
appears in non-uniform shapes and variable sizes, which makes it challenging to identify. The study
implemented a 2.5D method that utilizes axial, sagittal, and coronal views to include additional con-
textual spatial information. Combining 2.5D model with InRN network architecture yielded optimal
segmentation results as detailed in Table 2. However, MRI scans exhibit variations due to different clin-
ical setting, such as different scanners vendors, scanning parameters, and applicator types. To enhance
model robustness and adaptability to various applicators and MRI scanners, two studies incorporated
diverse settings in their training data, aiming to develop a more generalized model. Zabihollahy et al.
6
[140] trained a 2-step CNN (3D Dense U-Net) with different MRI setups: MR1 with a repetition time
(TR) of 2600 ms and time to echo (TE) of 95 ms, and MR2 with a TR of 3500 ms and TE of 97 ms. Ni
et al. [82] fine-tuned a pre-trained model with diverse training groups mixing different MRI scanners
(1.5T Siemens Espree and 3T Siemens Verio) and brachytherapy applicators (tandem and ring, Syed-
Neblett template, Venezia applicator). The optimal segmentation results of OARs from both studies
suggest that training with mixed data improves generalizability, making the models more viable for
future clinical implementation. The rapid segmentation of AI models can also facilitate MRI-guided
brachytherapy planning. Kim et al. [61] developed a dual convolution-transformer U-Net (DCT-UNet)
that provided HR-CTV and OAR segmentation along with a real-time active needle tracking function to
enhance the efficiency of MRI-guided brachytherapy procedures. The model is also incorporated into
the treatment planning system (TPS), to assist radiation oncologists in precisely placing catheters un-
der MRI guidance. The approach involved deformably registering the MRI from the primary treatment
planning image with contours (T2SPACE) to the MRI acquired during the procedure (T2QM), enabling
accurate and real-time tracking for improved guidance. The model achieved high DSC scores for OAR
segmentations, despite having less optimal results for the HR-CTV on T2QM. This demonstrates its
potential for improving catheter placement in MRI-guided brachytherapy.
CT has been extensively used in image-guided adaptive brachytherapy, facing similar time-constraint
challenges as MRI. Several studies have employed neural network-based models to simultaneously seg-
ment HR-CTV and OARs on CT images [25, 56, 70, 124, 133, 146, 153]. Li et al. [70], Duprez et al. [25],
and Xue et al. [133] employed nnU-Net-based models to segment HR-CTV and OARs across various
configurations, including 2D U-Net, 3D U-Net, and 3D Cascade U-Net. Unlike regular U-Net, the nnU-
Net handles training parameters autonomously for new tasks and each segmentation task uses the best
corresponding architecture. Li and Duprez’s studies produced similar results in terms of DSC, HD95,
and MSD with the 3D Cascade U-Net configuration in nnU-Net. However, Xue’s study incorporated a
prompt-based feature, which allows users to define a box around the target, outperforming the original
nnU-Net model with a remarkably high DSC of 0.96 for HR-CTV and 0.91 for the rectum. All three
studies found no statistically significant dosimetric differences between the manual and generated con-
tours. These findings support the incorporation of a prompt-based feature in segmentation models to
improve the results of complex structure segmentation, like the HR-CTV.
2.2.2
Prostate
For prostate brachytherapy, the target is typically visualized using PET/CT, TRUS, MRI, or CT during
different procedures. Several studies have employed deep learning to segment the prostate, lesion, and
OARs across different imaging modalities.
Dose boost on the dominant intraprostatic lesion (DIL) could potentially enhance the treatment out-
comes [37, 121, 127]. Accurately segmenting the DIL is thus important for an effective DIL boost in
prostate HDR brachytherapy plans, to ensure precise delivery of boost dose. PET/CT imaging can
provide detailed morphological/anatomical information about the prostate and DIL. Matkovic et al.
[78] used the Cascaded Regional-Net to automatically segment the prostate and DIL on PET/CT im-
ages. The Cascaded Regional-Net used a Dual Attention Network to extract deep features and identify
the volume-of-interest (VOI) of the prostate, narrowing the location range of the DIL. Subsequently,
a mask scoring regional convolutional neural network (MSR-CNN) detected the VOIs of the DILs and
segmented the DIL from the prostate VOI. The MSDs were 0.666 ± 0.696 mm and 0.814 ± 1.002 mm,
with DSCs of 0.932 ± 0.059 and 0.801 ± 0.178 for the prostate and DIL, respectively. The DSC for DIL
is relatively low due to its small size and irregular shape. The CT scans provide additional anatomical
structures of the patient but may add complexity in lesion segmentation. Wang et al. [126] conducted
a similar study using Cascaded U-net to segment the lesions on PET scans with and without CT infor-
mation. More lesions were detected on PET only compared to PET/CT scans (153/155 vs. 144/155),
but there was no statistically significant difference in the DSC and HD95 between the PET only and
PET/CT images as indicated in Table 3. Additionally, other studies investigated lesion segmentation
using different methods. Li et al. [69] tested their model on 56 PET/CT scans from external institutions
and showed no statistically significant difference compared to the internal testing results.
Prostate brachytherapy relies on TRUS images to guide implants. Accurate delineation of the prostate
and OARs may enable a TRUS-based planning workflow, eliminating the need for additional CT or MR
scans. However, as illustrated in Figure 5, the segmentation process is challenging due to the unclear
7
TABLE II. AI-based GYN-related segmentation results
Image Modality
Number of Patients
Model
Evaluation Parameters
Results
Citation
HR-CTV
Bladder
Rectum
Sigmoid Colon
Small Intestine
GTV
MRI
39
ResNet50 and InRN
DSC
0.85 ± 0.06
0.90 ± 0.05
0.76 ± 0.07
0.65 ± 0.12
0.54 ± 0.12
0.62 ± 0.14
[135]
HD95 (mm)
4.87 ± 2.19
6.28 ± 3.42
8.20 ± 4.07
20.44 ± 11.70
22.3 ± 13.66
6.83 ± 2.89
MRI
129 for MR1 and 52 for MR2
3D Dense U-Net
DSC (MR1)
-
0.93 ± 0.04
0.87 ± 0.03
0.80 ± 0.10
-
-
[140]
DSC (MR2)
-
0.94 ± 0.05
0.88 ± 0.04
0.80 ± 0.05
-
-
HD95 (mm) (MR1)
-
4.18 ± 0.52
2.54 ± 0.41
5.03 ± 1.31
-
-
HD95 (mm) (MR2)
-
2.89 ± 0.33
2.24 ± 0.40
3.28 ± 1.08
-
-
MRI
136
nnU-Net and transfer learning
vDSC
(US)
-
0.93 ± 0.04
0.87 ± 0.06
0.7 ± 0.2
0.7 ± 0.1
-
[82]
sDSC
(US)
-
0.80 ± 0.07
0.78 ± 0.09
0.7 ± 0.1
0.5 ± 0.1
-
HD95 (mm)
(US)
-
4 ± 5
7 ± 5
20 ± 15
20 ± 15
-
vDSC
(UA)
-
0.93 ± 0.04
0.85 ± 0.06
0.7 ± 0.1
0.7 ± 0.3
-
sDSC
(UA)
-
0.76 ± 0.07
0.70 ± 0.08
0.6 ± 0.1
0.5 ± 0.2
-
HD95 (mm)
(UA)
-
4 ± 2
10 ± 8
20 ± 15
30 ± 30
-
MRI
121
DCT-UNet
DSC (T2SPACE)
0.70 ± 0.12
0.94 ± 0.10
0.92 ± 0.11
0.84 ± 0.15
-
-
[61]
DSC (T2QM)
0.66 ± 0.10
0.98 ± 0.02
0.81 ± 0.04
0.80 ± 0.09
-
-
MRI
195
3D nnU-Net
DSC
-
-
-
-
-
0.73 [0.50-0.80]
[101]
HD95 (mm)
-
-
-
-
-
6.8 [4.2 –12.5]
MSD (mm)
-
-
-
-
-
1.4 [0.9 – 2.8
MRI
125
3D U-Net
DSC
0.85 ± 0.03
-
-
-
-
-
[141]
HD95 (mm)
3.70 ± 0.35
-
-
-
-
-
MRI and CT
65
Dual-path CNN
DSC
0.76 ± 0.06
-
-
-
-
-
[11]
HD95 (mm)
5.99 ± 1.68
-
-
-
-
-
Sensitivity
0.81 ± 0.04
-
-
-
-
-
Precision
0.83 ± 0.04
-
-
-
-
-
CT
91
DSD-UNET
DSC
0.83 ± 0.04
0.87 ± 0.03
0.82 ± 0.05
0.65 ± 0.08
0.80 ± 0.06
-
[146]
HD (mm)
8.1 ± 2.3
12.1 ± 4.0
9.2 ± 4.6
19.6 ± 8.7
27.8 ± 10.8
-
Jaccard Index
0.72 ± 0.04
0.78 ± 0.03
0.72 ± 0.05
0.52 ± 0.08
0.69 ± 0.06
-
CT
200
RefineNet
DSC
0.861
0.86
0.858
0.664
0.563
-
[56]
HD (mm)
6.005
19.98
12.27
98.41
68.12
-
Overlap Index
0.839
0.783
0.894
0.601
0.811
-
CT
62
nnU-Net
DSC
0.84 ± 0.07
0.94 ± 0.05
0.83 ± 0.07
-
-
-
[70]
HD95 (mm)
7.42 ± 5.02
3.50 ± 1.96
7.58 ± 5.86
-
-
-
MSD (mm)
2.09 ± 1.31
0.94 ± 0.50
3.60 ± 3.49
-
-
-
CT
100
nnU-Net
DSC
0.81 ± 0.05
0.92 ± 0.04
0.84 ± 0.04
-
-
-
[25]
HD95 (mm)
6.03 ± 2.01
3.00 ± 1.09
5.25 ± 1.78
-
-
-
MSD (mm)
2.23 ± 0.75
0.84 ± 0.30
1.36 ± 0.43
-
-
-
Precision
0.80
0.91
0.84
-
-
-
CT
60
Modified CNN
DSC
0.87
0.94
0.86
0.79
0.92
-
[124]
HD95 (mm)
1.45
4.52
2.52
10.92
8.83
-
CT
98
SEResU-Net
DSC
0.81 ± 0.05
0.92 ± 0.03
0.85 ± 0.05
0.60 ± 0.12
0.83 ± 0.09
-
[153]
HD95 (mm)
5.23 ± 1.39
4.75 ± 1.48
4.06 ± 1.68
30 ± 8.16
20.5 ± 9.88
-
CT
321
Prompt-nnUnet
DSC
0.96 ± 0.02
-
0.91 ± 0.02
-
-
-
[133]
HD95 (mm)
1.66 ± 1.11
-
3.07 ± 0.94
-
-
-
IoU
0.92 ± 0.04
-
0.84 ± 0.03
-
-
-
CT
113
ResU-Net
DSC
-
0.96 ± 0.04
0.97 ± 0.02
0.92 ± 0.03
-
-
[79]
HD (mm)
-
4.05 ± 5.17
1.96 ± 2.19
3.15 ± 2.03
-
-
MSD (mm)
-
1.04 ± 0.97
0.45 ± 0.09
0.79 ± 0.25
-
-
CT
51
3D U-Net and Long Short-Term Memory
DSC
0.87 ± 0.063
0.86 ± 0.049
0.77 ± 0.084
0.73 ± 0.0102
-
0.72 ± 0.091
[12]
CT
53
Mnet IM
sDSC
0.81 ± 0.007
-
-
-
-
-
[50]
vDSC
0.88 ± 0.001
-
-
-
-
-
Surface Overlap
0.78 ± 0.007
-
-
-
-
-
HD (mm)
3.20 ± 2.00
-
-
-
-
-
MSD (mm)
0.69 ± 0.06
-
-
-
-
-
Note: Abbreviations: US (unseen scanner), UA (unseen applicator), vDSC (volumetric dice similarity
coefficient), sDSC (surface dice similarity coefficient), DSD-UNET (dilated convolution and deep
supervision U-Net), SEResU-Net (U-Net with squeeze-and-excitation ResNet), IoU (intersection over
union), Mnet IM (improved M-Net model).
8
Figure 5: Example of organ segmentation on TRUS used in prostate brachytherapy. Red: prostate;
blue: rectum; green: urethra.
boundary between the prostate and rectum, making it highly dependent on the clinician’ experience.
Several studies have successfully used different deep-learning models to segment the prostate on TRUS
images, and the detailed results are summarized in Table 3 [7, 33, 34, 40, 60, 65, 86, 91, 132, 144].
Among these studies, the most accurate contours are reported from the semi-automatic models by
Girum et al. [34] and Peng et al. [91], both of which achieved DSCs of more than 0.96 for the prostate
segmentation. Girum et al. used a weakly supervised deep learning method, which is a fully connected
CNN with a prior knowledge generator block and a segmentation block. The weakly supervised method
is more effective and less time-consuming than training with fully labeled image data. The segmenta-
tion highly accurate DSC of 0.969 ± 0.009 and HD of 4.25 ± 4.58 mm on TRUS. Interestingly, this model
can also be directly implemented on CT, despite being trained on TRUS images, with only slightly in-
ferior results (DSC = 0.954 ± 0.009 and HD = 5.17 ± 1.41 mm). This study showed the potential for
model adaptation between different imaging modalities, which enables the model to be implemented
in imaging modalities with limited data. Peng et al. built an A-ProSeg model, which combines a closed-
principle-curve-based method, a global closed polygonal segment method, and memory-based differ-
ential evolution model to identify prostate vertices and create a smooth prostate contour. The model
demonstrated robustness as was trained and tested on a diverse dataset containing 226 patients with
945 TRUS slices in total. It achieved a DSC of 0.962 ± 0.024 and a HD of 1.9 ± 0.9 mm and maintained
a similar segmentation performance even when the TRUS images were set to a low signal-to-noise ratio
(SNR) of 0.8. It is worth to note that both methods were semi-automatic, requiring pseudo-landmarks
or radiologist-defined points as inputs, which may be prone to user errors or inter-observer variability.
Fully automatic models can further simplify the contouring process as they do not require human in-
tervention. Orlando et al. [87] trained and validated an accurate fully automatic model for segmenting
the prostate on TRUS on a large dataset of 246 patients. A 2D modified U-Net was built to predict the
2D radial slices from TRUS images and the predicted prostate slices were reconstructed in 3D. Using
this configuration, the median DSC was 0.94, HD was 2.89 mm, volume percent differences (VPD) was
5.78%, and MSD was 0.89 mm. This group conducted another study comparing U-Net and U-Net++
with different training configurations. [88] The U-Net++ model achieved the most optimal segmenta-
tion performance with a training dataset size of 1,000 2D images, regardless of the imaging acquisition
9
TABLE III
Image Modality
Number of Patients
Model
Evaluation Parameters
Results
Citation
Prostate
Prostate Lesion
Bladder
Rectum
Urethra
Seminal Vesicles
PET/CT
49
Cascaded Regional Net and MSR-CNN
DSC
0.932 ± 0.059
0.801 ± 0.178
-
-
-
-
[78]
MSD (mm)
0.666 ± 0.696
0.814 ± 1.002
-
-
-
-
PET/CT
84
U-net, Cascaded U-net, and cascaded detection segmentation network
DSC (PET/CT)
-
0.68 ± 0.15
-
-
-
-
[126]
DSC (PET only)
-
0.68 ± 0.17
-
-
-
-
HD95 (mm) (PET/CT)
-
3.98 ± 2.23
-
-
-
-
HD95 (mm) (PET only)
-
4.16 ± 2.33
-
-
-
-
Detection rate (PET/CT)
-
144/155
-
-
-
-
Detection rate (PET only)
-
153/155
-
-
-
-
PET/CT
137
UNETR
DSC (internal)
-
0.70
-
-
-
-
[69]
DSC (external)
-
0.68
-
-
-
-
IoU (internal)
-
0.566
-
-
-
-
IoU (external)
-
0.548
-
-
-
-
Precision (internal)
-
0.809
-
-
-
-
Precision (external)
-
0.749
-
-
-
-
Recall (internal)
-
0.66
-
-
-
-
Recall (external)
-
0.74
-
-
-
-
TRUS and CT
145
Weakly-supervised CNN and unsupervised CNN
DSC (TRUS)
0.969 ± 0.009
-
-
-
-
-
[34]
DSC (CT)
0.954 ± 0.009
-
-
-
-
-
3D HD (mm) (TRUS)
4.25 ± 4.58
-
-
-
-
-
3D HD (mm) (CT)
5.17 ± 1.41
-
-
-
-
-
Volumetric Overlap Ratio (TRUS)
0.939 ± 0.180
-
-
-
-
-
Volumetric Overlap Ratio (CT)
0.913 ± 0.170
-
-
-
-
-
TRUS
226
A-ProSeg
DSC
0.962 ± 0.024
-
-
-
-
-
[91]
JI
0.944 ± 0.033
-
-
-
-
-
Accuracy
95.7 ± 2.7%
-
-
-
-
-
TRUS
246
2D modified U-Net and 3D reconstruction
DSC
0.941 [0.926, 0.949]
-
-
-
-
-
[87]
Precision
93.2 [88.8, 95.4] %
-
-
-
-
-
Recall
96.0 [93.1, 98.5] %
-
-
-
-
-
VPD
5.78 [2.49, 11.50] %
-
-
-
-
-
HD (mm)
2.89 [2.37, 4.35]
-
-
-
-
-
MSD (mm)
0.89 [0.73, 1.09]
-
-
-
-
-
TRUS
44
Multidirectional Deeply Supervised V-Net
DSC
0.92 ± 0.03
-
-
-
-
-
[65]
HD (mm)
3.94 ± 1.55
-
-
-
-
-
MSD (mm)
0.60 ± 0.23
-
-
-
-
-
TRUS
675
CNN
DSC
0.939 ± 0.035
-
-
-
-
-
[60]
HD (mm)
2.7 ± 2.3
-
-
-
-
-
TRUS
145
Encoder–decoder CNN and DNN
DSC
0.88 ± 0.02
-
-
-
-
-
[33]
HD95 (mm)
2.01 ± 0.54
-
-
-
-
-
Accuracy
96 ± 1%
-
-
-
-
-
MSD (mm)
0.1 ± 0.06
-
-
-
-
-
TRUS
598
ResU-Net
DSC
0.937 ± 0.037
-
-
-
-
-
[7]
HD (mm)
3.0 ± 2.05
-
-
-
-
-
MSD (mm)
1.05 ± 0.71
-
-
-
-
-
TRUS
590
Multi-label method with K-SVD
Volumetric Error (CTV)
9.95 ± 3.53%
-
-
-
-
-
[86]
Volumetric Error (PTV)
8.84 ± 3.13%
-
-
-
-
-
HD (mm) (CTV)
5.40 ± 1.38
-
-
-
-
-
HD (mm) (PTV)
5.48 ± 1.51
-
-
-
-
-
MSD (mm) (CTV)
0.98 ± 0.39
-
-
-
-
-
MSD (mm) (PTV)
1.19 ± 0.48
-
-
-
-
-
TRUS
315
PTN and CPTTA
DSC
0.899 ± 0.035
-
-
-
-
-
[132]
HD (mm)
7.07 ± 3.19
-
-
-
-
-
MSD (mm)
1.30 ± 0.61
-
-
-
-
-
TRUS
132
2D U-Net CNN
DSC
0.872 [0.841, 0.888]
-
-
-
-
-
[40]
HD (mm)
6.0 [5.3, 8.0]
-
-
-
-
-
MSD (mm)
1.6 [1.2, 2.0]
-
-
-
-
-
TRUS and MRI
598
End-to-end CNN
DSC
0.909 ± 0.022
-
-
-
-
-
[144]
TRUS
83
Anchor-free mask CNN
DSC (cross validation)
0.93 ± 0.03
-
0.75 ± 0.012
0.90 ± 0.07
0.86 ± 0.07
-
[68]
DSC (hold-out)
0.94 ± 0.03
-
0.76 ± 0.13
0.92 ± 0.03
0.85 ± 0.06
-
HD (mm) (cross validation)
2.28 ± 0.64
-
2.58 ± 0.7
1.65 ± 0.52
1.85 ± 1.71
-
HD (mm) (hold-out)
2.27 ± 0.79
-
2.93 ± 1.29
1.90 ± 0.28
1.81 ± 0.72
-
MRI
200
2D and 3D U-Net FCNs
DSC (T2)
0.90 ± 0.04
-
0.96 ± 0.04
0.91 ± 0.06
-
0.80 ± 0.12
[105]
DSC (T1)
0.82 ± 0.07
-
0.88 ± 0.05
0.87 ± 0.06
-
0.46 ± 0.21
CT
215
DLAS
DSC (re-trained)
0.82
-
-
0.92
-
0.48
[23]
DSC (built-in)
0.73
-
-
0.81
-
0.37
Note: Abbreviations: DNN (deep neural network), PTN (polar transform network), CPTTA (centroid
perturbed test-time augmentation), K-SVD (K-singular value decomposition), A-ProSeg (accurate
prostate segmentation framework).
type (end-fire, side-fire, or mixed). Interestingly, increasing the training data size does not significantly
enhance the segmentation results but add additional training time.
In prostate brachytherapy, the OARs include the rectum, bladder, and seminal vesicles due to their
proximity to the prostate. An efficient and accurate segmentation method of these organs is essential
to minimize unnecessary radiation dose and reduce treatment toxicity. Three groups have provided
methods to segment the prostate and OARs on TRUS, MRI, or CT images.
Lei et al. [68] designed an anchor-free mask CNN that utilized a fully convolutional one-state object
detector that simultaneously segment the prostate, bladder, rectum, and urethra on 3D TRUS images
within 5 seconds per patient. This model provides fast segmentation on all organs but has limitations
in the segmentation accuracy of the bladder, constrained by the low contrast on ultrasound images.
Sanders et al. [105] trained 120 models with different combinations of 18 FCNs and different convolu-
tional encoders, to segment the prostate and the OARs in T1, T2/T1, and T2-weighted MRIs. Among
all combinations, an FCN with a DenseNet201 encoder yielded the most optimal results besides the
external urinary sphincter and the seminal vesicles, due to their complex shapes. While the model was
trained exclusively on T2-weighted and T2/T1-weighted contrast images, it can also be transferred to
T1-weighted MRI with slightly reduced accuracy. Duan et al. [23] conducted a study to assess the effi-
cacy of the commercial deep learning auto-segmentation (DLAS) software in automatically segmenting
the male pelvis on CT scans. Retraining the DLAS software with institutional data, the segmentation
results improved for the prostate and OARs compared to the built-in setup.
2.3
Other Imaging Applications
In addition to aiding with image registration and segmentation, AI can enhance medical image quality
and generate synthetic images to improve diagnosis and other steps in the brachytherapy workflow.
10
Figure 6. Example of metal artifacts caused by hip prothesis in a CT scan for prostate brachytherapy.
These applications are presented in Table 4.
Metal artifacts on CT images may complicate organ and applicator visualization. Figure 6 is an example
of how metal artifacts caused by hip prothesis can complicate the visualization of patient anatomy and
applicators on CT scans. Huang et al. [51] built a residual learning method based on CNN (RL-ARCNN)
to reduce metal artifacts on CT images for GYN cancer brachytherapy. They generated 600 simulated
artifact image slices from 20 GYN cancer patients to train and validate the RL-ARCNN model. Using
residual learning, the peak signal-to-noise ratio (PSNR) was the highest among all different image patch
sizes with the best result of 38.09 dB in 50 by 50 patch size.
Photoacoustic imaging, used for detecting prostate low-dose-rate (LDR) brachytherapy seeds, is sus-
ceptible to acoustic wave reflection artifacts . Allman et al. [6] built a CNN to identify artifacts and
true source (LDR seeds), and reduce the reflection artifact created by photoacoustic waves to improve
the quality of seed reconstruction. The CNN model achieved a low localization error of point source
with mean ± SD of 0.40 ± 0.22 mm and 0.38 ± 0.25 mm, and a high classification accuracy of 100
Deep learning can also improve image quality by increasing resolution. In practice, 3D TRUS images
with low resolution are usually captured with thick slice of (2-5 mm) in brachytherapy due to time
constraint. He et al. [43] developed a GAN-based framework integrated with a deeply supervised
attention model to construct high-resolution ultrasound images from the low-resolution TRUS images.
The model was trained with high and low-resolution TRUS image pairs from 20 patients, with the high-
resolution images served as ground truths. The proposed model achieved a mean absolute error (MAE)
of 6.5 ± 0.5 and a high PSNR of 38.0 ± 2.4 dB.
In addition to enhancing image resolution, deep learning models can also synthesize images, such as
generating synthetic MRI from CT images, combing the cost efficiency of CT with the soft tissue con-
trast of MRI. Podgorsak et al. [95] and Kang et al. [59] both utilized Pix2Pix [54] and CycleGAN to build
DL-based models to create synthetic MRI from CT scans, namely PCGAN and PxCGAN. Both studies
evaluated the quality of the synthetic MRIs by comparing contour differences between real and syn-
thetic MRIs within the same radiation oncologist (inter-modality) and segmentation differences among
different radiation oncologists within the same image (inter-observer). As presented in Table 4, both
studies successfully visualized the prostate and catheters on the synthetic MRIs and showed that the
11
TABLE IV. AI implementation on other imaging applications
Application
Cancer Site
Image Modality
Number
of
Patients
Model
Result Summary
Citation
Reduce metal arti-
facts
GYN
CT
35
RL-ARCNN
PSNR = 33.83 dB, 38.09 dB, and 36.80 dB for
25 by 25, 50 by 50, and 100 by 100 image
patch sizes respectively.
[51]
Remove artifacts
Prostate
Photoacoustic
17,340
im-
ages
CNN
From the water bath and phantom test, the
mean localization error of point source were
0.40 ± 0.22 mm and 0.38 ± 0.25 mm.
[6]
Improve resolution
Prostate
TRUS
20
GAN
and
Attention
Mean MAE for reconstructed images = 6.5 ±
0.5 and PSNR = 38.0 ± 2.4 dB.
[43]
Synthetic
MRI
from CT
Prostate
CT - MRI
78
PCGAN
DSC = 0.852 ± 0.057 and MSD = 2.47 ±
0.50 mm for interobserver contour dif-
ferences;
DSC = 0.846 ± 0.045 and MSD = 2.19
± 0.69 mm for intermodality contour
differences.
[95]
Synthetic
MRI
from CT
Prostate
CT - MRI
78
PxCGAN
DSC = 0.84 ± 0.05, MSD = 2.30 ± 0.67
mm, and HD = 10.11 ± 2.71 mm for
interobserver contour differences;
DSC = 0.84 ± 0.05, MSD = 2.19 ± 0.69
mm, and HD = 8.34 ± 2.27 mm for in-
termodality contour differences.
MAE = 0.14 ± 0.03, MSE = 0.04 ± 0.01,
PSNR = 68.69 ± 1.44 dB, and struc-
tural similarity index (SSIM) = 0.73 ±
0.11 for differences between synthetic
MRI and real MRI.
[59]
Synthetic
MRI
from CT
Prostate
CT - MRI
140
CycleGAN
and
deep
attention
U-Net
For synthetic MRI generated segmen-
tations: DSC = 0.95 ± 0.03 and MSD =
0.52 ± 0.22 mm for bladder,
DSC = 0.87 ± 0.04 and MSD = 0.93 ±
0.51 mm for the prostate,
DSC = 0.89 ± 0.04 and MSD = 0.92 ±
1.03 mm for the rectum.
[22]
Synthetic
MRI
from CT
Prostate
CT - MRI
49
CycleGAN
For synthetic MRI generated segmen-
tations: DSC = 0.92 ± 0.09, HD = 4.38
± 4.66 mm, and MSD = 0.62 ± 0.89 mm
for the leave-one-out test.
DSC = 0.91 ± 0.07, HD = 4.57 ± 3.03
mm, and MSD = 0.62 ± 0.65 mm for
the hold-out test.
[67]
DSC and MSD from inter-modality contours are comparable or better than the inter-observer differ-
ences. However, the limited patient dataset used in these studies may impact the generalizability of
the models, which affect the image quality and corresponding dosimetric parameters on new synthetic
MRIs. For instance, the dosimetric parameters for PTV and OARs from the synthetic MRI were gen-
erally higher than those from the actual MRI but had no statistically significant difference except for
bladder D1cc, potentially caused by the effect of catheter position on the target. Overall, this approach
has the potential to improve clinical workflows by reducing the need for additional MRI scans.
3
Preplanning
The preplanning process involves developing a brachytherapy treatment plan outside the operating
room prior to its delivery to the patient. As summarized in Table 5, this process can be applied to both
GYN and prostate brachytherapy treatments.
3.1
GYN
For HDR GYN brachytherapy, preplanning involves selecting the appropriate applicators, either intra-
cavitary (IC) or interstitial (IS), based on patient anatomy. Stenhouse et al. [113] trained 12 machine
learning models to predict the use of IC or IS applicator for different GYN cancer patients, selecting
the top three models for final predictions. Important features, such as the needle patterns and clini-
cal contours, were selected by an F-test to reduce complexity introduced by unnecessary features. An
AdaBoost Classifier, a Gradient Boosting Classifier, and a Random Forest (RF) Classifier were selected
to form a voting model because of their superior performances. The final voting model, formulated
from the weighted average of the predicted probabilities from the three models, provides applicator
12
decisions with an accuracy of 91.5 ± 0.9% and a F1 Score of 90.6 ± 1.1%. Another important factor to
consider when selecting the appropriate applicator is soft-tissue deformation caused by applicator in-
sertion. Applicators used in GYN brachytherapy can induce an average point-to-point displacement of
25.0 mm in the uterus. [31] Knowing the changes in soft tissue location post-insertion gives physicians
additional information for selecting the most effective applicator. Ghosh et al. [31] used a deep-learning
model to automatically predict uterus deformation caused by applicators using only pre-surgery MRI
as input. The model utilized a deep CNN model with auto-encoders for pre-surgery uterus segmenta-
tion and a modified U-Net for predicting the uterus deformation during implantation. The predicted
new uterine position had a DSC of 0.881 ± 0.038 and an HD of 5.8 ± 3.6 mm compared to the true
deformed position.
3.2
Prostate
In LDR prostate brachytherapy, preplanning involves determining the seed distribution and the re-
sulting dose distribution prior to the treatment day. Manual planning is time-consuming and heavily
dependent on the experience level of the planner. Nicolae et al. [85] built a machine learning-based
prostate implant planning algorithm (PIPA) system to automate treatment planning for LDR prostate
patients. The percentages of plans that need minor or major modification were approximately the same
for PIPA and manual method, but the algorithm reduced the planning time to 2.38 ± 0.96 minutes
compared to 43.13 ± 58.70 minutes. However, this model was only tested on a small cohort of 41
patients and did not report differences in dose-volume histogram (DVH). In 2021, Aleef et al. [3] em-
ployed a two-stage framework that consisted of conditional generative adversarial networks (cGAN) to
automatically generate treatment plans. The cGAN model predicted needle distribution, and a simu-
lated annealing algorithm optimized seed location. Overall, 90% of the generated treatment plans were
acceptable, with 60% requiring minor modifications and 30% requiring major modifications. Addi-
tionally, significantly less time (3 minutes compared to 20 minutes) was required for an automatic plan
with similar quality to manual plans (a CTV V100 value of 98.98% compared to 99.36%). In a later
study [4], the researchers developed a treatment planning generative adversarial network (TP-GAN)
and achieved similar DVH with less urethral doses in only 2.5 minutes or 3 seconds per plan with and
without fine-tuning. Deep learning models are also potential substitutes for traditional computational
models such as Monte Carlo (MC) simulations due to their fast-processing speed. Berumen et al. used
a 3D U-Net based model that learns MC simulations to predict single-seed dose to medium (DM,M) on
CT images, which produced similar DVH metrics but in significantly less time than MC methods (1.8
ms compared to 2 hours).
In intraoperative LDR brachytherapy, extra seeds are usually ordered to ensure sufficient coverage.
However, unused seeds in the procedure would require physicist to spend additional time documenting
and safely returning them to the vendor. To address this, Boussion et al. [10] used several machine-
learning models to predict the number of seeds needed for LDR prostate brachytherapy. The best-
performing ML model, support vector machines for regression (SVR), reduced the unused seeds from
23 ± 4 to 10 ± 4 when tested on 38 unseen treatments, though a 10% margin is still necessary to prevent
seed depletion during implant.
4
Treatment Planning
In HDR brachytherapy, both prostate and GYN, the treatment planning procedure is often performed
intraoperatively with patients under anesthesia or immobilized, which makes it highly time sensitive.
The detailed methods and results of recent studies employing AI to support the treatment planning
process are summarized in Table 6.
4.1
Prostate
In HDR brachytherapy, the dose distribution largely depends on the positions of applicators, which vary
per patient, making the dose prediction challenging. Figure 7 is an example of prostate HDR treatment
13
TABLE V. AI application in preplanning
Preplanning Task
Cancer Site
Number of Patients
Model
Result Summary
Citation
Select Applicator
GYN
233
AdaBoost,
Gradient
Boost,
and
RF classifier
Accuracy = 91.5 ± 0.9% and F1 Score = 90.6
± 1.1%.
[113]
Predict Applicator
Induced
Uterine
Deformation
GYN
92
CNN
and
modified
U-Net
For predicted uterine position after applica-
tor insertion: DSC = 0.881 ± 0.038 and HD =
5.8 ± 3.6 mm.
[31]
Generate Preplans
Prostate
150
K-nearest
neighbor
No significant difference in prostate V100,
prostate D90, urethra D0.1cc, rectum D1cc,
PTV V100% between manual and ML plans,
except prostate V150% was 4% lower for ML
plans.
[83]
Generate Preplans
Prostate
41
PIPA
No significant differences in prostate D90%,
V100%, rectum V100%, or rectum D1cc between
manual and PIPA plans.
[84]
Predict
Needles
and Seeds Distri-
bution
Prostate
931
cGAN
98.98% achieved 100% of the prescribed
dose, 90% of the generated plans were accept-
able with 60% minor and 30% major mod-
ifications. PTV V100% = 96.55 ± 1.44, PTV
V150% = 56.23 ± 4.37, CTV V100% = 99.36 ±
0.96, CTV V150% = 63.03 ± 5.15.
[3]
Predict Seeds Dis-
tribution
Prostate
961
TP-GAN
98.98% achieved 100% of the prescribed
dose. TPGAN only: PTV V100% = 94.6 ± 3.9,
PTV V150% = 55.0 ± 11.9, CTV V100% = 97.8
± 2.5, CTV V150% = 60.8 ± 13.7. TPGAN and
Simulating Annealing: PTV V100%= 95.9 ±
1.6, PTV V150% = 53.0 ± 3.5, CTV V100%= 98.8
± 0.9, CTV V150% = 59.1 ± 5.0.
[4]
Predict
Single-
Seed
Dose
to
Medium
in
Medium
Prostate
44
3D
U-Net
CNN
The average differences of the predicted and
MC-based calculations were 0.1% for CTV
D90 and 1.3%, 0.07%, and 4.9% for the D2cc
of rectum, bladder, and the urethra.
[9]
Predict Number of
Seeds
Prostate
409
SVR
MSE = 2.55, MAE = 1.21, and maximum er-
ror = 7.29 seeds. When tested on 38 unseen
treatments, reduced unused seeds from 23 ±
4 to 10 ± 4 seeds, and saved 493 seeds in total.
[10]
Note: Abbreviations: MSE (mean squared error).
14
Figure 7. Example of HDR prostate brachytherapy treatment planning on TRUS.
planning performed intraoperatively on TRUS. AI can analyze large datasets and identify complex
patterns, allowing it to predict DVH for the CTV and OARs. [2, 14, 18, 55, 71, 77, 122, 138, 147, 151]
Mao et al. [77] trained a 3D deep convolutional neural network (DCNN), RapidBrachyDL, to perform
dose calculations based on the MC method. RapidBrachyDL had errors less than 1.5% for the CTV
and OAR DVH metrics as shown in Table 6, comparable to the conventional MC method. Interestingly,
although trained with prostate patient data only, it showed transferability to cervical cancer patient CT
scans with errors below 3%. Similarly, Villa et al. [122] trained a DCNN model with MC-based method
generated database, achieving fast inverse planning in only 1.7 seconds with small mean percent errors
(MPE).
To guide the placement of catheters in HDR prostate brachytherapy, Lei et al. [66] developed Reg-Net,
a 3D CNN based deformable multi-atlas registration network. Reg-Net used distance maps of targets
and OARs to register with new patient CT images and predict catheter locations. The resulting DVH
metrics from predicted catheter distributions differed by no more than 5% different from clinical plans,
though there were hot spots in the prostate and excessive dose in OARs. This model quantifies dose
distribution on CT simulations prior to catheter placements, locates catheters more effectively, which
potentially reduces intuitive decision-making in the HDR procedures.
4.2
GYN
For HDR GYN brachytherapy, knowledge-based planning (KBP) using traditional methods has been
proven to standardize treatment plans and effectively predict dose volumes by learning the correlation
between the final plan dosimetry and patient-specific characteristics [139]. KBP uses spatial informa-
tion of patient anatomy and applicators to predict 3D dose for high-quality treatment plans. Cortes et
al. [18] applied a 3D U-Net CNN to conduct KBP for HDR cervical cancer brachytherapy using tandem
and ovoids (T&O) applicators. This model provided objective dose measures for HR-CTV D90 and OAR
D2cc, potentially offering quality checks for planners. MC-based dose calculation can also be performed
with DL models. Akhavanallaf et al. [2] developed a deep neural network (DNN)-based model, per-
sonalized brachytherapy dosimetry simulator (PBrDoseSim), to calculate dose with the MC method.
15
PBrDoseSim predicted the specific energy deposition kernel when the radioactive source is positioned
at the dwell position, which exhibited good agreement with MC calculations. Additionally, the study
provided a baseline comparison and an evaluation of their full dose distribution against the ground
truth as shown in Table 6. The study is limited by using the contours instead of the direct density maps
from the CT images.
To increase model robustness against different GYN brachytherapy settings, Li et al. [71] trained a
Squeeze and Excitation Attention Net (SEAN) with various clinically common applicators including
vaginal applicator, ovoid applicator, T&O applicators, free needles, and multi-channel applicator. The
smallest MAE of the predicted DVH metrics from SEAN, U-Net, and 3D Cascaded U-Net were summa-
rized in Table 6. While SEAN did not outperform U-Net and Cascaded U-Net in rectum and HRCTV
MAEs, it predicted the correct number and dose gradient for each applicator setup and closely resem-
bled the ground truth and had the highest gamma passing rate of 92 ± 10
Several studies showed AI model’s capability in predicting OAR dose [14, 55, 147, 151]. Two studies
[14? ] predicted the rectal toxicity levels from treatment plans, where the best sensitivity, or recall, is
84.75%, and the best specificity, or precision, is 79.87%. When using statistically significant features
only, the model achieved an area under the receiver operating characteristic curve (AUC) of 0.91, indi-
cating its strong discriminative ability. Zhang et al. [147] predicted the dose in bladder, rectum, and
sigmoid using neural networks, correlating D2cm3/D90 for each OAR and its sub-organ, showing strong
correlation and small mean squared error (MSE). Additionally, this model only requires sub-organ vol-
umes of the OAR without additional voxel information, allowing physicians without programming
experience to perform dose predictions.
Besides dose prediction, AI has several other applications in enhancing the treatment planning of HDR
brachytherapy. For example, Gao et al. [30] used dose prediction network (DPN) and a plan-approval
probability network (PPN) to predict the HDR plan approval probability. Pu et al. [96] provided a
method, the intelligent treatment planner network (ITPN) to automatically adjust HDR source dwell
times, optimizing until the objective function converges. Compared to a common clinical model, in-
verse planning simulated annealing (IPSA), the ITPN model preserved OARs better, notably reducing
the bladder D2cc, rectum V150, sigmoid V150, rectum V200, and sigmoid V200. Shen et al. [110] used
deep reinforced learning-based optimization to provide automatic weight tuning in inverse treatment
planning. The method consisted of a Weight Tuning Policy Network (WTPN) which automatically ad-
just the weights of OARs to produce high-quality plans, showing proficiency even trained on a limited
dataset and potential for integration into other treatment planning optimization.
TABLE VI. AI application in treatment planning
Treatment
Planning Task
Cancer
Site
Number
of
Patients
Model
Results
Citation
Inverse Plan-
ning
Prostate
273
DCNN
For prostate: MPE = -0.8 ± 1.0 % for V100,
-0.6 ± 3.1% for V150, and 0.2 ± 1.3% for
V200. For OARs: MPE = 1.7 ± 3.5% for ure-
thra D10, 0.9 ± 2.7% for urethra D30; 0.4 ±
2.6% for rectum D2cc, 2.8 ± 9.2% for rec-
tum D0.1cc.
[122]
Predict
Catheter
Placements
Prostate
90
Reg-Net
The difference between the clinical and
predicted prostate V150, V200, and D90
were 5.0 ± 6.5%, 2.9 ± 4.3%, 0.9 ± 1.5%,
bladder D2cc, V75 were 3.5 ± 3.4% and 0.2
± 0.4 cc, rectum D2cc, V75 were 1.5 ± 4.1%
and 0.1 ± 0.4 cc, urethra V125= 0.3 ± 0.4cc.
[66]
CTV
and
OAR
dose
prediction
Prostate/GYN61
RapidBrachyDL For prostate cancer: dose prediction errors
= 0.73%, 1.1%, 1.45%, 1.05%, for CTV D90,
rectum D2cc, urethra D0.1cc, and bladder
D2cc; For cervical cancer: dose prediction
errors = 1.73%, 2.46%, 1.68%, and 1.74%
for CTV D90, rectum D2cc, sigmoid D2cc,
and bladder D2cc, respectively.
[77]
16
CTV
and
OAR
dose
prediction
GYN
126
3D
U-Net
CNN
Isodose DSC = [0.87, 0.94], mean difference
of the DVH metrics were -0.09 ± 0.67 Gy
for HRCTV D90, -0.17 ± 0.67 Gy for blad-
der D2cc, -0.04 ± 0.46 Gy for rectum D2cc,
and 0.00 ± 0.44 Gy for sigmoid D2cc.
[18]
CTV
and
OAR
dose
prediction
GYN
78
PBrDoseSim
For
model
predicted
single-dwell
dose
kernels,
MRAE
=
1.16±0.42%
MAE=4.2±2.7x10−4
(Gy.sec−1/voxel).
Conformity
index
=
0.24,
dose
non-
uniformity ratio = 0.65, and dose homo-
geneity index = 0.34. The MRAE for CTV
between DNN and MC were 1.5±0.88% for
D95, 1.8±0.86% for D90, 1.3 ± 1% for D50,
0.85 ± 0.43% for V200, 0.56 ± 0.56% for
V150, 1.48 ± 0.72% for V100, 0.26 ± 0.38%
for V50. The MRAE for OARs were 2.7 ±
1.7% and for bladder D5cc and D2cc, 1.9
± 1.3% and 2.4 ± 1.6% for sigmoid D5cc
and D2cc, and 2.1 ± 1.7% and 2.5 ± 2% for
rectum D5cc and D2cc.
[2]
CTV
and
OAR
dose
prediction
GYN
81
SEAN
SEAN: MAE = 0.37 ± 0.25 for HRCTV D90,
0.23 ± 0.14 for bladder D2cc, 0.28 ± 0.20 for
rectum D2cc. U-Net: MAE = 0.34 ± 0.24 for
HRCTV D90, 0.25 ± 0.20 for bladder D2cc,
0.25 ± 0.21 for rectum D2cc. Cascaded U-
Net: MAE = 0.42 ± 0.31 for HRCTV D90,
0.24 ± 0.19 for bladder D2cc, 0.23 ± 0.19
for rectum D2cc.
[71]
CTV
and
OAR
dose
prediction
GYN
224
3D
mask-
guided
dose
prediction
model
Dose prediction errors = 0.63 ± 0.63, 0.60 ±
0.61, 0.53 ± 0.61, 1.21 ± 0.85, 0.71 ± 0.61,
1.16 ± 1.09, and 0.86 ± 0.58, for HRCTV
D95, HRCTV D95, HRCTV D100, bladder
D2cc, sigmoid D2cc, rectum D2cc, and intes-
tine D2cc.
[138]
Rectum
dose
prediction
GYN
42
VGG-16
and
RSDM
10-fold cross validation:
sensitivity =
61.1%, specificity = 70%, and AUC = 0.7.
leave-one-out cross validation: sensitivity
= 75% specificity = 83.3%, and AUC = 0.89.
[151]
Rectum
dose
prediction
GYN
42
SVM
Using principal component analysis (PCA)
features: sensitivity = 74.75%, specificity
= 72.67%, and AUC = 0.82; Using sta-
tistically significant features: sensitivity =
84.75%, specificity = 79.87%, and AUC =
0.91.
[14]
OAR
dose
prediction
GYN
59
LM algorithm
R= 0.80 for bladder, 0.88 for rectum,
and 0.86 for sigmoid; MSE = 5.543×10−3
for bladder D2cc/D90, 5.028x10−3 for rec-
tum D2cc/D90, and 8.815x10−3 for sigmoid
D2cc/D90.
[147]
Predict
Plan
Approval
Probability
GYN
63
DPN and PPN
Dose prediction error = 11.51% ± 6.92%
for bladder, 8.23% ± 5.75% for rectum,
7.12% ± 6.00% for sigmoid D2cc, and
10.16% ± 10.42% for CTV D90. Plan ap-
proval prediction: accuracy = 0.70, sensi-
tivity = 0.74, specificity = 0.65, and AUC =
0.74.
[30]
Inverse plan-
ning - dwell
times
GYN
20
ITPN
Directly output the dwell times of prese-
lected dwell positions of HDR BT for cer-
vical cancer, generate higher quality plans
with better CTV coverage and OARs spar-
ing compared to clinically accepted IPSA.
[96]
17
Organ
Weighting
Factor Adjust-
ment
GYN
10
WTPN
Plan quality score was improved by 8.5%
compared to the initial plan with arbitrar-
ily set weights, and by 10.7% compared to
the plans generated by human planners.
[110]
Intra-
fractional
OARs
dose-
volume
histogram
prediction
GYN
30
ANN
MPE = 6%, 5%, 8%, 7%, 10% for predicting
intra-fractional dose variations of bladder,
rectum, sigmoid, CTVIR, and CTVHR, re-
spectively.
[55]
Optimizing
Treatment
Parameters
Prostate
35
MANN
and
GA
The optimal values for Therapy Dose (TD)
= 47.3 Gy, TD coverage index (CI100%) =
1.14, and PSA nadir = 0.047 ng cm−3 for
low-risk group; TD = 50.4 Gy and CI100%
= 1.6, and PSA nadir = 0.25 ng cm−3 for
high-risk group.
[97]
Note: Abbreviations: MRAE (mean relative absolute error), VGG-16 (16-layers CNN developed by the visual geom-
etry group), SVM (support vector machine), RSDM (rectum surface dose maps), LM (Levenberg–Marquardt), ANN
(artificial neural network), MANN (multilayer artificial neural network), GA (genetic algorithm), MO-RV-GOMEA
(multi-objective real-valued gene-pool optimal mixing evolutionary algorithm), BRIGHT (brachytherapy via arti-
ficial Intelligent GOMEA-Heuristic based treatment planning), Rad-TRaP (radiomics based targeted radiotherapy
planning).
5
Applicator Reconstruction
5.1
GYN
Accurate digitization of applicators and catheters is crucial in HDR GYN treatment planning as it
largely affect the final dose distribution. Table 7 provides a summary of the AI implementation in
different types of applicator reconstruction, such as the T&O, tandem & ring (T&R), fletcher applica-
tors, catheters, and needles. To automatically segment the T&O applicators on CT images, Jung et al.
[57] trained a U-net model with additional spectral clustering and polynomial curve-fitting methods to
identify the locations and the central paths of applicators. Although trained solely on CT images T&O
applicators, the model accurately digitized not only T&O applicators but also Y-tandem and cylinder
applicators on CT, as well as T&O applicators on cone-beam CT. Both tip position errors and HD were
less than 1 mm under various testing setup, indicating the model’s robustness and transferability when
applied to different applicator structures and lower image quality.
MRI provides better visibility of soft tissue but has difficulty imaging applicators due to the inconsis-
tent appearance of contrast fiducials on the applicator lumen. However, HDR cervical brachytherapy
applicators can be reconstructed using MRI alone with library applicator models from the manufactur-
ers, which is potentially more reproducible than digitization on CT images. Hrinvich et al. [47] utilized
a circular Hough transform (CHT) model [24] to identify tandem and ring applicators on T2-weighted
MR images. It is followed by a 3D rotation matrix and a 3D translation vector with a stochastic evo-
lutionary optimizer [114] to obtain the positions of T&R applicators with a rigid registration method.
The proposed method achieved a mean reconstruction accuracy of 0.60 ± 0.24 mm for the ring and
0.58 ± 0.24 mm for the tandem, with mean variability smaller than the inter-observer variability. Plas-
tic catheters have a low hydrogen composition, which makes them difficult to identify on the MRI.
Zaffino et al. [142] developed a 3D U-Net model to automatically segment multiple closely spaced
brachytherapy catheters in MRI. The model produced a good accuracy with an average distance error
of 2.0 ± 3.4 mm, DSC of 0.60 ± 0.17, and a HD of 15.9 ± 20.5 mm. The false positive and false negative
catheters were 6.7% and 1.5%, respectively. Similar results were provided by Shaaer et al. [108], as
outlined in Table 7. The accuracy of the segmentation struggled with tubular structure reconstruction,
especially on MR images where other tubular tissue structures exist.
18
TABLE VII. AI in GYN applicator reconstruction
Type of Ap-
plicator
Image
Modality
Number
of
Patients
Model
Results
Citation
T&O
appli-
cator
CT
13
U-Net
For applicator segmentation: DSC =
0.937 ± 0.064 in 2D and 0.931 in 3D.
HD = 1.01 mm, 1.21 mm, and 1.18
mm. For tip position errors: tandem =
0.5 mm, right ovoid 0.74 mm, and left
ovoid = 0.67 mm.
[57]
T&O
appli-
cator
CT
91
DSD-UNET
For the intrauterine and ovoid tubes:
DSC = 0.921, HD = 2.3 mm. Between
channel paths: HD = 0.88 ± 0.12 mm,
0.95 ± 0.16 mm, and 0.96 ± 0.15 mm
for the intrauterine, left ovoid, and
right ovoid tubes, respectively.
[146]
T&O
appli-
cator
CT
10
HDBSCAN
Mean contour error = 0.3 mm, HD¡=1
mm. For HR-CTV D90, HR-CTV D95,
bladder D2cc, rectum D2cc, large bowel
D2cc, and small bowel D2cc, the median
and mean difference in DVH parame-
ters are all ¡= 1%.
[21]
Fletcher ap-
plicator
CT
70
U-Net
For applicator segmentation: DSC =
0.89 ± 0.09, HD = 1.66 ± 0.42 mm,
shaft error ¡ 0.5 mm, tip error = 0.8
mm. Dosimetric differences = 0.29%
for the D90 of HRCTV, and less than
2.64% for OAR D2cc.
[48]
T&R
appli-
cator
MRI
10
CHT
For dwell positions, the mean errors =
0.60 ± 0.24 mm and 0.58 ± 0.24 mm,
variability = 0.72 ± 0.32 mm and 0.70
± 0.29 mm, and inter-observer vari-
ability = 0.83 ± 0.31 mm and 0.78 ±
0.29 mm, for the ring and tandem, re-
spectively.
[47]
Catheters
MRI
50
3D U-Net
DSC = 0.60 ± 0.17, HD = 15.9 ± 20.5
mm, MDE = 2.0 ± 3.4 mm, false posi-
tive = 6.7%, and false negative = 1.5%.
[142]
Catheters
MRI
20
2D U-Net
DSC = 0.59 ± 0.10 and HD = 4.20 ±
2.40 mm.
Average variation = 0.97
± 0.66 mm with 98.32% ¡ 2 mm and
1.68% ¿= 3 mm
[108]
Interstitial
needles
CT
15
2.5D U-Net
DSC = 0.93 for needle segmentation,
HD = 0.71 mm for needle trajectories,
and HD = 0.63 mm for needle tip posi-
tions.
[58]
Interstitial
needles
CT
17
3D
Unet
with
atten-
tion gates
DSC = 0.937 ± 0.014, JI = 0.882 ±
0.025, HD =3.0 ± 1.9 mm, tip error =
1.1 ± 0.7 mm, and shaft error = 1.8 ±
1.6 mm
[128]
Needle
structures
Ultrasound 5
prostate
patients,
6
GYN
patients
CNN
with
modified
U-Net
For prostate:
needle tip error = 1.5
[0.9, 8.3] mm, angular error = 0.4 [0.3,
0.7] °, HD = 6.8 [1.3, 17.6] mm, DSC
= 0.789 [0.738, 0.847], recall = 73.2
[62.4, 81.9] %, and precision = 87.9
[84.8, 95.8] %. For GYN: needle tip er-
ror = 0.3 [0.2, 0.4] mm, angular error
= 0.4 [0.2, 0.7] °, HD = 0.5 [0.4, 0.9]
mm, DSC = 88.7 [84.6, 93.5] %, recall
= 85.2 [80.9, 91.1] %, and precision =
93.2 [89.6, 97.0] %.
[32]
T&R
appli-
cator
and
interstitial
needles
CT
48
nnU-Net
and
3D
U-Net
DSC = 0.646 for T&R applicators and
0.738 ± 0.034 for interstitial needles.
[16]
Note: Abbreviations: HDBSCAN (density-based linkage clustering algorithm), DSD-UNET (dilated
convolution and deep supervision U-Net). Duplicate paper from Zhang et al. [146] as in Table 2.
19
Figure 8. Needle reconstruction on CT image for HDR prostate brachytherapy.
5.2
Prostate
For prostate brachytherapy, both HDR and LDR, are unique procedures with comparable workflow
and distinct advantages [143]. As detailed in Table 8, approximately the same number of studies used
deep learning to assist HDR applicator reconstruction (identifying needles and catheters) and LDR
(reconstructing needles and seeds under different imaging modalities). HDR brachytherapy for prostate
cancer involves the insertion of interstitial needles through the perineum, followed by imaging using
techniques such as TRUS, MRI, or CT. TRUS is the most common imaging modality used to guide
the insertion of needles for HDR prostate brachytherapy. Anders´en et al. [8] used a 3D CNN U-Net
model to identify needles in TRUS images. The model was trained on a large dataset consisting of 1102
brachytherapy treatments, with a total of 24422 individual needles. The model achieved a root-mean-
square distance (RMSD) of 0.55mm compared to the clinical ground truth and 0.75 mm compared
to another physicist’s digitization, which is lower than the inter-observer variability of 0.80 mm. To
further enhance the needle digitization workflow, Zhang et al. conducted three studies using different
deep-learning approaches to detect multiple needles on 3D TRUS images simultaneously. The three
approaches were using an order-graph regularized dictionary learning (ORDL)-based method [148], a
deeply supervised attention U-Net with a total variation (TV) regularization method [149], and a large
margin mask R-CNN model (LMMask R-CNN) with a needle-based density-based spatial clustering
method [150]. The shaft and tip errors and accuracies were similar in all three methods but the LMMask
R-CNN-based model had the most superior result, detecting 98% of needles with shaft and tip errors
of 0.091 ± 0.043 mm and 0.33 ± 0.363 mm, respectively.
CT images can also be used to reconstruct the needles, as illustrated in Figure 8. Weishaupt et al. [129]
developed a deep-learning method to automatically digitize the HDR prostate needles on CT images.
Using 2D U-Net architecture, the model segmented the catheters and reconstructed their geometries in
3D using a density-based linkage clustering algorithm. The model accurately digitized all needles in
under one minute with a mean tip distance difference of -0.1 ± 0.6 mm and a mean shaft distance of
0.13 ± 0.09 mm. This method is highly efficient when compared to manual approach, which typically
takes an average of two minutes per needle.
MRI-guided HDR prostate brachytherapy has the potential to optimize the dose distribution due to
MRI’s superior visualization of the DIL compared to TRUS or CT. [93] However, digitizing catheters in
MRI is challenging because of their dark and diffuse appearance. To address this issue, Dai et al. [19]
utilized an attention-gated U-Net to automatically digitize catheters and a TV regularization to remove
excessive noise. The catheter tip error was found to be 0.37 ± 1.68 mm, with 87% of the tips within
localization error of no greater than 2.0 mm. Regarding catheter shaft localization, the error was 0.93
± 0.50 mm, with 97% of catheters detected with an error of less than 2.0 mm. The precision, recall,
and F1 score of shaft localization were 0.96, 0.86, and 0.91, respectively. These results indicate that AI
20
can simplify catheter digitization on MRI, potentially facilitate the use of MRI-guided brachytherapy
in clinical practice and leading to improved patient outcomes.
During the LDR prostate brachytherapy procedures, the placement of the radioactive seeds is guided
by TRUS and often updated based on real-time changes in the prostate. It is crucial to compare the
locations and orientations of implanted seeds to the planned seeds to adjust plans accordingly before
the next seed implantation. Golshan et al. [35] implemented a CNN model with a coarse sliding
window approach to identify the needle tracks, then detect stranded seeds within each identified track.
It achieved a precision of 78 ± 8%, recall of 64 ± 10%, and an F1 score of 70 ± 8%. The relatively low
recall suggests that many seeds were not identified on TRUS images, primarily due to poor ultrasound
image quality and a small training cohort. Due to similar challenges, Holupka et al. [46] was also not
able to provide high identification precision for loose seeds on TRUS.
CT is commonly used to assess LDR treatment quality post-implant. Nguyen et al. [81] used several
machine learning-based models to segment single seeds and groups of closely spaced (union) seeds
on CT images. They first used a k-means method to categorize the seeds into single or union seeds
groups, then a modified k-means for seeds (k-means-FS) and a Gaussian mixture model (GMM) with
expectation-maximization (EM) method to separate the union seed groups. The training and testing
dataset comprised 14 patients with a total of 1063 implanted seeds, along with two phantoms (1 seed
and 73 seeds). The seed angular orientation errors and MDE were greater in patients compared to the
phantoms due to the complex anatomical structures, and the false detection rate ranged from 1.8% to
4.8% for different setups.
The implanted radioactive seeds can also be identified on MRI only, as it offers superior soft tissue
contrast than CT. However, it can be challenging since they do not produce MR signals. An endorectal
coil (ERC) can enhance seed visibility on MRI scans but may entail additional costs and discomfort for
patients. To reduce the manual labor of identifying radioactive seeds on MRI after LDR brachytherapy,
Sanders et al. [104] developed a sliding-window CNN algorithm (SeedNet), which automatically iden-
tifies implanted radioactive seeds on prostate MRI scans. SeedNet demonstrated superior accuracy and
reduced identification time compared to results from dosimetrists for patients both with and without
an ERC. With ERC, seed detection achieved the highest recall (97.6%), precision (97.2%), and F1 score
(97.4%). Additionally, detection exhibited a low false discovery rate (2.8%), low false-negative rate
(2.4%), and low root mean square error (RMSE) of 0.19 mm ± 0.04 mm, all within an inference time of
56.6 ± 18.2 seconds.
6
Quality Assurance (QA)
Brachytherapy QA includes multiple aspects, such as safety, equipment, and plan QA. While the safety
and equipment QAs are typically performed manually before the treatment delivery, the plan QA can
be performed with the assistance of AI to identify suboptimal plans and improve plan qualities. Three
studies used AI models to assist the plan QA, as summarized in Table 9. Reijtenbagh et al. [99] trained
an RF model using Overlap Volume Histograms (OVHs) to predict the DVH for each OAR in HDR GYN
brachytherapy. It predicted doses for the target, bladder, rectum, sigmoid, and small bowels, allowing
perform patient anatomy-based QA. This QA model detected sub-optimal plans by identifying when
D2cc values fell outsize the 95% confidence interval, showing strong correlation with MSE ranged be-
tween 0.13 and 0.40 Gy. Testing on a different institution’s data, the model identified several clinically
compliant plans, proving the model’s effectiveness in multi-center settings. The OVH-based QA model
also has advantage in its short training time of less than a minute and short prediction times within
seconds. Reijtenbagh et al. [100] found an average reduction of 0.62 Gy for all OARs utilizing their
QA model, further proving its effectiveness. Another group from EMC [90] utilized a fully automated
planning model (Erasmus-iCycle) and successfully reduced a mean D2cc of 0.87 Gy in bladder and 1.4
Gy in rectum, yet the study was constrained to single-institute setting. Further, Fan et al. [28] devel-
oped a DNN model for HDR brachytherapy plan QA, focusing on checking dwell times and positions.
The model, based on Inception network by Szegedy et al. [116], used a small number of parameters
to reduce the computation burden and the risks of overfitting in traditional CNN methods. Given the
non-linear nature of the regression problem, Fan et al. employed a heatmap to represent the probability
distribution of the dwell position, reducing the complexity of the task. Predictions of dwell times were
within a 2% deviation from the ground truth, and dwell positions were within one pixel of the planned
21
TABLE VIII. AI in prostate applicator reconstruction
Type of Ap-
plicator
Image
Modality
Number
of
Patients
Model
Result Summary
Citation
Needles
TRUS
1102
3D CNN U-
Net
RMSD = 0.55 [0.35 0.86] mm
[8]
Needles
TRUS
21
ORDL
Shaft error = 0.19 ± 0.13 mm, tip error = 1.01
± 1.74 mm, accuracy = 0.95
[148]
Needles
TRUS
23
Deep super-
vised atten-
tion U-Net
Shaft error = 0.29 ± 0.23 mm, tip error = 0.44
± 0.93 mm, accuracy = 0.96
[149]
Needles
TRUS
23
LMMask
R-
CNN
Shaft error = 0.091 ± 0.043 mm, tip error =
0.330 ± 0.363 mm, accuracy = 0.98
[150]
Needles
TRUS
823
Modified
deep
U-Net
and VGG-16
Needle trajectories: resolutions = 0.668 mm
and 0.319 mm in x and y direction. Needle
tips: resolution = 0.721 mm, 0.369 mm, and
1.877 mm in x, y, and z directions.
[123]
Catheters
TRUS
242
U-Net
and
3D
recon-
struction
80% of catheter reconstructions were accu-
rate within 2 mm along 90% of the catheter
lengths. Reconstruction accuracy = 91% ex-
cluding 27% outliers.
[73]
Catheters
TRUS
49
3D U-Net
AUC = 0.85, recall = 0.97, and true positive
rate = 0.95.
[49]
Needles
CT
57
U-Net
Mean tip distance difference = -0.1 ± 0.6 mm
with range from -1.8 mm to 1.4 mm. Mean
shaft distance = 0.13 ± 0.09 mm with maxi-
mum distance = 0.96 mm.
[129]
Catheters
MRI
20
Deep super-
vised atten-
tion U-Net
Precision = 0.96, recall = 0.86, F1 = 0.91.
Catheter tips localization: error = 0.37 ± 1.68
mm, 87% with errors less than ± 2.0 mm,
and more than 71% within 1.0 mm. Catheter
shaft localization: error = 0.93 ± 0.50 mm,
97% with errors less than ± 2.0 mm, and 63%
within 1.0 mm.
[19]
Catheters
MRI
35
3D U-Net
F1 score = 0.73, precision = 0.65, recall =
0.85, percentage of needles detected = 97.5%
[5]
Stranded
Seeds
TRUS
13
CNN
Precision = 78 ± 8%, recall = 64 ± 10%, F1
score = 70 ± 8%
[35]
Needles
TRUS
9
Bayesian
classifier
23 fully visible needles: mean tip error = 1.4
mm, mean axis error = 1.5o.19 partially visi-
ble needles: mean tip error = 4.2 mm, mean
axis error = 6o.
[136]
Stranded
and
Loose
Seeds
TRUS
1 phantom
Bayesian
classifier
and SVM
Mean error = 1.09 mm ± 0.61 mm on phan-
tom image and 1.44 ± 0.45 mm on clinical im-
ages. Angle orientation error = 4.33 ± 8.5o.
[137]
Loose Seeds
TRUS
10
DetectNet
Location error = 2.29 mm,
precision =
81.07% and recall = 82.29%.
[46]
Loose Seeds
CT
14
patients
and 1 phan-
tom
k-means,
k-means-FS,
and
GMM
with
EM
method
For the phantom, the angular orientation er-
ror = 0.96 ± 0.4°, and MDE = 0.08 ± 0.04 mm.
For the patient, the maximum angular orien-
tation error = 3.18 ± 0.9°, and MDE = 0.50
± 0.16 mm. The least false detection rate =
1.8%
[81]
Stranded
Seeds
with
Positive
Contrast
MR-signal
Seed
Mark-
ers
MRI
68
SeedNet
With ERC: seed localization recall = 97.6 ±
2.2%, precision = 97.2 ± 1.9%, F1 score =
97.4 ± 1.5%, and RMSE = 0.19 ± 0.04 mm.
Without ERC: seed localization recall = 96.5
± 2.3%, precision = 90.5 ± 4.6%, F1 score =
93.3 ± 3.0%, and RMSE = 0.24 mm ± 0.03
mm.
[104]
Stranded
Seeds
MRI
1 phantom
QSM,
k-
means,
and
k-medoids
clustering
The average length and width of detected
seeds = 4.6 ± 0.3 mm and 0.9 ± 0.2 mm, com-
pared with the actual dimensions of 4.5 mm
and 0.8 mm. Maximum seed centroids differ-
ence = 7 mm. Dose distribution differences
range from -8 to 8 Gy/U.
[85]
Note: Abbreviations: VGG-16 (16-layers CNN developed by the visual geometry group), SVM (support
vector machine), QSM (quantitative susceptibility mapping).
22
TABLE IX. AI in QA
QA Tasks
Cancer
Site
Number
of
Patients
Model
Result Summary
Citation
Predict
dose-volume
histograms
GYN
145
RF
MSE between planned and predicted OARs
D2cc ranged between 0.13 and 0.40 Gy
[139]
Treatment
Plan QA
GYN
34
Erasmus-
iCycle
62 out of 63 plans were comparable or better
than clinically generated plans, desired dose
obtained in 14 out of 16 plans.
[141]
Predict
dwell
po-
sitions
and
times
GYN
130
DNN
Dwell times prediction error within 2% from
the ground truth, dwell positions within one
pixel of the planned positions.
[142]
positions. Despite the long training time of the model, it offered fast and accurate plan QA, predicting
dwell times and positions within seconds.
7
Outcome Prediction
AI demonstrates proficiency in predicting clinical outcomes, aiding oncologists and physicists can make
informed decisions regarding patient treatment. As summarized in Table 10, three different outcome
prediction tasks are carried out by machine learning models for both prostate and GYN brachyther-
apy. Two studies utilized ML methods to make the predictions for locally advanced cervical cancer
(LACC) patients. Abdalvand et al. [1] studied the effectiveness of four different ML algorithms, LASSO
(Least Absolute Shrinkage and Selection Operator) regression, Ridge regression, support vector ma-
chine (SVM), and RF in terms of LACC outcome prediction. The ten most important features when
considering LACC outcomes were selected from LASSO algorithm to reduce complexity and mitigate
over-fitting. The study specifically incorporated patient-specific applicator insertion geometries from
3D MR or CT imaging, as well as the physical, dosimetric, radiobiological, and clinical factors when
training the models. The RF algorithm had the highest discriminating ability with an area AUC of 0.82,
but an accuracy of only 0.77, limited by the small patient sample (111 selected) and the traditional
ML model’s ability to address heterogeneities of the clinical data. Tian et al. [118] devised a nonlin-
ear kernel based SVM classifier to predict fistula formation from the high radiation doses in patients
undergoing interstitial brachytherapy for LACC. Using sequential backward selection and sequential
floating backward selection methods, Tian et al. identified 7 most crucial features for model training.
Consequently, their model achieved an AUC of 0.904, with sensitivity and specificity rates of 97.1%
and 88.5%, respectively.
The recurrence rate of prostate cancer after the initial treatment can be predicted with ML models,
offering potential guidance in identifying patients who would benefit the most from salvage HDR
brachytherapy. Valdes et al. [120] used MediBoost (a decision-tree based model built by the same
group [119]) and a classification and regression tree (CART) model to predict the 5 years recurrence
rate. Only the most important features were selected such as the percentage of positive cores after
biopsy and disease-free interval after the first definitive treatment to reduce complexity and risk of
over-fitting of the data. The study found a 5-year recurrence probability of 0.75 associated with posi-
tive cores ¿= 0.35 and a disease-free interval ¡ 4.12 years. Although the study was limited by the small
dataset of 52 patients, there is 70% probability that the results were not due to random variation.
8
Real-time Monitoring
While various devices and systems can achieve real-time monitoring of HDR brachytherapy, AI offers
an alternative solution that enhances quality and efficiency, as detailed in Table 11.
Malignant tumors frequently exhibit distinct temperature distributions compared to normal tissue.
Therefore, thermal imaging can aid in identifying tissue-specific changes in the cervix during brachyther-
apy that are necessary for plan adjustments. Hoffer et al. [45] used the k-means method to predict the
23
TABLE X. AI in brachytherapy outcome prediction
Application
Cancer
Site
Number
of
Patients
Model
Result Summary
Citation
Cervical
cancer
outcome
prediction
GYN
111
LASSO,
Ridge, SVM,
and RF
Best AUC = 0.82, sensitivity = 0.79, speci-
ficity = 0.76, and accuracy = 0.77
[1]
Predict
fistula
de-
velopment
from
high
radiation
dose
GYN
35
Nonlinear
kernel-
based
SVM
classifier
AUC = 0.904, sensitivity = 97.1%, and speci-
ficity = 88.5%
[118]
Select
pa-
tients
to
receive
sal-
vage
HDR
brachyther-
apy
after
first
recur-
rence
after
radiation
therapy
Prostate
52
CART
and
MediBoost
Positive cores ¿= 0.35 and a disease-free inter-
val ¡ 4.12 years has a second recurrence rate
of 0.75, the conclusion has a 70% probability
of not due to random variations
[120]
status of the cervix before and after brachytherapy, using skewness and entropy levels from the thermal
image of the cervix. Additionally, K Nearest Neighbors (KNN) and SVM validated a 100% detection rate
for structural and textual changes in cervical tumors before and after brachytherapy.
During HDR brachytherapy treatments, monitoring the source position and dwell times of the radioac-
tive source can be achieved through a gamma camera, which often suffers from blurring effect and
noise. Nakanishi et al. [80] proposed a DL based approach to estimate the actual image without blur-
ring effect and noise for better real-time monitoring of the Ir-192 source. The method consisted of two
P2P models, similar to the method provided by Isola et al. [54] The highest structural similarity index
measure (SSIM) value was 0.980.006 and the lowest MAE was 2.210−3110−3 Additionally, the full width
at half maximum (FWHM) of the estimated image in both horizontal and vertical directions differed by
less than 0.5 mm from the actual source size.
Finally, AI can be used in the calibration of plastic scintillation detectors in a multi-point configuration
(mPSD), which provides in-vivo dosimetry measurement and real-time source tracking. Rosales et al.
[72] trained and compared three algorithms, linear regression, RF, and ANN to calibrate mPSD for
real-time feedback in HDR brachytherapy. Among the tree algorithms, the RF had the most accurate
calibration result with dose deviation generally remained below 20% and below 2% when same range of
distances was used for calibration. This ML-based method requires only one calibration for the detector
but can still be time-consuming if calibration at multiple locations is needed. While improvements are
still needed, this algorithm can lead to more precise measurements of mPSD and help medical physicist
and oncologists with adjusting treatment plans.
9
Discussion
In this review, we summarized AI’s applications in different parts of brachytherapy workflows, high-
lighting its comparable performances to manual efforts in the segmentation, classification, and predic-
tion tasks with non-significant errors.
The imaging section has the most studies, this is because AI’s role in brachytherapy largely relies on
imaging tasks. With the advent of deep learning, image registration has not only become more effi-
cient but also more feasible, especially in terms of achieving precise multi-modal registration. Fast and
accurate multi-model image registration is particularly valuable in brachytherapy, where integrating
MRI information with TRUS can provide better guidance during the procedure. Besides MRI-TRUS fu-
sion, AI can also enhance resolution and reduce artifacts, providing an alternative solution to overcome
imaging challenges in brachytherapy.
24
TABLE XI. AI in brachytherapy real-time monitoring
Monitoring
Application
Cancer
Site
Number
of
Patients
Model
Results
Citation
Monitor
thermal
image
to
predict
structural
and
textual
changes
in
cervical
tu-
mors before
and
after
brachyther-
apy
GYN
6
k-means,
KNN,
and
SVM
100%
detection
rate
for
physiological
changes in cervical tumors before and after
brachytherapy
[45]
Reduce blur-
ring
and
statistical
noise
in
real-time
monitoring
of
Ir-192
sources
with gamma
camera
GYN
11
Pix2pix
models
The highest SSIM = 0.98 ± 0.006 and MAE =
2.2x10−3 ± 1.0x10−3. FWHM error 0.5 mm in
both horizontal and vertical directions.
[80]
Calibration
of mPSD
GYN and
Prostate
1936
dwell
positions
Linear
re-
gression, RF,
and ANN
Dose predictions accuracy within 7% of the
TG-43 U1 formalism with all models and less
than 2% deviations using RF model
[72]
Manual contouring is time-consuming and requires extensive training, while AI models can learn seg-
mentation techniques in hours or less. Most studies reported AI segmentation is dramatically faster
than human experts on all image modalities. Kraus et al. [64] demonstrated that using AI in clinical
workflows reduces and standardizes the time from contouring to approval, with AI workflows taking
71–111 minutes, compared to the large deviation of 29–304 minutes with manual workflows. AI’s effi-
ciency stems not only from its rapid processing capabilities, but also from the absence of clinical inter-
ruptions that human physicians frequently encounter. The reviewed studies showed that AI-generated
contours often closely resemble those produced manually in quantitative evaluations. Several other
studies have also shown AI’s superior segmentation from different perspectives. For instance, Sanders
et al. [106] evaluated the differences in dosimetric parameter between the prostate and OAR contours
generated solely by a deep-learning algorithm (MIM-Symphony) and those further refined by physi-
cians, finding no significant differences in the dosimetric parameters even after human refinement.
King et al. [62] conducted a survey on prostate contours on TRUS images generated by AI versus by
professionals. The AI algorithm, using nnU-Net, generated contours with median DSC of 0.92 and won
a median of 57.5% of clinical observer preference, showing strong performance both objectively and
subjectively. In addition, AI can standardize contours and reduce inter-observer variability errors. As
shown in Anders´en’s [8] study, their deep learning model produced results with errors smaller than
inter-observer variability.
Studies on AI applications in treatment planning suggest that AI can generate plans that are comparable
to, or even superior to, those created by human planners. This capability stems from AI’s ability to learn
from hundreds of high-quality treatment plans during its training phase. While manual planning can
indeed produce excellent results, it is heavily reliant on the planner’s experience and is often resource-
intensive and time-consuming. In contrast, AI can generate superior plans much more efficiently. The
plan check function from automatic dose prediction models can also detect sub-optimal plans that
require proper adjustments.
AI application in applicator reconstruction, particularly multi-needle digitization, offer the potential to
facilitate real-time treatment planning and dosimetric adjustments, mitigating discrepancies between
the planning stage and the operating room, although there are still challenges in ground truth contour
accuracy and needle trajectory prediction. AI models have the potential to simplify more advanced
procedures, such as MRI-guided brachytherapy. Overall, AI can improve brachytherapy workflow effi-
ciency, producing similar results to conventional methods in significantly less time with more consis-
25
Figure 9. Distribution of Number of Patients in investigated studies.
tency.
The outcome prediction, QA, and real-time monitoring sections have the least AI application for several
reasons. While AI can assist with brachytherapy preparation, the treatment quality heavily depends on
the skills of the physicians or surgeons in the operation room, and physicists responsible for monitoring
source delivery and performing QA. The human-centered nature of QA and real-time monitoring limits
AI integration in these areas. Outcome prediction, however, is particularly challenging due to the need
for long-term data, which is often limited. This lack of comprehensive patient data makes it difficult
to apply AI effectively in predicting outcomes, as the reviewed studies only included 35, 52, and 111
patients.
Two common limitations for the current application of AI in brachytherapy are the small patient co-
horts and single institution setups. Analyzing patient numbers involved in each study, we found the
mean and the median numbers to be 149 and 66, respectively. A histogram in Figure 9 illustrates
the distribution, showing skewness due to a few studies with exceptionally large patient cohorts, while
most studies have less than 200 patients. Despite the advantages of brachytherapy, such as shorter over-
all treatment time and positive outcomes compared to surgery or EBRT [38], there has been a general
decline in its use [92]. This is due to procedural difficulty and a reduction in residency brachytherapy
training, which has decreased the number of physicians competent in performing this procedure [130].
Many studies addressed this challenge by employing cross-validation methods, in which the total avail-
able data is partitioned into several groups, with one group for validation and the rest for training. The
training of the model ends when each group is used as the validation set once. The cross-validation
methods allow the maximum utilization of available data for training and validation by assessing the
model’s performance across different subsets and enhancing its reliability and generalizability. Data
augmentation, which involves applying elastic deformations or transformation such as rotating and
resizing, allows different U-Net-based models to gain more training dataset and learn invariance to de-
formations. Besides, transfer-learning can also be used to leverage the challenge of limited data size,
several studies [4, 82, 105, 140, 151] showed that fine-tuning with limited clinical data could lead to
more precise task outcomes although requiring longer training times.
Although various methods exist to mitigate the adverse effects of limited image data, addressing the
26
challenge posed by the lack of patient anatomy and setup variations remains difficult. For example,
Holupka et al. [46] used 950 training US images from 10 patients, resulting in large seed location
errors as large as 2.29 mm when evaluated on new patient sets due to lack of patient variation. AI
models may struggle anatomies deviating significantly from training datasets, making reliance on AI-
generated results risky, particularly in HDR settings. Diverse data, in terms of various tumor sites,
heterogeneities, shielded applicators, and different radiation sources, is crucial for obtaining a model
suitable for general-propose brachytherapy dosimetry [77].
Another significant issue in current AI studies is the reliance on training data from single institutions.
Brachytherapy procedures are heavily dependent on imaging protocols, and AI models trained on data
from one institution may not perform well elsewhere. Although there were a few studies [69? ] showed
no statistically significant difference in cross-institution testing, majority of studies lack this valida-
tion. Even using the same imaging protocol, applicator types for brachytherapy could vary, further
complicating the implementation of pretrained models [18, 100]. For example, in the applicator recon-
struction of cervical cancer brachytherapy section, most of the applicator reconstruction studies used
T&O applicators, but the studies on other types of applicators, such as the T&R and fletcher applica-
tors, could be further investigated. Although the geometries of applicators may be somewhat similar, it
is challenging to apply a model trained on one type of applicator to another type. Dose prediction tasks
can be complex due to applicator variability, typically involving one type of applicator setup per study.
In addition, the acceptability of AI predicted plans is heavily reliant on the quality of clinical ground
truth, influence by variability in treatment planning system and protocol adopted in each institution.
Therefore, future generalized AI models development should include large and diverse patient datasets
from multiple institutions to address these limitations [99].
One potential solution for these common issues is to implement foundation models, especially in the
field of imaging. Segment Anything Model (SAM) [63], introduced by the Meta AI Research team, is a
foundation model for segmenting any object in the input image by detecting valid masks within images
and operating through a supervised routine rather than relying solely on unsupervised learning. SAM
was pre-trained on a comprehensive dataset with over 1 billion masks sourced from 11 million images.
This extensive training employs a task that promotes robust generalization, enabling zero-shot adapt-
ability to unfamiliar objects and images without necessitating supplementary training. Following the
introduction of SAM, Ma et al. introduced MedSAM [76], a refined foundation model that specifically
integrates medical knowledge to segment medical images. It is designed for universal segmentation
tasks and trained on a vast and diverse dataset of over a million medical image-mask pairs that cover
15 modalities and 30 cancer types. MedSAM addresses the issue of the task-specific nature of previous
models, which are only capable of segmenting certain image modalities or specific tissues. The diversity
in data sources effectively eliminates the need for additional training, addressing the limitation of large
biomedical image datasets. MedSAM’s capability to manage various anatomical structures and imaging
modalities makes it a potential tool for brachytherapy imaging applications. Large foundation models
such as MedSAM can be integrated with fine-tuning methods for more accurate organ segmentation
[39], which is applicable to brachytherapy procedures. Future studies aiming to automate the segmen-
tation of organs, tumors, or applicators can achieve this by fine-tuning these models, even with limited,
single-institution datasets, potentially leading to more reliable results that are acceptable clinically.
Since many aspects of brachytherapy, such as image registration, treatment planning, and applicator
placement, depend heavily on accurate image segmentation, the implementation of large foundation
models could offer a possible solution to the current challenges in applying AI to brachytherapy.
The final issue with current implementation of AI is the lack of universal standards in evaluating the
performance of AI models. While image segmentation has some commonly accepted evaluation meth-
ods, other aspects lack clear reporting guidelines, making it difficult to compare the results of different
AI models. Without consistent metrics for comparison, identifying areas for improvement and propos-
ing clear solutions for future studies becomes challenging. The establishment of universal evaluation
guidelines for each procedure category is necessary to ensure the future improvement of AI application,
thereby enhancing brachytherapy treatments.
27
10
Conclusion
This review covers the current development of AI in brachytherapy, focusing on prostate and GYN
cancer treatments. Unlike EBRT, which benefits from extensive patient data and straightforward AI
implementation, brachytherapy requires additional manual procedural accuracy, introducing unique
difficulties for AI integration. Current challenges include the lack of patient data from diverse insti-
tutions, and a possible solution is to adopt new foundational models to enhance image segmentation,
which paves the way for improvements in other procedures as well. Establishing universal standards in
validating the application of AI in brachytherapy is also essential to improve AI’s performance. Despite
these challenges, AI has the potential to enhance image segmentation accuracy, provide high-quality
plans, simplify real-time planning, which significantly enhance the brachytherapy workflow and en-
courage the use of brachytherapy. By addressing these challenges and investing in further development,
AI can improve the quality of brachytherapy treatment and patient outcomes in the foreseeable future.
ACKNOWLEDGEMENT
This research is supported in part by the National Institutes of Health under Award Number R01CA215718,
R01DE033512, R01CA272991 and P30CA008748.
Disclosures
The authors declare no conflicts of interest.
References
[1] N. Abdalvand, M. Sadeghi, S. R. Mahdavi, H. Abdollahi, Y. Qasempour, F. Mohammadian, M. J. T.
Birgani, and K. Hosseini. Brachytherapy outcome modeling in cervical cancer patients: A pre-
dictive machine learning study on patient-specific clinical, physical and dosimetric parameters.
Brachytherapy, 21(6):769–782, 2022.
ISSN 1873-1449 (Electronic) 1538-4721 (Linking).
doi:
10.1016/j.brachy.2022.06.007. URL https://www.ncbi.nlm.nih.gov/pubmed/35933272.
[2] A. Akhavanallaf, R. Mohammadi, I. Shiri, Y. Salimi, H. Arabi, and H. Zaidi.
Personalized
brachytherapy dose reconstruction using deep learning. Comput Biol Med, 136:104755, 2021.
ISSN 1879-0534 (Electronic) 0010-4825 (Linking).
doi: 10.1016/j.compbiomed.2021.104755.
URL https://www.ncbi.nlm.nih.gov/pubmed/34388458.
[3] T. A. Aleef, I. T. Spadinger, M. D. Peacock, S. E. Salcudean, and S. S. Mahdavi. Centre-specific
autonomous treatment plans for prostate brachytherapy using cgans. Int J Comput Assist Radiol
Surg, 16(7):1161–1170, 2021. ISSN 1861-6429 (Electronic) 1861-6410 (Linking). doi: 10.1007/
s11548-021-02405-1. URL https://www.ncbi.nlm.nih.gov/pubmed/34050909.
[4] Tajwar Abrar Aleef, Ingrid T Spadinger, Michael D Peacock, Septimiu E Salcudean, and S Sara
Mahdavi. Rapid treatment planning for low-dose-rate prostate brachytherapy with tp-gan. In
International Conference on Medical Image Computing and Computer-Assisted Intervention, pages
581–590. Springer.
[5] Amanda M Aleong, Alejandro Berlin, Jette Borg, Joelle Helou, Akbar Beiki-Ardakani, Alexandra
Rink, Srinivas Raman, Peter Chung, and Robert A Weersink. Rapid multi-catheter segmentation
for magnetic resonance image-guided catheter-based interventions. Medical Physics, 2024. ISSN
0094-2405.
[6] D. Allman, A. Reiter, and M. A. L. Bell. Photoacoustic source detection and reflection artifact
removal enabled by deep learning. IEEE Trans Med Imaging, 37(6):1464–1477, 2018. ISSN 1558-
254X (Electronic) 0278-0062 (Print) 0278-0062 (Linking). doi: 10.1109/TMI.2018.2829662. URL
https://www.ncbi.nlm.nih.gov/pubmed/29870374.
[7] Emran Mohammad Abu Anas, Saman Nouranian, S. Sara Mahdavi, Ingrid Spadinger, William J.
Morris, Septimu E. Salcudean, Parvin Mousavi, and Purang Abolmaesumi.
Clinical target-
volume delineation in prostate brachytherapy using residual neural networks. Medical Image
28
Computing and Computer Assisted Intervention-MICCAI 2017, pages 365–373. Springer Inter-
national Publishing. ISBN 978-3-319-66179-7.
[8] C. Andersen, T. Ryden, P. Thunberg, and J. H. Lagerlof. Deep learning-based digitization of
prostate brachytherapy needles in ultrasound images. Med Phys, 47(12):6414–6420, 2020. ISSN
2473-4209 (Electronic) 0094-2405 (Print) 0094-2405 (Linking). doi: 10.1002/mp.14508. URL
https://www.ncbi.nlm.nih.gov/pubmed/33012023.
[9] F. Berumen, S. A. Enger, and L. Beaulieu.
Fastd(m,m)calculation in ldr brachytherapy using
deep learning methods. Phys Med Biol, 68(11), 2023. ISSN 1361-6560 (Electronic) 0031-9155
(Linking).
doi: 10.1088/1361-6560/accd42.
URL https://www.ncbi.nlm.nih.gov/pubmed/
37059110.
[10] N. Boussion, U. Schick, G. Dissaux, L. Ollivier, G. Goasduff, O. Pradier, A. Valeri, and D. Visvikis.
A machine-learning approach based on 409 treatments to predict optimal number of iodine-125
seeds in low-dose-rate prostate brachytherapy. J Contemp Brachytherapy, 13(5):541–548, 2021.
ISSN 1689-832X (Print) 2081-2841 (Electronic) 2081-2841 (Linking).
doi: 10.5114/jcb.2021.
109789. URL https://www.ncbi.nlm.nih.gov/pubmed/34759979.
[11] Yufeng Cao, April Vassantachart, Omar Ragab, Shelly Bian, Priya Mitra, Zhengzheng Xu, Au-
drey Zhuang Gallogly, Jing Cui, Zhilei Liu Shen, and Salim Balik. Automatic segmentation of
high-risk clinical target volume for tandem-and-ovoids brachytherapy patients using an asym-
metric dual-path convolutional neural network. Medical physics, 49(3):1712–1722, 2022. ISSN
0094-2405.
[12] Jui-Hung Chang, Kai-Hsiang Lin, Ti-Hao Wang, Yu-Kai Zhou, and Pau-Choo Chung.
Image
segmentation in 3d brachytherapy using convolutional lstm. Journal of Medical and Biological
Engineering, 41:636–651, 2021. ISSN 1609-0985.
[13] Cyrus Chargari, Eric Deutsch, Pierre Blanchard, Sebastien Gouy, H´el`ene Martelli, Florent Gu´erin,
Isabelle Dumas, Alberto Bossi, Philippe Morice, and Akila N Viswanathan. Brachytherapy: An
overview for clinicians. CA: a cancer journal for clinicians, 69(5):386–401, 2019. ISSN 0007-9235.
[14] J. Chen, H. Chen, Z. Zhong, Z. Wang, B. Hrycushko, L. Zhou, S. Jiang, K. Albuquerque, X. Gu,
and X. Zhen. Investigating rectal toxicity associated dosimetric features with deformable ac-
cumulated rectal surface dose maps for cervical cancer radiotherapy. Radiat Oncol, 13(1):125,
2018.
ISSN 1748-717X (Electronic) 1748-717X (Linking).
doi: 10.1186/s13014-018-1068-0.
URL https://www.ncbi.nlm.nih.gov/pubmed/29980214.
[15] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille,
and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmenta-
tion. arXiv preprint arXiv:2102.04306, 2021.
[16] S Chen, S Shen, EA Simiele, Z Iqbal, DN Stanley, X Wu, J Peacock, MB Yusuf, S Marcrom, and
C Cardenas. Artificial intelligence-assisted automated applicator digitization for fully-automated
gynecological high-dose rate brachytherapy treatment planning. International Journal of Radiation
Oncology, Biology, Physics, 117(2):e651–e652, 2023. ISSN 0360-3016.
[17] Y. Chen, L. Xing, L. Yu, W. Liu, B. Pooya Fahimian, T. Niedermayr, H. P. Bagshaw, M. Buyy-
ounouski, and B. Han. Mr to ultrasound image registration with segmentation-based learning
for hdr prostate brachytherapy. Med Phys, 48(6):3074–3083, 2021. ISSN 2473-4209 (Electronic)
0094-2405 (Linking). doi: 10.1002/mp.14901. URL https://www.ncbi.nlm.nih.gov/pubmed/
33905566.
[18] K. G. Cortes, K. Kallis, A. Simon, J. Mayadev, S. M. Meyers, and K. L. Moore. Knowledge-based
three-dimensional dose prediction for tandem-and-ovoid brachytherapy. Brachytherapy, 21(4):
532–542, 2022. ISSN 1873-1449 (Electronic) 1538-4721 (Linking). doi: 10.1016/j.brachy.2022.
03.002. URL https://www.ncbi.nlm.nih.gov/pubmed/35562285.
[19] X. Dai, Y. Lei, Y. Zhang, R. L. J. Qiu, T. Wang, S. A. Dresser, W. J. Curran, P. Patel, T. Liu,
and X. Yang. Automatic multi-catheter detection using deeply supervised convolutional neu-
ral network in mri-guided hdr prostate brachytherapy. Med Phys, 47(9):4115–4124, 2020. ISSN
2473-4209 (Electronic) 0094-2405 (Print) 0094-2405 (Linking). doi: 10.1002/mp.14307. URL
https://www.ncbi.nlm.nih.gov/pubmed/32484573.
29
[20] D Jeffrey Demanes, Sheila Rege, Rodney R Rodriquez, Kathleen L Schutz, Gillian A Altieri, and
Thomas Wong. The use and advantages of a multichannel vaginal cylinder in high-dose-rate
brachytherapy. International Journal of Radiation Oncology* Biology* Physics, 44(1):211–219, 1999.
ISSN 0360-3016.
[21] Christopher L Deufel, Shulan Tian, Benjamin B Yan, Birjoo D Vaishnav, Michael G Haddock, and
Ivy A Petersen. Automated applicator digitization for high-dose-rate cervix brachytherapy using
image thresholding and density-based clustering.
Brachytherapy, 19(1):111–118, 2020.
ISSN
1538-4721.
[22] Xue Dong, Yang Lei, Sibo Tian, Tonghe Wang, Pretesh Patel, Walter J Curran, Ashesh B Jani, Tian
Liu, and Xiaofeng Yang. Synthetic mri-aided multi-organ segmentation on male pelvic ct using
cycle consistent deep attention network. Radiotherapy and Oncology, 141:192–199, 2019. ISSN
0167-8140.
[23] J. Duan, C. E. Vargas, N. Y. Yu, B. S. Laughlin, D. S. Toesca, S. Keole, J. C. M. Rwigema, W. W.
Wong, S. E. Schild, X. Feng, Q. Chen, and Y. Rong. Incremental retraining, clinical implemen-
tation, and acceptance rate of deep learning auto-segmentation for male pelvis in a multiuser
environment. Med Phys, 50(7):4079–4091, 2023. ISSN 2473-4209 (Electronic) 0094-2405 (Link-
ing). doi: 10.1002/mp.16537. URL https://www.ncbi.nlm.nih.gov/pubmed/37287322.
[24] Richard O Duda and Peter E Hart. Use of the hough transformation to detect lines and curves in
pictures. Communications of the ACM, 15(1):11–15, 1972. ISSN 0001-0782.
[25] D. Duprez, C. Trauernicht, H. Simonds, and O. Williams. Self-configuring nnu-net for auto-
matic delineation of the organs at risk and target in high-dose rate cervical brachytherapy, a
low/middle-income country’s experience. J Appl Clin Med Phys, 24(8):e13988, 2023. ISSN 1526-
9914 (Electronic) 1526-9914 (Linking). doi: 10.1002/acm2.13988. URL https://www.ncbi.
nlm.nih.gov/pubmed/37042449.
[26] S. Ecker, L. Zimmermann, G. Heilemann, Y. Niatsetski, M. Schmid, A. E. Sturdza, J. Knoth,
C. Kirisits, and N. Nesvacil.
Neural network-assisted automated image registration for mri-
guided adaptive brachytherapy in cervical cancer. Z Med Phys, 32(4):488–499, 2022. ISSN 1876-
4436 (Electronic) 0939-3889 (Print) 0939-3889 (Linking). doi: 10.1016/j.zemedi.2022.04.002.
URL https://www.ncbi.nlm.nih.gov/pubmed/35570099.
[27] Issam El Naqa and Martin J Murphy. What is machine learning? Springer, 2015. ISBN 3319183044.
[28] Jiawei Fan, Lei Xing, and Yong Yang. Independent verification of brachytherapy treatment plan
by using deep learning inference modeling. Physics in Medicine & Biology, 66(12):125014, 2021.
ISSN 0031-9155.
[29] Yabo Fu, Hao Zhang, Eric D Morris, Carri K Glide-Hurst, Suraj Pai, Alberto Traverso, Leonard
Wee, Ibrahim Hadzic, Per-Ivar Lønne, and Chenyang Shen. Artificial intelligence in radiation
therapy. IEEE transactions on radiation and plasma medical sciences, 6(2):158–181, 2021. ISSN
2469-7311.
[30] Yin Gao, Yesenia Gonzalez, Chika Nwachukwu, Kevin Albuquerque, and Xun Jia. Predicting
treatment plan approval probability for high-dose-rate brachytherapy of cervical cancer using
adversarial deep learning. Physics in Medicine & Biology, 69(9):095010, 2024. ISSN 0031-9155.
[31] S. Ghosh, K. Punithakumar, F. Huang, G. Menon, and P. Boulanger. Deep learning using pre-
brachytherapy mri to automatically predict applicator induced complex uterine deformation.
Annu Int Conf IEEE Eng Med Biol Soc, 2022:3826–3829, 2022. ISSN 2694-0604 (Electronic) 2375-
7477 (Linking). doi: 10.1109/EMBC48229.2022.9871157. URL https://www.ncbi.nlm.nih.
gov/pubmed/36086328.
[32] Derek J Gillies, Jessica R Rodgers, Igor Gyacskov, Priyanka Roy, Nirmal Kakani, Derek W
Cool, and Aaron Fenster. Deep learning segmentation of general interventional tools in two-
dimensional ultrasound images. Medical Physics, 47(10):4956–4970, 2020. ISSN 0094-2405.
30
[33] K. B. Girum, A. Lalande, R. Hussain, and G. Crehange.
A deep learning method for real-
time intraoperative us image segmentation in prostate brachytherapy. Int J Comput Assist Ra-
diol Surg, 15(9):1467–1476, 2020.
ISSN 1861-6429 (Electronic) 1861-6410 (Linking).
doi:
10.1007/s11548-020-02231-x. URL https://www.ncbi.nlm.nih.gov/pubmed/32691302.
[34] Kibrom Berihu Girum, Gilles Cr´ehange, Raabid Hussain, and Alain Lalande. Fast interactive
medical image segmentation with weakly supervised deep learning method. International Journal
of Computer Assisted Radiology and Surgery, 15:1437–1444, 2020. ISSN 1861-6410.
[35] M. Golshan, D. Karimi, S. Mahdavi, J. Lobo, M. Peacock, S. E. Salcudean, and I. Spadinger. Au-
tomatic detection of brachytherapy seeds in 3d ultrasound images using a convolutional neural
network. Phys Med Biol, 65(3):035016, 2020. ISSN 1361-6560 (Electronic) 0031-9155 (Linking).
doi: 10.1088/1361-6560/ab64b5. URL https://www.ncbi.nlm.nih.gov/pubmed/31860899.
[36] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the
ACM, 63(11):139–144, 2020. ISSN 0001-0782.
[37] Christopher D Goodman, Hatim Fakir, Stephen Pautler, Joseph Chin, and Glenn S Bauman. Dosi-
metric evaluation of psma pet-delineated dominant intraprostatic lesion simultaneous infield
boosts. Advances in radiation oncology, 5(2):212–220, 2020. ISSN 2452-1094.
[38] Peter Grimm, Ignace Billiet, David Bostwick, Adam P Dicker, Steven Frank, Jos Immerzeel, Mira
Keyes, Patrick Kupelian, W Robert Lee, and Stefan Machtens. Comparative analysis of prostate-
specific antigen free survival outcomes for patients with low, intermediate and high risk prostate
cancer treatment by radical therapy. results from the prostate cancer results study group. BJU
international, 109:22–29, 2012. ISSN 1464-4096.
[39] Hanxue Gu, Haoyu Dong, Jichen Yang, and Maciej A Mazurowski. How to build the best medical
image segmentation algorithm using foundation models: a comprehensive empirical study with
segment anything model. arXiv preprint arXiv:2404.09957, 2024.
[40] P. Hampole, T. Harding, D. Gillies, N. Orlando, C. Edirisinghe, L. C. Mendez, D. D’Souza,
V. Velker, R. Correa, J. Helou, S. Xing, A. Fenster, and D. A. Hoover. Deep learning-based ul-
trasound auto-segmentation of the prostate with brachytherapy implanted needles. Med Phys, 51
(4):2665–2677, 2024. ISSN 2473-4209 (Electronic) 0094-2405 (Linking). doi: 10.1002/mp.16811.
URL https://www.ncbi.nlm.nih.gov/pubmed/37888789.
[41] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Land-
man, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation.
In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 574–584.
[42] Sheng He, Rina Bao, P Ellen Grant, and Yangming Ou. U-netmer: U-net meets transformer for
medical image segmentation. arXiv preprint arXiv:2304.01401, 2023.
[43] Xiuxiu He, Yang Lei, Yingzi Liu, Zhen Tian, Tonghe Wang, Walter J Curran, Tian Liu, and Xi-
aofeng Yang. Deep attentional gan-based high-resolution ultrasound imaging. In Medical Imaging
2020: Ultrasonic Imaging and Tomography, volume 11319, pages 61–66. SPIE.
[44] TP Hellebust. Place of modern imaging in brachytherapy planning. Cancer/Radioth´erapie, 22(4):
326–333, 2018. ISSN 1278-3218.
[45] O. Hoffer, T. Rabin, R. R. Nir, R. Y. Brzezinski, Y. Zimmer, and I. Gannot.
Automated ther-
mal imaging monitors the local response to cervical cancer brachytherapy. J Biophotonics, 16
(1):e202200214, 2023. ISSN 1864-0648 (Electronic) 1864-063X (Linking). doi: 10.1002/jbio.
202200214. URL https://www.ncbi.nlm.nih.gov/pubmed/36063415.
[46] Edward J Holupka, John Rossman, Tye Morancy, Joseph Aronovitz, and Irving D Kaplan. The de-
tection of implanted radioactive seeds on ultrasound images using convolution neural networks.
2022.
31
[47] W. T. Hrinivich, M. Morcos, A. Viswanathan, and J. Lee. Automatic tandem and ring recon-
struction using mri for cervical cancer brachytherapy. Med Phys, 46(10):4324–4332, 2019. ISSN
2473-4209 (Electronic) 0094-2405 (Linking). doi: 10.1002/mp.13730. URL https://www.ncbi.
nlm.nih.gov/pubmed/31329302.
[48] H. Hu, Q. Yang, J. Li, P. Wang, B. Tang, X. Wang, and J. Lang. Deep learning applications in auto-
matic segmentation and reconstruction in ct-based cervix brachytherapy. J Contemp Brachyther-
apy, 13(3):325–330, 2021. ISSN 1689-832X (Print) 2081-2841 (Electronic) 2081-2841 (Linking).
doi: 10.5114/jcb.2021.106118. URL https://www.ncbi.nlm.nih.gov/pubmed/34122573.
[49] Zoe Hu, Harry Brastianos, Tamas Ungi, Csaba Pinter, Tim Olding, Martin Korzeniowski, and Ga-
bor Fichtinger. Automated catheter segmentation using 3d ultrasound images in high-dose-rate
prostate brachytherapy. In Medical Imaging 2021: Image-Guided Procedures, Robotic Interventions,
and Modeling, volume 11598, pages 252–259. SPIE.
[50] Mingxu Huang, Chaolu Feng, Deyu Sun, Ming Cui, and Dazhe Zhao. Segmentation of clini-
cal target volume from ct images for cervical cancer using deep learning. Technology in Cancer
Research & Treatment, 22:15330338221139164, 2023. ISSN 1533-0346.
[51] X. Huang, J. Wang, F. Tang, T. Zhong, and Y. Zhang. Metal artifact reduction on cervical ct images
by deep residual learning. Biomed Eng Online, 17(1):175, 2018. ISSN 1475-925X (Electronic)
1475-925X (Linking). doi: 10.1186/s12938-018-0609-y. URL https://www.ncbi.nlm.nih.gov/
pubmed/30482231.
[52] Elizabeth Huynh, Ahmed Hosny, Christian Guthier, Danielle S Bitterman, Steven F Petit,
Daphne A Haas-Kogan, Benjamin Kann, Hugo JWL Aerts, and Raymond H Mak. Artificial in-
telligence in radiation oncology. Nature Reviews Clinical Oncology, 17(12):771–781, 2020. ISSN
1759-4774.
[53] Fabian Isensee, Jens Petersen, Andre Klein, David Zimmerer, Paul F Jaeger, Simon Kohl, Jakob
Wasserthal, Gregor Koehler, Tobias Norajitra, and Sebastian Wirkert. nnu-net: Self-adapting
framework for u-net-based medical image segmentation. arXiv preprint arXiv:1809.10486, 2018.
[54] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1125–1134.
[55] R. Jaberi, Z. Siavashpour, M. R. Aghamiri, C. Kirisits, and R. Ghaderi. Artificial neural network
based gynaecological image-guided adaptive brachytherapy treatment planning correction of
intra-fractional organs at risk dose variation. J Contemp Brachytherapy, 9(6):508–518, 2017. ISSN
1689-832X (Print) 2081-2841 (Electronic) 2081-2841 (Linking). doi: 10.5114/jcb.2017.72567.
URL https://www.ncbi.nlm.nih.gov/pubmed/29441094.
[56] X. Jiang, F. Wang, Y. Chen, and S. Yan. Refinenet-based automatic delineation of the clinical target
volume and organs at risk for three-dimensional brachytherapy for cervical cancer. Ann Transl
Med, 9(23):1721, 2021. ISSN 2305-5839 (Print) 2305-5847 (Electronic) 2305-5839 (Linking). doi:
10.21037/atm-21-4074. URL https://www.ncbi.nlm.nih.gov/pubmed/35071415.
[57] H. Jung, Y. Gonzalez, C. Shen, P. Klages, K. Albuquerque, and X. Jia. Deep-learning-assisted
automatic digitization of applicators in 3d ct image-based high-dose-rate brachytherapy of gy-
necological cancer. Brachytherapy, 18(6):841–851, 2019. ISSN 1873-1449 (Electronic) 1538-4721
(Linking). doi: 10.1016/j.brachy.2019.06.003. URL https://www.ncbi.nlm.nih.gov/pubmed/
31345749.
[58] Hyunuk Jung, Chenyang Shen, Yesenia Gonzalez, Kevin Albuquerque, and Xun Jia.
Deep-
learning assisted automatic digitization of interstitial needles in 3d ct image based high dose-rate
brachytherapy of gynecological cancer. Physics in Medicine & Biology, 64(21):215003, 2019. ISSN
0031-9155.
[59] Hyejoo Kang, Alexander R Podgorsak, Bhanu Prasad Venkatesulu, Anjali L Saripalli, Brian
Chou, Abhishek A Solanki, Matthew Harkenrider, Steven Shea, John C Roeske, and Mohammed
Abuhamad.
Prostate segmentation accuracy using synthetic mri for high-dose-rate prostate
brachytherapy treatment planning. Physics in Medicine & Biology, 68(15):155017, 2023. ISSN
0031-9155.
32
[60] D. Karimi, Q. Zeng, P. Mathur, A. Avinash, S. Mahdavi, I. Spadinger, P. Abolmaesumi, and S. E.
Salcudean. Accurate and robust deep learning-based segmentation of the prostate clinical tar-
get volume in ultrasound images. Med Image Anal, 57:186–196, 2019. ISSN 1361-8423 (Elec-
tronic) 1361-8415 (Linking). doi: 10.1016/j.media.2019.07.005. URL https://www.ncbi.nlm.
nih.gov/pubmed/31325722.
[61] Gayoung Kim, Majd Antaki, Ehud J Schmidt, Michael Roumeliotis, Akila N Viswanathan, and
Junghoon Lee.
Intraoperative mri-guided cervical cancer brachytherapy with automatic tis-
sue segmentation using dual convolution-transformer network and real-time needle tracking.
In Medical Imaging 2024: Image-Guided Procedures, Robotic Interventions, and Modeling, volume
12928, pages 263–270. SPIE.
[62] M. T. King, C. E. Kehayias, T. Chaunzwa, D. B. Rosen, A. R. Mahal, T. D. Wallburn, M. G. Milligan,
M. A. Dyer, P. L. Nguyen, P. F. Orio, T. C. Harris, I. Buzurovic, and C. V. Guthier. Observer pref-
erence of artificial intelligence-generated versus clinical prostate contours for ultrasound-based
high dose rate brachytherapy. Med Phys, 50(10):5935–5943, 2023. ISSN 2473-4209 (Electronic)
0094-2405 (Linking). doi: 10.1002/mp.16716. URL https://www.ncbi.nlm.nih.gov/pubmed/
37665729.
[63] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, Spencer Whitehead, Alexander C Berg, and Wan-Yen Lo. Segment anything. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 4015–4026.
[64] Abigayle C Kraus, Zohaib Iqbal, Rex A Cardan, Richard A Popple, Dennis N Stanley, Sui Shen,
Joel A Pogue, Xingen Wu, Kevin Lee, and Samuel Marcrom. Prospective evaluation of automated
contouring for ct-based brachytherapy for gynecologic malignancies. Advances in Radiation On-
cology, 9(4):101417, 2024. ISSN 2452-1094.
[65] Y. Lei, S. Tian, X. He, T. Wang, B. Wang, P. Patel, A. B. Jani, H. Mao, W. J. Curran, T. Liu, and
X. Yang. Ultrasound prostate segmentation based on multidirectional deeply supervised v-net.
Med Phys, 46(7):3194–3206, 2019. ISSN 2473-4209 (Electronic) 0094-2405 (Print) 0094-2405
(Linking). doi: 10.1002/mp.13577. URL https://www.ncbi.nlm.nih.gov/pubmed/31074513.
[66] Y. Lei, T. Wang, Y. Fu, J. Roper, A. B. Jani, T. Liu, P. Patel, and X. Yang. Catheter position prediction
using deep-learning-based multi-atlas registration for high-dose rate prostate brachytherapy.
Med Phys, 48(11):7261–7270, 2021. ISSN 2473-4209 (Electronic) 0094-2405 (Print) 0094-2405
(Linking). doi: 10.1002/mp.15206. URL https://www.ncbi.nlm.nih.gov/pubmed/34480801.
[67] Yang Lei, Xue Dong, Zhen Tian, Yingzi Liu, Sibo Tian, Tonghe Wang, Xiaojun Jiang, Pretesh
Patel, Ashesh B Jani, and Hui Mao. Ct prostate segmentation based on synthetic mri-aided deep
attention fully convolution network. Medical physics, 47(2):530–540, 2020. ISSN 0094-2405.
[68] Yang Lei, Tonghe Wang, Justin Roper, Ashesh B Jani, Sagar A Patel, Walter J Curran, Pretesh Patel,
Tian Liu, and Xiaofeng Yang. Male pelvic multi-organ segmentation on transrectal ultrasound
using anchor-free mask cnn. Medical Physics, 48(6):3055–3064, 2021. ISSN 0094-2405.
[69] Y. Li, M. R. Imami, L. Zhao, A. Amindarolzarbi, E. Mena, J. Leal, J. Chen, A. Gafita, A. F. Voter,
X. Li, Y. Du, C. Zhu, P. L. Choyke, B. Zou, Z. Jiao, S. P. Rowe, M. G. Pomper, and H. X. Bai. An
automated deep learning-based framework for uptake segmentation and classification on psma
pet/ct imaging of patients with prostate cancer. J Imaging Inform Med, 2024. ISSN 2948-2933
(Electronic) 2948-2925 (Linking). doi: 10.1007/s10278-024-01104-y. URL https://www.ncbi.
nlm.nih.gov/pubmed/38587770.
[70] Z. Li, Q. Zhu, L. Zhang, X. Yang, Z. Li, and J. Fu. A deep learning-based self-adapting ensemble
method for segmentation in gynecological brachytherapy. Radiat Oncol, 17(1):152, 2022. ISSN
1748-717X (Electronic) 1748-717X (Linking). doi: 10.1186/s13014-022-02121-3. URL https:
//www.ncbi.nlm.nih.gov/pubmed/36064571.
[71] Z. Li, Z. Yang, J. Lu, Q. Zhu, Y. Wang, M. Zhao, Z. Li, and J. Fu. Deep learning-based dose
map prediction for high-dose-rate brachytherapy. Phys Med Biol, 68(17), 2023. ISSN 1361-6560
(Electronic) 0031-9155 (Linking). doi: 10.1088/1361-6560/acecd2. URL https://www.ncbi.
nlm.nih.gov/pubmed/37589292.
33
[72] H. M. Linares Rosales, G. Couture, L. Archambault, S. Beddar, P. Despres, and L. Beaulieu. On the
use of machine learning methods for mpsd calibration in hdr brachytherapy. Phys Med, 91:73–79,
2021. ISSN 1724-191X (Electronic) 1120-1797 (Linking). doi: 10.1016/j.ejmp.2021.10.003. URL
https://www.ncbi.nlm.nih.gov/pubmed/34717139.
[73] Derek Liu, Shayantonee Tupor, Jaskaran Singh, Trey Chernoff, Nelson Leong, Evgeny Sadikov,
Asim Amjad, and Sandra Zilles. The challenges facing deep learning–based catheter localization
for ultrasound guided high-dose-rate prostate brachytherapy. Medical Physics, 49(4):2442–2451,
2022. ISSN 0094-2405.
[74] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 3431–3440.
[75] J Nicholas Lukens, Mauricio Gamez, Kenneth Hu, and Louis B Harrison. Modern brachytherapy.
In Seminars in oncology, volume 41, pages 831–847. Elsevier. ISBN 0093-7754.
[76] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical
images. Nature Communications, 15(1):654, 2024. ISSN 2041-1723.
[77] X. Mao, J. Pineau, R. Keyes, and S. A. Enger. Rapidbrachydl: Rapid radiation dose calculations
in brachytherapy via deep learning. Int J Radiat Oncol Biol Phys, 108(3):802–812, 2020. ISSN
1879-355X (Electronic) 0360-3016 (Linking). doi: 10.1016/j.ijrobp.2020.04.045. URL https:
//www.ncbi.nlm.nih.gov/pubmed/32413546.
[78] L. A. Matkovic, T. Wang, Y. Lei, O. O. Akin-Akintayo, O. A. Abiodun Ojo, A. A. Akintayo, J. Roper,
J. D. Bradley, T. Liu, D. M. Schuster, and X. Yang. Prostate and dominant intraprostatic lesion
segmentation on pet/ct using cascaded regional-net. Phys Med Biol, 66(24), 2021. ISSN 1361-
6560 (Electronic) 0031-9155 (Print) 0031-9155 (Linking). doi: 10.1088/1361-6560/ac3c13. URL
https://www.ncbi.nlm.nih.gov/pubmed/34808603.
[79] R. Mohammadi, I. Shokatian, M. Salehi, H. Arabi, I. Shiri, and H. Zaidi. Deep learning-based
auto-segmentation of organs at risk in high-dose rate brachytherapy of cervical cancer. Radiother
Oncol, 159:231–240, 2021. ISSN 1879-0887 (Electronic) 0167-8140 (Linking). doi: 10.1016/j.
radonc.2021.03.030. URL https://www.ncbi.nlm.nih.gov/pubmed/33831446.
[80] K. Nakanishi, S. Yamamoto, T. Yabe, K. Yogo, Y. Noguchi, K. Okudaira, N. Kawachi, and
J. Kataoka. Estimating blurless and noise-free ir-192 source images from gamma camera images
for high-dose-rate brachytherapy using a deep-learning approach. Biomed Phys Eng Express, 10
(1), 2023. ISSN 2057-1976 (Electronic) 2057-1976 (Linking). doi: 10.1088/2057-1976/ad0bb2.
URL https://www.ncbi.nlm.nih.gov/pubmed/37948761.
[81] H. G. Nguyen, C. Fouard, and J. Troccaz.
Segmentation, separation and pose estimation of
prostate brachytherapy seeds in ct images. IEEE Trans Biomed Eng, 62(8):2012–24, 2015. ISSN
1558-2531 (Electronic) 0018-9294 (Linking). doi: 10.1109/TBME.2015.2409304. URL https:
//www.ncbi.nlm.nih.gov/pubmed/25769143.
[82] Ruiyan Ni, Kathy Han, Benjamin Haibe-Kains, and Alexandra Rink. Generalizability of deep
learning in organ-at-risk segmentation: A transfer learning study in cervical brachytherapy. Ra-
diotherapy and Oncology, 197:110332, 2024. ISSN 0167-8140.
[83] Alexandru Nicolae, Gerard Morton, Hans Chung, Andrew Loblaw, Suneil Jain, Darren Mitchell,
Lin Lu, Joelle Helou, Motasem Al-Hanaqta, and Emily Heath. Evaluation of a machine-learning
algorithm for treatment planning in prostate low-dose-rate brachytherapy. International Journal
of Radiation Oncology* Biology* Physics, 97(4):822–829, 2017. ISSN 0360-3016.
[84] Alexandru Nicolae, Mark Semple, Lin Lu, Mackenzie Smith, Hans Chung, Andrew Loblaw, Ger-
ard Morton, Lucas Castro Mendez, Chia-Lin Tseng, and Melanie Davidson.
Conventional vs
machine learning–based treatment planning in prostate brachytherapy: results of a phase i ran-
domized controlled trial. Brachytherapy, 19(4):470–476, 2020. ISSN 1538-4721.
34
[85] Reyhaneh Nosrati, Abraam Soliman, Habib Safigholi, Masoud Hashemi, Matthew Wronski, Ger-
ard Morton, Ana Pejovi´c-Mili´c, Greg Stanisz, and William Y Song. Mri-based automated de-
tection of implanted low dose rate (ldr) brachytherapy seeds using quantitative susceptibility
mapping (qsm) and unsupervised machine learning (ml). Radiotherapy and Oncology, 129(3):
540–547, 2018. ISSN 0167-8140.
[86] S. Nouranian, M. Ramezani, I. Spadinger, W. J. Morris, S. E. Salcudean, and P. Abolmae-
sumi. Learning-based multi-label segmentation of transrectal ultrasound images for prostate
brachytherapy.
IEEE Trans Med Imaging, 35(3):921–32, 2016.
ISSN 1558-254X (Electronic)
0278-0062 (Linking). doi: 10.1109/TMI.2015.2502540. URL https://www.ncbi.nlm.nih.gov/
pubmed/26599701.
[87] N. Orlando, D. J. Gillies, I. Gyacskov, C. Romagnoli, D. D’Souza, and A. Fenster. Automatic
prostate segmentation using deep learning on clinically diverse 3d transrectal ultrasound images.
Med Phys, 47(6):2413–2426, 2020. ISSN 2473-4209 (Electronic) 0094-2405 (Linking). doi: 10.
1002/mp.14134. URL https://www.ncbi.nlm.nih.gov/pubmed/32166768.
[88] N. Orlando, I. Gyacskov, D. J. Gillies, F. Guo, C. Romagnoli, D. D’Souza, D. W. Cool, D. A. Hoover,
and A. Fenster. Effect of dataset size, image quality, and image type on deep learning-based
automatic prostate segmentation in 3d ultrasound. Phys Med Biol, 67(7), 2022. ISSN 1361-6560
(Electronic) 0031-9155 (Linking). doi: 10.1088/1361-6560/ac5a93. URL https://www.ncbi.
nlm.nih.gov/pubmed/35240585.
[89] Keiron O’shea and Ryan Nash. An introduction to convolutional neural networks. arXiv preprint
arXiv:1511.08458, 2015.
[90] Michelle Oud, Inger-Karine Kolkman-Deurloo, Jan-Willem Mens, Danny Lathouwers, Zolt´an
Perk´o, Ben Heijmen, and Sebastiaan Breedveld. Fast and fully-automated multi-criterial treat-
ment planning for adaptive hdr brachytherapy for locally advanced cervical cancer. Radiotherapy
and oncology, 148:143–150, 2020. ISSN 0167-8140.
[91] T. Peng, Y. Dong, G. Di, J. Zhao, T. Li, G. Ren, L. Zhang, and J. Cai. Boundary delineation in
transrectal ultrasound images for region of interest of prostate. Phys Med Biol, 68(19), 2023.
ISSN 1361-6560 (Electronic) 0031-9155 (Linking). doi: 10.1088/1361-6560/acf5c5. URL https:
//www.ncbi.nlm.nih.gov/pubmed/37652058.
[92] Daniel G Petereit, Steven J Frank, Akila N Viswanathan, Beth Erickson, Patricia Eifel, Paul L
Nguyen, and David E Wazer. Brachytherapy: where has it gone? Journal of Clinical Oncology, 33
(9):980, 2015.
[93] Max Peters, Marieke J van Son, Marinus A Moerland, Linda GW Kerkmeijer, Wietse SC Eppinga,
Richard P Meijer, Jan JW Lagendijk, Taimur T Shah, and Hashim U Ahmed. Mri-guided ultrafocal
hdr brachytherapy for localized prostate cancer: median 4-year results of a feasibility study.
International Journal of Radiation Oncology* Biology* Physics, 104(5):1045–1053, 2019. ISSN 0360-
3016.
[94] Malvika Pillai, Karthik Adapa, Shiva K Das, Lukasz Mazur, John Dooley, Lawrence B Marks,
Reid F Thompson, and Bhishamjit S Chera. Using artificial intelligence to improve the quality
and safety of radiation therapy. Journal of the American College of Radiology, 16(9):1267–1272,
2019. ISSN 1546-1440.
[95] Alexander R Podgorsak, Bhanu P Venkatesulu, Mohammad Abuhamad, Matthew M Harkenrider,
Abhishek A Solanki, John C Roeske, and Hyejoo Kang.
Dosimetric and workflow impact of
synthetic-mri use in prostate high-dose-rate brachytherapy. Brachytherapy, 22(5):686–696, 2023.
ISSN 1538-4721.
[96] Gang Pu, Shan Jiang, Zhiyong Yang, Yuanjing Hu, and Ziqi Liu. Deep reinforcement learning for
treatment planning in high-dose-rate cervical brachytherapy. Physica Medica, 94:1–7, 2022. ISSN
1120-1797.
[97] Katarina M Rajkovi´c, Jovan Stankovi´c, Miodrag A´cimovi´c, Nina Dukanovi´c, and Borislava
Nikolin.
Modelling and optimisation of treatment parameters in high-dose-rate mono
35
brachytherapy for localised prostate carcinoma using a multilayer artificial neural network and
a genetic algorithm: Pilot study. Computers in Biology and Medicine, 126:104045, 2020. ISSN
0010-4825.
[98] Pranav Rajpurkar, Emma Chen, Oishi Banerjee, and Eric J Topol. Ai in health and medicine.
Nature medicine, 28(1):31–38, 2022. ISSN 1078-8956.
[99] Dominique Reijtenbagh,
J´er´emy Godart,
Astrid de Leeuw,
Yvette Seppenwoolde,
Ina
J¨urgenliemk-Schulz, Jan-Willem Mens, Remi Nout, and Mischa Hoogeman. Multi-center analysis
of machine-learning predicted dose parameters in brachytherapy for cervical cancer. Radiother-
apy and Oncology, 170:169–175, 2022. ISSN 0167-8140.
[100] Dominique MW Reijtenbagh, J´er´emy Godart, Astrid AC de Leeuw, Ina M J¨urgenliemk-Schulz,
Jan-Willem M Mens, Mich`ele Huge, and Mischa S Hoogeman. Multi-center dosimetric predic-
tions to improve plan quality for brachytherapy for cervical cancer treatment. Radiotherapy and
Oncology, 182:109518, 2023. ISSN 0167-8140.
[101] Roque Rodr´ıguez Outeiral, Patrick J Gonz´alez, Eva E Schaake, Uulke A van der Heide, and Rita
Sim˜oes. Deep learning for segmentation of the cervical cancer gross tumor volume on magnetic
resonance imaging for brachytherapy. Radiation Oncology, 18(1):91, 2023. ISSN 1748-717X.
[102] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks for
biomedical image segmentation.
In Medical image computing and computer-assisted interven-
tion–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceed-
ings, part III 18, pages 234–241. Springer. ISBN 3319245732.
[103] M. Salehi, A. Vafaei Sadr, S. R. Mahdavi, H. Arabi, I. Shiri, and R. Reiazi. Deep learning-based
non-rigid image registration for high-dose rate brachytherapy in inter-fraction cervical cancer. J
Digit Imaging, 36(2):574–587, 2023. ISSN 1618-727X (Electronic) 0897-1889 (Print) 0897-1889
(Linking). doi: 10.1007/s10278-022-00732-6. URL https://www.ncbi.nlm.nih.gov/pubmed/
36417026.
[104] J. W. Sanders, S. J. Frank, R. J. Kudchadker, T. L. Bruno, and J. Ma.
Development and clin-
ical implementation of seednet: A sliding-window convolutional neural network for radioac-
tive seed identification in mri-assisted radiosurgery (mars). Magn Reson Med, 81(6):3888–3900,
2019.
ISSN 1522-2594 (Electronic) 0740-3194 (Linking).
doi: 10.1002/mrm.27677.
URL
https://www.ncbi.nlm.nih.gov/pubmed/30737827.
[105] J. W. Sanders, G. D. Lewis, H. D. Thames, R. J. Kudchadker, A. M. Venkatesan, T. L. Bruno,
J. Ma, M. D. Pagel, and S. J. Frank. Machine segmentation of pelvic anatomy in mri-assisted
radiosurgery (mars) for prostate cancer brachytherapy. Int J Radiat Oncol Biol Phys, 108(5):1292–
1303, 2020. ISSN 1879-355X (Electronic) 0360-3016 (Linking). doi: 10.1016/j.ijrobp.2020.06.
076. URL https://www.ncbi.nlm.nih.gov/pubmed/32634543.
[106] J. W. Sanders, R. J. Kudchadker, C. Tang, H. Mok, A. M. Venkatesan, H. D. Thames, and S. J.
Frank.
Prospective evaluation of prostate and organs at risk segmentation software for mri-
based prostate radiation therapy. Radiol Artif Intell, 4(2):e210151, 2022. ISSN 2638-6100 (Elec-
tronic) 2638-6100 (Linking). doi: 10.1148/ryai.210151. URL https://www.ncbi.nlm.nih.gov/
pubmed/35391775.
[107] Amani Shaaer, Moti Paudel, Melanie Davidson, Mark Semple, Alexandru Nicolae, Lucas Castro
Mendez, Hans Chung, Andrew Loblaw, Chia-Lin Tseng, and Gerard Morton. Dosimetric evalu-
ation of mri-to-ultrasound automated image registration algorithms for prostate brachytherapy.
Brachytherapy, 19(5):599–606, 2020. ISSN 1538-4721.
[108] Amani Shaaer, Moti Paudel, Mackenzie Smith, Frances Tonolete, and Ananth Ravi.
Deep-
learning-assisted algorithm for catheter reconstruction during mr-only gynecological interstitial
brachytherapy. Journal of Applied Clinical Medical Physics, 23(2):e13494, 2022. ISSN 1526-9914.
[109] Mohammed Yousef Shaheen. Applications of artificial intelligence (ai) in healthcare: A review.
ScienceOpen Preprints, 2021.
36
[110] Chenyang Shen, Yesenia Gonzalez, Peter Klages, Nan Qin, Hyunuk Jung, Liyuan Chen, Dan
Nguyen, Steve B Jiang, and Xun Jia. Intelligent inverse treatment planning via deep reinforce-
ment learning, a proof-of-principle study in high dose-rate brachytherapy for cervical cancer.
Physics in Medicine & Biology, 64(11):115013, 2019. ISSN 0031-9155.
[111] Janusz Skowronek. Current status of brachytherapy in cancer treatment–short overview. Journal
of contemporary brachytherapy, 9(6):581–589, 2017. ISSN 1689-832X.
[112] Xinrui Song, Hengtao Guo, Xuanang Xu, Hanqing Chao, Sheng Xu, Baris Turkbey, Bradford J
Wood, Ge Wang, and Pingkun Yan. Cross-modal attention for mri and ultrasound volume reg-
istration. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part IV
24, pages 66–75. Springer. ISBN 3030872017.
[113] Kailyn Stenhouse, Michael Roumeliotis, Philip Ciunkiewicz, Robyn Banerjee, Svetlana Yanushke-
vich, and Philip McGeachy. Development of a machine learning model for optimal applicator
selection in high-dose-rate cervical brachytherapy. Frontiers in Oncology, 11:611437, 2021. ISSN
2234-943X.
[114] M. Styner, C. Brechbuhler, G. Szekely, and G. Gerig. Parametric estimate of intensity inhomo-
geneities applied to mri. IEEE Trans Med Imaging, 19(3):153–65, 2000. ISSN 0278-0062 (Print)
0278-0062 (Linking). doi: 10.1109/42.845174. URL https://www.ncbi.nlm.nih.gov/pubmed/
10875700.
[115] Jamema Swamidas, Christian Kirisits, Marisol De Brabandere, Taran Paulsen Hellebust, Frank-
Andr´e Siebert, and Kari Tanderup. Image registration, contour propagation and dose accumula-
tion of external beam and brachytherapy in gynecological radiotherapy. Radiotherapy and Oncol-
ogy, 143:1–11, 2020. ISSN 0167-8140.
[116] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9.
[117] Kari Tanderup, Cynthia M´enard, Csaba Polgar, Jacob Christian Lindegaard, Christian Kirisits,
and Richard P¨otter. Advancements in brachytherapy. Advanced drug delivery reviews, 109:15–25,
2017. ISSN 0169-409X.
[118] Z. Tian, A. Yen, Z. Zhou, C. Shen, K. Albuquerque, and B. Hrycushko. A machine-learning-
based prediction model of fistula formation after interstitial brachytherapy for locally advanced
gynecological malignancies. Brachytherapy, 18(4):530–538, 2019. ISSN 1873-1449 (Electronic)
1538-4721 (Linking).
doi: 10.1016/j.brachy.2019.04.004.
URL https://www.ncbi.nlm.nih.
gov/pubmed/31103434.
[119] G. Valdes, J. M. Luna, E. Eaton, 2nd Simone, C. B., L. H. Ungar, and T. D. Solberg. Mediboost: a
patient stratification tool for interpretable decision making in the era of precision medicine. Sci
Rep, 6:37854, 2016. ISSN 2045-2322 (Electronic) 2045-2322 (Linking). doi: 10.1038/srep37854.
URL https://www.ncbi.nlm.nih.gov/pubmed/27901055.
[120] G. Valdes, A. J. Chang, Y. Interian, K. Owen, S. T. Jensen, L. H. Ungar, A. Cunha, T. D. Solberg, and
I. C. Hsu. Salvage hdr brachytherapy: Multiple hypothesis testing versus machine learning anal-
ysis. Int J Radiat Oncol Biol Phys, 101(3):694–703, 2018. ISSN 1879-355X (Electronic) 0360-3016
(Linking). doi: 10.1016/j.ijrobp.2018.03.001. URL https://www.ncbi.nlm.nih.gov/pubmed/
29709315.
[121] Eric Vigneault, Khaly Mbodji, Louis-Gabriel Racine, Eric Chevrette, Marie-Claude Lavall´ee,
Andr´e-Guy Martin, Philippe Despr´es, and Luc Beaulieu.
Image-guided high-dose-rate
brachytherapy boost to the dominant intraprostatic lesion using multiparametric magnetic res-
onance imaging including spectroscopy: results of a prospective study. Brachytherapy, 15(6):
746–751, 2016. ISSN 1538-4721.
[122] Mateo Villa, Julien Bert, Antoine Valeri, Ulrike Schick, and Dimitris Visvikis. Fast monte carlo-
based inverse planning for prostate brachytherapy by using deep learning. IEEE Transactions on
Radiation and Plasma Medical Sciences, 6(2):182–188, 2021. ISSN 2469-7311.
37
[123] Fuyue Wang, Lei Xing, Hilary Bagshaw, Mark Buyyounouski, and Bin Han. Deep learning appli-
cations in automatic needle segmentation in ultrasound-guided prostate brachytherapy. Medical
Physics, 47(9):3797–3805, 2020. ISSN 0094-2405.
[124] J. Wang, Y. Chen, Y. Tu, H. Xie, Y. Chen, L. Luo, P. Zhou, and Q. Tang. Evaluation of auto-
segmentation for brachytherapy of postoperative cervical cancer using deep learning-based
workflow. Phys Med Biol, 68(5), 2023. ISSN 1361-6560 (Electronic) 0031-9155 (Linking). doi:
10.1088/1361-6560/acba76. URL https://www.ncbi.nlm.nih.gov/pubmed/36753762.
[125] Shijun Wang and Ronald M Summers. Machine learning and radiology. Medical image analysis,
16(5):933–951, 2012. ISSN 1361-8415.
[126] T. Wang, Y. Lei, E. Schreibmann, J. Roper, T. Liu, D. M. Schuster, A. B. Jani, and X. Yang. Lesion
segmentation on (18)f-fluciclovine pet/ct images using deep learning. Front Oncol, 13:1274803,
2023. ISSN 2234-943X (Print) 2234-943X (Electronic) 2234-943X (Linking). doi: 10.3389/fonc.
2023.1274803. URL https://www.ncbi.nlm.nih.gov/pubmed/38156106.
[127] Tonghe Wang, Robert H Press, Matt Giles, Ashesh B Jani, Peter Rossi, Yang Lei, Walter J Curran,
Pretesh Patel, Tian Liu, and Xiaofeng Yang. Multiparametric mri-guided dose boost to dominant
intraprostatic lesions in ct-based high-dose-rate prostate brachytherapy. The British journal of
radiology, 92(1097):20190089, 2019. ISSN 0007-1285.
[128] Yuenan Wang, Wanwei Jian, Lin Zhu, Chunya Cai, Bailin Zhang, and Xuetao Wang. Attention-
gated deep-learning–based automatic digitization of interstitial needles in high-dose-rate
brachytherapy for cervical cancer.
Advances in Radiation Oncology, 9(1):101340, 2024.
ISSN
2452-1094.
[129] L. L. Weishaupt, H. K. Sayed, X. Mao, R. Choo, B. J. Stish, S. A. Enger, and C. Deufel. Approaching
automated applicator digitization from a new angle: Using sagittal images to improve deep learn-
ing accuracy and robustness in high-dose-rate prostate brachytherapy. Brachytherapy, 21(4):520–
531, 2022. ISSN 1873-1449 (Electronic) 1538-4721 (Linking). doi: 10.1016/j.brachy.2022.02.005.
URL https://www.ncbi.nlm.nih.gov/pubmed/35422402.
[130] Vonetta M Williams, Jenna M Kahn, Nikhil G Thaker, Sushil Beriwal, Paul L Nguyen, Douglas
Arthur, Daniel Petereit, and Brandon A Dyer. The case for brachytherapy: why it deserves a
renaissance. Advances in Radiation Oncology, 6(2):100605, 2021. ISSN 2452-1094.
[131] Menglin Wu, Xuchen He, Fan Li, Jie Zhu, Shanshan Wang, and Pablo D Burstein. Weakly super-
vised volumetric prostate registration for mri-trus image driven by signed distance map. Com-
puters in Biology and Medicine, 163:107150, 2023. ISSN 0010-4825.
[132] X. Xu, T. Sanford, B. Turkbey, S. Xu, B. J. Wood, and P. Yan.
Polar transform network for
prostate ultrasound segmentation with uncertainty estimation.
Med Image Anal, 78:102418,
2022. ISSN 1361-8423 (Electronic) 1361-8415 (Print) 1361-8415 (Linking). doi: 10.1016/j.media.
2022.102418. URL https://www.ncbi.nlm.nih.gov/pubmed/35349838.
[133] Xian Xue, Dazhu Liang, Kaiyue Wang, Jianwei Gao, Jingjing Ding, Fugen Zhou, Juan Xu, Hefeng
Liu, Quanfu Sun, and Ping Jiang. A deep learning-based 3d prompt-nnunet model for auto-
matic segmentation in brachytherapy of postoperative endometrial carcinoma. Journal of Applied
Clinical Medical Physics, page e14371, 2024. ISSN 1526-9914.
[134] Xin Yi, Ekta Walia, and Paul Babyn.
Generative adversarial network in medical imaging: A
review. Medical image analysis, 58:101552, 2019. ISSN 1361-8415.
[135] SA Yoganathan, Siji Nojin Paul, Satheesh Paloor, Tarraf Torfeh, Suparna Halsnad Chandramouli,
Rabih Hammoud, and Noora Al-Hammadi. Automatic segmentation of magnetic resonance im-
ages for high-dose-rate cervical cancer brachytherapy using deep learning. Medical Physics, 49
(3):1571–1584, 2022. ISSN 0094-2405.
[136] Hatem Younes, Sandrine Voros, and Jocelyne Troccaz. Automatic needle localization in 3d ul-
trasound images for brachytherapy. In 2018 IEEE 15th International Symposium on Biomedical
Imaging (ISBI 2018), pages 1203–1207. IEEE. ISBN 1538636360.
38
[137] Hatem Younes, Jocelyne Troccaz, and Sandrine Voros.
Machine learning and registration for
automatic seed localization in 3d us images for prostate brachytherapy. Medical Physics, 48(3):
1144–1156, 2021. ISSN 0094-2405.
[138] Lang Yu, Wenjun Zhang, Jie Zhang, Qi Chen, Lu Bai, Nan Liu, Tingtian Pang, Bo Yang, and
Jie Qiu. A cnn-based dose prediction method for brachytherapy treatment planning of patients
with cervical cancer. Journal of Radiation Research and Applied Sciences, 17(3):101013, 2024. ISSN
1687-8507.
[139] T. I. Yusufaly, S. M. Meyers, L. K. Mell, and K. L. Moore. Knowledge-based planning for intact
cervical cancer. Semin Radiat Oncol, 30(4):328–339, 2020. ISSN 1532-9461 (Electronic) 1053-
4296 (Linking). doi: 10.1016/j.semradonc.2020.05.009. URL https://www.ncbi.nlm.nih.gov/
pubmed/32828388.
[140] F. Zabihollahy, A. N. Viswanathan, E. J. Schmidt, M. Morcos, and J. Lee. Fully automated multior-
gan segmentation of female pelvic magnetic resonance images with coarse-to-fine convolutional
neural network. Med Phys, 48(11):7028–7042, 2021. ISSN 2473-4209 (Electronic) 0094-2405
(Print) 0094-2405 (Linking). doi: 10.1002/mp.15268. URL https://www.ncbi.nlm.nih.gov/
pubmed/34609756.
[141] Fatemeh Zabihollahy, Akila N Viswanathan, Ehud J Schmidt, and Junghoon Lee. Fully automated
segmentation of clinical target volume in cervical cancer from magnetic resonance imaging with
convolutional neural network. Journal of applied clinical medical physics, 23(9):e13725, 2022. ISSN
1526-9914.
[142] P. Zaffino, G. Pernelle, A. Mastmeyer, A. Mehrtash, H. Zhang, R. Kikinis, T. Kapur, and
M. Francesca Spadea.
Fully automatic catheter segmentation in mri with 3d convolutional
neural networks: application to mri-guided gynecologic brachytherapy.
Phys Med Biol, 64
(16):165008, 2019. ISSN 1361-6560 (Electronic) 0031-9155 (Print) 0031-9155 (Linking). doi:
10.1088/1361-6560/ab2f47. URL https://www.ncbi.nlm.nih.gov/pubmed/31272095.
[143] Nicholas G Zaorsky, Brian J Davis, Paul L Nguyen, Timothy N Showalter, Peter J Hoskin, Yasuo
Yoshioka, Gerard C Morton, and Eric M Horwitz. The evolution of brachytherapy for prostate
cancer. Nature Reviews Urology, 14(7):415–439, 2017. ISSN 1759-4812.
[144] Q. Zeng, G. Samei, D. Karimi, C. Kesch, S. S. Mahdavi, P. Abolmaesumi, and S. E. Salcud-
ean. Prostate segmentation in transrectal ultrasound using magnetic resonance imaging priors.
Int J Comput Assist Radiol Surg, 13(6):749–757, 2018. ISSN 1861-6429 (Electronic) 1861-6410
(Linking). doi: 10.1007/s11548-018-1742-6. URL https://www.ncbi.nlm.nih.gov/pubmed/
29589259.
[145] Q. Zeng, Y. Fu, Z. Tian, Y. Lei, Y. Zhang, T. Wang, H. Mao, T. Liu, W. J. Curran, A. B. Jani, P. Patel,
and X. Yang. Label-driven magnetic resonance imaging (mri)-transrectal ultrasound (trus) reg-
istration using weakly supervised learning for mri-guided prostate radiotherapy. Phys Med Biol,
65(13):135002, 2020. ISSN 1361-6560 (Electronic) 0031-9155 (Print) 0031-9155 (Linking). doi:
10.1088/1361-6560/ab8cd6. URL https://www.ncbi.nlm.nih.gov/pubmed/32330922.
[146] D. Zhang, Z. Yang, S. Jiang, Z. Zhou, M. Meng, and W. Wang. Automatic segmentation and appli-
cator reconstruction for ct-based brachytherapy of cervical cancer using 3d convolutional neural
networks. J Appl Clin Med Phys, 21(10):158–169, 2020. ISSN 1526-9914 (Electronic) 1526-9914
(Linking). doi: 10.1002/acm2.13024. URL https://www.ncbi.nlm.nih.gov/pubmed/32991783.
[147] H. W. Zhang, X. M. Zhong, Z. H. Zhang, and H. W. Pang. Dose prediction of organs at risk in pa-
tients with cervical cancer receiving brachytherapy using needle insertion based on a neural net-
work method. BMC Cancer, 23(1):385, 2023. ISSN 1471-2407 (Electronic) 1471-2407 (Linking).
doi: 10.1186/s12885-023-10875-6. URL https://www.ncbi.nlm.nih.gov/pubmed/37106444.
[148] Y. Zhang, X. He, Z. Tian, J. J. Jeong, Y. Lei, T. Wang, Q. Zeng, A. B. Jani, W. J. Curran, P. Patel,
T. Liu, and X. Yang. Multi-needle detection in 3d ultrasound images using unsupervised order-
graph regularized sparse dictionary learning. IEEE Trans Med Imaging, 39(7):2302–2315, 2020.
ISSN 1558-254X (Electronic) 0278-0062 (Print) 0278-0062 (Linking). doi: 10.1109/TMI.2020.
2968770. URL https://www.ncbi.nlm.nih.gov/pubmed/31985414.
39
[149] Y. Zhang, Y. Lei, R. L. J. Qiu, T. Wang, H. Wang, A. B. Jani, W. J. Curran, P. Patel, T. Liu, and
X. Yang. Multi-needle localization with attention u-net in us-guided hdr prostate brachytherapy.
Med Phys, 47(7):2735–2745, 2020. ISSN 2473-4209 (Electronic) 0094-2405 (Print) 0094-2405
(Linking). doi: 10.1002/mp.14128. URL https://www.ncbi.nlm.nih.gov/pubmed/32155666.
[150] Y. Zhang, Z. Tian, Y. Lei, T. Wang, P. Patel, A. B. Jani, W. J. Curran, T. Liu, and X. Yang. Automatic
multi-needle localization in ultrasound images using large margin mask rcnn for ultrasound-
guided prostate brachytherapy. Phys Med Biol, 65(20):205003, 2020. ISSN 1361-6560 (Electronic)
0031-9155 (Linking). doi: 10.1088/1361-6560/aba410. URL https://www.ncbi.nlm.nih.gov/
pubmed/32640435.
[151] X. Zhen, J. Chen, Z. Zhong, B. Hrycushko, L. Zhou, S. Jiang, K. Albuquerque, and X. Gu. Deep
convolutional neural network with transfer learning for rectum toxicity prediction in cervical
cancer radiotherapy: a feasibility study. Phys Med Biol, 62(21):8246–8263, 2017. ISSN 1361-6560
(Electronic) 0031-9155 (Linking). doi: 10.1088/1361-6560/aa8d09. URL https://www.ncbi.
nlm.nih.gov/pubmed/28914611.
[152] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++:
A nested u-net architecture for medical image segmentation. In Deep Learning in Medical Image
Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA
2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018,
Granada, Spain, September 20, 2018, Proceedings 4, pages 3–11. Springer. ISBN 3030008886.
[153] J Zhu, J Yan, J Zhang, L Yu, A Song, Z Zheng, Y Chen, S Wang, Q Chen, and Z Liu. Automatic
segmentation of high-risk clinical target volume and organs at risk in brachytherapy of cervical
cancer with a convolutional neural network. Cancer/Radioth´erapie, 2024. ISSN 1278-3218.
40
