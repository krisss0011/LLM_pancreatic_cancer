Retrospective Comparative Analysis of Prostate
Cancer In-Basket Messages: Responses from
Closed-Domain LLM vs. Clinical Teams
Yuexing Hao1, Jason M. Holmes1, Jared Hobson2, Alexandra Bennett2, Daniel K. Ebner2,
David M. Routman2, Satomi Shiraishi2, Samir H. Patel1, Nathan Y. Yu1, Chris L.
Hallemeier2, Brooke E. Ball2, Mark R. Waddle2, and Wei Liu1
1Department of Radiation Oncology, Mayo Clinic Phoenix, AZ, 85054, USA
2Department of Radiation Oncology, Mayo Clinic Rochester, MN, 55905, USA
ABSTRACT
In-basket message interactions play a crucial role in physician-patient communication, occurring during all phases (pre-, during,
and post) of a patient’s care journey. However, responding to these patients’ inquiries has become a significant burden on
healthcare workflows, consuming considerable time for clinical care teams. To address this, we introduce RadOnc-GPT,
a specialized Large Language Model (LLM) powered by GPT-4 that has been designed with a focus on radiotherapeutic
treatment of prostate cancer with advanced prompt engineering, and specifically designed to assist in generating responses.
We integrated RadOnc-GPT with patient electronic health records (EHR) from both the hospital-wide EHR database and an
internal, radiation-oncology-specific database. RadOnc-GPT was evaluated on 158 previously recorded in-basket message
interactions. Quantitative natural language processing (NLP) analysis and two grading studies with clinicians and nurses were
used to assess RadOnc-GPT’s responses. Our findings indicate that RadOnc-GPT slightly outperformed the clinical care
team in "Clarity" and "Empathy," while achieving comparable scores in "Completeness" and "Correctness." RadOnc-GPT is
estimated to save 5.2 minutes per message for nurses and 2.4 minutes for clinicians, from reading the inquiry to sending the
response. Employing RadOnc-GPT for in-basket message draft generation has the potential to alleviate the workload of clinical
care teams and reduce healthcare costs by producing high-quality, timely responses.
1 Introduction
In-Basket is the online portal messaging system integrated within Epic Applications functioning similarly to email for
communication between patients and their clinical care team. In-basket messaging system is often used to exchange messages
regarding patient concerns, appointments, and follow-up care, particularly when real-time communication is not possible.
During or following treatment, patients may not always receive immediate support from their care team. Patients with limited
clinical literacy and understanding still need to communicate with healthcare professionals for various needs, including disease
monitoring, medication, appointments, and billing or insurance issues. In this context, the In-basket serves as a vital tool to
bridge the communication gap between patients and clinical professionals1.
However, clinical care teams struggle to draft responses on time due to the increasing complexity of patients’ supportive
care needs2. Several studies have shown that an increased workload from responding to in-basket messages can negatively
impact clinicians’ burnout rates and overall well-being2–5. Further, patient messaging volumes increased by more than 50%
after COVID-19, placing an undue burden on clinical teams6–8. Though these added avenues of communication are beneficial,
generally responding to these messages is non-reimbursable as well9,10.
Since In-basket messages often contain important real-world concerns from patients, the text-based in-basket message
dataset is valuable for demonstrating patient-centered interactions. We propose using large language models (LLMs), which
are connected with the electrical health record (EHR) system, to provide timely and layman-friendly responses to various
categories of In-basket message inquiries11–16. LLMs have already shown strong technical capabilities in clinical context
learning, summarization, response generation, decision-support, and Q&A17–29. Here, we aim to evaluate the performance of
LLM and clinical care teams in three key areas: 1) capturing and interpreting all sources of data, 2) generating personalized and
prompt responses, and 3) upholding high clinical standards in terms of completeness, correctness, clarity, and empathy.
Rather than applying LLMs across all types of disease sites, we focused on prostate cancer patients who received treatment
at the Mayo Clinic’s radiation oncology department. Specializing in a specific field allows the LLM to generate more accurate
and relevant responses. We developed RadOnc-GPT, an OpenAI GPT-4o-powered LLM, which is integrated with EHR30. Since
many in-basket messages require external context for proper understanding and interpretation, RadOnc-GPT can generate more
arXiv:2409.18290v1  [cs.AI]  26 Sep 2024
personalized responses with greater details in a zero-shot without training. This approach helps save time for the clinical care
team by reducing the need to consult multiple sources of information to draft a response.
2 Method
This was a retrospective study approved by the Institutional Review Board of the Mayo Clinic. Our study included patients
with prostate cancer who were managed at Mayo Clinic (Rochester, MN) in the calendar years 2022-2024. RadOnc-GPT is a
Retrieval-Augmented Generation (RAG) system that connects with both the hospital wide electrical medical record database,
Epic, developed by Epic Systems, and the radiation oncology specific database, Aria, developed by Varian Medical Systems.
The data RadOnc-GPT may access includes clinical notes, radiology notes, pathology notes, urology notes, radiology reports,
radiation treatment details, diagnosis details, patient details (demographics), in-basket messages, and more. RadOnc-GPT is
able to retrieve data by way of specifying the patient ID and which dataset to retreive to the backend system. Once retrieved,
the data is inserted into the conversation history.
Subject demographic information retrieved from the EHR system included sex, age, race, ethnicity, preferred language, and
the attending physician’s name. Information collected from Aria included demographic information (sex, age, race, ethnicity,
preferred language, and the attending physician’s name), prostate cancer treatment-specific information (course description,
plan intent, treatment orientation, radiation type, radiation oncology machine type, number of fractions, dose prescription, dose
delivered, radiation technique, and treatment duration), and diagnosis details (cancer stage, ICD (International Classification
of Diseases) diagnosis code and code type, onset date). Information collected from Epic included clinical notes, ordered by
date. For RadOnc-GPT, the information retrieval order starts with patient demographic details, followed by treatment details,
diagnosis details, and lastly, clinical notes.
Prostate Cancer
Patient Inquiries
Care Team
Responses
Assess
RadOnc-GPT
Responses
NLP Quantitative
Evaluation
In-Basket
Messages
Patient ID
Electronic Health Records
(Patient details, treatment,
diagnosis, clinical notes)
Clinical
Professional
Gradings
Retrieve
[Optional] Check & Match
Connect
Figure 1. In-Basket Comparison Study Workflow Overview. From the in-basket messages dataset, we extracted prostate
cancer patient inquiries and their corresponding care team responses. RadOnc-GPT, integrated with patients’ EHR profiles,
generates responses to these inquiries. A randomized dataset containing both RadOnc-GPT and clinical care team responses is
then created for NLP-based quantitative evaluation and single-blinded grading by clinical professionals. Clinician and nurse
graders can optionally review and match specific responses to patient EHRs using the patient ID.
To ensure every patient inquiry was consistent and under the same GPT generation environment, we developed a GUI
interface for RadOnc-GPT that was re-initialized for each test. This approach ensured that RadOnc-GPT did not generate
biased responses from its memory of the previous patient’s pair of inquiries and responses.
Our study’s evaluation was divided into two main components: natural language processing (NLP) quantitative assessments
and clinical professional grading, as illustrated in Figure 1.
For NLP evaluation, we performed four types of measurements31,32: natural language understanding, reasoning, context
readability, and natural language generation.
For the grading study, we focused on six dimensions of evaluation33–35:
1. completeness (ranging from 1-5, the higher the better),
2. correctness (ranging from 1-5, the higher the better),
3. clarity (ranging from 1-5, the higher the better),
2/16
4. empathy (ranging from 1-5, the higher the better),
5. estimated time to respond (in minutes),
6. extensive editing required (No use, major editing, minor editing, no editing needed),
7. (Optional) text comments section.
We enlisted four medical doctors from the Department of Radiation Oncology, all with significant in-basket response experience,
with a mean Years of Experience (YoE) of 5. Of these, two medical residents (C1 and C2) independently graded all 158
messages. A third chief resident (C3) reviewed discrepancies when conflicts arose, and a board-certified radiation oncologist
specializing in prostate cancer (C4) provided the final grading decision if disagreements persisted. Given that nurses typically
initiate responses to in-basket messages, we also recruited four nurses from the same department to evaluate their capability
(whether they can answer the questions or not) and estimate the time in minutes required to answer 158 patient inquiries (mean
YoE = 5.25). The nurses provided anonymized estimates of the time in minutes spent responding to and redirecting these
in-basket messages to other advanced practice providers (APPs) or clinicians. The graders details are displayed in Table 1.
Sample grading details are displayed in Appendix Figure 10.
Clinician
Clinical Domain
Gender
YoE
Nurse
Cancer Domain
Gender
YoE
C1
Radiation Oncology
Male
3 yrs
N1
Prostate & Breast Cancer
Female
13 yrs
C2
Radiation Oncology
Female
3 yrs
N2
Prostate Cancer
Female
4 yrs
C3
Radiation Oncology
Male
5 yrs
N3
Prostate Cancer
Female
2 yrs
C4
Radiation Oncology
Male
9 yrs
N4
Prostate Cancer
Female
2 yrs
Table 1. Clinician and Nurse Grader Profiles.
3 Results
3.1 In-Basket Message Dataset
In-basket message interactions can often be disorganized. Without a standardized format for patient inquiries, under one
subject, patients may send multiple messages for a single issue or combine several unrelated questions into one message. This
makes it difficult to categorize the messages, as they frequently span multiple categories. Additionally, a single thread may
include several conversation pairs, where a pair is defined as one or more patient inquiries followed by one or more care team
responses in a time-sequenced manner. For inclusion in our evaluation dataset, each patient-clinician conversation pair must
have consisted of one patient inquiry and one care team response.
Figure 2. Nine Categories of In-Basket Message Patients’ Inquiries
3/16
We selected 90 non-metastatic prostate cancer patients from the Mayo Clinic Department of Radiation Oncology in-Basket
message database. After filtering patient inquiries that are not relevant to medical advice seeking or receiving no or unrelated
care team replies, we finally selected 158 patient inquiries, with each of them containing a clinical care team’s reply. We only
selected the message type under the Epic category of "Patient Medical Advice Request." We then pulled 158 patient inquiries’
human care team’s responses and utilized the patient inquiries to generate 158 RadOnc-GPT responses. We randomized the 316
responses and did not disclose the graders’ response source.
We manually summarized the 158 patient inquiries into 9 main categories: ’Test Results’, ’Side Effects’, ’Medication
Questions’, ’Radiation Treatment Questions’, ’Medical Oncology Questions’, ’Surgical Oncology Questions’, ’Care Coordina-
tion/Logistics’, ’Lab/Radiology/Pathology Reports’, and ’Care Journey Questions’ (Figure 2). The three most common patient
inquiries are ’Side Effects’, ’Medication Questions’, and ’Radiation Treatment Questions’.
3.2 NLP Analysis
3.2.1 Sentiment Analysis
To understand the sentiment differences from human care team and the RadOnc-GPT, we conducted TextBlob and VADER
analysis. In the TextBlob Sentiment Distribution (Figure 3 (A)), RadOnc-GPT responses are observed to skew towards a more
positive sentiment, with the majority of responses clustering around a sentiment score of 0.25. In contrast, human care team
responses present a more evenly distributed sentiment profile, with a significant concentration around the neutral score of 0
(grey line in Figure 3 (A)). RadOnc-GPT responses tend to be more positive, whereas Care Team responses consist of a broader
spectrum of sentiments, including neutral and negative tones. The VADER Sentiment Distribution (Figure 3 (B)) provides
further insight into these differences. The box plot reveals that RadOnc-GPT responses exhibit a high median sentiment score,
nearing 1.0, indicative of a predominantly positive sentiment. However, there are notable outliers reflecting occasional negative
sentiment. Clinical care team responses, by comparison, display a wider range of sentiment scores, with a lower median,
indicating a more varied and contextually nuanced sentiment expression. Our sentiment analysis collectively suggests that
while RadOnc-GPT responses are generally more positive, care team responses offer a more balanced sentiment distribution,
reflecting a greater sensitivity to the contextual nuances of the input data.
3.2.2 Natural Language inference (NLI) analysis
Figure 3. Sentiment Analysis. (A) TextBlob Sentiment Distribution; (B) VADER Sentiment Distribution; (C) NLI
Distributions between GPT and Care Team Responses; (D) Semantic Similarity Scores
4/16
To understand how human care team and RadOnc-GPT responses’ inferences with the patients’ inquiries, we conducted an
NLI analysis36. RadOnc-GPT responses were predominantly Neutral, with 92.41% of responses in this category, suggesting
a tendency towards generalized statements. In contrast, clinician responses were more varied, with 70.25% Neutral and
29.11% Entailment, indicating greater relevance and specificity. Both response types showed low contradiction rates, though
RadOnc-GPT responses had a slightly higher rate at 3.16%, which may point to occasional inconsistencies. The NLI label
distribution comparison is shown in Figure 3 (C).
Comparing the semantic similarity37 between RadOnc-GPT and human care team responses provided additional context,
showing a mean similarity score of 0.85 between RadOnc-GPT and human care team responses. This high score indicated a
strong alignment in content, even though RadOnc-GPT responses are generally more neutral. The findings suggested that while
RadOnc-GPT responses may lack the specificity found in human care team responses, they still captured the core semantic
contents, reflecting contextually relevant information. Figure 3 (D) shows the distribution of the semantic similarity scores.
3.2.3 Readability Scores
We compared the average readability scores across several indices, comparing patient inquiry, RadOnc-GPT, and clinical care
team responses. The Flesch Reading Ease scores38,39, where higher values indicate easier readability, showed that the human
care team responses were the most accessible (66.2), followed by RadOnc-GPT (59.9). This suggested that human care team
responses were slightly easier than RadOnc-GPT to read. For the Flesch-Kincaid Grade Level, Gunning Fog Index, SMOG
Index, Automated Readability Index, and Coleman-Liau Index, lower scores indicated better readability. Across these metrics,
human care team responses consistently scores lower than RadOnc-GPT, implying that human care team are written at a lower
reading level and are easier to understand. RadOnc-GPT and Human Care Team responses, while similar across these indices,
generally reflect higher complexity, particularly in the SMOG Index and Gunning Fog Index.
Figure 4. Readability Scores and Word and Sentence Counts Comparison.
The relationship between word counts and sentence counts in RadOnc-GPT, and human care team responses exhibited a
positive correlation (Figure 4). RadOnc-GPT responses tended to use more words per sentence than clinic care team responses.
The steeper slope of the RadOnc-GPT regression line indicated that RadOnc-GPT responses became more verbose as the
sentence count increases. Human care team responses were more clustered at lower word and sentence counts, reflecting a
more concise communication style. Figure 4 clearly distinguished GPT’s verbosity from the clinic care team’s brevity.
On average, RadOnc-GPT responses were more detailed, with about 135 words and 9 sentences per response. Human care
team responses, while similar in length to RadOnc-GPT responses, average around 132 words and 7 sentences per response,
indicating that care team responses were slightly more concise in terms of sentence structure.
3.3 Clinician Grader Study
In the single-blinded grader study, two clinician graders first graded all 316 responses (158 human care team responses, 158
RadOnc-GPT responses). The average of two graders’ results showed that GPT consistently performs better in "Empathy"
and "Clarity’, while human responses show higher averages in "Completeness" and "Correctness". The grading rubrics are
dispalyed in Appendix 12.3.
The mean clinician scores on "Completeness," "Correctness," "Clarity," and "Empathy" for "Human" responses vary
between 16.00 and 19.25 across categories, with the highest in Surgical Oncology Question (19.25). In contrast, RadOnc-GPT
5/16
Figure 5. Average Score Across all Four Categories
Figure 6. Radar Chart Comparing Clinical Care Team and RadOnc-GPT Performance Across Nine Categories. The four
grading dimensions are combined, with a maximum possible score of 20.
scores range from 16.72 to 19.21, with the highest in Test Results (19.21). RadOnc-GPT outperforms clinical care team in Test
Results, Care Coordination/Logistics, and Care Journey Questions, as displayed in Figure 6.
3.3.1 Time Comparison
The nurse graders study focused solely on two criteria: "Can you answer this patient inquiry?" and "Estimated time to answer
this patient inquiry." We compared the clinician graders’ estimated times to those of the nurses. On average, clinicians took
3.60 minutes (SD 1.44) to respond to an in-basket message, compared to the nurses’ 6.39 minutes (SD 4.05). While both
clinician graders were able to answer all 158 messages, nurses indicated "No" for 90 inquiries, requiring referral to clinicians,
and "Yes" for 68 inquiries. For the inquiries marked "Yes," the average response time was 5.54 minutes, and for those marked
"No," the average time was 8.83 minutes. Even though nurses may struggle with some inquiries, they still need to conduct
proper research and gather relevant patient information to determine whether the in-basket message should be escalated to
clinicians.
6/16
Figure 7. [Left] Comparisons on Two Graders’ Average Human Care Team and RadOncGPT Editing Time; [Right]
Comparative Analysis of Clinicians and Nurses Average Time in Responding In-Basket Messages.
4 Discussion
RadOnc-GPT was well able to provide medical advice to individualized patient In-basket messages on this retrospective
comparison study to both trained radiation oncologists as well as radiation-oncology-specific nurses. Although RadOnc-GPT
responses are human-like and generally similar to responses generated by the original human care teams in many aspects,
caution is still needed before deploying its messages without human oversight in real-world healthcare settings.
Judges
Non-Experts
Hierarchy of Clinician-Patient Relationship
(Depth)
Medical Knowledge
(Breadth)
Experiential Knowledge & 
Preferences
With Rad Onc GPT
Patients 
With Rad Onc GPT
Clinician 
Clinician 
Patients 
AI
Higher
Experts
Figure 8. Patient-Clinician Hierarchy Structure Shift with RadOncGPT in In-Basket Message Generation. With
RadOnc-GPT’s assistance in in-basket message generation, human care team can gradually switch its roles in initiating the
response drafts to judging the RadOnc-GPT generated drafts.
Since human care team may still need to confirm the evidence in responses by pulling out the imaging or lab/exam results
to avoid hallucination, RadOnc-GPT may be able to accelerate the response turn-out rate and alleviate the human care team’s
response pressure.
Our study observed that the human care team responses typically addressed the immediate action items to instruct patients
what to do next. The care team seldom provides sufficient patient education, clinical concepts clarifications, or informed
explanations. As RadOnc-GPT responses provided more information that clinical care team’s responses might not include,
RadOnc-GPT pushed non-expert patients to gain more expertise. While RadOnc-GPT prepared a draft in-basket message
response, clinicians went from Experts to Judges. The shift of both patients’ and clinicians’ roles and expertise in healthcare
7/16
was illustrated in Figure 8.
4.1 Prompt Engineering
We considered prompts to be one of the key factors determining the quality of RadOnc-GPT responses. For the final prompts,
we provided instructions in 1) steps of retrieving information to ensure responsibility; 2) acting as the attending physician
and provider; 3) step-by-step reasoning from patient health profiles to address patient’s inquiries; 4) handling the medications
(prioritizing over-the-counter medications); 5) determining the clarity of patient’s inquiry and asking for more information if
needed; 6) patient’s health literacy; 7) providing the original patient’s inquiry. The full prompts were presented in the appendix
12.2.
Additionally, there is a lack of standardized scales or metrics to evaluate the GPT-generated messages. A few studies
have included clear evaluation methods and scoring rubrics for grading. However, the studies in the medical domain are quite
specific, and researchers found it challenging to generalize the grading across all types of medical domains or diseases. Also,
in the reader study, which included human evaluators, the subjective grading could potentially introduce bias from years of
experience, practicing domain, clinical roles, and clinics.
4.2 Economic Potential Impacts
Figure 9. Wait Time for Human Care Team’s Response
The average of this 158 patient inquiry messages wait time for clinical care team response is 22.42 hrs (sd = 32.83, median
= 11.73 hrs), as shown in Figure 9.
The purpose of using RadOnc-GPT to generate in-basket message response was not to replace the human care team’s role
in managing the prostate cancer patients’ inquiries. Instead, RadOnc-GPT was intended to streamline the response process and
save time for the care team. Typically, responses to in-basket messages were handled sequentially, starting with nurses, then
progressing to nurse practitioners or APPs, and finally to clinicians.
Based on our estimation, using RadOnc-GPT to assist in in-basket messages generation, average words in patient inquiries
were 88.89 (SD: 64.93), estimated reading time (for an average English reader 175 words per min) would be 0.51 min for each
message (SD: 0.37 min). GPT response average words were 119.55 (SD: 49.72), with an estimated reading time of 0.68 min
(SD: 0.28 min). The clinical professionals review time would be 1.19 min for each message. Based on the clinicians and nurses
estimation, RadOnc-GPT could save approximately 5.2 minutes per message for nurses and 2.41 minutes for clinicians, from
reading the patient inquiry mesage to drafting and sending the response. With Mayo Clinic receiving around 5,000 in-basket
messages daily40 and assuming that one-fifth of these are requests for medical advice (which is 1000 messages), the potential
time savings for nurses alone would amount to 5200 minutes (or 86.67 hours) per day. Based on the NIH salary table1, this
equates to an annual savings of at least $2.28 million in nurse time ($72 per hour). 2
2https://hr.nih.gov/benefits/pay/pay-guide
8/16
5 Limitation
The retrospective study feature limited our study since we can’t ask the patients to add more information or reply to the
RadOnc-GPT generated responses. We only compared a pair of interactions under one subject, which consists of a patient
inquiry and a response message from either RadOnc-GPT or the clinical care team. It might deviate from the real-world
interaction since sometimes either the clinical care team or patients send out multiple messages under one subject to explain
their health concerns. Additionally, RadOnc-GPT processes only text and is currently unable to handle images or files. While
no images or files were involved in the in-basket message grading, interpreting such information typically takes longer than
reading text messages.
Although RadOnc-GPT generated responses can be comparable to clinical care team responses, this was a retrospective
study, and the human care team’s responses were affected by multiple factors (i.e., different care team roles’ response, the busy
time, clinical department), and likely the responses were not the best from the clinical care team.
Another limitation is that we only use GPT-4o as the backend LLM for RadOnc-GPT to generate responses. We didn’t
compare our backend engine GPT-4o with other LLMs such as LLama 3, Gemini, GPT-4 or GPT-3.5. The performance based
on GPT-4o may not generalize to other LLMs.
6 Conclusion
In this single-blinded comparison study, we evaluated 158 in-basket message interactions between RadOnc-GPT and clinical
care teams. The results demonstrated RadOnc-GPT’s ability to answer patient inquiries, though we observed limitations in its
capacity to capture the nuanced information that clinical professionals provide. Utilizing RadOnc-GPT as a foundational tool
for generating in-basket message responses allows clinical professionals to serve more as reviewers than primary authors. This
approach not only saves time and improves workflow efficiency but also enables clinicians to be more comprehensive in their
responses and to focus more on the direct patient interaction care. Future studies should further explore the limitations of LLMs
in assisting with in-basket message generation.
7 Data Availability
The authors declare that the data supporting the findings of this study are available upon request. The dataset is not public because
it contains patient health information (PHI). However, sample data consisting of 18 pairs of patient inquiries and responses, with
PHI removed, is available on GitHub: https://github.com/YuexingHao/In-Basket-Message-Evaluation/blob/main/In-Basket-QA-
Dataset.xlsx.
8 Code Availability
The code is available on GitHub: https://github.com/YuexingHao/In-Basket-Message-Evaluation
9 Acknowledgments
We thank the nurse practitioners Derek S. Remme, D.N.P., and Jonathan Moonen, D.N.P., as well as the nurses who contributed
to this study: Brittainy Johnson, R.N., Shyanne Dobbs, R.N., Brooke Kelly, R.N., and Bailey Kirchner, R.N. This research
was supported by the National Cancer Institute (NCI) R01CA280134, the Eric & Wendy Schmidt Fund for AI Research
& Innovation, the Fred C. and Katherine B. Anderson Foundation, and the Kemper Marley Foundation. The authors also
acknowledged support from Paul Calabresi Program in Clinical/Translational Research at the Mayo Clinic Comprehensive
Cancer Center K12CA090628.
10 Author Contributions
Y.H., J.M.H., M.R.W., and W.L. conceptualized the study. Y.H. and J.M.H. were responsible for data collection, data
preprocessing, model development, and validation. J.H., A.B., D.K.E., S.S., S.H.P., B.E.B., C.L.H., and M.R.W. offered
expertise in clinical grading studies and interpreted the results. Y.H. J.M.H., D.M.R., N.Y.Y., B.E.B., D.K.E., M.R.W., and W.L.
interpreted the experimental results and provided feedback on the study. All authors contributed to writing the manuscript and
reviewed and approved the final version. The study was co-supervised by M.R.W. and W.L.
11 Competing Interests
The authors declare no competing interests.
9/16
References
1. Han, H.-R. et al. Using patient portals to improve patient outcomes: systematic review. JMIR human factors 6, e15038
(2019).
2. Sandford, L. M. et al. Tracking health care team response to electronic health record asynchronous alerts: Role of in-basket
message burden. J. Patient-Centered Res. Rev. 3, 201–202 (2016).
3. Tai-Seale, M. et al. Physicians’ well-being linked to in-basket messages generated by algorithms in electronic health
records. Heal. Aff. 38, 1073–1078 (2019).
4. Baxter, S. L. et al. Association of electronic health record inbasket message characteristics with physician burnout. JAMA
Netw. Open 5, e2244363–e2244363 (2022).
5. Overhage, J. M. & McCallie Jr, D. Physician time spent using the electronic health record during outpatient encounters: a
descriptive study. Annals internal medicine 172, 169–174 (2020).
6. Nath, B. et al. Trends in electronic health record inbox messaging during the covid-19 pandemic in an ambulatory practice
network in new england. JAMA network open 4, e2131490–e2131490 (2021).
7. Holmgren, A. J., Byron, M. E., Grouse, C. K. & Adler-Milstein, J. Association between billing patient portal messages as
e-visits and patient messaging volume. Jama 329, 339–342 (2023).
8. Lieu, T. A. et al. Primary care physicians’ experiences with and strategies for managing electronic messages. JAMA
network open 2, e1918287–e1918287 (2019).
9. Adler-Milstein, J., Zhao, W., Willard-Grace, R., Knox, M. & Grumbach, K. Electronic health records and burnout: time
spent on the electronic health record after hours and message volume associated with exhaustion but not with cynicism
among primary care clinicians. J. Am. Med. Informatics Assoc. 27, 531–538 (2020).
10. Ayers, J. W. et al. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public
social media forum. JAMA internal medicine 183, 589–596 (2023).
11. Achiam, J. et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).
12. Matulis, J. & McCoy, R. Relief in sight? chatbots, in-baskets, and the overwhelmed primary care clinician. J. general
internal medicine 38, 2808–2815 (2023).
13. Chen, S. et al. The effect of using a large language model to respond to patient messages. The Lancet Digit. Heal. 6,
e379–e381 (2024).
14. Gandhi, T. K. et al. How can artificial intelligence decrease cognitive and work burden for front line practitioners? JAMIA
open 6, ooad079 (2023).
15. Baxter, S. L., Longhurst, C. A., Millen, M., Sitapati, A. M. & Tai-Seale, M. Generative artificial intelligence responses to
patient messages in the electronic health record: early lessons learned. JAMIA open 7, ooae028 (2024).
16. Small, W. R. et al. Large language model–based responses to patients’ in-basket messages. JAMA network open 7,
e2422399–e2422399 (2024).
17. Eriksen, A. V., Möller, S. & Ryg, J. Use of gpt-4 to diagnose complex clinical cases (2023).
18. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of gpt-4 on medical challenge problems.
arXiv preprint arXiv:2303.13375 (2023).
19. Hao, Y., Liu, Z., Riter, R. N. & Kalantari, S. Advancing patient-centered shared decision-making with ai systems for older
adult cancer patients. In Proceedings of the CHI Conference on Human Factors in Computing Systems, 1–20 (2024).
20. Holmes, J. et al. Evaluating large language models on a highly-specialized topic, radiation oncology physics. Front. Oncol.
13, 1219326 (2023).
21. Garcia, P. et al. Artificial intelligence–generated draft replies to patient inbox messages. JAMA Netw. Open 7, e243201–
e243201 (2024).
22. Rezayi, S. et al. Clinicalradiobert: Knowledge-infused few shot learning for clinical notes named entity recognition. In
Machine Learning in Medical Imaging: 13th International Workshop, MLMI 2022, Held in Conjunction with MICCAI
2022, Singapore, September 18, 2022, Proceedings, 269–278, DOI: 10.1007/978-3-031-21014-3_28 (Springer-Verlag,
Berlin, Heidelberg, 2022).
23. Wu, Z. et al. Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific
radiology nli task. IEEE Transactions on Big Data DOI: 10.48550/arXiv.2304.09138 (2024). Accepted.
10/16
24. Liao, W. et al. Mask-guided bert for few-shot text classification. Neurocomputing 610, 128576, DOI: https://doi.org/10.
1016/j.neucom.2024.128576 (2024).
25. Holmes, J. et al. Benchmarking a foundation large language model on its ability to relabel structure names in accordance
with the american association of physicists in medicine task group-263 report. Pract. Radiat. Oncol. DOI: https://doi.org/
10.1016/j.prro.2024.04.017 (2024).
26. Liu, Z. et al. Tailoring large language models to radiology: A preliminary approach to llm adaptation for a highly
specialized domain. In Machine Learning in Medical Imaging: 14th International Workshop, MLMI 2023, Held in
Conjunction with MICCAI 2023, Vancouver, BC, Canada, October 8, 2023, Proceedings, Part I, 464–473, DOI: 10.1007/
978-3-031-45673-2_46 (Springer-Verlag, Berlin, Heidelberg, 2023).
27. Liu, C. et al. Artificial general intelligence for radiation oncology. Meta-Radiology 1, 100045, DOI: https://doi.org/10.
1016/j.metrad.2023.100045 (2023).
28. Dai, H. et al. Chataug: Leveraging chatgpt for text data augmentation. IEEE Transsations on Big Data (2024). Accepted.
29. Xiao, Z. et al. Instruction-vit: Multi-modal prompts for instruction learning in vision transformer. Inf. Fusion 104, 102204,
DOI: https://doi.org/10.1016/j.inffus.2023.102204 (2024).
30. Liu, Z. et al. Radonc-gpt: A large language model for radiation oncology (2023). 2309.10160.
31. Chang, Y. et al. A survey on evaluation of large language models. ACM Transactions on Intell. Syst. Technol. 15, 1–45
(2024).
32. Iroju, O. G. & Olaleke, J. O. A systematic review of natural language processing in healthcare. Int. J. Inf. Technol. Comput.
Sci. 8, 44–50 (2015).
33. Liu, L. et al. Towards automatic evaluation for llms’ clinical capabilities: Metric, data, and algorithm. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 5466–5475 (2024).
34. Abeysinghe, B. & Circi, R. The challenges of evaluating llm applications: An analysis of automated, human, and llm-based
approaches. arXiv preprint arXiv:2406.03339 (2024).
35. Wei, Q. et al. Evaluation of chatgpt-generated medical responses: a systematic review and meta-analysis. J. Biomed.
Informatics 104620 (2024).
36. MacCartney, B. Natural language inference (Stanford University, 2009).
37. Miller, G. A. & Charles, W. G. Contextual correlates of semantic similarity. Lang. cognitive processes 6, 1–28 (1991).
38. Flesch, R. A new readability yardstick. J. applied psychology 32, 221 (1948).
39. Jindal, P. & MacDermid, J. C. Assessing reading levels of health information: uses and limitations of flesch formula. Educ.
for Heal. 30, 84–88 (2017).
40. Cognetta-Rieke, C. Mayo clinic department of nursing leveraging artificial intelligence for automating draft patient
message responses. Mayo AI Summit 2024 .
11/16
12 Appendix
12.1 Grading Study Details
Completeness
Correctness
Clarity
Empathy
Extensive Editing Needed?
Comments
Subject: Bladder Control Issue
Patient EHR details 
Patient ID: XXXXXXXX
Patient Inquiries
Hi Dr. XXX,Just letting you know I had my PSA today and it is 0.48,  I have been feeling
very tired lately.  Let me know if you think we should be doing anything. Thanks, XXX
Response 
Hi XXX,
Thank you for the update.  I think we should wait till your PSA is a little bit higher before
we proceed with imaging, as this will improve our chances of clearly being able to identify
any site of recurrence ......
Clinician Graders
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
Use without Editing
Major Editing
Minor Editing (< 1 min)
Will not use this
Demographic: 69M, White...
Diagnosis: Stage IVA, T3b, N1, M0,
PSA 4.3, Gleason score 4+4 
Clinical notes: Increased urinary
frequency, urgency, weak stream,....
Y
N
Can you answer this inquiry?
Estimate Time to answer
Subject: Bladder Control Issue
Patient Inquiries
Hi Dr. XXX,Just letting you know I had
my PSA today and it is 0.48,  I have
been feeling very tired lately.  Let me
know if you think we should be doing
anything. Thanks, XXX
Nurse Graders
1 min or less
1 min - 3 mins
3 mins - 5 mins
5 mins - 7 mins
7 mins - 10 mins
10 mins - 13 mins
13 mins - 15 mins
15 mins - 20 mins
more than 20 mins
Figure 10. In-Basket Message Grading Study Details and Sample GUI.
12.2 Prompt Engineering
From the instructions above, we tested several different prompts and finally used this as our final prompt: "Patient #ID has sent
an in-basket message. Please generate a response to their message. Before generating the response, first retrieve the patient
details, patient treatment details, patient diagnosis details, and patient clinical notes. Do not pull the in-basket messages.
Retrieve all types of patient data simultaneously. In writing your response, feel free to make recommendations as if you were
the attending physician (since your response will be approved by the attending physician). When handling prescriptions,
prioritize over-the-counter if appropriate. Sign off as the attending physician. Do not mention that the patient should contact
their provider, since you are acting as their provider. Prior to giving your response, explain your reasoning step by step in an
analysis section. As part of your analysis, indicate whether the patient has provided enough information to adequately respond
to the message. If you determine that the patient has not provided enough information, please ask for more information in your
message to the patient. Assume the patient has a high school education level. Here is the in-basket message that you should
respond to: Message Details."
12/16
12.3 In-Basket Messages Grading Rubric
0 - Not applicable; 1 - Strongly disagree; 2 - Disagree; 3 - Neutral; 4 - Agree; 5 - Strongly agree
Completeness (6-point scale)
Definition: The extent to which the response addresses all parts of the patient’s message.
Key Points:
• Does the response cover all the questions or concerns raised by the patient?
• Are there any missing elements that the patient might need to know?
• Does the response provide a thorough and comprehensive answer?
Correctness
Definition: The accuracy and reliability of the information provided in the response.
Key Points:
• Is the information factually correct?
• Are medical terms and treatment options accurately described?
• Does the response avoid any misleading or incorrect statements?
Clarity
Definition: The ease with which the patient can understand the response.
Key Points:
• How clear and easy is the language to understand?
• Are medical terms explained in a way that a layperson can comprehend?
• Is the response well-organized and logically structured?
Empathy
Definition: The degree to which the response shows understanding and sensitivity to the patient’s emotional state.
Key Points:
• Does the response acknowledge the patient’s feelings and concerns?
• Is the tone compassionate and supportive?
• Does the response make the patient feel heard and cared for?
Extensive Editing (4-point scale)
Definition: The degree to which the response can be sent to the patient directly.
Scale:
• Would use this without editing.
• Minor (<1 minute) editing.
• Major editing.
• Would not use this.
13/16
12.4 Grader Study Result Statistics
Metrics
Completeness
Correctness
Clarity
Empathy
Two Graders Mean (SD) of RadOnc-GPT Responses
4.56 (0.85)
4.33 (1.05)
4.91 (0.43)
4.71 (0.57)
Two Graders Mean (SD) of Human Care Team Responses
4.65 (0.66)
4.69 (0.67)
4.90 (0.43)
4.52 (0.75)
t-statistics
-3.96
-1.08
0
-4.23
p-Value
< 0.01
> 0.05
> 0.05
< 0.01
Wilcoxon signed-rank test
1033.0
2168.0
196.0
854.0
R-Squared
0.30
0.40
0.23
0.31
ICC
0.68
0.77
0.65
0.67
Pearson Correlation Coefficient
0.55
0.63
0.48
0.56
Cohen’s Kappa Score
0.27
0.30
0.21
0.32
Three Graders’ Mean Variability [41 messages]
0.94
0.98
0.74
0.74
Table 2. Clinician Grader Result Statistics.
12.5 More Results
12.5.1 Grader Bias
The correlation of 0.717 indicates a strong positive relationship between the results of the two graders. This means that when
one grader gives a higher score, the other grader tends to give a higher score as well, showing that their evaluations are largely
consistent and in agreement. Small T-test and >0.05 P-Value indicate that there is no statistically significant difference between
the results of the two graders, meaning their assessments are generally consistent with each other. To understand the bias
between two independent graders, we used a Radar Chart (Figure 12 Right) to understand the differences between the four
categories.
Figure 11. Comparisons Between Two Graders’ Scorings on Four Categories: Completeness, Correctness, Clarity, and
Empathy.
The third grader graded 41 responses when the differences between two graders are greater than one. The fourth grader
graded 2 responses when third grader still introduced big differences.
The analysis of "Completeness," "Correctness," "Clarity," and "Empathy" scores between two clinician graders revealed
notable differences in certain areas. In the category of "Completeness," the t-test yielded a t-statistic of -3.960 with p < 0.05,
indicating a statistically significant difference between the two graders. Similarly, in "Empathy," a t-statistic of -4.235 with p <
0.05, highlighting a significant difference in graders’ perspectives interacting with patients. On the other hand, the "Correctness"
category presents a different picture. The t-test of -1.076 with p > 0.05 suggest no statistically significant difference between
the two clinicians, indicating that both clinician graders are similarly accurate in their assessments. Moreover, the "Clarity"
14/16
scores show complete parity between the two clinicians, with a t-statistic of 0.0 and p-value > 0.05, meaning their clarity in
communication is identical.
Figure 12. Heatmap of Two Clinician Graders’ Correlation Across Four Categories.
12.6 Analysis of Qualitative Comments from Clinician Graders
Clinician graders optionally provided comments, which indicated a focus on patient concerns, with words like "patient,"
"radiation," "prostate," and "symptoms" standing out (Figure 13). Issues related to treatment side effects, proper assessment,
and clarity in responses were frequently mentioned, highlighting the importance of addressing specific patient needs and
improving communication.
Figure 13. Wordcloud of Qualitative Comments from Clinician Graders
12.7 LLM Graders
We also conducted a study to evaluate whether different LLMs could match the evaluation quality of clinician graders. Using
the same study design as for the clinicians—randomized single-blinded grading, the same grading rubrics, and independent
15/16
grading without knowing the clinicians’ scores—we tested GPT-4o and Gemini on four criteria: "Completeness," "Correctness,"
"Clarity," and "Empathy". The LLMs were integrated with the EHR and provided with the same grading rubrics as prompts
used by the clinician graders (Appendix 12.3).
Under a zero-shot prompt engineering approach, GPT-4o rated RadOnc-GPT responses at 17.70 and clinical care team
responses at 15.83, while Gemini rated RadOnc-GPT responses at 15.05 and clinical care team responses at 13.14, all out of
20. The LLM graders generally favored RadOnc-GPT responses but rated all responses lower overall compared to clinician
graders. This discrepancy may be due to factors such as the format preferred by LLMs, an unclear grading rubric, or the
need for more domain-specific or clinical knowledge. Although the study was randomized and single-blinded, the LLMs
likely recognized whether the responses were generated by RadOnc-GPT or the human care team, showing a preference for
RadOnc-GPT responses. The detailed results for each category are displayed in Table 3 and the radar chart Figure 14.
Category
Response Type
Clinician Graders
GPT-4o
Gemini
Completeness
RadOnc-GPT
4.55 (0.77)
3.89 (0.84)
3.23 (0.89)
Clinical Care Team
4.65 (0.59)
3.38 (0.91)
2.78 (0.88)
Correctness
RadOnc-GPT
4.32 (0.94)
4.89 (0.40)
4.23 (0.70)
Clinical Care Team
4.69 (0.58)
4.72 (0.58)
3.77 (0.87)
Clarity
RadOnc-GPT
4.89 (0.45)
4.82 (0.43)
4.31 (0.64)
Clinical Care Team
4.89 (0.41)
4.49 (0.73)
3.81 (0.88)
Empathy
RadOnc-GPT
4.71 (0.49)
4.11 (0.98)
3.28 (0.90)
Clinical Care Team
4.52 (0.67)
3.24 (1.19)
2.78 (0.98)
Table 3. Mean and Standard Deviation of Scores for Clinician Graders, GPT-4o, and Gemini
Research into the evaluation process with GPT-4o and Gemini showed that it is feasible to train LLMs to serve as evaluators
on par with human experts. As LLMs become more aligned with clinician graders, they could also improve the quality of their
own responses. However, existed LLMs relying solely on zero-shot prompts, without proper domain-specific training and
guidance, still require further refinement.
Figure 14. Radar Charts to compare four dimensions with three different graders.
16/16
