Efficient Self-Supervised Barlow Twins from Limited
Tissue Slide Cohorts for Colonic Pathology Diagnostics
Cassandre Nottona, Vasudev Sharmaa, Vincent Quoc-Huy Trinhc, Lina
Chend, Minqi Xue, Sonal Varma1, Mahdi S. Hosseinia,b
aUniversity of Concordia, 1455 De Maisonneuve Blvd. W., Montreal, H3G
1M8, Quebec, Canada
bMila–Quebec Artificial Intelligence Institute, 6666, St-Urbain, #200, Montreal, H2S
3H1, Quebec, Canada
cUniversity of Montreal, 2900, boul. Edouard-Montpetit, Montreal, K3T
1J4, Quebec, Canada
dSunnybrook Health Science Centre, 2075 Bayview Avenue, Toronto, M4N
3M5, Ontario, Canada
eKingston General Hospital, 76 Stuart Street, Kingston, K7L 2V7, Ontario, Canada
Abstract
Colorectal cancer (CRC) is one of the few cancers that have an established
dysplasia-carcinoma sequence that benefits from screening. Everyone over
50 years of age in Canada is eligible for CRC screening. About 20% of those
people will undergo a biopsy for a pre-neoplastic polyp and, in many cases,
multiple polyps. As such, these polyp biopsies make up the bulk of a pathol-
ogist’s workload. Developing an efficient computational model to help screen
these polyp biopsies can improve the pathologist’s workflow and help guide
their attention to critical areas on the slide. DL models face significant chal-
lenges in computational pathology (CPath) because of the gigapixel image
size of whole-slide images and the scarcity of detailed annotated datasets.
It is, therefore, crucial to leverage self-supervised learning (SSL) methods
to alleviate the burden and cost of data annotation. However, current re-
search lacks methods to apply SSL frameworks to analyze pathology data
effectively. This paper aims to propose an optimized Barlow Twins frame-
work for colorectal polyps screening. We adapt its hyperparameters, aug-
mentation strategy and encoder to the specificity of the pathology data to
enhance performance. Additionally, we investigate the best Field of View
(FoV) for colorectal polyps screening and propose a new benchmark dataset
for CRC screening, made of four types of colorectal polyps and normal tissue,
Preprint submitted to Elsevier
November 12, 2024
arXiv:2411.05959v1  [eess.IV]  8 Nov 2024
by performing downstream tasking on MHIST and NCT-CRC-7K datasets.
Furthermore, we show that the SSL representations are more meaningful and
qualitative than the supervised ones and that Barlow Twins benefits from
the Swin Transformer when applied to pathology data. Codes are avaialble
from https://github.com/AtlasAnalyticsLab/PathBT.
Keywords:
Computational Pathology, Self Supervised Learning,
Fine-tuning, Colorectal Cancer, Colorectal Polyps, Benchmark
List of Figures
1
Five slides from the five classes with their annotations (in black
and blue) and patches from the ROI for the four polyps and
from tissue regions for the Normal WSI. We observe that the
annotations are not complete, as only one layer of the sample
has been annotated (HP5, SSL6, TA3 and TVA2). The normal
slide N9 does not present with any annotations. . . . . . . . .
12
2
Impact of the different components of the augmentation strat-
egy on the performance. We observe that the baseline, pro-
posed in the original work [1] does not perform well on our
benchmark dataset. . . . . . . . . . . . . . . . . . . . . . . . .
14
3
Two sets of new transformations are applied to the input of
Barlow Twins to create distorted views.
Examples of each
transformation applied to different types of tissues are given
as examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
4
The WSI are tiled into patches from ROI annotations or non-
annotated regions. All patches are forwarded into the Bar-
low Twins framework. The input patches undergo two sets of
distortions. The distorted patches are forwarded through an
encoder, and their representations are projected into the loss
space using a projector. The Barlow Twins loss LBT aims at
making the cross-correlation matrix of these two embeddings
close to the identity matrix. The learned representations are
then evaluated on the patch level by training a Fully Con-
nected Layer on top of the linear encoder and on the slide
level using CLAM . . . . . . . . . . . . . . . . . . . . . . . . .
16
2
5
Polar graphs of the Accuracy and AUC of the patch classi-
fication for the different models and datasets. The proposed
models, in blue and purple, perform relatively well and out-
perform other models with the same encoder for three out of
the four datasets. . . . . . . . . . . . . . . . . . . . . . . . . .
19
6
UMAP visualizations for the ResNet-50 encoders trained in
a supervised way (on the left) and within pathBT settings
(on the right). Here, SSL corresponds to the Sessile Serrated
Lesions. We observe that the self-supervised method yields
more distinct and separable representations than the super-
vised method.
Furthermore, the three groups identified by
pathBT, Normal, HP/SSLe and TA/TVA, align with the di-
agnostic ambiguities observed in the data.
. . . . . . . . . . .
20
7
Polar graphs of the Accuracy and AUC of the slide classifica-
tion for the different models and datasets. The results do not
follow the same trends as in Figure 5. However, we remark
that swinBT performs relatively well. Slide evaluation does
not rely on the same features as the patch classification.
. . .
21
8
A TVA WSI annotated by the different models. We observe
that the different models highlight different regions of high in-
terest. The sample above is a zoomed-in view of the annotated
sample.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
9
Six patches with the highest diagnostic values for the six mod-
els across the slide showing TVA, presented in Figure 8. The
patches with the highest diagnostic value are on the left. Coloured
shapes highlight that the same patches were found to be of
high diagnosis value across different encoders. The red con-
tour highlights the noisy patches. . . . . . . . . . . . . . . . .
24
A.10 Normal and pathological tissue from our dataset.
We can
recognize some serrated structures (HP and SSLe) and some
tubular structures (TA and TVA) . . . . . . . . . . . . . . . .
29
A.11 Lightweight CNN for background detection and noise removal.
First, patches are extracted from the WSI using the Otsu
thresholding concept, integrated into TIAToolbox. Then, the
patches labelled as tissue by this first filter are forwarded to a
lightweight CNN of three convolutional layers with ReLU ac-
tivation, which labels the patches as tissue or background/noise. 30
3
A.12 Number of extracted patches within and outside ROI, for the
training and test data
. . . . . . . . . . . . . . . . . . . . . .
31
A.13 Five classes of the KGH dataset under different FoV in µm×µm 31
B.14 Influence of the batch size on the performance . . . . . . . . .
32
B.15 Influence of the λ parameter on the model performance . . . .
33
B.16 Influence of the projector dimension on the model performance 33
B.17 Influence of the solarization transformation on the model per-
formance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
B.18 Influence of the sizes of the crops on the two batches on the
model performance . . . . . . . . . . . . . . . . . . . . . . . .
35
B.19 Influence of the posterization on the two batches on the model
performance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
B.20 Influence of the rotation on the model performance
. . . . . .
36
B.21 Influence of CutMix and MixUp in the evaluation phase on
the test accuracy . . . . . . . . . . . . . . . . . . . . . . . . .
37
D.22 Training and validation losses for basicBT, imBT, pathBT and
swinBT trained on pkgh, pkgh-800, pkgh-600 and pkgh-410.
We can see that the training losses are decreasing over time
however, the validation losses present with a different behaviour 39
D.23 Training and validation losses (at the top from left to right)
and validation accuracy (at the bottom) for basicBT, imBT,
pathBT and swinBT trained on pkgh.
. . . . . . . . . . . . .
40
D.24 Training and validation losses and errors (in red and blue)
and validation AUC (in yellow) for CLAM framework trained
on top of basicBT, imBT, pathBT and swinBT encoders and
trained on pkgh. The models all overfit but with pathBT for
which we observe an overall increase of the validation AUC
throughout the training . . . . . . . . . . . . . . . . . . . . . .
42
E.25 Confusion matrices and ROC curves for basicBT, imBT, pathBT
and swinBT trained on pkgh. Here Sessile Serrated Lesions
are abbreviated as SSL. We observe that the models have a
very accurate ROC curve when it comes to Normal classifica-
tion however, they struggle to differentiate HP and SSL, and
TA and TVA, as the confusion matrices can emphasize. . . . .
44
E.26 CLAM heatmaps for (from top to bottom) Hyperplastic Polyps,
Sessile Serrated Lesions, Tubular Adenoma and Normal WSIs
45
4
List of Tables
1
Dataset statistics: slides and annotations in KGH dataset . . .
12
2
Different trained encoders. RN50s, Swins correspond to ResNet-
50 and Swin-Tiny trained in a supervised manner. For the
pretraining, Y corresponds to an initialization of the weights
with ImageNet weights and N to random initialization. For
the augmentation, Y corresponds to the new augmentation
introduced in Section 3.2.2 and N corresponds to the default
augmentation strategy. . . . . . . . . . . . . . . . . . . . . . .
17
3
Accuracy and AUC (acc - AUC) for patch classification of the
different pathBT encoders evaluated on different FoV . . . . .
21
4
Accuracy and AUC (acc - AUC) for patch classification of the
different swinBT encoders evaluated on different FoV . . . . .
21
5
Accuracy and AUC (acc - AUC) for the slide classification
of the different swinBT encoders evaluated on different FoV
using CLAM framework
. . . . . . . . . . . . . . . . . . . . .
22
6
Accuracies and AUC of PCam test set after linear evaluation
on the encoders trained in the settings mentionned in Table 2
with a batch size of 256. The training set was used to train
the encoder and 5% of this training set was used for training
the linear layer. . . . . . . . . . . . . . . . . . . . . . . . . . .
25
7
Accuracy - AUC for downstream tasking on MHIST dataset
using the encoders pretrained on three FoV of the KGH dataset
using different methods . . . . . . . . . . . . . . . . . . . . . .
26
8
Accuracy - AUC for downstream tasking on NCT-CRC-7k
dataset using the encoders pretrained on three FoV of the
KGH dataset using different methods . . . . . . . . . . . . . .
27
C.9 Main hyperparameters for the supervised training of ResNet-
50 and Swin-T. LR for Learning Rate and BS for Batch Size .
38
D.10 Number of slides, extracted by CLAM, per class in the training
and test sets . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
E.11 Patch top-1 accuracy and AUC of the test set for all models
and for all datasets. benchBT was trained for 200 ImageNet
epochs [2]. The number of epochs of Barlow Twins training
after which the linear evaluation with the best performance
was performed is in italic. The two best results for each
dataset are in bold. Best result overall is in red. . . . . . .
42
5
E.12 Slide level accuracy, AUC and F1 score (from top to bottom)
of the test slides for all models and all datasets.
The two
best results for each dataset are in bold. The two best results
overall are in red. . . . . . . . . . . . . . . . . . . . . . . . . .
43
6
1. Introduction
Computational Pathology (CPath) is an innovative field at the inter-
section of pathology and computer science. It aims to successfully create
a framework of digital diagnostics that helps extract meaningful represen-
tations from the raw data in the oncology domain. Leveraging tools such
as Deep Learning (DL), CPath enhances histopathological Whole Slide Im-
ages (WSIs) analysis and provides valuable diagnosis insights [3]. CPath is
expected to integrate easily with existing diagnostic workflows, improving
diagnostic accuracy, reproducibility, and disease detection and grading effi-
ciency [4]. The recent advancements in DL methods in CPath have led to
enhanced performance, but new challenges have emerged.
DL methods in CPath often require large amounts of annotated data.
However, annotating data is a time-consuming and expensive task that re-
quires the expertise of pathologists with years of extensive clinical experience.
This leads to a scarcity of publicly available, detailed annotated datasets and
the need to leverage these limited annotations to train accurate models. As
digital pathology expands worldwide, identifying methods less dependent on
costly annotations becomes critical. To exploit large unlabeled or weakly an-
notated datasets, it is crucial to capitalize on Self-Supervised Learning (SSL),
which trains on large amounts of unlabeled data and can outperform super-
vised pre-training [5]. Additionally, recent works have shown that SSL pre-
training on pathology data can improve performance on downstream pathol-
ogy tasks [2]. This paper focuses on addressing DL challenges in CPath to
classify colorectal polyps and prevent colorectal cancer (CRC). CRC stands
as one of the most prevalent causes of cancer-related mortality, but it is also
one of the most preventable cancers. However, bottlenecks in patient screen-
ing schedules due to a shortage of pathologists induce delayed diagnoses [6].
Therefore, integrating CPath into clinical workflows could enhance diagnosis
and lead to rapid care, increasing survival as deadly diseases, notably cancer,
are detected precisely and efficiently. Hence, developing efficient SSL screen-
ing tools for accurately classifying colorectal polyps is crucial for effective
CRC screening [7].
Integrated into Gastrointestinal (GI) cancer screening, CPath has shown
excellent diagnosis accuracy, on par with board-certified pathologists. With
the need for precise cancer grading for treatment personalization, DL is now a
cornerstone of the fight against cancer and has achieved high Area Under the
Curve (AUC) scores above 0.98. Additionally, it was shown that the deepest
7
models obtain the highest performance due to their superior generalization
capabilities [7, 3].
However, such models require more extensive training data [7]. Pathol-
ogy images are highly specific as they have neither canonical orientations
nor high colour variations. The Field of View (FoV) of the patches used for
model training is also significant, as different FoV convey varying contextual
information [2]. Despite advancements in GI and CRC screening, current
methods do not adequately study the appropriate FoV for CRC screening.
Additionally, they lack a thorough comparison of features learned through
supervised and self-supervised learning (SSL) approaches. Furthermore, a
more comprehensive analysis is needed on how self-supervised learning meth-
ods, originally developed for natural images, can be adapted to the unique
characteristics of pathological data, particularly through data augmentation
techniques.
The future of CPath relies on SSL and its adaptation to the pathology
data’s specific characteristics. In this paper, we conduct an in-depth anal-
ysis of the Barlow Twins [1] framework and propose to optimize this SSL
framework for feature embedding from colorectal polyps for CRC screening.
Our study yields several contributions: we propose (1) an enhanced Bar-
low Twins framework for pathology data by adapting the hyperparameters,
augmentation strategy and pretraining strategy, (2) an evaluation of Bar-
low Twins representations on the patch and slide level introducing this SSL
method in a MIL framework, (3) an investigation on the best FoV for CRC
screening, (4) quantitative and qualitative comparisons of the SSL and su-
pervised features, and (5) a new benchmark dataset for CRC screening after
transferring the weights from our private dataset to MHIST and NCT-CRC-
7k [8, 9, 10]. Code and pre-trained model weights (in PyTorch) are available
at https://github.com/AtlasAnalyticsLab/PathBT for further contributions
to the research community.
The paper is organized as follows: Section 2 reviews the related work
and discusses the existing approaches to self-supervised learning in CPath.
Section 3 describes the proposed methodology, detailing the patch dataset
curation and the different techniques used. Section 4 presents the experimen-
tal results and analyzes the qualitative and quantitative performance of the
proposed methods, including heatmap and UMAP examination. In Section
5, we discuss the implications of our findings and suggest potential directions
for future research.
8
2. Related Work
2.1. Computational pathology
The development of rapid and efficient whole-slide imaging technology
has laid the groundwork for CPath.
This discipline leverages computa-
tional methods to analyze and characterize histopathological features within
gigapixel digital images, or Whole Slide Images (WSIs), generated from
biomedical microscopy. CPath aims at improving cancer diagnosis, prognosis,
and treatment planning [3]. The introduction of Deep Learning techniques
for WSI representation learning has significantly enhanced diagnostic perfor-
mance, achieving results comparable to those of board-certified pathologists
[11].
2.2. Self-Supervised Representation Learning
Self-Supervised Learning (SSL) methods learn representations of a train-
ing dataset through pre-text tasks, or SSL paradigms. Nowadays, three of
them are particularly present in the literature.
On the one hand, Con-
trastive Learning (CL) methods, such as SimCLR [12] or MoCO [13], have
become increasingly successful. These methods consist of learning represen-
tations that attract distorted views of the same input and push apart the
representations of different inputs. On the other hand, non-contrastive
methods, such as Barlow Twins [1], are also built on the objective of learn-
ing similar representations for distorted views of the same input, but repre-
sentations from different inputs are not used in the process. Barlow Twins is
designed to learn invariant representations of the data under different input
distortions. This framework passes two batches of distorted views through
an encoder (ResNet-50) and a projector. The embeddings from both dis-
torted batches are used to calculate the cross-correlation matrix. The core
objective of this framework is to minimize the redundancy in these represen-
tations, and this is achieved by pushing the cross-correlation matrix closer
to the identity matrix through the Barlow Twins loss:
LBT =
X
i
(1 −Cii)2 + λ
X
i
X
j̸=i
C2
ij
(1)
Minimizing redundancy aligns with the redundancy-reduction principle of
H. Barlow, suggesting that the brain efficiently processes visual information
by discarding redundant features across the visual neural pathway. These
9
two methods rely on a Convolutional Neural Network (CNN) encoder, such
as ResNet-50 [14].
Besides, the effectiveness of Vision Transformers (ViT) has been demon-
strated on various computer vision tasks [15, 16]. DINO framework has
introduced the possibility of training ViT in a Self-Supervised manner [5].
2.3. Self-Supervised Learning for pathology data
As the annotation of large cohorts of data is significantly time-consuming
for pathologists, the development of SSL methods for unlabeled pathology
data has become increasingly crucial. While various studies in digital pathol-
ogy have explored the use of contrastive learning for acquiring robust rep-
resentations for classification tasks [17, 18], recent studies suggest limita-
tions to its efficiency for histopathology image analysis [19]. The limitation
arises from the inherent similarity in morphological features between adja-
cent patches within a WSI, rendering them less effective negative pairs for
CL. This is one of the reasons why the DINO framework remains a pop-
ular choice for pathology representation learning (HIPT, Clam, Vim4Path
[20, 8, 21]).
A recent benchmark study compared four SSL methods, among which
MoCov2, Barlow Twins and DINO, on four pathology datasets [2].
This
study highlighted the need for domain-aligned pretraining with different aug-
mentation techniques such as random vertical flips, weak colour jittering,
RandStainNA, and multiple Fields of View. The authors emphasize the po-
tential of domain-specific SSL methods to improve the performance of models
fine-tuned for downstream pathology tasks. A study investigating augmen-
tation techniques within the MoCov2 framework for Chest X-rays highlights
the need for future research on the impact of various data augmentations as
well as their combination for SSL in pathology [22]. A recent work proposes a
rotation method on the patch level to apply random rotation to one patch or
a crop of that patch and show great results [23]. This emphasis underscores
the critical role of SSL frameworks in digital pathology, particularly when
complemented by domain-aligned fine-tuning. While prior works, such as
[24], which introduces automatic adversarial style augmentation (AdvStyle)
to simulate covariate shifts similar to staining variations in pathology data,
and [25], which explores diverse augmentation techniques for pathology im-
ages within the DINO framework, have made significant contributions, have
made significant contributions, no published research to date has investigated
fine-tuning Barlow Twins framework for pathology downstream tasks.
10
Barlow Twins has performed excellently on diverse pathology datasets [2]
and requires less computational resources than DINO [5]: it does not require
a large batch size and only pushes one encoder on GPUs. Therefore, in this
work, we study the hyperparameters and augmentation techniques of this
SSL method for colorectal polyps screening tasks.
2.4. Multiple Instance Learning for WSIs
Pathologists use different resolutions of the image to dress their final di-
agnosis.
Therefore, Multi Instance Learning (MIL) consists in leveraging
the hierarchy of the data to learn a global representation of a WSI or a pa-
tient. In such processes, the training is done by aggregating different levels
of the data. In digital pathology, Campanella et al. propose an end-to-end
weak-supervision framework using a RNN for slide aggregation [26]. Lu et al.
show that MIL can leverage low-quality representations for weakly-supervised
slide-level tasks [27]. Following this work, SSL frameworks such as SimCLR,
Moco or DINO have been used as instance-level feature extractions. Recent
works such as DeepMIL, followed by DeepSMILE, introduce the use of atten-
tion pooling methods to weigh the patches by importance for classification
[28, 29]. The authors demonstrate that attention-based MIL outperform reg-
ular MIL. More recently, the SOTA method CLAM uses attention to identify
subregions of high diagnostic value within a WSI [8]. Following recent wors
[30, 31] employing CLAM to compare different encoders, we leverage CLAM
as a MIL framework as well to compare the different architectures on the
slide level.
3. Methods
3.1. Benchmark dataset
Our work aims to answer a clinical problem defined by pathologists: the
need to develop a screening method for colorectal polyps to recognize and
classify different types of polyps. Indeed, this work exploits a Kingston Gen-
eral Hospital (KGH) dataset comprising 1037 WSIs of normal colon tissue
and four different types of colorectal polyps: Hyperplastic Polyps (HP), Ses-
sile Serrated Lesions (SSLe), Tubular Adenoma (TA) and Tubulovillous Ade-
noma (TVA). An expert pathologist partially annotated this dataset on the
Region of Interest (ROI) level. A description of these precancerous lesions
is provided in Section Appendix
A. The WSIs have been acquired under
4 different magnification levels: 20X, 5X, 1.25X and 0.3125X, with a pixel
11
resolution at 20X of 0.4 microns per pixel (mpp). Table 1 presents the slides
and annotations throughout the dataset, and Figure 1 presents five slides
from the five classes with the annotations and patches from the ROI when
applicable.
Class
# of slides
# of ROIs
Normal
200
0
TA
207
465
SSLes
201
548
HP
212
284
TVA
217
842
Table 1: Dataset statistics: slides and annotations in KGH dataset
Figure 1: Five slides from the five classes with their annotations (in black and blue) and
patches from the ROI for the four polyps and from tissue regions for the Normal WSI. We
observe that the annotations are not complete, as only one layer of the sample has been
annotated (HP5, SSL6, TA3 and TVA2). The normal slide N9 does not present with any
annotations.
For a better understanding of the annotations, we denote Ann the set
of annotated tissue in the ROIs, nonAnn the set of tissue non-annotated
in the pathological slides, N the tissue from the normal WSIs and P the
pathological tissues. We highlight that:
• Ann ⊂P and Ann ∩N = ∅as all ROI regions identify pathological
tissues either from HP, SSLes, TA or TVA.
12
• nonAnn ⊂(N ∪P) as not all pathological regions have been annotated
and regions outside ROIs could be either normal tissue, pathological
tissue, or distorted tissue.
Distorted tissue is an abnormal alteration in the histological structure,
organization, or architecture of tissue caused by tumours, inflammation, med-
ical procedures, or other diseases.
Therefore, we define our dataset as a weakly annotated dataset.
We
reserve 100 slides for testing purposes. We extract patches throughout all
WSIs using TIAToolBox [32] and a custom MLP to remove artifacts and
background under four different fields of view: 1400 microns, 800 microns,
600 microns and 410 microns. These datasets will be referred as pkgh, pkgh-
800, pkgh-600 and pkgh-410. Figure A.13 shows the different pathologies at
different resolutions. Figure A.12 presents the number of patches, within and
outside the ROI annotations, for train and test WSIs. It is important to note
that using a smaller field of view results in more image patches; however, the
same quantity of tissue is represented throughout each dataset.
This work leverages the KGH dataset to optimize Barlow Twins archi-
tecture. This approach aims to achieve accurate classification at both the
patch and slide levels. Moreover, this approach will be evaluated on PCam
dataset [33], a well-known challenging benchmark for classifying metastatic
cancer in breast cancer patients’ lymph nodes.
3.2. Enhanced Barlow Twins methods
This work aims to fine-tune Barlow Twins [1] for pathology downstream
tasks.
We initially conducted ablation studies on a representative subset of the
dataset pkgh-410, made of 5,000 patches per class from ROI to gain insights
into the framework’s behaviour and identify potential improvements.
3.2.1. Barlow Twins hyperparameters
We first led an ablation study on the main hyperparameters of
this framework: the batch size, the projector dimension and the trade-off
parameter λ. The details are shown in Section Appendix
B. Our initial
ablation study revealed a positive correlation between batch size and perfor-
mance. However, this improvement comes at the cost of increased compu-
tational demands. Therefore, a pragmatic choice of a batch size of 512 was
made. Additionally, the default value of the hyperparameter λ was adapted
13
to this specific dataset. The ablation study investigating projector dimension
yielded a surprising result. While the original paper suggested a direct re-
lationship between accuracy and projector dimension, our findings indicated
that a dimension of 2048 was optimal for this dataset, suggesting that the
representations from the polyps overfit in a larger dimensional space.
3.2.2. Barlow Twins augmentation strategy
We conducted a subsequent ablation study to investigate data augmenta-
tion strategies’ impact further. Firstly, we evaluate the default augmentation
strategy of Barlow Twins, defined in [1]. This augmentation strategy includes
random crop, horizontal flip, colour jitter, random grayscale, Gaussian blur-
ring and solarization, and normalization. According to the original work,
this augmentation strategy is best suited when dealing with natural images.
However, when applying this strategy to our dataset, we observe in Figure 2,
that the baseline performance is the second worst. The only case with worse
performance occurs when horizontal flips are removed. Eliminating solar-
ization, colour jitter, Gaussian blur, and grayscale enhances performance,
as these augmentations alter the intrinsic colours within each patch. This
alteration compromises crucial information regarding specific tissue charac-
teristics carried by the stain’s colour.
Figure 2: Impact of the different components of the augmentation strategy on the perfor-
mance. We observe that the baseline, proposed in the original work [1] does not perform
well on our benchmark dataset.
To adapt this augmentation strategy to pathology, we first propose to
study the existing transformation by analyzing the effect of different thresh-
olds on the performance. We conclude that solarization with a high threshold
(250) and weak jitter (as supported by [2]) benefits the training. However,
14
Gaussian blurring, grayscale, or asymmetric crops, such as in DINO [5], do
not fit the pathology data.
Secondly, we propose to study the potential of new transformations in
this framework. We conclude that high posterization on one of the batches,
vertical flips and affine transformations boost the training. We conduct an
in-depth analysis of this ablation study in Section Appendix B. We propose
a new augmentation strategy, presented in Figure 3.
Figure 3: Two sets of new transformations are applied to the input of Barlow Twins
to create distorted views. Examples of each transformation applied to different types of
tissues are given as examples.
3.2.3. Encoder
Vision Transformers (ViTs) have shown promising results in various com-
puter vision tasks.
The Swin Transformer processes images by merging
patches in deeper layers [34]. Applied to pathology, the Swin Transform-
ers acts as a local-global feature extractor and enhances the learning of both
local fine structure and global context [35]. In this last part, and for the
first time in CPath, we want to study the potential of the Swin encoder in
the Barlow Twins setting. For a fair comparison with ResNet-50 (26 mil-
lion of trainable parameters), we study Swin-Tiny (28 million of trainable
parameters).
15
3.3. Evaluation details
This work aims to compare the different methods mentionned above: pre-
training, augmentation strategy, and encoders. Figure 4 presents the general
framework, from the slide to the patch and slide classification, using the
Barlow Twins method.
Figure 4: The WSI are tiled into patches from ROI annotations or non-annotated regions.
All patches are forwarded into the Barlow Twins framework. The input patches undergo
two sets of distortions. The distorted patches are forwarded through an encoder, and their
representations are projected into the loss space using a projector. The Barlow Twins loss
LBT aims at making the cross-correlation matrix of these two embeddings close to the
identity matrix. The learned representations are then evaluated on the patch level by
training a Fully Connected Layer on top of the linear encoder and on the slide level using
CLAM
3.3.1. Pretraining details
We train Swin-Tiny and ResNet-50 in a supervised manner, using Normal
and ROI patches for 20 epochs, and in a self-supervised manner, using Barlow
Twins and all patches (Normal, ROI and non-ROI). More precisely, we train
six models, defined in Table 2. For the training in Barlow Twins settings, the
LARS optimizer is used with a batch size of 512 with initial learning rates
16
of 0.2 for the weights and 0.0048 for the biases. The projection head is of
dimensions 8192. The encoder is trained for 100 epochs.
RN50s
Swins
basicBT
imBT
pathBT
swinBT
Training
sup
sup
BT
BT
BT
BT
Pretraining
Y
Y
N
Y
Y
Y
Augmentation
N
N
N
N
Y
Y
Encoder
RN50
Sw-T
RN50
RN50
RN50
Sw-T
Table 2: Different trained encoders. RN50s, Swins correspond to ResNet-50 and Swin-
Tiny trained in a supervised manner. For the pretraining, Y corresponds to an initial-
ization of the weights with ImageNet weights and N to random initialization. For the
augmentation, Y corresponds to the new augmentation introduced in Section 3.2.2 and N
corresponds to the default augmentation strategy.
Additionally, we will compare the performance of these models to those
of benchBT, the ResNet-50 encoder pre-trained on more than 36,000,000
WSIs from [2].
3.3.2. Linear evaluation
To evaluate the quality of the representations learned during the first
phase, we perform a linear evaluation. We train a linear classifier on top
of a frozen encoder to map the representations to the 5 classes. The top-
1 accuracy and the Area Under the Curve (AUC) are the metrics used for
comparison. While accuracy is a straightforward metric, it lacks insights into
the specific errors made by the model. The AUC provides insight into the
trade-off between sensitivity and specificity and can quantify the classifier’s
overall performance. Therefore, employing accuracy and AUC provides a
more comprehensive understanding of our classifier’s performance. The lin-
ear classifier is trained for 100 epochs using the SGD optimizer, the Cosine
Annealing scheduler, with an initial learning rate of 0.3. Section Appendix
D provides more details about this setup. The datasets used for linear evalu-
ation are balanced training sets of 3,500 patches per class and balanced test
sets of 300 patches per class, randomly selected.
3.3.3. Multi Instance evaluation
To assess the quality of the learned representations for slide-label clas-
sification, we employ CLAM method [8]. The performance of CLAM will
be evaluated using the AUC metric. Moreover, we will compare the gener-
ated WSI attention maps with the ROI annotations to assess the enocoders’
relevance to the classification task.
17
3.4. Ablation Studies
Due to space limitations, we place the ablation studies in the Appendix.
We first introduce the preprocessing steps in Section Appendix
A. Then,
in Section Appendix
B, we discuss the diverse augmentation and hyper-
parameters effect.
We also present the training curves of the supervised
and self-supervised encoders, the linear layers, and the CLAM framework in
Section Appendix D. We share some additional results, such as confusion
matrices and additional heatmaps in Appendix E.
4. Experimental Results
In this section, we first carry out various experiments for colorectal polyps
classification at the patch and slide levels. Then, we emphasize the impor-
tance of the FoV used for pretraining by evaluating the encoders on di-
verse FoV. We analyze CLAM heatmaps with respect to the partial anno-
tations of the KGH dataset. To conclude, we reproduce the patch classifi-
cation on PCam [33] benchmark dataset and perform downstream tasking
on MHIST [9] and NCT-CRC-7k [10] leveraging the encoders trained on the
KGH dataset.
4.1. Patch classification
The classifier is trained two times on the same encoder. Figure 5 presents
the accuracy and AUC for the patch classification. The encoders are eval-
uated after 10, 50 and 100 epochs of self-supervised training. The epoch
of self-supervised training for which the model performs the best is also
reported.
We observe that in all cases, the self-supervised methods pro-
vide better results than the supervised baseline. Then, we observe that the
benchmark weights do not generalize well to our dataset under linear eval-
uation. Additionally, we can observe that for pkgh and pkgh-800, the two
proposed models perform better than the other ones. However, for pkgh-600,
pathBT is outperformed by imBT, which is still outperformed by SwinBT.
Results for pkgh-410 are surprising: basicBT outperforms imBT, pathBT
and swinBT. The ImageNet initialization did not benefit the training for
this large dataset.
The new data augmentation strategy did not benefit
the quality of the extracted features either. The proposed methods perform
well on smaller datasets (with less than 500,000 patches) and bigger FoVs
18
Figure 5: Polar graphs of the Accuracy and AUC of the patch classification for the different
models and datasets. The proposed models, in blue and purple, perform relatively well
and outperform other models with the same encoder for three out of the four datasets.
(minimum 800 µm). For large datasets (1.6 million images), ImageNet ini-
tialization makes it hard for the model to converge properly. For all models,
swinBT outperforms all ResNet-50-based models on the patch level.
Figure 6 displays the UMAP visualizations of the classified features for
the ResNet-50 trained supervised and in pathBT settings. The UMAP for
ResNet-50 supervised shows that the Normal class is quite separated from
the rest, but the four other classes are still mixed. However, the UMAP
for the pathBT model shows three distinct groups: Normal, HP/SSLe and
TA/TVA, meaning that the model struggles to separate HP from SSLe and
TA from TVA. This visualization suggests that HP and SSLe share a similar
feature. The same conclusion can be drawn for TA and TVA. These results
can be explained by the fact that TVA is TA with more than 20% of villous
architecture. Therefore, TA structures can still have a bit of villous architec-
ture. Additionally, SSLe is more common in the right colon, and HP in the
left [36]. By default, a growth in the right colon will be considered SSLe until
proven otherwise. The fact that the model is able to catch this ambiguity
shows that it is learning meaningful features.
4.2. Slide classification
If patch results can help guide pathologists at the microscopic level, per-
forming a slide evaluation can help pathologists by sorting the slides by order
19
Figure 6: UMAP visualizations for the ResNet-50 encoders trained in a supervised way
(on the left) and within pathBT settings (on the right). Here, SSL corresponds to the
Sessile Serrated Lesions.
We observe that the self-supervised method yields more dis-
tinct and separable representations than the supervised method. Furthermore, the three
groups identified by pathBT, Normal, HP/SSLe and TA/TVA, align with the diagnostic
ambiguities observed in the data.
of importance. In this section, the MIL framework, CLAM [8], is trained
once on the same encoder. Figure 7 presents the slide classification’s accu-
racy, AUC and F1 score. Here, the results are more nuanced. The benchmark
weights do not provide a good enough representation of the data and the MIL
model fails to converge for pkgh-600 and pkgh-410. If the supervised base-
lines do not perform greatly overall, the supervised ResNet-50 reaches the
second-best AUC on pkgh-600 and the second-best accuracy and F1-score on
pkgh-410, emphasizing the possibility of training models on a subset of the
training set. However, the self-supervised models generally perform better
than the supervised models. For pkgh, the two proposed methods perform
better on all metrics. SwinBT performs better than pathBT in terms of AUC
metrics and always provides a very satisfying AUC above 0.9891. For pkgh-
800, it seems that basicBT surpasses all models for accuracy and F1-score.
For pkgh-600, pathBT is outperformed by imBT on all metrics, but swinBT
provides the best results. The best classification overall is obtained by imBT
encoder on pkgh-410 dataset.
4.3. Pretraining on Different Fields of View
In this section, we aim to utilize encoders pretrained on one FoV, to
classify patches or slides at a different FoV. For linear evaluation, we freeze
20
Figure 7: Polar graphs of the Accuracy and AUC of the slide classification for the different
models and datasets. The results do not follow the same trends as in Figure 5. However,
we remark that swinBT performs relatively well. Slide evaluation does not rely on the
same features as the patch classification.
the pathBT encoder trained on pkgh, pkgh-600 and pkgh-800 and we train
a linear layer on a dataset with a different FoV.
pkgh
pkgh-800
pkgh-600
Trained
pkgh
83.43 - 0.9652
77.5 - 0.9399
75.3 - 0.9472
on
pkgh-800
79.76 - 0.9591
83.84 - 0.9676
82.34 - 0.9608
pkgh-600
78.14 - 0.9509
80.17 - 0.9589
81.84 - 0.9599
Table 3: Accuracy and AUC (acc - AUC) for patch classification of the different pathBT
encoders evaluated on different FoV
pkgh
pkgh-800
pkgh-600
Trained
pkgh
86.15 - 0.9764
83.33 - 0.973
81.33 - 0.967
on
pkgh-800
85.05 - 0.9757
86.47 - 0.9822
82.8 - 0.9666
pkgh-600
83.33 - 0.9688
85.73 - 0.9754
84.17 - 0.9728
Table 4: Accuracy and AUC (acc - AUC) for patch classification of the different swinBT
encoders evaluated on different FoV
Table 3 shows the transferability results of pathBT models. Each model
performs better on the model they were trained on using Barlow Twins rather
than on the model they are evaluated on. However, pathBT trained on pkgh-
800 performs better on pkgh-600 than pathBT trained on pkgh-600. It is
possible that models trained on pkgh-600 or smaller struggled to generalize
to new data.
Table 4 shows the transferability results of swinBT models. The overall
results are better than those with pathBT, highlighting the ability of the Swin
21
Transformer to generalize to unseen data and its pertinence for pathology
data.
We also observe that the models generalize better to the datasets whose
FoV are close to the FoV used during pretraining, highlighting the fact that
the features learned by the models are linked to the FoV and the visible
structures in the patches.
Secondly, on the slide level, we train the CLAM framework [8] on top of
the frozen encoders. Table 5 shows that a closer FoV to the one used during
pretraining will provide better results than a much different FoV. This is
highlighting again the fact that the features learned by the models are linked
to the FoV.
pkgh
pkgh-800
pkgh-600
Trained
pkgh
91.78 - 0.9908
91.78 - 0.9858
90.41 - 0.9912
on
pkgh-800
91.78 - 0.9805
87.67 - 0.9891
89.04 - 0.9892
pkgh-600
84.93 - 0.9726
89.04 - 0.9892
93.16 - 0.9932
Table 5: Accuracy and AUC (acc - AUC) for the slide classification of the different swinBT
encoders evaluated on different FoV using CLAM framework
These results also emphasize that generalizable models should be able
to adapt to different FoV and, therefore, be trained on different FoV. This
observation aligns with the observation made by [2].
4.4. CLAM heatmaps and ROI Correlation
A strength of the CLAM framework is to determine areas with high diag-
nosis value. However, this asset is not taken into consideration here. As the
analysis of the decision of the CLAM framework carries meaningful explana-
tions, Figure 8 shows the different CLAM heatmaps of a TVA slide after the
MIL framework on six different encoders. First, all heatmaps are different
despite correlating with the pathologist annotation (red line). After clari-
fication with our expert pathologist, we found that these tissue samples all
comprise villous features at the bottom border of all tissue samples (as shown
by the pathBT model, for example). The center of the samples is normal
tissue. Both supervised encoders highlight patches within the ROI. However,
as mentionned in section 3.1, ROI annotations should be transferrable to the
other tissue samples below the top right one. For the two supervised models,
the heatmap within the ROI annotation does not map to the heatmaps from
the samples underneath. For the Barlow Twins encoders, mainly for imBT,
22
pathBT and swinBT, the local portions of the heatmaps corresponding to
the ROI are reproducible to other tissue samples in the slide. This result
shows that the self-supervised encoders caught the general organization of
the slide, while the supervised ones, only fed with patches from the non-ROI,
tend to generalize to the whole slide. To complete this analysis, Figure A.13
Figure 8: A TVA WSI annotated by the different models. We observe that the different
models highlight different regions of high interest. The sample above is a zoomed-in view
of the annotated sample.
shows the six patches with the highest diagnosis values. We observe that
patches of the highest diagnosis values for Resnet-50 supervised, basicBT
and imBT present artifacts, such as tissue fold and torn tissue. However, the
supervised swin-T, pathBT, and swinBT do not focus on these patches, and
the final diagnosis is made using relevant tissue patches. Moreover, pathBT
23
and swinBT share three patches of high diagnosis values in a different order.
This observation underscores the effectiveness of the proposed augmentation
strategy in bridging the differences between encoders.
Figure 9: Six patches with the highest diagnostic values for the six models across the slide
showing TVA, presented in Figure 8. The patches with the highest diagnostic value are
on the left. Coloured shapes highlight that the same patches were found to be of high
diagnosis value across different encoders. The red contour highlights the noisy patches.
4.5. Patch classification on PCam
As the proposed augmentation strategy was studied and established on
a subset of the KGH dataset, it seems fair to reproduce the experiments
described in Table 2 on another dataset. PCam is a well-known challenging
benchmark for binary class breast cancer type classification and is, therefore,
a robust dataset to evaluate our method.
To reproduce similar settings,
where the encoder is trained on all available data while the linear evaluation
is performed on limited annotated data, the encoder is trained on the PCam
training dataset while the linear layer is trained on a random 5% of the PCam
training set. The encoder was trained once for 100 epochs with a batch size
of 256 to alleviate the need for computational resources, and the linear layer
was trained for 100 epochs on the same frozen encoder with two different
24
folds of 5% of the training set. The model is then evaluated on the PCam
test set. Table 6 presents the accuracy and AUC of the different methods.
basicBT
imBT
pathBT
swinBT
ACC
69.2
69.84
70.5
79.63
AUC
0.692
0.6984
0.705
0.7963
Table 6: Accuracies and AUC of PCam test set after linear evaluation on the encoders
trained in the settings mentionned in Table 2 with a batch size of 256. The training set
was used to train the encoder and 5% of this training set was used for training the linear
layer.
We observe that imBT performs better than basicBT (+0.6%). There-
fore, it seems that the weight initialization still matters. Secondly, PathBT
performs slightly better (+0.6% in accuracy and AUC) than basicBT and
imBT; therefore, the augmentation strategy developed for the KGH dataset
can still be applied to this dataset. Additionally, SwinBT is still outperform-
ing all other methods (+9%), highlighting the potential of Swin transformers
for pathology data.
4.6. Downstream tasking on MHIST and CRC
In this section, the capacity of our model to generalize to other colon
datasets is evaluated. We train a linear layer on top of the frozen encoders
to map the representations to the classes for two datasets: MHIST [9] and
NCT-CRC-7k [10].
MHIST. MHIST dataset is designed to classify colorectal polyps into two
histological categories: Hyperplastic Polyps and Sessile Serrated Adenomas.
It comprises 3,152 patches which are H&E stained, and it was chosen because
of the similarity of these two classes to those of our KGH dataset.
The
original split train/test is used to train and evaluate the linear layer. Table
7 presents the accuracy and AUC of the diverse models pretrained on three
different FoV of the KGH dataset using four different methods presented in
Table 2. Additionally, the linear evaluation of benchBT on MHIST provides
an accuracy of 80.76 and an AUC of 0.7943. For MHIST dataset, basicBT
and imBT outperform the benchmark weights and the two proposed methods.
If basicBT provides the best accuracy (for the model pre-trained on pkgh-
600), imBT provides the best AUC (for the model pre-trained on pkgh-800).
If the accuracy for imBT remains constant throughout the three datasets, it
seems the representations given by basicBT, pathBT and swinBT are highly
25
dependent on the pretraining dataset. These results show that our proposed
methods do not generalize well to this downstream task. However, the default
transformations generalize better. The weak colour jitter could be a reason
for this lack of generalizability, as the model will have a lower ability to
generalize to other stains. These results also highlight the potential of the
KGH dataset to be used as a pretraining dataset for diverse downstream
tasks, as imBT and basicBT outperform the benchmark model, trained on a
massive cohort of data (36k WSIs).
pkgh
pkgh-800
pkgh-600
basicBT
82.5 - 0.8061
83.32 - 0.8194
84.95 - 0.7971
imBT
82.5 - 0.8033
82.6 - 0.8264
82.5 - 0.8217
pathBT
76.05 - 0.7563
74.62 - 0.7392
75.54 - 0.7361
swinBT
79.12 - 0.7634
76.56 - 0.7408
77.79 - 0.7628
Table 7: Accuracy - AUC for downstream tasking on MHIST dataset using the encoders
pretrained on three FoV of the KGH dataset using different methods
NCT-CRC. The NCT-CRC-7k dataset is the validation set of the NCT-
CRC-100k dataset and is made of 7,180 H&E patches containing human col-
orectal cancer and normal tissue patches. This dataset comprises nine classes
(adipose, background, debris, lymphocytes, mucus, smooth muscle, normal
colon mucosa, cancer-associated stroma and colorectal adenocarcinoma) and
was chosen because this large repository presents tasks similar to the KGH
dataet. 50% of this dataset was used for training the linear layer and 50%
for evaluating the model. Table 8 presents the accuracy and AUC of the
diverse models pretrained on three different FoV of the KGH dataset using
four different methods presented in Table 2. Additionally, the linear evalu-
ation of benchBT on the test split of NCT-CRC-7k provides an accuracy of
96.6 and an AUC of 0.9985. The results from Table 8 are inconsistent with
those of MHIST in the precedent section. Here, we observe that swinBT
outperforms all other models for all pretraining datasets regarding accuracy.
Representations learnt by pathBT are the ones giving the top-2 best AUC
across all pre-trained datasets. Here, all four models perform better than
the benchmark model for accuracy but not for the AUC (third best). In this
case, we show that the two proposed methods align better with the CRC
dataset and that the KGH dataset has a high potential for model pretraining
for colorectal screening.
The evaluation of the encoders pretrained on KGH on these two colorectal
26
pkgh
pkgh-800
pkgh-600
basicBT
97.99 - 0.9963
98.52 - 0.9986
99.08 - 0.9995
imBT
98.08 - 0.9937
98.69 - 0.9966
98.8 - 0.9971
pathBT
98.47 - 0.9971
98.5 - 0.9978
98.83 - 0.9986
swinBT
99.11 - 0.998
99.03 - 0.9984
99.19 - 0.9984
Table 8: Accuracy - AUC for downstream tasking on NCT-CRC-7k dataset using the
encoders pretrained on three FoV of the KGH dataset using different methods
polyps datasets has shown that the KGH dataset has a high potential to be
used as a dataset for model pretraining for colorectal polyp screening.
5. Concluding remarks and Future Work
Feature embedding from colorectal polyps for colorectal cancer (CRC)
screening relies on efficient and effective representation of Whole Slide Im-
ages (WSIs). In this paper, we propose an enhanced Barlow Twins framework
for CRC screening by adapting the augmentation strategy to the pathology
downstream task and leveraging the Swin Transformer (1). We demonstrate
that the proposed models deliver robust performance at both patch and slide
levels and generate relevant representations of the patches, facilitating mean-
ingful and accurate diagnoses for WSIs. We also highlighted that supervised
methods provide sub-optimal results on our weakly annotated dataset (2).
Furthermore, we showed that, for WSI classification, a small Field of View
(FoV) of 410 µm showed high AUCs above 0.99 across the four datasets,
while a larger FoV results in less effective WSI classification (3). We also
established that the self-supervised methods learn more realistic, robust and
meaningful features than the supervised models (4). We demonstrated that
encoders trained on the KGH dataset can be used for model pretraining for
colorectal polyp classification (5), and we release the weights. The proposed
method has the potential to be applied to a wide range of other pathology-
related tasks. We believe that adapting frameworks developed for natural
images to pathology data could result in improved performance.
These results motivate future research directions. Firstly, SSL methods
need to be tailored to pathology downstream tasks. Investigating the aug-
mentation strategy and the different encoders in DINO [5] or SwAV [37]
could pave the way for other enhanced models in CPath. Secondly, the data
augmentation study was done on the patch level, and we observed that the
27
results between the patch and slide levels were not necessarily consistent.
Therefore, future works should investigate the best augmentation strategy
for slide-level classification. We believe that further exploration of these top-
ics will yield considerable improvements for pathology-specific SSL.
Acknowledgments
This work is funded by the Fonds de recherche du Qu´ebec - Nature
et technologies (FRQNT). In addition, the data collected for this study
is supported by Huron Digital Pathology and Ontario Molecular Pathol-
ogy Research Network (OMPRN) funding grant. We also extend our grati-
tude to the students at Atlas Analytics Lab, in particular to Damien Mar-
tins Gomes and Ali Nasiri-Sarvi, for their fruitful discussions. In addition,
we thank Resources for Research Groups(RRG)–Digital Research Alliance
Canada (DRAC), Computer Science Cluster at Concordia University, and
Huron Digital Pathology for providing essential computational resources.
Data availability
The dataset for this study is offered to editors and peer reviewers at
the time of submission for the purposes of evaluating the manuscript upon
request.
The data is not publicly available due to privacy concerns and
Intellectual Property matters related to Huron Digital Pathology.
Supplementary Material
Appendix A. More details about the KGH dataset
Appendix A.1. Colorectal Polyps of the KGH dataset
The focus of the research presented in this thesis primarily revolves around
four distinct types of polyps, presented in Figure A.10:
• Hyperplastic Polyps (HP): These noncancerous growths carry a
low risk of malignant transformation and are characterized by an over-
growth of normal cells in the mucosal lining of the colon or distal colon;
[38, 39]
28
Figure A.10: Normal and pathological tissue from our dataset. We can recognize some
serrated structures (HP and SSLe) and some tubular structures (TA and TVA)
• Sessile Serrated Lesions (SSLe):
SSLes, a subtype of serrated
polyps, constitute at least 20% of all serrated polyps. They are consid-
ered precancerous and are characterized by a flat or slightly elevated
shape, predominantly found in the cecum and ascending colon. Distin-
guishing SSLes from hyperplastic polyps may pose challenges, although
pathologists can recognize certain distortions; [40, 41]
• Tubular Adenomas (TA): These are usually small and benign polyps,
prevalent in more than 80% of cases. While they are considered pre-
cancerous, less than 10% of them have the potential to progress into
cancer; [42, 43, 44]
• Tubulovillous Adenomas (TVA): TVAs are a subtype of colonic
adenomas exhibiting a combination of tubular and villous features.
Considered precancerous, they have the potential to transform into
malignant structures. [42, 45, 46]
Appendix A.2. Lightweight CNN for artefact detection
While state-of-the-art tools, such as Clam [8] or TIAToolbox [32], are
available for extracting patches from Whole Slide Images for Deep Learning
computation, they fail to detect the noise and artifacts present in our dataset,
such as glass artifacts and marker annotations.
Therefore, we propose a
lightweight Convolutional Neural Network (CNN) model that acts as a second
filter for the patches extracted by the TIAToolbox Otsu method [47]. Figure
A.11 shows the lightweight CNN. It is made of three convolutional layers with
ReLU activation and a Fully Connected layer to map the representations to
the two classes (tissue and background/noise). This CNN comprises 469,442
29
trainable parameters and was trained on a custom dataset made of 10,000
patches from each class, manually extracted at different Fields of View (FoV).
It reaches a test accuracy of 98.6% and is relatively fast, as predicting one
patch takes 0.9 ms. The code for this patch extraction tool is available at
https://github.com/CassNot/pKGH.
Figure A.11: Lightweight CNN for background detection and noise removal. First, patches
are extracted from the WSI using the Otsu thresholding concept, integrated into TIATool-
box. Then, the patches labelled as tissue by this first filter are forwarded to a lightweight
CNN of three convolutional layers with ReLU activation, which labels the patches as tissue
or background/noise.
Appendix A.3. Patch extraction
Using this model, patches from the KGH dataset mentionned in Section
3.1 have been extracted with four different FoVs: 410 µm, 600 µm, 800 µm
and 1400 µm. Figure A.12 presents the patches extracted across all train and
test slides and shows the counts of patches within and outside of the ROI
annotations for each of the five classes. Figure 9 shows patches from each
pathology at the four different FoVs. We can see that a larger FoV shows the
general organization of the polyp while a smaller FoV focuses on the specific
regions, such as some crypts.
Appendix B. Ablation studies for PathBT
In this section, we present the main ablation studies lead to understand
the influence of the main hyperparameters and augmentation strategy on
30
Figure A.12: Number of extracted patches within and outside ROI, for the training and
test data
Figure A.13: Five classes of the KGH dataset under different FoV in µm × µm
31
the Barlow Twins performance. For this study, the baseline is defined with a
batch size of 128, λ = 0.0051 and a projector dimension of 4096. The study
is led on a subset of ROI patches of pkgh-410, made of 5,000 patches per
class.
Appendix B.1. Hyperparameters
The parameters studied here are the projector dimension, the batch size
and the λ (lambda) parameter in the loss function (Equation 1). Several key
observations emerge from these experiments:
A larger batch size enhances performance: in agreement with the origi-
nal study lead by [1], we conclude that a larger batch size, because it includes
more batch statistics into the computation of the cross-correlation matrix,
provides a more accurate loss and therefore leads to better representation.
Figure B.14 shows the accuracy after linear evaluation of the model trained
in Barlow Twins settings with different batch sizes.
Figure B.14: Influence of the batch size on the performance
The default λ works well with pathology images: as shown in Figure
B.15, the value of the parameter does not influence much the training for
λ < 0.01. However, a bigger λ induces a high instability in the training.
A smaller projector dimensions leads to better performance with
the default augmentation strategy: as shown in Figure B.16, the better
performance is obtained with a batch size of 2048 which comes in contradic-
tion with the findings of the original paper [1]. This can be explained by the
fact that this augmentation strategy causes the representations to overfit in
a high-dimensional space. Moreover, a smaller projection head may result in
underfitting, potentially eliminating meaningful data representations.
With the new augmentation strategy, a larger projection head is
needed: if we reproduce the ablation study on the projector dimension us-
ing the augmentation strategy proposed in this paper, we observe that the
32
Figure B.15: Influence of the λ parameter on the model performance
Figure B.16: Influence of the projector dimension on the model performance
best results are obtained with a projector dimension of 8192. It means the
representations learned with this new augmentation strategy require a higher
dimensional space for proper representation.
Appendix B.2. Augmentation strategy
In this study, we first study the effects of removing specific data augmen-
tations of the original augmentation strategies. Then, we study the thresh-
olds used for Solarization, Colour Jitter, and Random Crops used in the
original strategy before studying the potential of new transformations. Sev-
eral key observations emerge from these experiments:
Grayscale and Gaussian blur harm the learning of good represen-
tations: from Figure 2, we understand that random grayscale and Gaussian
blur, because they modify the meaningful colours and borders of the patches,
destroy valuable information.
Gaussian blur creates out-of-focus patches,
which can harm representation learning in pathology.
A weak colour jitter benefits the training: as supported by [2], a strong
colour jitter results in unrealistic patches. One objective of the colour jitter
is to expose the model to various colours and lighting conditions. It also
33
prevents the model from overfitting to the specific colour characteristics of
the training data and, therefore, is very useful when it comes to transfer
learning, as the model will learn to generalize better and be able to adapt to
diverse environments. In our case, the training dataset and the downstream
dataset are the same, and therefore, the colour jittering is not very useful,
even though it can provide more robustness to the training data. Applying a
random weak colour jitter fosters the learning of meaningful representations
by adding a weak and meaningful distortion.
A high solarization benefits the training: removing solarization results
in a modest boost in test accuracy. This observation aligns with previous
research [48] in which the authors highlight that solarization can harm pathol-
ogy models because it produces unrealistic and unreliable images. However,
as shown in Figure B.17, a very high threshold of 250 colours the back-
ground or very thin tissue in blue and neutrophils, basophils, eosinophils
and macrophages in dark blue, therefore emphasizing specific regions highly
relevant to cancer diagnoses and reactive changes.
Figure B.17: Influence of the solarization transformation on the model performance
Barlow Twins does not benefit from different sizes of crops: moti-
vated by DINO settings [5], the possibility of using two different crop sizes
for the input (local and global crops) is studied. Leveraging the symmetry
of the Barlow Twins setup, the first batch is allocated to the global crop
and the second batch to the local crop. We compare random crops using
the final size or scale of the original patch. However, from Figure B.18, we
observe that having asymmetric crops does not benefit the training. Because
different crops have different FoV, the model struggles to find useful data
representations. Moreover, upsampling needs to be performed with a crop
smaller than 224 × 224, and the data could become less realistic.
34
Figure B.18: Influence of the sizes of the crops on the two batches on the model perfor-
mance
A high posterization can benefit the self-supervised training: from
Figure B.19, we observe that applying posterization with a limited number
of bits does not benefit the training, whereas posterization on more than 5
bits outperforms the baseline (we note that the posterization on 8 bits stays
within the range of the standard deviations of the baseline). The image on
7 bits is of slightly lower resolution than on 8 bits, which can contribute to
catching high-frequency features, such as borders and contours.
Figure B.19: Influence of the posterization on the two batches on the model performance
Rotations leverage the non-canonical orientations of the pathol-
ogy images: we apply random rotations under different angles on the input
patches. From Figure B.20, we find that the best rotation angle is 180, which
means the patches are rotated from any angle between 0 and 180. As flipping
is already used, some rotations values could be redundant with the flips and
35
lead back to the original orientations of the patch. This observation is sup-
ported by a recent paper introducing HistoRotate [23], a rotation method
on the patch level to apply random rotation to one patch or to a crop of that
patch and show great results.
Figure B.20: Influence of the rotation on the model performance
Affine transformations enhance the self-supervised training: using
the same arguments as for the rotation, affine transformations leverage the
non-canonical orientation of the patches and benefit the training. Moreover,
affine transformation with an angle between 0 and 45 and a translation fac-
tor of (0.5, 0.5) benefits the training and perform better than a sole rotation
between 0 and 180.
RandStainNA does not benefit the training: RandStainNA, for Ran-
dom Stain Normalization and Augmentation [49], augments the images with
more realistic stain styles and is a solution to bridge to stain augmentation
and stain normalization. We observe that this augmentation does not benefit
the training. However, our training and test data come from the same dataset
and have the same stain. Therefore, this augmentation is not necessary in
our case as we do not need to normalize the stain across multiple datasets
and staining as they did in [2], for example. This augmentation could, how-
ever, be necessary if we want to transfer knowledge from the KGH dataset to
another one (which could be H&E stained, contrary to KGH, which is HPS
stained). In this case, the training must be performed on the KGH dataset
with RandStainNA augmentation. The results will be suboptimal but more
transferrable.
MixUP & CutMix are not appropriate for pathology downstream
tasks: Cutmix [50] is an augmentation technique particularly used for image
36
classification tasks. It combines pairs of images and their labels to create new
training examples. A crop from one patch is pasted onto another patch, and
the labels are mixed according to the cut’s proportions. Similar to CutMix,
MixUp operates by blending images, but it does so by taking a linear com-
bination of pairs of images and their labels. Both methods operate on the
images and the labels. If used during the pretraining, these methods perturb
the learning of good representations. The highest test accuracy obtained
was 85.96% or 6 points below the baseline. One possible explanation is that
the transformation alters the inherent structure within individual patches.
However, in the context of pathology, the spatial arrangement of cells in
the original image holds significance due to intercellular communication and
potential interactions. Consequently, these augmentations introduce unreal-
istic data into the model. Despite concerns about realism, the demonstrated
performance improvement for classification tasks suggests potential utility.
Therefore, we propose exploring the application of these augmentations dur-
ing the second phase, where patch classification is performed with the benefit
of labelled data. Figure B.21 presents the test accuracy when these augmen-
tations are applied to the training data during the evaluation phase. We
observe that the baseline performs better. We conclude that as our patches
can contain critical, subtle features that are key to diagnosis, applying MixUp
or CutMix might obscure or distort these features, leading to misleading or
uninterpretable images that do not resemble realistic pathological conditions.
Figure B.21: Influence of CutMix and MixUp in the evaluation phase on the test accuracy
37
Appendix C. Supervised Training
To train the supervised ResNet-50 and Swin-Tiny, we performed an ab-
lation study on the main hyperparameters: batch size (BS), learning rates,
optimizer (opt), scheduler (sch) and weight decay (wd).
The pretraining
method (pr) was also evaluated. Table C.9 presents the selected hyperpa-
rameters.
Encoder
BS
opt
LR
wd
sch
pr
ResNet-50
64
Adam
0.0001
1e−5
none
ImageNet
Swin-T
64
AdamW
0.0001
1e−5
none
ImageNet
Table C.9: Main hyperparameters for the supervised training of ResNet-50 and Swin-T.
LR for Learning Rate and BS for Batch Size
Following this first study, we analyzed different data augmentation tech-
niques and their impact on the training outcome. This study was made on
a subset of pkgh-410 dataset and the ResNet-18 encoder for five epochs. In
conclusion, and to reduce the overfitting of the models on the training data,
the data augmentation techniques comprised:
• random and vertical flips with a probability of 0.5;
• invertion with a probability of 0.5;
• rotation with a probability of 0.5 and degrees=(0,90).
Then, the models were trained twice for 20 epochs and the model with
the highest validation accuracy before overfitting was retained for further
evaluation.
Appendix D. Training Curves
This section shows the training curves for Barlow Twins training, linear
evaluation and CLAM training.
Appendix D.1. Barlow Twins
We follow the settings from Table 2 to train Barlow Twins. For the first
phase, the encoders are trained on all patches (ROI, normal and non-ROI)
for 100 epochs. Training and validation losses are reported in Figure D.22.
From the training losses, we observe that the encoder is learning. Moreover,
38
the validation losses in the case of Self-Supervised Learning reflect how well
the model learns to understand the data structure through the pretext task
and therefore, its measure of the quality of the learned representations is
indirect. This is why it is more informative to evaluate the quality of the
representations on a supervised downstream task: overfitting on the pretext
task does not necessarily mean that the learnt representations are not of good
quality.
Figure D.22: Training and validation losses for basicBT, imBT, pathBT and swinBT
trained on pkgh, pkgh-800, pkgh-600 and pkgh-410. We can see that the training losses
are decreasing over time however, the validation losses present with a different behaviour
Appendix D.2. Linear Evaluation
We follow the original code to train the last linear layer. We observe
that a lower initial learning rate than 0.3 provides highly fluctuating results
and high overfitting. With an initial learning rate of 0.3, the loss fluctuates
widely but decreases and the results are reproducible, showing that the model
is learning. Figure D.23 shows the training and validation losses (at the top,
from left to right) and the validation accuracy (at the bottom) for the linear
layers on top of basicBT, imBT, pathBT and swinBT models trained on
pkgh.
39
Figure D.23: Training and validation losses (at the top from left to right) and validation
accuracy (at the bottom) for basicBT, imBT, pathBT and swinBT trained on pkgh.
40
Appendix D.3. CLAM
To train CLAM framework, we follow the original code which includes
patch extraction, features extraction and MIL training. The original code
was used for patch extraction, and therefore, artifact patches were extracted.
However, the encoders were trained on the clean patches extracted following
Section Appendix A. Additionally, the CLAM patches extraction pipeline
does not find any contour to process in some WSIs, where tissue samples are
small and torn. This is a known issue from CLAM extraction pipeline. As
a result, the number of slides used to train and test the models for the slide
evaluation is modified. Table D.10 presents the number of patches for the
different classes and split.
class
train
test
Normal
180
20
HP
123
16
SSLes
144
20
TA
142
13
TVA
192
19
TOTAL
781
88
Table D.10: Number of slides, extracted by CLAM, per class in the training and test sets
Figure D.24 shows the training and validation losses and errors as well
as the validation AUC (yellow) of the training of the MIL model using the
different pre-trained encoders. We can see that the models overfit, which is
on par with current concerns regarding MIL method [51, 52]. However, we
note that all models overfit regarding the validation AUC, but pathBT is
the one for which the AUC increases throughout the training. This result
highlights the pertinence of the novel augmentation for this model.
Appendix E. Additional results and visualization
Appendix E.1. Detailed results for patch and slide evaluation
Table E.11 and Table E.12 present the accuracy, AUC and F1-score (if
applicable) of the patch and slide classification taks. The polar graphs in
Figures 5 and 7.
41
Figure D.24: Training and validation losses and errors (in red and blue) and validation
AUC (in yellow) for CLAM framework trained on top of basicBT, imBT, pathBT and
swinBT encoders and trained on pkgh. The models all overfit but with pathBT for which
we observe an overall increase of the validation AUC throughout the training
Dataset
RN-50 sup.
Swin-T sup.
basicBT
imBT
benchBT
pathBT
swinBT
pkgh
81.85±0.05
82.67±0.75
78.77±1.87
81.32±0.6
75.86±0.09
83.43±1.54
86.15±0.33
0.893
0.9119
0.9592
0.9618
0.8478
0.9652
0.9764
100
100
.
50
100
pkgh-800
80.6±0.18
84.17±0.23
81.27±0.57
81.3±0.14
75.9±1.94
83.84±0.05
86.47±2.4
0.909
0.9161
0.9668
0.9576
0.842
0.9676
0.9822
20
100
100
.
100
100
pkgh-600
81.43±0.14
83.17±0.9
80.54±0.09
82.26±1.03
74.9±0.33
81.84±0.49
84.17±1.6
0.91
0.8519
0.9613
0.9613
0.8424
0.9599
0.9728
100
50
.
100
100
pkgh-410
80.52±0.64
80.86±
80.06±
82.57±1.27
74.03±0.37
80.4±0.8
82.37±0.65
0.8774
0.8965
0.9720
0.9649
0.8427
0.9515
0.9657
100
50
.
100
100
Table E.11: Patch top-1 accuracy and AUC of the test set for all models and for all
datasets. benchBT was trained for 200 ImageNet epochs [2]. The number of epochs of
Barlow Twins training after which the linear evaluation with the best performance was
performed is in italic. The two best results for each dataset are in bold. Best result
overall is in red.
42
Dataset
RN-50 sup.
Swin-T sup.
basicBT
imBT
benchBT
PathBT
SwinBT
pkgh
78.08
87.67
86.3
90.41
80.82
91.8
91.78
0.9647
0.9807
0.9819
0.9854
0.9552
0.9862
0.9908
76.89
87.86
86.29
90.29
78.86
91.8
91.79
pkgh-800
91.78
91.78
94.52
91.7
89.04
89.04
87.67
0.9935
0.9951
0.9894
0.9899
0.976
0.9812
0.9891
91.89
92.05
94.51
91.6
89.01
89
88.07
pkgh-600
87.67
89.04
89.04
91.8
89.04
93.15
0.9856
0.9908
0.9842
0.9896
.
0.9812
0.9932
87.26
88.91
88.65
91.73
89
93.16
pkgh-410
91.78
87.67
90.41
95.89
87.67
89.04
0.9905
0.984
0.9941
0.9966
.
0.9819
0.992
91.73
87.74
89.62
95.89
87.24
89.11
Table E.12: Slide level accuracy, AUC and F1 score (from top to bottom) of the test slides
for all models and all datasets. The two best results for each dataset are in bold. The two
best results overall are in red.
Appendix E.2. Confusion matrices
Figure E.25 presents the confusion matrices and ROC curves for the su-
pervised ResNet-50, basicBT, imBT, pathBT and swinBT. The first observa-
tion is that, even though the supervised baseline can separate the histology
from the pathology patches (high ROC curve for Normal), it does not per-
form well to dissociate between the pathologies. The benchmark model can
recognize HP and SSLe but struggles to recognize TA and TVA and does not
separate histology well from pathology. The proposed models perform very
well in recognizing histology from pathology. The swinBT obtains the best
ROC curves. We remark that they struggle to separate HP from SSLe and
TA from TVA even though imBT seems stronger in recognizing TVA from
TA (almost no TVA are predicted as TA, but TA can be predicted as TVA).
However, all models obtain better ROC curves for the classes TA/TVA than
HP/SSLe. This can be explained by the fact that TVA is TA with more
than 20% of villous architecture. Therefore, TA structures can still have a
bit of villous architecture. Additionally, SSLe is more common in the right
colon, and HP in the left [36]. By default, a growth in the right colon will
be considered SSLe until proven otherwise.
According to our expert pathologist, these results show that the models
are sharp enough to detect this ambiguity. These results are highly valu-
able as the models catch these villous architectures that pathologists should
consider.
Appendix E.3. CLAM heatmaps
To complete the analysis of Section 4.4, sile additional heatmaps are
displayed in Figure E.26. Overall, all the highest attention scores for the
SSL methods map to the ROI from our expert pathologist. However, the
43
Figure E.25: Confusion matrices and ROC curves for basicBT, imBT, pathBT and swinBT
trained on pkgh. Here Sessile Serrated Lesions are abbreviated as SSL. We observe that the
models have a very accurate ROC curve when it comes to Normal classification however,
they struggle to differentiate HP and SSL, and TA and TVA, as the confusion matrices
can emphasize.
patches with the highest attention scores are not always consistent for the
supervised methods. Additionally, pathBT and swinBT provide more ac-
curate heatmaps for the Hyperplastic Polyps (top) and Normal WSIs, we
observe that basicBT and imBT are getting distracted by the tissue overlap
regions, whereas pathBT and swinBT label them as non-relevant. pathBT
and swinBT have similar relevant regions, and basicBT and imBT share
similar diagnostically relevant regions as well.
Example citation, See ? ].
References
[1] J. Zbontar, L. Jing, I. Misra, Y. LeCun, S. Deny, Barlow twins: Self-
supervised learning via redundancy reduction (2021).
arXiv:2103.
03230.
[2] M. Kang, H. Song, S. Park, D. Yoo, S. Pereira, Benchmarking self-
supervised learning on diverse pathology datasets, in: 2023 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), 2023,
pp. 3344–3354. doi:10.1109/CVPR52729.2023.00326.
[3] M. S. Hosseini, B. E. Bejnordi, V. Q.-H. Trinh, D. Hasan, X. Li, T. Kim,
H. Zhang, T. Wu, K. Chinniah, S. Maghsoudlou, R. Zhang, S. Yang,
J. Zhu, L. Chan, S. Khaki, A. Buin, F. Chaji, A. Salehi, B. N. Nguyen,
44
Figure E.26: CLAM heatmaps for (from top to bottom) Hyperplastic Polyps, Sessile
Serrated Lesions, Tubular Adenoma and Normal WSIs
45
D. Samaras, K. N. Plataniotis, Computational pathology: A survey
review and the way forward (2024). arXiv:2304.05482.
[4] J. N. Kather, J. Calderaro, Development of ai-based pathology biomark-
ers in gastrointestinal and liver cancer, Nature Reviews Gastroenterol-
ogy & Hepatology 17 (10) (2020) 591–592.
[5] M. Caron, H. Touvron, I. Misra, H. J´egou, J. Mairal, P. Bojanowski,
A. Joulin, Emerging properties in self-supervised vision transformers
(2021). arXiv:2104.14294.
[6] B.-J. MacKinnon, N.B. urged to consider interprovincial licensing of
pathologists to deal with ’critical’ shortage, CBC News (2023).
[7] A. N. N. Wong, Z. He, K. L. Leung, C. C. K. To, C. Y. Wong, S. C. C.
Wong, J. S. Yoo, C. K. R. Chan, A. Z. Chan, M. D. Lacambra, M. H. Y.
Yeung, Current developments of artificial intelligence in digital pathol-
ogy and its future clinical applications in gastrointestinal cancers, Can-
cers (Basel) 14 (15) (2022) 3780.
[8] M. Y. Lu, D. F. Williamson, T. Y. Chen, R. J. Chen, M. Barbi-
eri, F. Mahmood, Data-efficient and weakly supervised computational
pathology on whole-slide images, Nature biomedical engineering 5 (6)
(2021) 555–570.
[9] Mhist: A minimalist histopathology image analysis dataset, https://
bmirds.github.io/MHIST/, accessed: 2024-7-18.
[10] J. N. Kather, N. Halama, A. Marx, 100,000 histological images of human
colorectal cancer and healthy tissue (May 2018).
[11] L. Hou, D. Samaras, T. M. Kurc, Y. Gao, J. E. Davis, J. H. Saltz,
Patch-based convolutional neural network for whole slide tissue image
classification, in: Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 2424–2433.
[12] T. Chen, S. Kornblith, M. Norouzi, G. Hinton, A simple framework
for contrastive learning of visual representations (2020). arXiv:2002.
05709.
46
[13] K. He, H. Fan, Y. Wu, S. Xie, R. Girshick, Momentum contrast for
unsupervised visual representation learning (2020). arXiv:1911.05722.
[14] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image
recognition, CoRR abs/1512.03385 (2015). arXiv:1512.03385.
URL http://arxiv.org/abs/1512.03385
[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, N. Houlsby, An image is worth 16x16 words: Transformers
for image recognition at scale (2021). arXiv:2010.11929.
[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, I. Polosukhin, Attention is all you need (2023). arXiv:1706.
03762.
[17] P. A. Fashi, S. Hemati, M. Babaie, R. Gonzalez, H. R. Tizhoosh, A
self-supervised contrastive learning approach for whole slide image rep-
resentation in digital pathology, J. Pathol. Inform. 13 (100133) (2022)
100133.
[18] A. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, F. Makedon, A survey
on contrastive self-supervised learning (2020). arXiv:2011.00362.
[19] W. Wu, C. Gao, J. DiPalma, S. Vosoughi, S. Hassanpour, Improving rep-
resentation learning for histopathologic images with cluster constraints
(2023). arXiv:2310.12334.
[20] R. J. Chen, C. Chen, Y. Li, T. Y. Chen, A. D. Trister, R. G. Krish-
nan, F. Mahmood, Scaling vision transformers to gigapixel images via
hierarchical self-supervised learning, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), 2022,
pp. 16144–16155.
[21] A. Nasiri-Sarvi, V. Q.-H. Trinh, H. Rivaz, M. S. Hosseini, Vim4path:
Self-supervised vision mamba for histopathology images, in: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2024, pp. 6894–6903.
[22] H. Sowrirajan, J. Yang, A. Y. Ng, P. Rajpurkar, Moco pretraining
improves representation and transferability of chest x-ray models, in:
47
M. Heinrich, Q. Dou, M. de Bruijne, J. Lellmann, A. Schl¨afer, F. Ernst
(Eds.), Proceedings of the Fourth Conference on Medical Imaging with
Deep Learning, Vol. 143 of Proceedings of Machine Learning Research,
PMLR, 2021, pp. 728–744.
URL https://proceedings.mlr.press/v143/sowrirajan21a.html
[23] S. Alfasly, A. Shafique, P. Nejat, J. Khan, A. Alsaafin, G. Alabtah,
H. Tizhoosh, Rotation-agnostic image representation learning for digital
pathology (2023). arXiv:2311.08359.
[24] D. Maleki, S. Rahnamayan, H. Tizhoosh, A self-supervised framework
for cross-modal search in histopathology archives using scale harmoniza-
tion, Scientific Reports 14 (1) (2024) 9724.
[25] G. Vray, D. Tomar, B. Bozorgtabar, J.-P. Thiran, Distill-soda: Distill-
ing self-supervised vision transformer for source-free open-set domain
adaptation in computational pathology, IEEE Transactions on Medical
Imaging (2024).
[26] G. Campanella, M. G. Hanna, L. Geneslaw, A. Miraflor, V. Werneck
Krauss Silva, K. J. Busam, E. Brogi, V. E. Reuter, D. S. Klimstra, T. J.
Fuchs, Clinical-grade computational pathology using weakly supervised
deep learning on whole slide images, Nat. Med. 25 (8) (2019) 1301–1309.
[27] M. Y. Lu, D. F. Williamson, T. Y. Chen, R. J. Chen, M. Barbi-
eri, F. Mahmood, Data-efficient and weakly supervised computational
pathology on whole-slide images, Nature Biomedical Engineering 5 (6)
(2021) 555–570.
[28] M. Ilse, J. M. Tomczak, M. Welling, Attention-based deep multiple in-
stance learning, arXiv preprint arXiv:1802.04712 (2018).
[29] Y. Schirris, E. Gavves, I. Nederlof, H. M. Horlings, J. Teuwen,
Deepsmile:
Contrastive self-supervised pre-training benefits msi and
hrd classification directly from h&e whole-slide images in colorec-
tal and breast cancer, Medical Image Analysis 79 (2022) 102464.
doi:https://doi.org/10.1016/j.media.2022.102464.
URL
https://www.sciencedirect.com/science/article/pii/
S1361841522001116
48
[30] A. Mammadov, L. L. Folgoc, J. Adam, A. Buronfosse, G. Hayem,
G. HOCQUET, P. Gori, Self-supervision revives simple multiple in-
stance classification methods in pathology, in: Medical Imaging with
Deep Learning, 2024.
URL https://openreview.net/forum?id=yxX1D556ic
[31] A. Nasiri-Sarvi, V. Q.-H. Trinh, H. Rivaz, M. S. Hosseini, Vim4path:
Self-supervised vision mamba for histopathology images, in: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops, 2024, pp. 6894–6903.
[32] J. Pocock, S. Graham, Q. D. Vu, M. Jahanifar, S. Deshpande, G. Had-
jigeorghiou, A. Shephard, R. M. S. Bashir, M. Bilal, W. Lu, D. Epstein,
F. Minhas, N. M. Rajpoot, S. E. A. Raza, TIAToolbox as an end-to-end
library for advanced tissue image analytics, Communications Medicine
2 (1) (2022) 120. doi:10.1038/s43856-022-00186-5.
URL https://www.nature.com/articles/s43856-022-00186-5
[33] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M. Welling, Rotation
equivariant CNNs for digital pathology (Jun. 2018). arXiv:1806.03962.
[34] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin
transformer: Hierarchical vision transformer using shifted windows, in:
2021 IEEE/CVF International Conference on Computer Vision (ICCV),
IEEE, 2021, pp. 9992–10002.
[35] X. Wang, S. Yang, J. Zhang, M. Wang, J. Zhang, W. Yang, J. Huang,
X. Han,
Transformer-based unsupervised contrastive learning for
histopathological image classification, Medical Image Analysis 81 (2022)
102559. doi:https://doi.org/10.1016/j.media.2022.102559.
URL
https://www.sciencedirect.com/science/article/pii/
S1361841522002043
[36] P. J. Pickhardt, B. D. Pooler, D. H. Kim, C. Hassan, K. A. Matkowskyj,
R. B. Halberg, The natural history of colorectal polyps: overview of
predictive static and dynamic features, Gastroenterology Clinics 47 (3)
(2018) 515–536.
[37] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, A. Joulin, Unsu-
pervised learning of visual features by contrasting cluster assignments,
Advances in neural information processing systems 33 (2020) 9912–9924.
49
[38] Canadian
Cancer
Society
/
Soci´et´e
canadienne
du
can-
cer,
Non-cancerous
tumours
of
the
colon
or
rectum,
https:
//cancer.ca/en/cancer-information/cancer-types/colorectal/
what-is-colorectal-cancer/non-cancerous-tumours,
accessed:
2024-1-12.
[39] T. Jewell, Hyperplastic polyp in colon or stomach: Follow-up and treat-
ment,
https://www.healthline.com/health/hyperplastic-polyp,
accessed: 2024-1-12 (Jan. 2018).
[40] Serrated
polyps,
https://my.clevelandclinic.org/health/
diseases/17462-serrated-polyps, accessed: 2024-1-12.
[41] Understanding
your
pathology
report:
Colon
polyps
(sessile
or
traditional
serrated
adenomas),
https://www.cancer.org/
cancer/diagnosis-staging/tests/biopsy-and-cytology-tests/
understanding-your-pathology-report/colon-pathology/
colon-polyps-sessile-or-traditional-serrated-adenomas.html,
accessed: 2024-1-12.
[42] M. Taherian, S. Lotfollahzadeh, P. Daneshpajouhnejad, K. Arora, Tubu-
lar Adenoma, StatPearls Publishing, 2023.
[43] J. Marks, Tubular adenoma:
Definition, treatment, outlook, and
more, https://www.healthline.com/health/tubular-adenoma, ac-
cessed: 2024-1-12 (Sep. 2017).
[44] Tubular
adenoma,
https://www.webmd.com/colorectal-cancer/
colorectal-tubular-adenoma, accessed: 2024-1-12.
[45] Colonic polyps, in: Imaging in Gastroenterology, Elsevier, 2018, pp.
276–277.
[46] M. Taherian, S. Lotfollahzadeh, P. Daneshpajouhnejad, K. Arora, Tubu-
lar adenoma (2023).
[47] D. Liu, J. Yu, Otsu method and k-means, in: 2009 Ninth International
Conference on Hybrid Intelligent Systems, Vol. 1, 2009, pp. 344–349.
doi:10.1109/HIS.2009.74.
50
[48] K. Faryna, J. Van der Laak, G. Litjens, Tailoring automated data aug-
mentation to h&e-stained histopathology, in: Medical imaging with deep
learning, 2021.
[49] Y. Shen, Y. Luo, D. Shen, J. Ke, Randstainna: Learning stain-agnostic
features from histology slides by bridging stain augmentation and nor-
malization, in: International Conference on Medical Image Computing
and Computer-Assisted Intervention, Springer, 2022, pp. 212–221.
[50] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, Y. Yoo, Cutmix: Reg-
ularization strategy to train strong classifiers with localizable features,
in: Proceedings of the IEEE/CVF international conference on computer
vision, 2019, pp. 6023–6032.
[51] M. Gadermayr, M. Tschuchnig, Multiple instance learning for digital
pathology: A review of the state-of-the-art, limitations & future poten-
tial, Computerized Medical Imaging and Graphics (2024) 102337.
[52] S. Yang, Y. Wang, H. Chen, Mambamil:
Enhancing long sequence
modeling with sequence reordering in computational pathology, arXiv
preprint arXiv:2403.06800 (2024).
51
