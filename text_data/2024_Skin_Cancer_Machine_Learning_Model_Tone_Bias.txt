SKIN CANCER MACHINE LEARNING MODEL TONE BIAS
James Pope
1, Md Hassanuzzaman
1, Mingmar Sherpa2, Omar Emara1, Ayush Joshi
1, and Nirmala Adhikari3
1University of Bristol, Intelligent Systems Laboratory, Bristol, United Kingdom
2Massachusetts Institute of Technology, Department of Biology, Cambridge, United States
3University of Alabama at Birmingham, Department of Physics, Birmingham, Alabama, United States
ABSTRACT
Background: Many open-source skin cancer image datasets are the result of clinical trials conducted
in countries with lighter skin tones. Due to this tone imbalance, machine learning models derived
from these datasets can perform well at detecting skin cancer for lighter skin tones. Though there
is less prevalence of skin cancer with darker tones, any tone bias in these models would introduce
fairness concerns and reduce public trust in the artificial intelligence health field.
Methods: We examine a subset of images from the International Skin Imaging Collaboration (ISIC)
archive that provide tone information. The subset has more light (83.3%) than dark (16.7%) images
and more benign (74.7%) than malignant (25.3%) images. These imbalances could explain a model’s
tone bias. To address this, we train models using the imbalanced dataset (3,623 images) and a
sampled balanced dataset (∼500 images) to compare against. The datasets are used to train a deep
convolutional neural network model to classify the images as malignant or benign. We then evaluate
the models’ disparate impact, based on selection rate, relative to dark or light skin tone.
Results: Using the imbalanced dataset, we found that the model is significantly better at detecting
malignant images in lighter tone (selection rate 27.5%) images versus darker tones (selection rate
15.9%). This results in a disparate impact of 0.577. Using the balanced dataset, we found that the
model is also significantly better at detecting malignant images in lighter (selection rate 50.0%)
versus darker tones (selection rate 34.2%). This results in a disparate impact of 0.684. Using the
imbalanced or balanced dataset to train the model still results in a disparate impact well below the
standard threshold of 0.80 which suggests the model is biased with respect to skin tone.
Conclusion: The results show that typical skin cancer machine learning models can be tone biased.
These results provide evidence that diagnosis or tone imbalance is not the cause of the bias. These
results provide evidence the models are learning tone related features. Other techniques will be
necessary to identify and address the bias in these models, an area of future investigation.
Keywords Machine learning bias · Skin cancer detection · Dataset imbalance · Disparate impact · Deep convolutional
neural network
1
Introduction
In the past decade numerous skin image datasets have been publicly released Cassidy et al. [2022] and used for
developing models to predict skin diseases, including skin cancer [Jain et al., 2021, Liu et al., 2020]. However, most of
the datasets are sampled from lighter skin tone populations [Wen et al., 2022]. This raises concerns about the developed
models being biased towards lighter skin tones.
Recent work on machine learning bias proposes methods to detect and mitigate model bias [Green et al., 2024]. Machine
learning models utilise relationships between the input features and the target feature to make predictions. There are
concerns when the input features include or are strongly correlated with protected features (Barocas et al. [2023]), also
known as protected characteristics. These protected features include race, age, gender, nationality, and religion. In
certain applications it is acceptable to use these features to improve model predictions, such as healthcare applications
Bonham et al. [2016].
arXiv:2410.06385v1  [eess.IV]  8 Oct 2024
Skin Cancer Machine Learning Model Tone Bias
However, in many applications using protected features to make predictions is highly undesirable and often illegal (such
as finance). Existing research regarding human diagnosis shows that healthcare workers familiar with lighter skin tones
are poor at diagnosing skin cancer in darker skin tones Mukwende et al. [2020], Barata et al. [2023]. Though tone is not
usually included as a protected feature, it can be used as a proxy. Even if this is acceptable, detecting and understanding
the source of tone bias motivates this study.
Deep convolutional neural network models (CNNs) have been shown to perform well for skin cancer diagnosis Jain
[2024]. Given a dermoscopic image of a lesion, the model is trained to predict malignant or benign. Though the models
often only use features derived from the image, there is concern the models are learning tone, or tone correlated, features.
Determining what deep learning models have learned is difficult and remains an active area of research. Additionally,
less than 2.1% of the datasets provide skin tone annotations Wen et al. [2022] (usually based on the Fitzpatrick skin
type Gupta and Sharma [2019]).
In this study, we use the publicly available International Skin Imaging Collaboration (ISIC) datasets Codella et al.
[2018]. We consider the subset of dermoscopic images that provide Fitzpatrick skin type. This derived dataset has three
times more light tone images than dark. We develop and train deep CNN models on this tone imbalanced dataset to
predict the diagnosis and find that it does have a tone bias. Believing the imbalance is the source of the bias, we train
models on tone balanced datasets and find that the bias remains. The results provide evidence that the model is learning
tone related features. The imbalanced dataset was limited to only dermoscopic images with skin tone information (3,623
of ∼81,000), greatly reducing the number of images considered. Future studies will consider using a tone classifier
to utilise all dermoscopic images and explainable AI techniques to determine what the model learned regarding tone.
Future investigation will also include exploring these approaches to potentially detect skin abnormalities related to the
presence of heavy metal contamination in the environment which present risk for neurotoxicity, reducing life expectancy.
Adhikari et al. [2024].
2
Material and Methods
In this section we first provide details about the study approach and datasets used. We consider model bias and
evaluation measures. We then present details about the deep CNN model used in the study followed by the experimental
setup.
2.1
Study design and population
The study here is retrospective design that harness the work of deep convolutional neural network model to classify the
images as malignant or benign. We used the machine learning approach where we trained the model using the images
from publicly available International Skin Imaging Collaboration (ISIC) dataset. The model is trained to perform the
specific classification of the images as malignant or benign. The images data comprises from patients globally, ensuring
a wide representation of demographics across the world. The diversity in skin images allow for generalisation of our
model across different population. Patient information such as name, age, sex and ethnicity are anonymised in the ISIC
dataset, where we ensure the research ethical standards. The images with skin tone came from the following institutes
• 1517 Hospital Italiano de Buenos Aires
• 1459 Sydney Melanoma Diagnostic Center at Royal Prince Alfred Hospital, Pascale Guitera
• 709 Memorial Sloan Kettering Cancer Center
Though this does not provide global representation, it does provide a more diverse patient population. Future work will
be to use 81,000 images that will include an even larger patient population.
2.2
Dataset Ethics
Our study uses images publicly available under Creative Commons licenses from the International Skin Imaging
Collaboration website Codella et al. [2018]. To the best of our knowledge, the images used in this study do not
have personally identifiable information and are in compliance with current US Health Insurance Portability and
Accountability Act (HIPPA) laws and EU General Data Protection Regulations.
2.3
Dataset
The International Skin Imaging Collaboration has over 20 datasets from around the world and included ∼81,155
dermoscopic images with the diagnosis of {malignant,benign}. Of this, there are 3,623 images that also have the
2
Skin Cancer Machine Learning Model Tone Bias
0
750
1,500
2,250
3,000
FST I
FST II
FST III
FST IV
FST V
FST VI
3
83
728
102
71
447
1,980
209
Benign
Malignant
Light Skin Tone
Dark Skin Tone
Fig. 1: Dataset Fitzpatrick Skin Types to Tone Mapping
Fitzpatrick Skin Type (FST). Figure 1 shows the number of instances for each skin type and delineated by diagnosis.
There are clearly far more FST II than any other skin type. Notably, there are not any images in the ISIC archives that
have FST V or FST VI. Based on the available images and annotated skin types, we decide to map FST I and FST II to
light skin tones and FST III and FST IV to dark skin tones. This decision is somewhat arbitrary. Future will explore
skin tone classification approaches using individual typology angle (ITA) Chardon et al. [1991].
Once the image skin tones have been mapped, Figure 2 shows the relative number for tone and diagnosis. There is
clearly a class imbalance regarding the diagnosis with 74.7% (2707/3623) benign and 25.3% (916/3623) malignant.
Furthermore, there is also an imbalance between tone with 83.3% light (3019/3623) and 16.7% light (604/3623).
Both imbalances are a problem. The imbalance with diagnosis, the target feature being predicted, can cause a classifier
to concentrate on this majority class and fail to learn how to classify the minority class. This is not a bias issue but still
undesirable. The skin tone imbalance can similarly result in a classifier that is better at classifying malignant lesions
for lighter tones and this potential bias is the focus of our work. There are a number of solutions to address these
imbalances, such as under-sampling the majority class, over-sampling the minority class (similar to bootstrapping), and
generative/augmentation approaches. We decide to under-sample benign images so that there are the same number of
benign and malignant images. This results in much fewer images. The resulting dataset will have 2 × the number of
malignant images, much fewer than the imbalanced dataset. We further under-sample light images to match dark tone
images. This slightly changes thee ratio for diagnosis. Figure 3 shows the result after under-sampling by diagnosis and
skin tone. Clearly the skin tone is perfectly balanced and diagnosis is much more balanced than before. However, there
are far fewer images in the balanced dataset versus the imbalanced dataset. We will use both these datasets to train
models and evaluate bias.
2.4
Evaluation Metrics
In this section we first define terms used in the confusion matrix, followed by common evalaution metrics, and finish
with a discussion of which metrics we chose.
3
Skin Cancer Machine Learning Model Tone Bias
Dark and 
Malignant
86
Dark and 
Benign
518
Light and 
Benign
2,189
Light and 
Malignant
830
Fig. 2: Imbalanced Dataset
Dark and 
Malignant
86
Dark and 
Benign
158
Light and 
Benign
116
Light and 
Malignant
128
Fig. 3: Balanced Dataset (Sampled)
2.4.1
Confusion Matrix Definition
For a given image, a binary skin cancer classification model will predict either benign or malignant and can be compared
against the true diagnosis for the image. For binary classification, the more general terms are positive and negative. We
define positive to indicate malignant and define negative to indicate benign. With these definitions, we further define
the following terms.
• true positive (TP) the classifier predicts malignant and the true diagnosis is malignant,
• true negative (TN) the classifier predicts benign and the true diagnosis is benign,
• false positive (FP) the classifier predicts malignant and the true diagnosis is benign,
• false negative (FN) the classifier predicts benign and the true diagnosis is malignant.
Given a large number of images to predict, the first step in evaluating the model is to compute number of true positives,
true negatives, false positives, false negatives. These four terms are usually presented in a confusion matrix with the
true predictions on the diagonal and the false predictions off-diagonal. This presentation is useful and we use it in our 3
section.
2.4.2
Model Evaluation Metric
The confusion matrix is used to compute several metrics that can be used to evaluate the performance of the model.
Given the confusion matrix, the accuracy is defined as follows.
4
Skin Cancer Machine Learning Model Tone Bias
Criterion
Statements
Condition
Independence
ˆY ⊥A
P{ ˆY = 1 | A = a} = P{ ˆY = 1 | A = b}
Separation
ˆY ⊥A | Y
P{ ˆY = 1 | Y = 1, A = a} = P{ ˆY = 1 | Y = 1, A = b}
P{ ˆY = 1 | Y = 0, A = a} = P{ ˆY = 1 | Y = 0, A = b}
Sufficiency
Y ⊥A | ˆY
P{Y = 1 | ˆY = 1, A = a} = P{Y = 1 | ˆY = 1, A = b}
P{Y = 1 | ˆY = 0, A = a} = P{Y = 1 | ˆY = 0, A = b}
Table 1: Model Bias Criterion
accuracy =
TP + TN
TP + TN + FP + FN
(1)
The accuracy includes all terms and measures the number of times the model is correct versus the number of predictions.
For a balanced dataset, where the number of malignant images is roughly equal to the benign images, accuracy is a useful
metric to evaluate the performance of a classifier. However, when there are significantly more benign than malignant
images in the datasets, known as a class imbalance, accuracy is not best choice. This is because a trivial majority class
classifier, that just predicts benign, can achieve a high accuracy. Other scores consider different combinations of TP,
TN, FP, and FN (such as precision, recall, and f1) and are also often used (for both balanced and imbalanced datasets).
For our purposes, we are concerned with the disparate impact (also derived from TP, TN, FP, and FN). However,
regarding accuracy, the model needs to perform better than the majority class classifier. For the imbalanced dataset, the
majority class classifier would achieve an accuracy of 74%. Any model that cannot achieve a higher accuracy would
indeed be considered poor. A model that is learning to differentiate between malignant and benign should result in an
accuracy higher than the majority class percentage.
2.4.3
Model Training
The number of epochs (i.e. how long to train the model) can greatly affect the accuracy of a model. During each epoch
the training loss is generally expected to decrease. A model is considered trained when the training loss ceases to
decrease. The model is said to have learned all it can from the dataset. The training stops using some criteria. We decide
to stop when the training loss generally stops decreasing. This stopping criteria is generally termed early stopping (this
can also combined with differences between the training and validation losses). Importantly, this approach is often used
and would be typical of a deployed model. We provide a detailed bias analysis using the trained models.
2.4.4
Evaluating Model Bias
Many statistical criteria have been proposed to determine whether a model is biased towards certain protected features.
We narrow our focus to the following three defined by Barocas, et al. Barocas et al. [2023], and detailed in Table 1.
• Independence
• Separation
• Sufficiency
Where random variable A is a protected feature, ˆY is the binary classifier, and Y is the target variable. We also represent
positive as a 1 and negative as a 0. We choose to use the Independence criterion, also known as disparate impact, as it
captures the notion of equal selection. Considering ˆY = 1 as selection, the condition requires all groups to be accepted
equally. Allowing a and b to be swapped, the equation is often relaxed to meet the following ratio where ϵ is often taken
to be 0.2 Feldman et al. [2015].
Disparate Impact = selection rate for group a
selection rate for group b = P{ ˆY = 1 | A = a}
P{ ˆY = 1 | A = b}
≥1 −ϵ
(2)
5
Skin Cancer Machine Learning Model Tone Bias
The disparate impact can be used to evaluate model bias and considers how well the model performs for one group
versus another group. It is computed as a ratio, as shown in Equation 2. We consider positive to indicate the lesion
is malignant. We define it in this manner because when the classifier fails to predict malignant it represents a lost
opportunity (e.g. further tests, examinations, treatments). Given this, the disparate impact can more intuitively be
defined for our problem as shown in Equation 3.
Tone Disparate Impact = classifier predicts cancer for dark tone
classifier predicts cancer for light tone
(3)
We use the the Tone Disparate Impact to evaluate our model for bias.
2.5
Model Bias Solutions
There are numerous proposed solutions to addressing model bias including regulatory and algorithmic. Here we only
consider algorithmic solutions to meet a criteria. The algorithmic solutions generally fall into one of three techniques.
• Preprocessing of the data
• Modify the model’s weights during training
• Postprocessing (reweighing) the model predictions
Each has known strengths and weaknesses and will be explored in future work. We provide here to facilitate our later
discussion.
2.6
Model Architecture
Many models have been proposed for image classification, notably those based using Convolutional Neural Networks
such Residual and Inception architectures He et al. [2016]. The canonical CNN model has a series of convolutional
blocks, then a flatten layer, followed by series of linear blocks, and finally the prediction / output layer Lecun et al.
[1998]. To allow more inspection of the model, we define a custom CNN architecture for training and classifying images
as either malignant or benign. We note that we do not expect this custom model to outperform current techniques but it
should outperform a majority class classifier.
Fig. 4: Skin Cancer Image Classifier Architecture / Model
The initial layer takes the raw image of width × height × features. The features are also known as channels. The input
is expected to be 244 × 244 with 3 features. As the image is transformed through each block, the number of features
increases while the image width and height reduce. Once the features are learned, they are flattened into one vector and
input to a series of regular dense neural layers that can combine the low-level features into fewer, high-level features.
The final layer combines the high-level features using a soft max function to produce a probability distribution of the
two classes. This is shown in Figure 4.
The convolutional blocks consist of a sequential convolutional layer (Conv2d), rectified linear unit activation (ReLU),
and max pooling layer (MaxPool2d). We chose the ReLU activation function as it is typical to use for image classification
architectures Krizhevsky et al. [2017]. Each time the image is passed through the max pooling layer, the image width
6
Skin Cancer Machine Learning Model Tone Bias
Skin Cancer (CNN) Model
Imbalanced
Input
Results
Conv2d
ReLU
MaxPool
2d
Conv2d
ReLU
MaxPool
2d
Linear
ReLU
Dropout
Linear
ReLU
Dropout
Conv2d
ReLU
MaxPool
2d
Flatten
Linear
LogSoft
Max
Series Convolutional Blocks
Series Linear Blocks
Output Block
In 3
Out 32
Kernel 7
In 32
Out 64
Kernel 3
In 64
Out 128
Kernel 3
In 100352
Out 512
Dropout 0.50
In 512
Out 256
Dropout 0.50
In 256
Out 2
Dark and 
Malignant
86
Dark and 
Benign
518
Light and 
Benign
2,189
Light and 
Malignant
830
Dark and 
Malignant
86
Dark and 
Benign
518
Light and 
Benign
2,189
Light and 
Malignant
830
Dark and 
Malignant
86
Dark and 
Benign
518
Light and 
Benign
2,189
Light and 
Malignant
830
Validation
Training
Forward / Back Propagation ~ Epoch
Random 
Split
Fig. 5: Imbalanced Experiment Setup
Conv2d
ReLU
MaxPool
2d
Conv2d
ReLU
MaxPool
2d
Linear
ReLU
Dropout
Linear
ReLU
Dropout
Conv2d
ReLU
MaxPool
2d
Flatten
Linear
LogSoft
Max
Series Convolutional Blocks
Series Linear Blocks
Output Block
In 3
Out 32
Kernel 7
In 32
Out 64
Kernel 3
In 64
Out 128
Kernel 3
In 100352
Out 512
Dropout 0.50
In 512
Out 256
Dropout 0.50
In 256
Out 2
Balanced
Input
Dark and 
Malignant
86
Dark and 
Benign
158
Light and 
Benign
116
Light and 
Malignant
128
Dark and 
Malignant
86
Dark and 
Benign
158
Light and 
Benign
116
Light and 
Malignant
128
Dark and 
Malignant
86
Dark and 
Benign
158
Light and 
Benign
116
Light and 
Malignant
128
Training
Validation
Results
Forward / Back Propagation ~ Epoch
Random 
Split
Skin Cancer (CNN) Model
Fig. 6: Balanced Experiment Setup
and height is reduced by two. The linear blocks consists of a sequential dense layer (Linear), ReLU activation, and
dropout layer. The dropout layer has been shown to prevent overfitting and required some percentage of disabling the
path/connection (simulating an ensemble of neural networks Srivastava et al. [2014].
2.6.1
Hyper-parameter Tuning
Deep learning architectures have a number of hyper-parameters that can have a significant effect on the model’s
prediction and computational training performance. These hyper-parameters are often selected manually using the
modeller’s intuition or, more recently, using automated search methods. The presented architecture has the following
hyper-parameters. The ranges explored are given in the braces.
• Number of convolutional blocks [2,6]
• Number of units for each convolutional layer [16,256]
• Number of linear blocks [2,6]
• Number of units for each linear layer [16,256]
• Percentage for each dropout layer [0.2,0.5]
• Learning rate step size [0.1, 0.00001]
• Optimiser {Adam, SGD, RMSProp}
Through a combination of automated (using the Optuna framework Akiba et al. [2019]) and manual hyper-parameter
tuning we found three convolutional blocks and two linear blocks provided the best results. This also informed our
selection of blocks, units, and dropout percentage. The tuning process also selected the Adam optimiser with a learning
rate of 0.00001.
2.7
Experimental Setup
Figures 5 and 5 depict the experimental setup. The given input dataset is first randomly split into a training and
validation set (2/3 and 1/3 respectively). For each epoch the training set is used in the forward pass through the model
to produce predictions. The predictions are compared against the validation set and results computed, to include the
training loss, accuracy. and selection rates. The backward pass then updates the model’s weights. This process is
repeated each epoch with the results saved to be plotted and compared.
7
Skin Cancer Machine Learning Model Tone Bias
Fig. 7: Imbalanced Dataset Training and DI Curves
The models were implemented using the PyTorch deep learning framework Paszke et al. [2019]. The experiments were
conducted on both an Apple M2 Max system on a chip (12 CPU cores, 38 GPU cores, 16 neural engine cores, 64 GB
RAM) and the Isambard-AI supercomputer (280 OpenMP cores, 916 GB RAM) ISA [20242]. The Metal Performance
Shaders (MPS) backend was used for the M2 Max. For Isambard-AI the OpenMP backend was used.
3
Results
We first present the results for a model trained using the imbalanced dataset followed by the balanced dataset results.
For each epoch a disparate impact is produced. After presenting the disparate impact for all epochs, we examine the
final epoch’s confusion matrix and computation in more detail.
3.1
Imbalanced Dataset Results
To provide some indication that the disparate impact is not just spurious, we add a control feature to compare against.
The control feature has random values and, therefore, not correlated with the diagnosis (or skin tone). We would expect
the control to have a disparate impact between 0.8 and 1.2. Anything outside (above or below) these thresholds would
indicate bias.
We trained the model for approximately 200 epochs. We compute the disparate impact for the tone and control features.
We also compute the training loss to provide an indication the model is learning. The tone and control disparate impact
are shown from 0.0 to 1.3. The training loss is also (conveniently) shown in the same range. Figure 7 shows the results
computed for each epoch. We see significant variation in the early epochs when the model is not well trained. After
about 50 epochs the control remains within the expected, unbiased range. We can clearly see that the tone disparate
impact is consistently well below the threshold. Between 150 to 200 epochs, it appears the model has stopped learning
(the training loss flattens).
To provide more context, we examine the final epoch’s results. Table 2 shows the confusion matrix for the imbalanced
model’s predictions for dark tone images. We can see that it predicts 23 positive images from the 189 in the validation
set.
8
Skin Cancer Machine Learning Model Tone Bias
True
Positive
Negative
Total
Model
Positive
15
15
30
Negative
12
147
159
Total
27
162
189
Table 2: Imbalanced Tone and Diagnosis Dataset Confusion Matrix (Epoch 200): Dark Tone Images
Table 3 shows the confusion matrix for the imbalanced model’s predictions for light tone images. The model predicts
187 positive images from the 898 in the validation set.
True
Positive
Negative
Total
Model
Positive
157
90
247
Negative
83
568
651
Total
240
658
898
Table 3: Imbalanced Tone and Diagnosis Dataset Confusion Matrix (Epoch 200): Light Tone Images
Using the confusion matrix results, Equation 4 shows that the disparate impact is 0.584. This is the Tone Disparate
Impact value shown near the last epoch of Figure 7.
Tone Disparate ImpactImbalanced = classifier predicts cancer for dark tone
classifier predicts cancer for light tone = 30/189
247/898 = 0.577
(4)
3.2
Balanced Dataset Results
We trained the balanced model for approximately 350 epochs. This was trained for more epochs than the imbalanced
case as it took more epochs before the training loss flattened (around epoch 300). Figure 7 shows the results computed
for each epoch. Similar to the imbalanced results, we see significant variation of the disparate impact in the early
epochs. After 50 epochs the control remains within the expected range but the tone remains below the 0.80 threshold,
though not as significant as with the imbalanced disparate impact.
To provide additional context, we examine the final epoch’s results. Table 4 shows the confusion matrix for the balanced
model’s predictions for dark tone images. We can see that it predicts 31 positive images from the 79 in the validation
set.
True
Positive
Negative
Total
Model
Positive
14
13
27
Negative
10
42
52
Total
24
55
79
Table 4: Balanced Tone and Diagnosis Dataset Confusion Matrix (Epoch 325): Dark Tone Images
Table 5 shows the confusion matrix for the balanced model’s predictions for light tone images. The model predicts 42
positive images from the 76 in the validation set.
True
Positive
Negative
Total
Model
Positive
32
10
42
Negative
12
22
34
Total
44
32
76
Table 5: Balanced Tone and Diagnosis Dataset Confusion Matrix (Epoch 325): Light Tone Images
Using the confusion matrix results, Equation 5 shows that the disparate impact is 0.684. This is the Tone Disparate
Impact value shown near the last epoch in Figure 8.
9
Skin Cancer Machine Learning Model Tone Bias
Fig. 8: Balanced Dataset Training and DI Curves
Model
# Accuracy
Majority Classifier Accuracy
Imbalanced Model
0.83
0.74
Balanced Model
0.72
0.55
Table 6: Model Accuracy Comparison (Final Epoch)
Tone Disparate ImpactBalanced = classifier predicts cancer for dark tone
classifier predicts cancer for light tone = 27/79
38/76 = 0.684
(5)
3.3
Model Accuracy Results
Table 6 shows the accuracy for the balanced and imbalanced models (technically the models trained using the imbalanced
and balanced datasets) computed using Equation 1. Note that the accuracy is derived from the last epoch after training
loss has stopped decreasing. We compare against the majority class classifier derived from Figures 2 and 3.
4
Discussion
4.1
Disparate Impact for Imbalanced and Balanced Models
The balanced model resulted in a disparate impact of 0.71. The imbalanced model resulted in a disparate impact of
approximately 0.58. Regardless of the relative number of malignant versus benign diagnosed images or relative number
of light versus dark tone images, the disparate impact indicates the model is better at selecting light tone images versus
dark tone images. The disparate impact for both imbalanced and balanced datasets is below the 0.80 threshold indicating
modal bias.
The results suggest that the balanced model results may have less bias than the imbalanced model. However, there is
clearly randomness in the disparate impact as the model’s are trained each epoch. We did repeat these experiments
10
Skin Cancer Machine Learning Model Tone Bias
and confirmed similar results where the disparate impact of the balanced model was less than the imbalanced model.
However, more experiments and tests are necessary to confirm this hypothesis and will be reported in future work.
4.2
Dataset Size, Class Imbalance, and Model Accuracy
We chose to address the class imbalance by under sampling benign diagnosis and then under sampling light tone images.
Of the 3,623 images, this left roughly 500 images to train and test the models. Deep learning models typically require
large amounts of data to achieve high accuracy. Our model’s accuracy was not significantly larger than the majority
class classifier, likely due to the fact this there was minimal training data.
With the 3,623 images, the model accuracy was reasonable. We are careful to note that many high accuracy models
include other features, such as exposure to the sun, location of the lesion, etc. Tschandl et al. [2018].
One solution to increase the number of images is to develop a tone classifier to label the skin tone of the remaining
78,000 images. Another, possibly more controversial, approach would be to use generative AI approaches to create
more malignant and dark images.
4.3
Model Bias Evaluation
As discussed in Section 2.4.4, there are several criterion to evaluate model bias. It can be shown that satisfying one of
the three criteria presented by Barocas, et al. Barocas et al. [2023], will violate one or both the other criterion. We
chose to evaluate the model using Independence (via the disparate impact measure). If we used one of the solutions
mentioned in Section 2.5 to meet this criteria, we would necessarily violate the Separation criteria. Unfortunately, this
means we cannot simply declare our model "free from bias" given we satisfy one of these criteria. Nevertheless, not
meeting any criteria or being completely unaware of a model’s bias is less desirable.
Balancing the dataset for tone and diagnosis is considered a data preprocessing solution to address the bias. Though this
did not work, we believe further data preprocessing techniques are important to consider. This motivates exploration of
other techniques (outlined in Section 2.5) to mitigate the model bias.
4.4
Skin Tone Definition
We somewhat arbitrary decided to delineate light and dark tone using the Fitzpatrick skin types. We note there are no V
or VI skin types in ISIC archive. There are other definitions of skin tone and future work will explore defining light and
dark using these definitions Mukwende et al. [2020].
4.5
Model Accuracy Comparison
The model accuracy results were compared against the majority class classifier to ensure the model is learning how
to differentiate the diagnosis versus just guess randomly or guessing the majority class (i.e. benign). Though both
models performed better than the majority classifier, the model trained on the balanced dataset performed relatively
better. However, these accuracy results are well below state-of-the-art Brinker et al. [2018] for more sophisticated deep
neural networks trained on much larger datasets. We believe this is due to the relatively small datasets used to train
our models. This work was limited due to the lack of skin type annotations. Future work will consider these more
sophisticated models with larger datasets derived using a tone classifier.
5
Acknowledgements
This work was initially support by a Jean Golding Institute, University of Bristol, Seed Corn Grant "Pilot Study to
determine Tone Bias in Open-Source Skin Cancer Datasets" awarded 8 November 2023. Subsequent computational
support was provided by Isambard-AI, Bristol Centre for Supercomputing ISA [20242]. The authors would like to
thank the Jean Golding Institute’s Dr. Huw Day and Will Chapman for their support throughout the project. We would
also like to thank Christopher Woods for support using the Isambard supercomputer.
References
Bill Cassidy, Connah Kendrick, Andrzej Brodzicki, Joanna Jaworek-Korjakowska, and Moi Hoon Yap. Analysis of
the isic image datasets: Usage, benchmarks and recommendations. Medical Image Analysis, 75:102305, 2022.
ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.2021.102305. URL https://www.sciencedirect.com/
science/article/pii/S1361841521003509.
11
Skin Cancer Machine Learning Model Tone Bias
Ayush Jain, David Way, Vishakha Gupta, Yi Gao, Guilherme de Oliveira Marinho, Jay Hartford, Rory Sayres, Kimberly
Kanada, Clara Eng, Kunal Nagpal, Karen B. DeSalvo, Greg S. Corrado, Lily Peng, Dale R. Webster, R. Carter
Dunn, David Coz, Susan J. Huang, Yun Liu, Peggy Bui, and Yuan Liu. Development and Assessment of an
Artificial Intelligence–Based Tool for Skin Condition Diagnosis by Primary Care Physicians and Nurse Practitioners
in Teledermatology Practices. JAMA Network Open, 4(4):e217249–e217249, 04 2021. ISSN 2574-3805. doi:
10.1001/jamanetworkopen.2021.7249. URL https://doi.org/10.1001/jamanetworkopen.2021.7249.
Yuan Liu, Ayush Jain, Clara Eng, David H. Way, Kang Lee, Peggy Bui, Kimberly Kanada, Guilherme de Oliveira Mar-
inho, Jessica Gallegos, Sara Gabriele, Vishakha Gupta, Nalini Singh, Vivek Natarajan, Rainer Hofmann-Wellenhof,
Greg S. Corrado, Lily H. Peng, Dale R. Webster, Dennis Ai, Susan J. Huang, Yun Liu, R. Carter Dunn, and David
Coz. A deep learning system for differential diagnosis of skin diseases. Nature Medicine, 26(6):900–908, Jun 2020.
ISSN 1546-170X. doi: 10.1038/s41591-020-0842-3. URL https://doi.org/10.1038/s41591-020-0842-3.
David Wen, Saad M. Khan, Antonio Ji Xu, Hussein Ibrahim, Luke Smith, Jose Caballero, Luis Zepeda, Carlos
de Blas Perez, Alastair K. Denniston, Xiaoxuan Liu, and Rubeta N. Matin. Characteristics of publicly available skin
cancer image datasets: a systematic review. The Lancet Digital Health, 4(1):e64–e74, Jan 2022. ISSN 2589-7500.
doi: 10.1016/S2589-7500(21)00252-1. URL https://doi.org/10.1016/S2589-7500(21)00252-1.
B. Lee Green, Anastasia Murphy, and Edmondo Robinson. Accelerating health disparities research with artificial
intelligence. Frontiers in Digital Health, 6, 2024. ISSN 2673-253X. doi: 10.3389/fdgth.2024.1330160. URL
https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2024.1330160.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning: Limitations and Opportunities.
MIT Press, 2023.
Vence L Bonham, Shawneequa L Callier, and Royal, Charmaine D. Will precision medicine move us beyond race? N
Engl J Med, 374(21):2003–2005, May 2016.
Malone Mukwende, Peter Tamony, and Margot Turner. Mind the Gap: A handbook of clinical signs in Black and Brown
skin. St Georges University of London, 8 2020. doi: 10.24376/rd.sgul.12769988.v1.
Catarina Barata, Veronica Rotemberg, Noel C F Codella, Philipp Tschandl, Christoph Rinner, Bengu Nisa Akay, Zoe
Apalla, Giuseppe Argenziano, Allan Halpern, Aimilios Lallas, Caterina Longo, Josep Malvehy, Susana Puig, Cliff
Rosendahl, H Peter Soyer, Iris Zalaudek, and Harald Kittler. A reinforcement learning model for AI-based decision
support in skin cancer. Nat Med, 29(8):1941–1946, July 2023.
Tanish Jain. Evaluating machine learning-based skin cancer diagnosis, 2024. URL https://arxiv.org/abs/2409.
03794.
Vishal Gupta and Vinod Kumar Sharma. Skin typing: Fitzpatrick grading and others. Clinics in Dermatology, 37
(5):430–436, 2019. ISSN 0738-081X. doi: https://doi.org/10.1016/j.clindermatol.2019.07.010. URL https:
//www.sciencedirect.com/science/article/pii/S0738081X1930121X. The Color of Skin.
Noel C. F. Codella, David Gutman, M. Emre Celebi, Brian Helba, Michael A. Marchetti, Stephen W. Dusza, Aadi
Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, and Allan Halpern. Skin lesion analysis toward
melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the
international skin imaging collaboration (isic), 2018. URL https://arxiv.org/abs/1710.05006.
Nirmala Adhikari, Dmitry Martyshkin, Vladimir Fedorov, Deblina Das, Veena Antony, and Sergey Mirov. Laser-induced
breakdown spectroscopy detection of heavy metal contamination in soil samples from north birmingham, alabama.
Applied Sciences, 14(17), 2024. ISSN 2076-3417. doi: 10.3390/app14177868. URL https://www.mdpi.com/
2076-3417/14/17/7868.
A Chardon, I Cretois, and C Hourseau. Skin colour typology and suntanning pathways. Int J Cosmet Sci, 13(4):
191–208, August 1991.
Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying
and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’15, page 259–268, New York, NY, USA, 2015. Association for Computing
Machinery. ISBN 9781450336642. doi: 10.1145/2783258.2783311. URL https://doi.org/10.1145/2783258.
2783311.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. doi: 10.1109/CVPR.2016.90.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings
of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791.
12
Skin Cancer Machine Learning Model Tone Bias
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural
networks. Commun. ACM, 60(6):84–90, May 2017. ISSN 0001-0782. doi: 10.1145/3065386. URL https:
//doi.org/10.1145/3065386.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple
way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):1929–1958, 2014.
URL http://jmlr.org/papers/v15/srivastava14a.html.
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation
hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD ’19, page 2623–2631, New York, NY, USA, 2019. Association for
Computing Machinery. ISBN 9781450362016. doi: 10.1145/3292500.3330701. URL https://doi.org/10.
1145/3292500.3330701.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
imperative style, high-performance deep learning library, 2019. URL https://arxiv.org/abs/1912.01703.
University
of
Bristol
to
host
UKs
most
powerful
supercomputer
to
advance
AI
discov-
ery
|
National
Composites
Centre
—
nccuk.com.
https://www.nccuk.com/news/
university-of-bristol-to-host-uks-most-powerful-supercomputer-to-advance-ai-discovery/,
20242. [Accessed 29-09-2024].
Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The HAM10000 dataset, a large collection of multi-source
dermatoscopic images of common pigmented skin lesions. Scientific Data, 5(1):180161, August 2018.
Titus Josef Brinker, Achim Hekler, Jochen Sven Utikal, Niels Grabe, Dirk Schadendorf, Joachim Klode, Carola Berking,
Theresa Steeb, Alexander H Enk, and Christof von Kalle. Skin cancer classification using convolutional neural
networks: Systematic review. J Med Internet Res, 20(10):e11936, October 2018.
13
