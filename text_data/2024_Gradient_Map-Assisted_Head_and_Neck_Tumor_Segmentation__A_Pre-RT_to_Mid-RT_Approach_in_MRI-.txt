Gradient Map-Assisted Head and Neck Tumor
Segmentation: A Pre-RT to Mid-RT Approach in
MRI-Guided Radiotherapy
Jintao Ren1,2[0000−0002−1558−7196], Kim Hochreuter1,2[0009−0000−7347−1688],
Mathis Ersted Rasmussen1,2[0000−0002−7853−3531], Jesper Folsted
Kallehauge1,2[0000−0003−3705−5390], and Stine Sofia
Korreman⋆1,2,3[0000−0002−3523−382X]
1 Aarhus University, Department of Clinical Medicine, Nordre Palle Juul-Jensens
Blvd. 11, 8200 Aarhus, Denmark
2 Aarhus University Hospital, Danish Centre for Particle Therapy, Palle Juul-Jensens
Blvd. 25, 8200 Aarhus, Denmark
3 Aarhus University, Department of Oncology, Palle Juul-Jensens Blvd. 35, 8200
Aarhus, Denmark
stine.korreman@clin.au.dk
Abstract. Radiation therapy (RT) is a vital part of treatment for head
and neck cancer, where accurate segmentation of gross tumor volume
(GTV) is essential for effective treatment planning. This study investi-
gates the use of pre-RT tumor regions and local gradient maps to en-
hance mid-RT tumor segmentation for head and neck cancer in MRI-
guided adaptive radiotherapy. By leveraging pre-RT images and their
segmentations as prior knowledge, we address the challenge of tumor lo-
calization in mid-RT segmentation. A gradient map of the tumor region
from the pre-RT image is computed and applied to mid-RT images to
improve tumor boundary delineation. Our approach demonstrated im-
proved segmentation accuracy for both primary GTV (GTVp) and nodal
GTV (GTVn), though performance was limited by data constraints. The
final DSCagg scores from the challenge’s test set evaluation were 0.534
for GTVp, 0.867 for GTVn, and a mean score of 0.70. This method shows
potential for enhancing segmentation and treatment planning in adaptive
radiotherapy. Team: DCPT-Stine’s group.
Keywords: Deep learning · Prior knowledge · Tumor Segmentation ·
Head and Neck Cancer · MRI
1
Introduction
Radiation therapy (RT) is a key treatment modality for head and neck cancer
(HNC), but tumor delineation for RT planning remains a major challenge. Tra-
ditional RT planning relies heavily on manual delineation of tumor volumes by
⋆Corresponding author: stine.korreman@clin.au.dk
arXiv:2410.12941v1  [cs.CV]  16 Oct 2024
2
J. Ren et al.
clinicians, a process that is both time-consuming and prone to high inter-observer
variability (IOV), particularly in HNC where complex anatomical structures and
critical organs at risk (OARs) lie in close proximity to the tumor [28,17,23]. MRI-
guided RT has emerged as a promising approach offering superior soft tissue con-
trast and the advantage of avoiding additional ionizing radiation during imaging.
Further, MRI-guided adaptive RT holds significant potential to improve clinical
outcomes by maximizing tumor control while minimizing side effects [3,15,14].
In addition, the use of multimodality images is also recommended [9].
With the advancements in imaging techniques and artificial intelligence (AI),
recent research has increasingly focused on automating the segmentation of HNC
tumors and OARs using deep learning methods [13,19]. These AI-driven ap-
proaches aim to overcome the limitations of manual segmentation by providing
faster and potentially more consistent delineations [5,6]. However, due to the lack
of gold-standard imaging, these applications face a common challenge of uncer-
tainty. Specifically, for HNC, tumors often have ambiguous borders, particularly
in the primary tumor volume (GTVp), where the lack of clear distinction be-
tween healthy and malignant tissues further complicates tumor delineation for
both human annotators and AI models. Strong IOV is particularly problematic
for GTVp, where inconsistent manual annotations can significantly impact the
training of deep learning models [4].
To further improve tumor segmentation performance for head and neck can-
cers, previous studies have explored methods that incorporate prior segmenta-
tions or prompts (e.g. bounding boxes, scribbles and clicks) to refine subsequent
segmentation tasks. For example, Outeiral et al. applied bounding box cropping
methods, which led to an increase in DSC for MRI images [18]. Ren et al. found
that adding a bounding box as an additional channel improved nnUNet perfor-
mance, raising GTVp/lymph node (GTVn) DSC from 0.68/0.63 to 0.88/0.89 on
multimodal data (CT, PET, MRI) [21]. Wang et al. used a RetinaNet model to
narrow segmentation fields on CT and PET scans, improving precision [33]. Wei
et al. [34] demonstrated that incorporating minimal training steps after human
interactions raised GTV accuracy from a DSC of 0.65 to 0.82. Interactions such
as single or multiple clicks within the GTV have also demonstrated significant
improvements in segmentation accuracy [20,25].
Building upon these advancements, this study addresses the challenges of
MRI-guided adaptive RT for HNC, focusing on the second task of the HNTS-
MRG 2024 challenge [29]. This task involves segmenting GTVp and GTVn using
pre-RT and mid-RT T2-weighted (T2w) MRI images, with mid-RT images taken
after RT treatment. The main challenge is accurately identifying all malignant
tumor regions and determining the correct contours, complicated by similar soft
tissue contrasts and the lack of definitive ground truth [24]. Tumor shrinkage or
disappearance in mid-RT images further complicates boundary delineation.
To overcome these challenges, pre-RT images and their corresponding delin-
eations can serve as valuable prior knowledge for assisting in the segmentation
of tumors on mid-RT images. However, the optimal way to leverage this pre-RT
information for mid-RT segmentation remains an open question [32].
Gradient Map-Assisted Mid-RT HNC Tumor Segmentation
3
Despite the temporal link, pre-RT and mid-RT images often differ in intensity,
shape, and texture, presenting a challenge for consistent segmentation. These
inter- and intra-patient variations may, however, enrich deep learning models by
providing diverse training examples for better generalization.
In this study, we present a novel approach that utilizes pre-RT tumor delin-
eations to improve mid-RT segmentation. We first use the deformably registered
pre-RT tumor delineations to identify bounding boxes, defining Regions of Inter-
est (ROIs) around the tumors. These ROIs are then employed to compute gradi-
ent maps on the mid-RT T2w images, which serve as additional input channels.
Additionally, we generate gradient maps from the original pre-RT images and
their ground truth (GT) delineations, incorporating them as extra training data.
This approach aims to leverage both pre-RT and mid-RT information, thereby
enhancing segmentation accuracy during the mid-RT phase.
2
Material and Methods
2.1
Data
The dataset used in this study was provided by the organizer of the HNTS-MRG
2024 challenge task2 which were 150 HNC patients, predominantly oropharyn-
geal cancer (OPC). Imaging provided for each patient was T2w fat saturated
anatomical sequences of the head and neck region taken at MD Anderson Cancer
Center [29]. Images include pre-RT (1-3 weeks before the start of RT) and mid-
RT (2-4 weeks intra-RT) scans. Multiple physician expert observers (n = 3 to 4)
have independently segmented GTVp and GTVn structures for all cases (pre-RT
and mid-RT) based on MRI images provided in the challenge. The ground truth
was obtained via the Simultaneous Truth And Performance Level Estimation al-
gorithm (STAPLE). Pre-RT images and delineations were deformably registered
(DR) to the mid-RT images (DR pre-RT).
2.2
Incorporating pre-RT Tumor Location with Gradient Maps for
mid-RT
We consider pre-RT images and their delineations as valuable prior knowledge for
segmenting tumors on mid-RT images, especially for identifying tumor locations.
To leverage this information, we performed connected component analysis on the
GTV segmentation masks from DR pre-RT images to identify individual tumor
instances (GTVp and GTVn). For each instance, 3D coordinates of the tumor
boundaries were extracted, and a bounding box was created around the tumor,
with random perturbations of 2-6 voxels in the x, y, and z directions to account
for spatial variations. Next, we computed the gradient magnitude (to construct
a gradient map) from mid-RT T2w MRI images within these bounding boxes to
capture intensity changes around tumor boundaries. The preprocessing involved
normalizing the T2w MRI based on its full intensity range and extracting the
regions defined by the pre-RT mask. A gradient map was then generated using a
4
J. Ren et al.
Gaussian filter (sigma = 1) to highlight intensity changes. Gradient values were
normalized to the range 0-1, with any values greater than 1 clipped to maintain
consistency across all patients.
Although pre-RT and mid-RT images are temporally related, their higher-
level features, such as tumor morphology and intensity patterns, can differ sig-
nificantly, likely due to treatment effects or natural variability. To account for
this, we treated the pre-RT and mid-RT images of the same patient as indepen-
dent data points, effectively doubling the dataset size from 150 to 300 samples.
Gradient maps were computed for both pre-RT and mid-RT images using the
similar process. This approach allowed the model to learn from a broader range
of spatial and intensity variations by expanding the training set.
At test time, the bounding boxes of tumors from DR pre-RT were used
to construct the gradient maps of mid-RT T2w MR images. Therefore, both
mid-RT T2w and the gradient maps were treated as two-channel inputs for the
network. The overall process is outlined in detail in the flowchart, as illustrated
in Figure 1.
Ref
Move
DR Pre-RT T2w
Mid-RT T2w
Tumor bounding boxes
Mid-RT Gradient Map
Deformable 
Registration
Pre-RT T2w
Mid-RT T2w & Gradient Map
Pre-RT Gradient Map
Train time
Mid-RT T2w & Gradient Map
Test time
DR Pre-RT T2w
Pre-RT T2w & Gradient Map
DR Pre-RT T2w
Fig. 1. Workflow for incorporating pre-RT tumor location and gradient maps in mid-
RT segmentation.
2.3
Deep learning configurations
We utilized the nnUNet framework [7] to implement and train the model, em-
ploying the nnUNetResEncM planner to design the network architecture[8]. The
Gradient Map-Assisted Mid-RT HNC Tumor Segmentation
5
model was based on a Residual Encoder U-Net with six stages. Each stage con-
tained a specified number of convolutional blocks: Stage 1 had 1 block, Stage 2
had 3 blocks, Stage 3 had 4 blocks, and Stages 4 through 6 each had 6 blocks.
The Conv3D layers employed a kernel size of 1x3x3 for the first layer, followed
by 3x3x3 kernels for the subsequent layers. Strides were set to 1x1x1 in the
first stage and either 1x2x2 or 2x2x2 in the later stages to reduce dimensional-
ity. InstanceNorm3D was used for normalization, and LeakyReLU served as the
activation function.
The training process used a batch size of 8 with a patch size of 48x192x192
voxels. Both the input T2w images and gradient maps were normalized using
Z-Score normalization for intensity standardization. Cubic interpolation (order
3) was applied for data resampling, while first-order interpolation was used for
segmentation masks. The median image size was 123x512x511 voxels, with voxel
spacing set at 1.2x0.5x0.5 mm. Training was performed using the SGD optimizer
with a PolyLR scheduler with exponent=0.9, starting with a learning rate of 0.01.
The loss function was a combination of cross-entropy and dice loss.
The 150 patients were randomly divided into 5 folds, with each fold con-
taining 240 images/delineations (comprising pre-RT and mid-RT images for 120
patients) for training, while 30 mid-RT images/delineations for validation. The
maximum number of epochs was set to 1000 for each model, and the final models
from the last epoch were saved for prediction. An ensemble of all models trained
across the five folds was used to generate predictions on the test set for the fi-
nal challenge submission. In accordance with guidelines for reproducibility and
verification [6], all source code and trained weights for gradient map generation
and deep learning have been made publicly available on GitHub†.
2.4
Evaluation Metrics
The HNTS-MRG challenge utilizes the mean aggregated Dice Similarity Coef-
ficient (DSCagg) as the primary evaluation metric for ranking [1]. In addition,
we also used the conventional Dice Similarity Coefficient (DSC) and the 95th
percentile Hausdorff Distance (HD95) and mean surface distance (MSD) as sup-
plementary metrics to assess the segmentation performance for both GTVp and
GTVn.
2.5
System Environment
The experiments were performed on a system featuring dual AMD Ryzen Thread-
ripper 3990X processors with 64 cores (128 threads) and 256GB of RAM. Train-
ing was conducted using an NVIDIA RTX A6000 GPU with 48GB of VRAM.
The software setup consisted of Python 3.12.4, PyTorch 2.4.0, CUDA 12.6 and
nnU-Net 2.5.1, while distance metrics were computed using MedPy 0.5.2.
† https://github.com/Aarhus-RadOnc-AI/GradientSegHNTS
6
J. Ren et al.
3
Results
3.1
5-Fold cross-validation results
We evaluated our approach using 5-fold cross-validation on the training data
(n=150). For each fold, average DSCagg, HD95, and MSD scores were calculated
for both GTVp and GTVn, as shown in Table 1 and Table 2. The results indicated
variability in GTVp segmentation accuracy, with DSCagg ranging from 0.469 to
0.697, while GTVn exhibited more consistent performance, with DSCagg ranging
from 0.786 to 0.871. The HD95 scores ranged from 9.3 to 15.6 mm for GTVp
and 4.2 to 7.4 mm for GTVn, while MSD scores varied between 2.5 to 5.4 mm
for GTVp and 1.0 to 1.8 mm for GTVn.
Table 1. GTVp performance on 5-Fold cross-validation
Metric
Fold 0 Fold 1 Fold 2 Fold 3 Fold 4 Average
DSCagg
0.682
0.493
0.469
0.636
0.697
0.595
HD95 [mm]
9.9
15.6
14.6
9.3
12.3
12.3
MSD [mm]
3.6
5.4
4.2
2.5
3.9
3.9
Table 2. GTVn performance on 5-Fold cross-validation
Metric
Fold 0 Fold 1 Fold 2 Fold 3 Fold 4 Average
DSCagg
0.871
0.786
0.868
0.859
0.827
0.842
HD95 [mm]
4.2
5.4
6.4
7.4
5.9
5.9
MSD [mm]
1.0
1.4
1.8
1.6
1.5
1.5
3.2
Comparison between with and without gradient map
We further compared the performance of using T2w images with (w/) and with-
out (w/o) gradient maps on fold-0 of the validation set (n=30). For GTVp, the
mean DSC improved from 0.355 to 0.538 (p < 0.005), and for GTVn, it increased
from 0.688 to 0.825 (p < 0.001), based on Wilcoxon signed-rank tests. In Fig-
ure 2, the violin plots illustrate this comparison, showing a clear shift toward
higher DSC values for both GTVp and GTVn when the gradient map is applied.
The distributions highlight the overall improvement in segmentation accuracy
when incorporating gradient maps.
Figure 3 presents two patient cases. In patient a, the use of the gradient
map successfully segmented a previously missed GTVn on the T2w-only image,
improving the DSC from 0.0 to 0.83. For patient b, both GTVp and GTVn
segmentations improved, with DSC scores increasing from 0.14 to 0.80 and 0.73
to 0.87, respectively. However, part of the GTVp was missed as it extended
beyond the bounding box range defined by the gradient map.
Gradient Map-Assisted Mid-RT HNC Tumor Segmentation
7
w/o Gradient Map
w/ Gradient Map
0.0
0.2
0.4
0.6
0.8
1.0
Dice Similarity Coefficient (DSC)
p = 0.001920
Improvement in GTVp using Gradient Map
w/o Gradient Map
w/ Gradient Map
0.0
0.2
0.4
0.6
0.8
1.0
Dice Similarity Coefficient (DSC)
p = 0.000236
Improvement in GTVn using Gradient Map
Fig. 2. Comparison of segmentation performance with (w/) and without (w/o) gradient
maps on fold-0 of the validation set (n=30).
(a)
T2w
Gradient Map
T2w + GT (2D)
Gradient Map + GT (2D)
GT (3D)
w/o Gradient (3D)
w/ Gradient (3D)
T2w
Gradient Map
T2w + GT (2D)
(b)
GTVp DSC 0.88
GTVn DSC 0.0
GTVp DSC 0.86
GTVn DSC 0.83
GTVp DSC 0.14
GTVn DSC 0.73
GTVp DSC 0.80
GTVn DSC 0.87
w/ Gradient (2D)
w/ Gradient (2D)
GT (3D)
w/o Gradient (3D)
w/ Gradient (3D)
Gradient Map + GT (2D)
Fig. 3. Example cases demonstrating the impact of using gradient maps. Patient a:
The gradient map enabled accurate segmentation of a previously undetected GTVn,
improving the DSC from 0.0 to 0.83. Patient b: The use of the gradient map increased
the DSC for both GTVp (from 0.14 to 0.80) and GTVn (from 0.73 to 0.87), although
part of the GTVp was missed as it extended beyond the gradient map’s bounding box.
8
J. Ren et al.
3.3
Correlation between tumor volume change and DSC scores
(a)
(b)
Fig. 4. Relationship between tumor volume change and DSC scores for GTVp and
GTVn. (a) The first row shows three subplots representing different bins of mid-RT
GTVp volumes. Each subplot’s x-axis represents the change in volume (pre-RT volume
minus mid-RT volume), and the y-axis represents the predicted DSC score. The size of
the dots corresponds to the mid-RT GTV volume, while red X marks indicate instances
where the current mid-RT volume is 0. (b) The second row displays similar subplots for
GTVn, illustrating the relationship between volume change and segmentation accuracy.
As shown in Figure 4, cross-validation results (n=150) revealed no strong
correlation between tumor volume change and DSC scores for either GTVp or
GTVn. Spearman’s correlation coefficients were used to measure these relation-
ships. We observed numerous instances where significant tumor shrinkage re-
sulted in false predictions, particularly for GTVp, with DSC scores of 0.0. Mod-
erate negative correlations were identified in specific volume ranges; for GTVp, a
mid-RT volume bin between 0.8 and 3.0 cubic centimeters (cc) showed a Spear-
man’s correlation coefficient of -0.41. Similarly, for GTVn, a volume bin ranging
from 2.567 to 8.5 cc had a coefficient of -0.34.
Gradient Map-Assisted Mid-RT HNC Tumor Segmentation
9
3.4
Final test score
After completing training and validation, we submitted our trained model’s
Docker container to the challenge platform. Predictions were generated using an
ensemble of all five models trained across the 5-folds. The final DSCagg scores
on the test set, as evaluated by the challenge, were 0.534 for GTVp, 0.867 for
GTVn, with a mean score of 0.70.
4
Discussion
In this study, we integrated pre-RT tumor location and gradient maps to enhance
tumor segmentation in mid-RT images for head and neck cancer. Results from
the 5-fold cross-validation show that leveraging pre-RT information improved
the delineation of both GTVp and GTVn. The average performance across folds,
measured by DSCagg, HD95, and MSD, demonstrated consistent and reliable
segmentation. The comparison of T2w without gradient map versus T2w with
gradient map revealed that gradient maps provided more precise boundary local-
ization. These findings highlight the potential of combining spatial and localized
gradient intensity information to enhance segmentation accuracy in MRI-guided
adaptive radiotherapy.
Deep learning-based AI approaches have garnered significant interest in en-
hancing RT patient treatment, with notable progress made in HNC tumor auto-
segmentation [5]. Much of this advancement has been driven by MICCAI public
data challenges, such as the HECKTOR challenges [1,2]. Numerous studies have
participated and contributed diverse approaches to the challenge. Notably, Xie
and Peng [35] achieved a DSC of 0.778 using a 3D SE UNet integrated with the
nnUNet pipeline. Similarly, Naser et al. [16] employed a Resnet U-Net, achieving
a mean Dice score of 0.69 and further validating the model on MRI, where the
mean DSC reached 0.73 ± 0.12 for T2w+ T1w imaging [30].
Despite these advancements, there remains a scarcity of studies or publicly
available datasets that leverage the temporal dependency between pre-RT and
mid-RT images for head and neck tumor segmentation, especially using MRI
data. The HNTS-MRG 2024 Challenge helps address this gap. A primary dif-
ficulty in this challenge is accurately identifying all malignant regions in the
mid-RT MRI, where using pre-RT ground truth delineations as prior informa-
tion can help overcome this difficulty. As demonstrated in our study, even a rough
propagation of pre-RT information, such as incorporating a bounding box, led to
significant improvements in segmentation accuracy. Specifically, the mean DSC
for GTVp improved from 0.355 to 0.538 (p < 0.005), and for GTVn, it increased
from 0.688 to 0.825 (p < 0.001). These results highlight the potential of uti-
lizing pre-RT data; however, accurately tracking tumor evolution and precisely
propagating contours from pre-RT to mid-RT images remains a challenging task,
suggesting that more refined methods are needed to fully leverage this temporal
information.
In addition to leveraging pre-RT information, our findings indicate that other
factors may influence segmentation accuracy. Specifically, in cases where the tu-
10
J. Ren et al.
mor was absent after treatment (mid-RT GTVp volume = 0), the model failed
to predict this absence, resulting in multiple cases of DSC=0.0. This limitation
may be exacerbated by the presence of gradient maps in the original tumor re-
gion, contributing to false-positive predictions. Furthermore, our results revealed
moderate negative correlations in certain volume bins, indicating that as tumors
shrink, particularly in the mid-RT moderate volume range GTVp (0.8-3 cc) and
GTVn (2.567-8.5 cc), segmentation accuracy tends to decline. This decrease is
likely due to the greater complexity in detecting and delineating smaller tumors
during the mid-RT phase, where reduced volume leads to greater boundary am-
biguity.
Deep learning-based tumor segmentation is inherently data-intensive, and the
limited availability of annotated datasets presents a significant challenge. This is-
sue is particularly pronounced in head and neck cancer for mid-RT, where tumor
volumes diminish significantly during treatment. Although the dataset consists
of 150 training cases, this may be insufficient for advanced techniques like defor-
mation or image propagation networks, which require large datasets to optimize
both image alignment and segmentation objectives [11]. Furthermore, variability
in patient data distributions complicates training, reducing the model’s ability
to generalize across diverse cases.
Our approach aimed to mitigate these challenges by simplifying the problem
through a data manipulation approach. Instead of propagating the full image
evolution from pre-RT to mid-RT, which would require more complex modeling
and a larger dataset, we focused on providing rough tumor localization through
the use of bounding boxes derived from pre-RT images. By narrowing the region
of interest, we reduced the complexity of the segmentation task. Moreover, we
calculated the gradient of the mid-RT image within the bounding box to sim-
plify the training process, allowing more detailed boundary information to be
incorporated alongside the location data fed into the network.
A limitation of our approach is the use of a bounding box with an arbitrary
expansion of 2-6 voxels around the deformable registered pre-RT image, which
may not always align well with the mid-RT image. This misalignment becomes
more critical if there are significant registration errors or substantial anatomical
changes in the patient during the course of treatment. The arbitrary choice of
expanding the bounding box by 2-6 voxels in all three dimensions carries inherent
risks. If the expansion is too small, there is a risk of missing part of the actual
tumor in the mid-RT image, as shown on Figure 3(b). On the other hand, if
the expansion is too large, it may reduce the precision of tumor localization,
as the box would include more irrelevant tissue, thereby diluting the intended
localization benefit.
In routine clinical practice, human involvement is often required to refine
such critical tumor definition tasks. Clinicians may define tumor locations in-
teractively, using tools such as scribbles, clicks, or bounding boxes, to provide
more accurate localization cues. Studies have demonstrated that integrating such
human-in-the-loop methods can significantly enhance detection and segmenta-
tion accuracy [25,21,34]. This approach is particularly useful when automated
Gradient Map-Assisted Mid-RT HNC Tumor Segmentation
11
registration struggles with large anatomical changes or complex tumor morpholo-
gies, making precise localization difficult without manual input.
Our results demonstrate a significant difference in segmentation accuracy
between GTVp and GTVn, with DSCagg scores of 0.534 for GTVp and 0.867
for GTVn. The validation performance for GTVp ranged widely (0.47 to 0.69),
whereas GTVn showed more consistency (0.79 to 0.87). This disparity is partly
because GTVp often diminishes after treatment, making it barely visible on mid-
RT images but still indicated by the gradient map. Additionally, the inherent
ambiguity in defining GTVp tumor contours contributes to this difference. The
challenge organizers used a STAPLE consensus from multiple annotators (3-4)
as the ground truth, but due to low contrast at GTVp boundaries, there may
exists considerable IOV, resulting in less agreement and higher uncertainty in
GTVp segmentations [26].
To address this issue, the ground truth delineations should strictly adhere
to established guidelines [10]. However, GTVp is often overestimated, leading
to a high false positive rate compared to histology, highlighting flaws in delin-
eations and the need for improved imaging techniques [27]. From a deep learning
developer’s view, an alternative approach could involve developing a probabilis-
tic model based on multiple annotators’ input for GTVp, rather than using a
consensus method like STAPLE or averaging. Such probabilistic models can sig-
nificantly improve uncertainty modeling and provide better confident calibration
scores, leading to more reliable segmentation maps [22,12,31].
5
Conclusion
In conclusion, our novel approach effectively leverages pre-RT information to
enhance mid-RT segmentation accuracy by incorporating gradient maps derived
from pre-RT tumor delineations. Our results show that utilizing pre-RT delin-
eations improves the model’s ability to delineate both GTVp and GTVn for mid-
RT, with significant gains in segmentation accuracy. However, the improvements
for GTVp are less pronounced, likely due to reduced tumor volume, inherent am-
biguity, and variability in tumor contours. We suggest that incorporating a semi-
automatic, human-in-the-loop approach could help mitigate false predictions for
GTVp, particularly when tumor boundaries are unclear in imaging. Future work
could focus on integrating probabilistic models based on multiple annotators to
better address this uncertainty. Overall, our approach demonstrates potential
for enhancing segmentation accuracy in MRI-guided adaptive radiotherapy.
References
1. Andrearczyk, V., Oreiller, V., Boughdad, S., Rest, C.C.L., Elhalawani, H., Jreige,
M., Prior, J.O., Vallières, M., Visvikis, D., Hatt, M., et al.: Overview of the heck-
tor challenge at miccai 2021: automatic head and neck tumor segmentation and
outcome prediction in pet/ct images. In: 3D head and neck tumor segmentation in
PET/CT challenge, pp. 1–37. Springer (2021)
12
J. Ren et al.
2. Andrearczyk, V., et al.: Overview of the hecktor challenge at miccai 2022: Auto-
matic head and neck tumor segmentation and outcome prediction in pet/ct. In:
Andrearczyk, V., Oreiller, V., Hatt, M., Depeursinge, A. (eds.) Head and Neck
Tumor Segmentation and Outcome Prediction. HECKTOR 2022, Lecture Notes
in Computer Science, vol. 13626. Springer, Cham (2023). https://doi.org/10.
1007/978-3-031-27420-6_1, https://doi.org/10.1007/978-3-031-27420-6_1
3. Benitez, C.M., Chuong, M.D., Künzel, L.A., Thorwarth, D.: Mri-guided adaptive
radiation therapy. In: Seminars in Radiation Oncology. vol. 34, pp. 84–91. Elsevier
(2024)
4. Henderson, E.G., Osorio, E.M.V., van Herk, M., Brouwer, C.L., Steenbakkers, R.J.,
Green, A.F.: Accurate segmentation of head and neck radiotherapy ct scans with
3d cnns: Consistency is key. Physics in Medicine & Biology 68(8), 085003 (2023)
5. Hindocha, S., Zucker, K., Jena, R., Banfill, K., Mackay, K., Price, G., Pudney,
D., Wang, J., Taylor, A.: Artificial intelligence for radiotherapy auto-contouring:
Current use, perceptions of and barriers to implementation. Clinical Oncology
35(4), 219–226 (2023)
6. Hurkmans, C., Bibault, J.E., Brock, K.K., van Elmpt, W., Feng, M., Fuller, C.D.,
Jereczek-Fossa, B.A., Korreman, S., Landry, G., Madesta, F., et al.: A joint estro
and aapm guideline for development, clinical validation and reporting of artificial
intelligence models in radiation therapy. Radiotherapy and Oncology 197, 110345
(2024)
7. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a
self-configuring method for deep learning-based biomedical image segmentation.
Nature methods 18(2), 203–211 (2021)
8. Isensee, F., Wald, T., Ulrich, C., Baumgartner, M., Roy, S., Maier-Hein, K., Jaeger,
P.F.: nnu-net revisited: A call for rigorous validation in 3d medical image segmen-
tation. arXiv preprint arXiv:2404.09556 (2024)
9. Jensen, K., Al-Farra, G., Dejanovic, D., Eriksen, J.G., Loft, A., Hansen, C.R.,
Pameijer, F.A., Zukauskaite, R., Grau, C.: Imaging for target delineation in head
and neck cancer radiotherapy. In: Seminars in nuclear medicine. vol. 51, pp. 59–67.
Elsevier (2021)
10. Jensen, K., Friborg, J., Hansen, C.R., Samsøe, E., Johansen, J., Andersen, M.,
Smulders, B., Andersen, E., Nielsen, M.S., Eriksen, J.G., et al.: The danish head
and neck cancer group (dahanca) 2020 radiotherapy guidelines. Radiotherapy and
Oncology 151, 149–151 (2020)
11. Kawula, M., Vagni, M., Cusumano, D., Boldrini, L., Placidi, L., Corradini, S.,
Belka, C., Landry, G., Kurz, C.: Prior knowledge based deep learning auto-
segmentation in magnetic resonance imaging-guided radiotherapy of prostate can-
cer. Physics and Imaging in Radiation Oncology 28, 100498 (2023)
12. Kohl, S.A., Romera-Paredes, B., Maier-Hein, K.H., Rezende, D.J., Eslami, S.,
Kohli, P., Zisserman, A., Ronneberger, O.: A hierarchical probabilistic u-net for
modeling multi-scale ambiguities. arXiv preprint arXiv:1905.13077 (2019)
13. McDonald, B.A., Cardenas, C.E., O’Connell, N., Ahmed, S., Naser, M.A., Wahid,
K.A., Xu, J., Thill, D., Zuhour, R.J., Mesko, S., et al.: Investigation of autoseg-
mentation techniques on t2-weighted mri for off-line dose reconstruction in mr-linac
workflow for head and neck cancers. Medical physics 51(1), 278–291 (2024)
14. McDonald, B.A., Dal Bello, R., Fuller, C.D., Balermpas, P.: The use of mr-guided
radiation therapy for head and neck cancer and recommended reporting guidance.
In: Seminars in radiation oncology. vol. 34, pp. 69–83. Elsevier (2024)
Gradient Map-Assisted Mid-RT HNC Tumor Segmentation
13
15. Mohamed, A.S., Bahig, H., Aristophanous, M., Blanchard, P., Kamal, M., Ding,
Y., Cardenas, C.E., Brock, K.K., Lai, S.Y., Hutcheson, K.A., et al.: Prospective in
silico study of the feasibility and dosimetric advantages of mri-guided dose adap-
tation for human papillomavirus positive oropharyngeal cancer patients compared
with standard imrt. Clinical and translational radiation oncology 11, 11–18 (2018)
16. Naser, M.A., van Dijk, L.V., He, R., Wahid, K.A., Fuller, C.D.: Tumor segmenta-
tion in patients with head and neck cancers using deep learning based-on multi-
modality pet/ct images. In: 3D Head and Neck Tumor Segmentation in PET/CT
Challenge, pp. 85–98. Springer (2020)
17. Nielsen, C.P., Lorenzen, E.L., Jensen, K., Eriksen, J.G., Johansen, J., Gyldenkerne,
N., Zukauskaite, R., Kjellgren, M., Maare, C., Lønkvist, C.K., et al.: Interobserver
variation in organs at risk contouring in head and neck cancer according to the
dahanca guidelines. Radiotherapy and Oncology 197, 110337 (2024)
18. Outeiral, R.R., Bos, P., Al-Mamgani, A., Jasperse, B., Simões, R., van der Heide,
U.A.: Oropharyngeal primary tumor segmentation for radiotherapy planning on
magnetic resonance imaging using deep learning. Physics and imaging in radiation
oncology 19, 39–44 (2021)
19. Rasmussen, M.E., Nijkamp, J.A., Eriksen, J.G., Korreman, S.S.: A simple single-
cycle interactive strategy to improve deep learning-based segmentation of organs-
at-risk in head-and-neck cancer. Physics and Imaging in Radiation Oncology 26,
100426 (2023)
20. Ren, J., Nijkamp, J., Rasmussen, M., Eriksen, J., Korreman, S.: Mo-0799 single-
click user input reduces false detection in deep learning head and neck tumor
segmentation. Radiotherapy and Oncology 182, S669–S671 (2023)
21. Ren, J., Rasmussen, M., Nijkamp, J., Eriksen, J.G., Korreman, S.: Segment any-
thing model for head and neck tumor segmentation with ct, pet and mri multi-
modality images. arXiv preprint arXiv:2402.17454 (2024)
22. Ren, J., Teuwen, J., Nijkamp, J., Rasmussen, M., Gouw, Z., Eriksen, J.G., Sonke,
J.J., Korreman, S.: Enhancing the reliability of deep learning-based head and
neck tumour segmentation using uncertainty estimation with multi-modal images.
Physics in Medicine & Biology 69(16), 165018 (2024)
23. Riegel, A.C., Berson, A.M., Destian, S., Ng, T., Tena, L.B., Mitnick, R.J., Wong,
P.S.: Variability of gross tumor volume delineation in head-and-neck cancer using ct
and pet/ct fusion. International Journal of Radiation Oncology* Biology* Physics
65(3), 726–732 (2006)
24. Rodriguez, J.D., Selleck, A.M., Razek, A.A.K.A., Huang, B.Y.: Update on mr imag-
ing of soft tissue tumors of head and neck. Magnetic Resonance Imaging Clinics
30(1), 151–198 (2022)
25. Saukkoriipi, M., Sahlsten, J., Jaskari, J., Orasmaa, L., Kangas, J., Rasouli, N.,
Raisamo, R., Hirvonen, J., Mehtonen, H., Järnstedt, J., et al.: Interactive 3d seg-
mentation for primary gross tumor volume in oropharyngeal cancer. arXiv preprint
arXiv:2409.06605 (2024)
26. Segedin, B., Petric, P.: Uncertainties in target volume delineation in radiotherapy–
are they relevant and what can we do about them? Radiology and oncology 50(3),
254–262 (2016)
27. Smits, H.J., Raaijmakers, C.P., de Ridder, M., Gouw, Z.A., Doornaert, P.A.,
Pameijer, F.A., Lodeweges, J.E., Ruiter, L.N., Kuijer, K.M., Schakel, T., et al.:
Improved delineation with diffusion weighted imaging for laryngeal and hypopha-
ryngeal tumors validated with pathology. Radiotherapy and Oncology 194, 110182
(2024)
14
J. Ren et al.
28. van der Veen, J., Gulyban, A., Nuyts, S.: Interobserver variability in delineation
of target volumes in head and neck cancer. Radiotherapy and Oncology 137, 9–15
(2019)
29. Wahid, K., Dede, C., Naser, M., Fuller, C.: Training dataset for hntsmrg 2024
challenge. https://doi.org/10.5281/zenodo.11199559 (2024), [Data set]
30. Wahid, K.A., Ahmed, S., He, R., van Dijk, L.V., Teuwen, J., McDonald,
B.A., Salama, V., Mohamed, A.S., Salzillo, T., Dede, C., et al.: Evaluation
of deep learning-based multiparametric mri oropharyngeal primary tumor auto-
segmentation and investigation of input channel effects: Results from a prospective
imaging registry. Clinical and translational radiation oncology 32, 6–14 (2022)
31. Wahid, K.A., Kaffey, Z.Y., Farris, D.P., Humbert-Vidan, L., Moreno, A.C., Ras-
mussen, M., Ren, J., Naser, M.A., Netherton, T.J., Korreman, S., Balakrishnan,
G., Fuller, C.D., Fuentes, D., Dohopolski, M.J.: Artificial intelligence uncertainty
quantification in radiotherapy applications - a scoping review. Radiotherapy and
Oncology 201, 110542 (2024). https://doi.org/https://doi.org/10.1016/j.
radonc.2024.110542, https://www.sciencedirect.com/science/article/pii/
S0167814024035205
32. Wang, X., Chang, Y., Pei, X., Xu, X.G.: A prior-information-based automatic
segmentation method for the clinical target volume in adaptive radiotherapy of
cervical cancer. Journal of Applied Clinical Medical Physics 25(5), e14350 (2024)
33. Wang, Y., Lombardo, E., Huang, L., Avanzo, M., Fanetti, G., Franchin, G.,
Zschaeck, S., Weingärtner, J., Belka, C., Riboldi, M., et al.: Comparison of deep
learning networks for fully automated head and neck tumor delineation on multi-
centric pet/ct images. Radiation Oncology 19(1), 3 (2024)
34. Wei, Z., Ren, J., Korreman, S.S., Nijkamp, J.: Towards interactive deep-learning for
tumour segmentation in head and neck cancer radiotherapy. Physics and Imaging
in Radiation Oncology 25, 100408 (2023)
35. Xie, J., Peng, Y.: The head and neck tumor segmentation based on 3d u-net. In: 3D
Head and Neck Tumor Segmentation in PET/CT Challenge, pp. 92–98. Springer
(2021)
