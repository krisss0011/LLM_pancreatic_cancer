PGDIFFSEG: PRIOR-GUIDED DENOISING DIFFUSION MODEL
WITH PARAMETER-SHARED ATTENTION FOR BREAST CANCER
SEGMENTATION
Feiyan Feng
School of Information
Science and Engineering,
Shandong Normal University,
Jinan 250358, China
1720696671@qq.com
Tianyu Liu
School of Information
Science and Engineering,
Shandong Normal University,
Jinan 250358, China
lty.tianyu@qq.com
Hong Wang*
School of Information
Science and Engineering,
Shandong Normal University,
Jinan 250358, China
111052@sdnu.edu.cn
Jun Zhao
School of Information
Science and Engineering,
Shandong Normal University,
Jinan 250358, China
zhaojun@sdnu.edu.cn
Wei Li
School of Information
Science and Engineering,
Shandong Normal University,
Jinan 250358, China
1966067505@qq.com
Yanshen Sun*
Department of Computer Science,
Virginia Tech,Virginia,
24061, USA
yansh93@vt.edu
October 24, 2024
ABSTRACT
Early detection through imaging and accurate diagnosis is crucial in mitigating the high mortality
rate associated with breast cancer. However, locating tumors from low-resolution and high-noise
medical images is extremely challenging. Therefore, this paper proposes a novel PGDiffSeg (Prior-
Guided Diffusion Denoising Model with Parameter-Shared Attention) that applies diffusion denoising
methods to breast cancer medical image segmentation, accurately recovering the affected areas from
Gaussian noise. Firstly, we design a parallel pipeline for noise processing and semantic information
processing and propose a parameter-shared attention module (PSA) in multi-layer that seamlessly
integrates these two pipelines. This integration empowers PGDiffSeg to incorporate semantic details at
multiple levels during the denoising process, producing highly accurate segmentation maps. Secondly,
we introduce a guided strategy that leverages prior knowledge to simulate the decision-making process
of medical professionals, thereby enhancing the model’s ability to locate tumor positions precisely.
Finally, we provide the first-ever discussion on the interpretability of the generative diffusion model in
the context of breast cancer segmentation. Extensive experiments have demonstrated the superiority
of our model over the current state-of-the-art approaches, confirming its effectiveness as a flexible
diffusion denoising method suitable for medical image research. Our code will be publicly available
later.
1
Introduction
According to the 2023 cancer statistics report [1], breast cancer ranks second in mortality rate among female cancers,
just behind lung cancer. Nevertheless, extensive research has shown that early screening plays a pivotal role in enhancing
the 5-year relative survival rate because initial morphology and location of tumors can be detected through medical
imaging [2]. This early detection is instrumental in achieving more favorable treatment outcomes [3, 4]. Furthermore,
advancements in treatment protocols, including targeted therapies and adjuvant chemotherapies, have contributed to
the rapid increase in the survival rate of breast cancer patients [5]. However, despite these positive developments, the
latest research reveals a concerning phenomenon: the exponential increase in medical images, including mammography,
arXiv:2410.17812v1  [eess.IV]  23 Oct 2024
A PREPRINT - OCTOBER 24, 2024
xT
xT-1
xt
xt-1
x1
x0
 
...
...
...
...
ROIs
condition
reverse
forword
Figure 1: The process of PGDiffSeg. It is visualized using a viridis color mapping, where the color ranges from deep
purple (representing 0) to bright yellow (representing 1). This process includes both the forward process (noising) and
the reverse process (denoising). At each step, a certain amount of noise is added until the image becomes a Gaussian
distribution. The model learns denoising schemes using images as conditions and generates regions of interest (ROIs).
magnetic resonance imaging (MRI) sequences, and breast ultrasound images, has significantly extended the time
required for radiologists to interpret and analyze these images, which overwhelms radiologists and leads to a high
misdiagnosis rate [6]. Accurately pinpointing tumors from low-resolution and noisy breast images is challenging [7],
requiring substantial effort and expertise for precise tumor segmentation in medical images [8].
In recent years, deep learning-based methods, such as deep convolutional neural networks (DCNNs), have emerged
as powerful tools for segmenting suspected regions and extracting features from segmented breast cancer ROIs [9].
Integrating computer-aided diagnosis (CAD) strategies based on DCNNs can assist radiologists in making informed
decisions, ultimately reducing unnecessary biopsies and alleviating patients’ discomfort caused by unnecessary invasive
procedures.
The U-shape architecture has shown significant progress in breast cancer segmentation by effectively integrating
low-level and high-level information through skip connections. Inspired by its success, many breast cancer segmentation
networks based on UNet have sprung up [10, 11, 12]. However, current CAD methods face several challenges. Firstly,
they tend to rely excessively on pre-processing phases, such as image denoising and enhancement, resulting in image
distortion and the loss of crucial primitive features. In the context of breast cancer segmentation, the utilization of
high-resolution images that capture various cancer-related features has the potential to enhance the discrimination ability
of the model [13]. Moreover, end-to-end segmentation methods lack interpretability due to their "black box" feature
[14]. Radiologists may question the authenticity of the segmentation results since the decision-making process needs
more guidance or expert knowledge. Furthermore, specific hybrid methods focus on exploring additional information,
such as global and local features, that are relevant to the object areas. These methods extract cancer-related features
from breast images and directly sum different representations. However, these approaches can widen the semantic gap
and neglect essential medical commonsense knowledge.
To address these challenges, we introduce a novel GFDiffSeg model, Prior-Guided Diffusion Denoising Model with
Parameter-Shared Attention, for breast cancer segmentation. Unlike the classical Denoising Diffusion Probabilistic
Model (DDPM) [15] , GFDiffSeg goes beyond iteratively transforming a noise-corrupted input into a clean sample
using the diffusion process and probabilistic modeling. It also leverages images as conditions to learn the denoising
process for corresponding segmentation masks. The process of noising and denoising in GFDiffSeg is depicted in
Fig. 1. By incorporating this principle, GFDiffSeg can directly obtain segmentation results from the raw images while
providing interpretability. Furthermore, we propose a parameter-shared attention (PSA) module to effectively fuse
noise and image features, bridging the semantic gap between different representations. Additionally, research has
demonstrated that incorporating prior knowledge into networks can provide benefits [16, 17], so we suggest adopting a
prior knowledge-guided strategy to facilitate rapid focus on regions of interest. These advancements in the GFDiffSeg
model contribute to improved segmentation performance and efficiency in breast cancer analysis. Our contributions can
be summarized as follows:
• PGDiffSeg represents a pioneering approach that enhances diffusion denoising techniques to achieve accurate
breast cancer medical image segmentation by effectively eliminating Gaussian noise. Notably, PGDiffSeg
holds the distinction of being the first diffusion generative model in the field of breast cancer segmentation,
equipped with valuable interpretability capabilities.
2
A PREPRINT - OCTOBER 24, 2024
• We introduce a parallel pipeline for separate processing of image and noise-added label information. Further-
more, we design a PSA module that enhances the model’s focus on lesion areas in both pipelines to address
the fusion of noise features and semantic information. This integration empowers PGDiffSeg to incorporate
semantic details at multiple levels during denoising.
• We incorporate prior knowledge by utilizing context encoding, allowing our model to simulate doctors’
decision-making process. This integration of prior knowledge successfully reduces the segmentation difficulty
for our model. This module operates akin to the expertise of physicians, guiding the denoising process and
facilitating quicker focus on lesion areas.
• We are the first to present the interpretability of diffusion models and analyze the attention transfer character-
istics in image segmentation tasks. In contrast to end-to-end models, we visualize intermediate results and
attention positions at each iteration, providing a clear view of the model’s learning process and enhancing trust
among medical professionals.
• Our method is extensively evaluated and consistently outperforms or achieves comparable results to the
current state-of-the-art models in breast cancer image segmentation. Moreover, experiments reveal our model’s
excellent transferability, allowing it to be seamlessly applied to diverse modalities such as MRI or ultrasound
datasets.
We organize the rest of this paper as followings. The related works are reviewed in Section 2. The proposed model and
its detailed workflow are presented in Section 3. The experiments, and analysis are demonstrated in Sections 4 and 5.
Ultimately, we summarize our work and provide future perspectives in Section 6.
2
Related Work
2.1
Diffusion model in medical image processing tasks
Due to its remarkable ability to generate realistic images, diffusion model has captured considerable attention in
computer vision [18, 19, 20, 21, 22, 23]. There are three generic diffusion modeling frameworks, each based on
denoising diffusion probabilistic models, noise-conditioned score networks, and stochastic differential equations [24].
When performing segmentation tasks, diffusion models add noise to the segmentation mask and use the image as an
additional input to denoise the noisy mask. However, existing segmentation methods are inadequate in maximizing the
use of image information to improve segmentation performance. They either concatenate the image and xt directly
[25], add the encoded image features and encoded xt [20], embed the xt features into the image features at each
downsampling step [26], or use cross-attention to fuse the image and xt before downsampling [27]. Therefore, this
study innovatively designs a fusion strategy where image information and noisy labels mutually reinforce each other,
facilitating the generation of enhanced and more effective feature representations.
2.2
Self-attention mechanism in medical image segmentation
The self-attention mechanism has gained significant popularity in medical image analysis due to its ability to capture
long-range dependencies and global features. Consequently, several pure transformer models have been developed
for medical image segmentation tasks [28, 29]. However, these pure transformer designs often neglect the extraction
of local information. Recent research has explored hybrid architectures that combine self-attention with CNNs for
more effective segmentation [30, 31, 32, 33, 34]. Nonetheless, these studies primarily focus on extracting multi-scale
information without effectively addressing the interaction of such information. In contrast, our proposed PSA module
emphasizes the fusion of two distinct types of information, thereby improving segmentation performance by bridging
the semantic gap.
3
PGDiffSeg model
3.1
Framework of the GFDiffSeg model
Our proposed network GFDiffSeg operates on the diffusion process and generates segmented results from Gaussian
noise through iterative sampling. The overall structure of the network is shown in Fig. 2.
Our model comprises feature pre-extraction, feature encoding, prior knowledge guidance, bottleneck, and feature
decoding stages. In the feature pre-extraction stage, the network separately extracts hidden features of the image and the
noised label xt using two slim dense blocks (SDB). Next, in the feature encoding stage, the downflow of the model is
divided into two branches. One branch, called the condition flow, takes the extracted image features as input to transmit
3
A PREPRINT - OCTOBER 24, 2024
30
··
SDB
SDB
(d) Bottlenek
Layer
MLP
Image
xt
Noise
Xt-1
With/-out cancers
Condition unit
Condition unit
Condition unit
condition unit
Denoising unit
Denoising unit
Denoising unit
Denoising unit
Up unit
Up unit
Up unit
Up unit
PSA
PSA
PSA
PSA
Supervised unit
Block
Supervised unit
Supervised unit
Supervised unit
Out_Conv
(c) Prior knowledge 
guidance
(b) Feature encoding
(e) Feature decoding
(a) Feature pre-extraction
Denoising flow
Condition flow
Up flow
Skip connection
Supervised flow
Element addition
Figure 2: Framework of PGDiffSeg model. (a) The feature pre-extraction module utilizes the slim dense block (SDB)
to extract high-level semantic features from the image and xt. (b) The feature encoding module performs denoising
and encodes the features of xt and the image using condition units. The features are fused using parameter-shared
attention (PSA) module after each layer of branches. (c) The prior knowledge guidance module plays a crucial role in
the bottleneck layer by injecting expert knowledge. (d) The bottleneck layer serves as the connection point between
feature encoding and feature decoding. It receives the output from feature encoding and passes it to feature decoding. (e)
The feature decoding module converts the high-level feature representations from the bottleneck into the corresponding
output for the original xt. The model predicts the noise added in this step to obtain xt−1.
spatial characteristics. And the other branch, the denoising flow, takes the extracted xt features as input to distinguish
noise information. Unlike conventional encoders, these two extractor flows work in tandem, with the parameter-shared
attention (PSA) module employed after each layer of the down-level unit, facilitating information exchange between the
two branches and promoting mutual improvement. After feature encoding, the features from both sides are fused by
element addition before the bottleneck. Furthermore, we also incorporate prior tumor information into the segmentation
task by training a classification network that guides the learning process of the segmentation task. Following the
bottleneck, the rich features are fed into a decoder consisting of four up units, ultimately obtaining the noise-adding
prediction from xt−1 to xt.
Next, we will mainly introduce our three main tasks, SDB in feature pre-extraction, PAS module in feature encoding,
and prior-supervision module used for prior knowledge guidance. We do not introduce too many innovations in other
modules and have followed the majority’s work; the detailed implementation can be found in the supplementary
materials.
3.2
Slim dense block
As shown in Fig 3, the slim dense block (SDB) is constructed using a lightweight dense block [35], and plays a crucial
role in extracting hidden information that may be lost or inaccessible during the information exchange process of the
PSA module in advance. By eliminating the growth rate of the Dense Block and adopting element-wise addition instead
of channel concatenation, the SDB ensures a consistent number of channels in each layer. This design enables seamless
integration of the SDB into our model, irrespective of the increasing number of layers.
This module consists of L layers, where each layer implements a non-linear transformation H(·), which involves a
composite function that includes the connection of two 3 × 3 convolution blocks, two-dimensional BatchNorm, and
LeakyReLU activation.
4
A PREPRINT - OCTOBER 24, 2024
LeakyReLU
BatchNorm2d
conv2D
conv2D
LeakyReLU
BatchNorm2d
conv2D
conv2D
LeakyReLU
BatchNorm2d
conv2D
conv2D
LeakyReLU
BatchNorm2d
conv2D
conv2D
Figure 3: The structure of the SDB as depicted in Fig. 2(a). Taking L=4 as an example, we showcase the flow of feature
vectors between different modules. Different feature vectors are combined by addition, and for better visual effects, we
do not reflect this process in the figure.
To fully exploit information, every layer in this module receives the output of all previous layers. This design ensures
comprehensive information utilization and prevents overfitting by allowing the model to capture relevant features
effectively. We denote the output of the ith layer as xi. The transmission of feature maps in the ith layer can be
represented as Equation 1.
xi = H (xi−1) ⊕
i−1
X
j=1
xj ◦χ
(1)
where χ is a scaling factor set to 0.2 in this model to optimize the overall architecture.
3.3
Parameter-shared cross-attention module
The parameter-shared cross-attention(PSA) module is designed to bridge the semantic gap between the two branches.
Inspired by [36] and [37], we propose a novel approach for integrating relevant information, overcome the isolated state
of the independent downflows between the two branches.
In our design, we introduce a PSA module after each layer of the denoising and condition units, as depicted in Fig.
4. Since xt and image have spatial consistency, the parameters of each layer’s PSA module can be trained with two
hidden features simultaneously: the feature that needs to be denoised and the pathological feature that contains tumor
information. This PSA block plays a bidirectional role in embedding denoised features and pathological features into
the same feature space, allowing them to complement each other.
V1
V2
T
Kshared
Qshared
Qshared
Kshared
T
T
Transpose
Softmax
Element addition
Shared 
parameters
x1
x2
R
C
H
W


C
N

C
N

C
N

C
N

N
N

C
H
W


C
H
W


C
H
W


C
H
W


C
H
W


C
H
W


R
R
R
R
R
C
N

C
N

N
C

N
C

R
C
N

Convolution operation
N
N

R C
H
W


R
Reshape
Matrix multiplication
x1 _output
x2 _output
Figure 4: Parameter-shared multi-layer cross-attention (PSA) module. After being embedded in the denoising unit and
condition unit of each layer, this module receives the two intermediate results of the denoising flow and condition flow
in the Feature encoder, fully fuses their related information, and then transmits them to the denoising unit and condition
unit of the next layer respectively.
The pathological features x1 ∈RC×H×W from the previous condition unit are transmitted to two semantic spaces
for query and key calculations, which are mapped to the R ˜
C×N space, where ˜C =
C
8 and N = H × W. The
denoised features x2 also utilize the correspondent noise spaces with identical parameters, ensuring consistency in the
computations and allowing one set of features to be mapped onto the other set. Then x1 and x2 use their separate value
5
A PREPRINT - OCTOBER 24, 2024
generation modules to obtain their value of x1 and x2 of dimensions C × N respectively. Therefore, the calculation of
query, key, and value for the two features can be represented as Equation 2.
qi, ki, vi = Qshared (xi) , Kshared (xi) , Vi (xi) ; i = 1, 2
(2)
where Q, K, V , are the generation of query, key, and value, respectively; and qi, ki, vi are query, key, and value of
xi. The x1 attention maps are first computed by multiplying the transpose of the projected q1 layer with k1. After
measuring similarity through softmax, they are multiplied by v1 to produce the final semantic maps with a shape of
N × C, which can be seen in Equation 3. Similarly, the denoising maps of x2 undergo the same process as described in
Equation 4.
Xs = v1 · Softmax
 qT
1 · k1

(3)
Xd = v2 · Softmax
 qT
2 · k2

(4)
where Xs and Xd denote semantic and denoising maps, respectively. Then the shape of Xs and Xd is reset to
C × H × W, and the two original feature maps can be updated to form the output, as represented by Equation 5.
x1 = η1Xs + x1; x2 = η2Xd + x2
(5)
where η1 is a learnable scalar and we initialize it as 0. This allows the model to focus more on the intrinsic features of
its branch in the early stages of training and gradually assign more weights to the supplemental features [37].
3.4
Prior-supervision module
In practical applications, achieving pixel-wise classification can be a challenging task, whereas determining the presence
of a tumor is relatively simpler [38]. Therefore, we incorporate a supervised flow generated by the prior supervision
module as prior semantic knowledge to offer position guidance and integrate it into the denoising process.
conv2D
stride=1
BatchNorm2d
Relu
conv2D
stride=2
BatchNorm2d
Relu
conv2D
stride=1
BatchNorm2d
Relu
30
··
Layer
MLP
With/-out cancers
Supervised unit
Block
Supervised unit
Supervised unit
Supervised unit
Dice + BCE loss
BCE loss
Total loss
Predict
Ground truth
Output
Figure 5: Details of prior-supervision module. To make the output of this module consistent with the Vector space of
the output of the feature encoder, we designed one block and four supervised units, respectively, corresponding to the
SDB and four layers units in the feature encoder
As shown in Fig. 5, this branch takes the image I as input and first goes through a 3×3 convolution layer, BatchNorm,
and ReLU. Then, the output is fed through four supervised units, each consisting of two blocks with a 3×3 convolution,
BatchNorm, and ReLU. We use convolution with a stride of 2 to reduce the size of the feature maps, ensuring that
the output size of the supervised flow matches the bottleneck of the main branch. Finally, the bottleneck-information
weight is adjusted by adding x to ˜x, depicted as Equation 6.
˜x = x ⊕˜x
(6)
where ˜x represents the feature map of the bottleneck and x is the output of the supervised flow. This module has an
independent loss function. We obtain a probability classification value p using linear mapping and train this part using
cross-entropy loss, as shown in Equation 7.
6
A PREPRINT - OCTOBER 24, 2024
lossCE = −1
N Σ (yi log (pi) + (1 −yi) log (1 −pi))
(7)
where N represents the number of samples, yi is the true value of the ith sample (whether a tumor exists), and pi is the
predicted probability of that sample.
Furthermore, to ensure that this supervised flow focuses on the correct positions, we map the penultimate supervised
unit from the space of RC×H×W to R1×H×W and compute the Dice loss and Binary Cross-Entropy loss by comparing
it with the resized segmentation label. The Dice loss and Binary Cross-Entropy loss are represented by Equation 8 and
Equation 9, respectively.
lossdice = 1 −
2
N
P
i=1
yiˆyi + ϵ
N
P
i=1
yi +
N
P
i=1
ˆyi + ϵ
(8)
lossBCE = −
N
X
i=1
yi log(ˆyi) + (1 −yi) log(1 −ˆyi)
(9)
where yi represents the true label, ˆyi represents the predicted result of the model, N represents the number of samples,
and ϵ is a small constant used to avoid division by zero. The final loss function is as Equation 10.
loss = λ1lossCE + λ2(lossdice + lossBCE)
(10)
4
Experiments
4.1
Datasets
To demonstrate the effectiveness of tumor segmentation, we conduct comprehensive experiments on two publicly
available datasets, the Breast-MRI-NACT-Pilot dataset [39] and the Breast Ultrasound Image (BUSI) dataset [40].
Breast-MRI-NACT-Pilot is published on TCIA [41] and used for locally advanced breast cancer segmentation. This
collection contains 64 patients with stage 2 or 3 locally advanced breast cancer (tumor size >= 3cm) enrolled from
1995 to 2002. All patients had breast cancer diagnosed based on histopathological reports of biopsy or surgical excision.
Using a bilateral phased array breast coil, the breast MRI was constructed by a 1.5-T scanner (Signa, GE Healthcare,
Milwaukee, WI). Our experiment obtained DCE-MRI scans of 64 patients before NACT treatment, of which 62 scans
have a resolution of 256 × 256 × 60, and the resolution of the two scans is 512x512x64. We trained our model using 2D
slices, with 43 patients for training, 7 for validation, and 14 for testing.
BUSI is the first publicly available breast ultrasound dataset. It was collected in 2018 using the LOGIQ E9 ultrasound
system and LOGIQ E90 Agile ultrasound system during the scanning process, including breast ultrasound images from
600 women between the ages of 25 and 75. The dataset comprises 780 grayscale images with an average size of 500
× 500 pixels. These images were collected and stored in DICOM format at Baheya Hospital and then converted to
PNG format using a DICOM converter application. The collected images are divided into three categories: normal (133
images), benign(437 images), and malignant(210 images).
4.2
Data preprocessing
In the Breast-MRI-NACT-Pilot dataset, approximately the first 70 % of patients in each dataset are used for training,
the middle 10% for validation, and the remaining 20% for testing. We applied the same partitioning for the BUSI
dataset’s three classes (normal, benign, and malignant) to ensure consistency in sample distribution across the training,
validation, and testing sets. We truncate all pixel intensities to a specific range. In the Breast-MRI-NACT-Pilot dataset,
the original HU range is from 0 to 96 but can vary for each patient. We normalize each patient’s range to [0,255] and
then truncate the intensity values to the range of [20, 200]. For the BUSI dataset, the original distribution is [0, 255],
and we truncate it to the range of [30, 235]. To prevent overfitting, data augmentation, including horizontal, vertical
flipping, and rotation by 90°, 180°, and 270°, were applied to the BUSI dataset to increase the size of the training set
six-fold. We resized all data to 128 × 128 and normalized it to [-1,1] based on the methodology proposed in DDPM
[15].
7
A PREPRINT - OCTOBER 24, 2024
T=1
T=1000
425
601
469
338
296
434
536
999
734
549
699
15
571
621
996
T=1
T=1000
condition flow
denoising flow
(a) Attention transfer characteristics on condition flow
(b) Attention transfer characteristics on denoising  flow
Figure 6: The attention positions in the (a) condition flow and (b) denoising flow exhibit variations across diffusion
steps. The four rows correspond to the four layers of the model. Within each row, the images represent the model’s
output at the same position but at different diffusion steps. Horizontal arrows illustrate the changes in diffusion steps,
while vertical arrows indicate the direction of parameter propagation within the model. The original images and their
corresponding ground truth are displayed on Fig. 7(c). The highlighted red regions signify the positions significantly
influencing the model’s decision-making process.
5
Discussion
In this work, we demonstrate the effectiveness and clinical feasibility of the proposed method in improving cancer
segmentation, which includes a visual analysis and discussion of the proposed modules, as well as the applicability
of existing efficient sampling studies to our tumor segmentation task. To directly reflect the execution process of our
model, we employed Grad-CAM [42] to highlight the regions in each layer that contribute to the segmentation results.
5.1
Attention transfer characteristics of denoising model
We demonstrate the attention transfer characteristics of the denoising model in the segmentation task, as shown in Fig.
6. It illustrates the spatiotemporal transition of the vital contribution regions when generating a segmentation mask.
In Fig. 6(a), for example, each row represents one layer’s output of the model’s condition flow, with diffusion steps
decreasing from left to right and the model’s depth increasing from top to bottom.
In the same layer in condition flow in Fig. 6(a), attention positions vary at different diffusion steps. This variation
often transitions smoothly as the timesteps decrease, but occasional jumps may occur. Additionally, we observe that
the first layer of the condition flow initially focuses on global information and gradually localizes to the tumor region.
The attention in the second layer appears more dispersed, similar to the distribution shown in Fig. 6(b), which may be
attributed to the influence of the sudden introduction of information from the denoising flow. However, as the model’s
depth increases, this bias is readjusted. The attention in the third layer shifts gradually from the background to the
foreground. In the deeper layer (the last row), the model’s attention becomes diverse, indicating that the deep network
attempts to learn more complex features that cannot be easily interpreted.
Similarly, Fig. 6(b) depicts the changes in attention positions of the denoising flow. We present the results for each layer,
allowing us to observe the variation in attention positions over time. Fig. 6(b) demonstrates that the denoising flow
exhibits a similar pattern to the condition flow, indicating that our model operates in a rich feature space. Although this
branch’s initial input is Gaussian noise (at T=1000), it can still accurately locate the tumor when T gradually decreases.
This ability can be attributed to introducing PSA.
5.2
Effectiveness of PSA Module
As shown in Fig. 7, we compared the changes in the heatmaps before and after applying the PSA module, where the top
row represents the attention positions of the latest convolutional layer before PSA, and the bottom row displays the
attention maps after PSA. In Fig. 7(a), we notice that the model focuses more on the tumor region after incorporating
the PSA module in the denoising flow. This change is crucial because we need PSA to introduce semantic information
8
A PREPRINT - OCTOBER 24, 2024
from the condition flow, enabling it to perform denoising effectively in this step. Additionally, Fig. 7(b) demonstrates
that although the condition flow already captures the semantic information and focuses on the tumor region, introducing
the PSA module further enhances its confidence in the target area.
(a) PSA on denoising flow
(b) PSA on condition flow
(c) Image and 
ground truth
Figure 7: The positions attended by the convolutional layers before and after PSA are shown in the top and bottom
rows, respectively. (a) represents the results from the denoising flow, while (b) represents the results from the condition
flow. Moreover, (c) is the original images and their corresponding ground truth.
5.3
The effectiveness of introducing prior knowledge
In this section, we discussed the effectiveness of prior knowledge. We examined the attention positions of the last layer
feature map in this branch using CAM, as shown in Fig. 8. It can be observed that this subnetwork can identify the
location of tumors, which means it can simulate prior knowledge provided by physicians and facilitate subsequent
denoising. In the Breast-MRI-NACT-Pilot dataset, tumor locations are often scattered. Although this subnetwork cannot
offer precise segmentation like the segmentation network, it can at least identify the approximate location of the tumor,
especially in areas where tumors are concentrated. While it’s not absolutely accurate, introducing this prior knowledge
provides convenience for the leading network.
(b) BUSI
Heatmap
Image
Ground Truth
(a) Breast-MRI-NACT-Pilot
Heatmap
Image
Ground Truth
Figure 8: The effectiveness of introducing prior knowledge. The heatmaps indicate the model’s greater attention at the
highlighted positions, represented by red.
5.4
Analyzing of the results
Fig 9 compares the performance of the GFDiffSeg model and others. The first two rows show the results of different
models on the Breast-MRI-NACT-Pilot dataset, while the last two rows depict the results on the BUSI dataset. It can
be observed that other models often exhibit more regions of over-segmentation or unsegmented areas, whereas our
model’s inaccurate position is primarily located at the edges. Furthermore, both datasets indicate that Swin-Unet is
prone to excessive segmentation and exhibits jagged edges. Moreover, SegNet and UNeXt struggle with segmenting
small detached tumors. The segmentation results of Swin-Unet tend to include some unrelated regions. On the BUSI
dataset, as depicted in the last row of Fig. 9, Unet and SegNet cannot fully segment tumors with a significant difference
in the grayscale range. UNeXt, on the other hand, fails to identify the intersection between two grayscale ranges as
tumor regions. Fortunately, this problem is effectively resolved in GFDiffSeg, as the denoising diffusion model is
powerful for learning data distributions.
5.5
Efficient sampling
Since diffusion models require extensive sampling iterations, which significantly hinders their application, we employed
accelerated algorithms [43, 44, 45, 46] to explore efficient sampling with reduced numbers of function evaluations
9
A PREPRINT - OCTOBER 24, 2024
Image
label
Unet
SegNet
UNeXt
Swin-Unet
Ours
Figure 9: Comparison of the GFDiffSeg model with state-of-the-art models. We use different colors to represent the
results of the models in terms of correct segmentation, over-segmentation, and unsegmented regions. Using A and B to
denote the ground truth region and the predicted label of the tumor, white represents A∩B (intersection of A and B); red
represents A-B, indicating the areas that the model failed to segment; blue represents B-A, showing regions that do not
contain tumors but were incorrectly segmented as tumors by the model.
(NFE). To mitigate the impact of randomness, we repeated each experiment five times and reported the average and
variance of the Dice similarity score (DSC). More details can be found in the supplemental materials. The visualized
results are shown in Figures 10 and 11, where the solid line represents the average DSC of the five repeated experiments,
the dashed line represents the variance, and the yellow horizontal line indicates the initial results of our model.
5
10
20
30
40
50
60
70
80
90
100
The number of function evaluations
0.50
0.55
0.60
0.65
0.70
0.75
DSC
uni_pc
ddim
dpm-solver
dpm-solver++
0.00
0.01
0.02
0.03
0.04
0.05
Variance of 5 identical experiments
Figure 10: The mean and variance of the DSC for different numbers of function evaluations on the Breast-MRI-NACT-
Pilot dataset.
5
10
20
30
40
50
60
70
80
90
100
The number of function evaluations
0.50
0.55
0.60
0.65
0.70
0.75
DSC
uni_pc
ddim
dpm-solver
dpm-solver++
0.00
0.01
0.02
0.03
0.04
0.05
Variance of 5 identical experiments
Figure 11: The mean and variance of the DSC for different numbers of function evaluations on the BUSI dataset.
6
Conclusion
We propose the GFDiffSeg model, a novel approach for breast cancer segmentation. Our study further demonstrates the
feasibility of the denoising strategy in medical image segmentation, specifically tailored for breast cancer. We enhance
tumor localization by incorporating the PSA module and a guided prior knowledge strategy. Experimental results show
10
A PREPRINT - OCTOBER 24, 2024
that the GFDiffSeg model outperforms or performs comparably to state-of-the-art approaches. Interpretability analysis
further confirms the effectiveness of our method.
However, there are limitations in the current version of the GFDiffSeg model that need to be addressed in future research.
We have only conducted experiments on breast datasets with different modalities. The denoising process also incurs
longer sampling times than other models, limiting the practical application and generalization of the denoising diffusion
model in medical image segmentation.
In the future, We aim to explore methods to reduce sampling time and extend our model to tumor segmentation in
other organs. Additionally, we will focus on developing more efficient model architectures, such as leveraging prior
knowledge to design prompts that guide the model in generating more accurate segmentation results.
Acknowledgment
This work is supported by National Nature Science Foundation of China (62072290), Natural Science Foundation of
Shandong Province (ZR2022QF022) and Jinan "20 new colleges and universities" Funded Project (202228110).
References
[1] R. L. Siegel, K. D. Miller, N. S. Wagle, and A. Jemal, “Cancer statistics, 2023,” CA: A Cancer Journal for
Clinicians, vol. 73, no. 1, pp. 17–48, 2023. [Online]. Available: https://acsjournals.onlinelibrary.wiley.com/doi/
abs/10.3322/caac.21763
[2] D. Crosby, S. Bhatia, K. M. Brindle, L. M. Coussens, C. Dive, M. Emberton, S. Esener, R. C. Fitzgerald, S. S.
Gambhir, P. Kuhn, T. R. Rebbeck, and S. Balasubramanian, “Early detection of cancer,” Science, vol. 375, no.
6586, p. eaay9040, 2022. [Online]. Available: https://www.science.org/doi/abs/10.1126/science.aay9040
[3] J. M. Croswell, D. F. Ransohoff, and B. S. Kramer, “Principles of cancer screening: Lessons from history and
study design issues,” Seminars in Oncology, vol. 37, no. 3, pp. 202–215, 2010, cancer Prevention I. [Online].
Available: https://www.sciencedirect.com/science/article/pii/S0093775410000710
[4] T. J. O’Grady, M. A. Gates, and F. P. Boscoe, “Thyroid cancer incidence attributable to overdiagnosis in the united
states 1981–2011,” International Journal of Cancer, vol. 137, no. 11, pp. 2664–2673, 2015. [Online]. Available:
https://onlinelibrary.wiley.com/doi/abs/10.1002/ijc.29634
[5] K. Sasaki, S. S. Strom, S. O’Brien, E. Jabbour, F. Ravandi, M. Konopleva, G. Borthakur, N. Pemmaraju,
N. Daver, P. Jain, S. Pierce, H. Kantarjian, and J. E. Cortes, “Relative survival in patients with
chronic-phase chronic myeloid leukaemia in the tyrosine-kinase inhibitor era: analysis of patient data from six
prospective clinical trials,” The Lancet Haematology, vol. 2, no. 5, pp. e186–e193, 2015. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S2352302615000484
[6] L. Singh, Z. Jaffery, Z. Zaheeruddin, and R. Singh, “Segmentation and characterization of breast tumor in
mammograms,” in 2010 International Conference on Advances in Recent Technologies in Communication and
Computing, 2010, pp. 213–216.
[7] J. Yang, L. Jiao, R. Shang, X. Liu, R. Li, and L. Xu, “Ept-net: Edge perception transformer for 3d medical image
segmentation,” IEEE Transactions on Medical Imaging, pp. 1–1, 2023.
[8] Z. Tian, H. Zhao, M. Shu, Z. Yang, R. Li, and J. Jia, “Prior guided feature enrichment network for few-shot
segmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 2, pp. 1050–1065,
2022.
[9] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos, “Image segmentation using
deep learning: A survey,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 7, pp.
3523–3542, 2022.
[10] J.-J. Liu, Q. Hou, Z.-A. Liu, and M.-M. Cheng, “Poolnet+: Exploring the potential of pooling for salient object
detection,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 1, pp. 887–904, 2023.
[11] X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, and P.-A. Heng, “H-denseunet: Hybrid densely connected unet for
liver and tumor segmentation from ct volumes,” IEEE Transactions on Medical Imaging, vol. 37, no. 12, pp.
2663–2674, 2018.
[12] R. Ke, A. Bugeau, N. Papadakis, P. Schuetz, and C.-B. Schönlieb, “Learning to segment microscopy images with
lazy labels,” in Computer Vision – ECCV 2020 Workshops, A. Bartoli and A. Fusiello, Eds.
Cham: Springer
International Publishing, 2020, pp. 411–428.
11
A PREPRINT - OCTOBER 24, 2024
[13] Y. Liu, F. Zhang, C. Chen, S. Wang, Y. Wang, and Y. Yu, “Act like a radiologist: Towards reliable multi-view
correspondence reasoning for mammogram mass detection,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 44, no. 10, pp. 5947–5961, 2022.
[14] T. Liu, H. Wang, S. Yu, F. Feng, and J. Zhao, “A soft-attention guidance stacked neural network
for neoadjuvant chemotherapy’s pathological response diagnosis using breast dynamic contrast-enhanced
mri,”
Biomedical Signal Processing and Control,
vol. 86,
p. 105145,
2023. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1746809423005785
[15] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in Advances in Neural Information
Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.
Curran
Associates, Inc., 2020, pp. 6840–6851. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2020/
file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf
[16] P. Li, Y. Liu, Z. Cui, F. Yang, Y. Zhao, C. Lian, and C. Gao, “Semantic graph attention with explicit anatomical
association modeling for tooth segmentation from cbct images,” IEEE Transactions on Medical Imaging, vol. 41,
no. 11, pp. 3116–3127, 2022.
[17] H. Huang, H. Zheng, L. Lin, M. Cai, H. Hu, Q. Zhang, Q. Chen, Y. Iwamoto, X. Han, Y.-W. Chen, and R. Tong,
“Medical image segmentation with deep atlas prior,” IEEE Transactions on Medical Imaging, vol. 40, no. 12, pp.
3519–3530, 2021.
[18] K. Gong, K. Johnson, G. El Fakhri, Q. Li, and T. Pan, “Pet image denoising based on denoising diffusion
probabilistic model,” European Journal of Nuclear Medicine and Molecular Imaging, vol. 51, no. 2, pp. 358–368,
2024.
[19] H. Chung and J. C. Ye, “Score-based diffusion models for accelerated mri,” Medical Image Analysis, vol. 80, p.
102479, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1361841522001268
[20] T. Amit, T. Shaharbany, E. Nachmani, and L. Wolf, “Segdiff: Image segmentation with diffusion probabilistic
models,” 2021. [Online]. Available: https://arxiv.org/abs/2112.00390
[21] R. S. Zimmermann, L. Schott, Y. Song, B. A. Dunn, and D. A. Klindt, “Score-based generative classifiers,” in
NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.
[22] B. Kim and J. C. Ye, “Diffusion deformable model for 4d temporal medical image generation,” in Medical Image
Computing and Computer Assisted Intervention – MICCAI 2022, L. Wang, Q. Dou, P. T. Fletcher, S. Speidel,
and S. Li, Eds.
Cham: Springer Nature Switzerland, 2022, pp. 539–548.
[23] P. A. Moghadam, S. Van Dalen, K. C. Martin, J. Lennerz, S. Yip, H. Farahani, and A. Bashashati, “A morphology
focused diffusion probabilistic model for synthesis of histopathology images,” in Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision (WACV), January 2023, pp. 2000–2009.
[24] F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, “Diffusion models in vision: A survey,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, pp. 1–20, 2023.
[25] J. Wolleb, R. Sandkühler, F. Bieder, P. Valmaggia, and P. C. Cattin, “Diffusion models for implicit image
segmentation ensembles,” in Proceedings of The 5th International Conference on Medical Imaging with Deep
Learning, ser. Proceedings of Machine Learning Research, E. Konukoglu, B. Menze, A. Venkataraman,
C. Baumgartner, Q. Dou, and S. Albarqouni, Eds., vol. 172.
PMLR, 06–08 Jul 2022, pp. 1336–1348. [Online].
Available: https://proceedings.mlr.press/v172/wolleb22a.html
[26] J. Wu, R. Fu, H. Fang, Y. Zhang, Y. Yang, H. Xiong, H. Liu, and Y. Xu, “Medsegdiff: Medical image segmentation
with diffusion probabilistic model,” in Medical Imaging with Deep Learning.
PMLR, 2024, pp. 1623–1639.
[27] G. J. Chowdary and Z. Yin, “Diffusion transformer u-net for medical image segmentation,” in International
conference on medical image computing and computer-assisted intervention.
Springer, 2023, pp. 622–631.
[28] D. Karimi, S. D. Vasylechko, and A. Gholipour, “Convolution-free medical image segmentation using transformers,”
in Medical Image Computing and Computer Assisted Intervention – MICCAI 2021, M. de Bruijne, P. C. Cattin,
S. Cotin, N. Padoy, S. Speidel, Y. Zheng, and C. Essert, Eds.
Cham: Springer International Publishing, 2021, pp.
78–88.
[29] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, “Swin-unet: Unet-like pure transformer
for medical image segmentation,” in Computer Vision – ECCV 2022 Workshops, L. Karlinsky, T. Michaeli, and
K. Nishino, Eds.
Cham: Springer Nature Switzerland, 2023, pp. 205–218.
[30] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, “Medical transformer: Gated axial-attention for
medical image segmentation,” in Medical Image Computing and Computer Assisted Intervention – MICCAI
2021, M. de Bruijne, P. C. Cattin, S. Cotin, N. Padoy, S. Speidel, Y. Zheng, and C. Essert, Eds.
Cham: Springer
International Publishing, 2021, pp. 36–46.
12
A PREPRINT - OCTOBER 24, 2024
[31] Y. Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and cnns for medical image segmentation,” in
Medical Image Computing and Computer Assisted Intervention – MICCAI 2021, M. de Bruijne, P. C. Cattin,
S. Cotin, N. Padoy, S. Speidel, Y. Zheng, and C. Essert, Eds.
Cham: Springer International Publishing, 2021, pp.
14–24.
[32] A. Lin, B. Chen, J. Xu, Z. Zhang, G. Lu, and D. Zhang, “Ds-transunet: Dual swin transformer u-net for medical
image segmentation,” IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1–15, 2022.
[33] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision
transformer using shifted windows,” in Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV), October 2021, pp. 10 012–10 022.
[34] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, and D. Xu, “Unetr:
Transformers for 3d medical image segmentation,” in 2022 IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV), 2022, pp. 1748–1758.
[35] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
[36] A. M. Shaker, M. Maaz, H. Rasheed, S. Khan, M.-H. Yang, and F. S. Khan, “Unetr++: delving into efficient and
accurate 3d medical image segmentation,” IEEE Transactions on Medical Imaging, 2024.
[37] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-attention generative adversarial networks,” in
Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning
Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97.
PMLR, 09–15 Jun 2019, pp. 7354–7363. [Online].
Available: https://proceedings.mlr.press/v97/zhang19d.html
[38] J. Wang, Y. Zheng, J. Ma, X. Li, C. Wang, J. Gee, H. Wang, and W. Huang, “Information bottleneck-based
interpretable multitask network for breast cancer classification and segmentation,” Medical Image Analysis, vol. 83,
p. 102687, 2023. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1361841522003152
[39] S. C. Partridge, J. E. Gibbs, Y. Lu, L. J. Esserman, D. Tripathy, D. S. Wolverton, H. S. Rugo, E. S. Hwang,
C. A. Ewing, and N. M. Hylton, “Mri measurements of breast tumor volume predict response to neoadjuvant
chemotherapy and recurrence-free survival,” American Journal of Roentgenology, vol. 184, no. 6, pp. 1774–1781,
2005, pMID: 15908529. [Online]. Available: https://doi.org/10.2214/ajr.184.6.01841774
[40] W. Al-Dhabyani, M. Gomaa, H. Khaled, and A. Fahmy, “Dataset of breast ultrasound images,” Data
in Brief, vol. 28, p. 104863, 2020. [Online]. Available: https://www.sciencedirect.com/science/article/pii/
S2352340919312181
[41] K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby, P. Koppel, S. Moore, S. Phillips, D. Maffitt, M. Pringle,
L. Tarbox, and F. Prior, “The cancer imaging archive (tcia): Maintaining and operating a public information
repository,” Journal of Digital Imaging, vol. 26, no. 6, pp. 1045–1057, Dec 2013. [Online]. Available:
https://doi.org/10.1007/s10278-013-9622-7
[42] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, “Grad-cam: Visual explanations
from deep networks via gradient-based localization,” in Proceedings of the IEEE International Conference on
Computer Vision (ICCV), Oct 2017.
[43] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,” in International Conference on Learning
Representations, 2021. [Online]. Available: https://openreview.net/forum?id=St1giarCHLP
[44] C. Lu, Y. Zhou, F. Bao, J. Chen, C. LI, and J. Zhu, “Dpm-solver:
A fast ode solver for diffusion
probabilistic model sampling in around 10 steps,” in Advances in Neural Information Processing Systems,
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35.
Curran Associates,
Inc., 2022, pp. 5775–5787. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2022/file/
260a14acce2a89dad36adc8eefe7c59e-Paper-Conference.pdf
[45] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, “Dpm-solver++: Fast solver for guided sampling of diffusion
probabilistic models,” arXiv preprint arXiv:2211.01095, 2022.
[46] W. Zhao, L. Bai, Y. Rao, J. Zhou, and J. Lu, “Unipc:
A unified predictor-corrector framework for
fast sampling of diffusion models,” in Advances in Neural Information Processing Systems, A. Oh,
T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., vol. 36.
Curran Associates,
Inc., 2023, pp. 49 842–49 869. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2023/file/
9c2aa1e456ea543997f6927295196381-Paper-Conference.pdf
[47] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep unsupervised learning using
nonequilibrium thermodynamics,” in Proceedings of the 32nd International Conference on Machine Learning,
13
A PREPRINT - OCTOBER 24, 2024
ser. Proceedings of Machine Learning Research, F. Bach and D. Blei, Eds., vol. 37.
Lille, France: PMLR, 07–09
Jul 2015, pp. 2256–2265. [Online]. Available: https://proceedings.mlr.press/v37/sohl-dickstein15.html
[48] A. Q. Nichol and P. Dhariwal, “Improved denoising diffusion probabilistic models,” in Proceedings of
the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research,
M. Meila and T. Zhang, Eds., vol. 139.
PMLR, 18–24 Jul 2021, pp. 8162–8171. [Online]. Available:
https://proceedings.mlr.press/v139/nichol21a.html
[49] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, “Score-based generative modeling
through stochastic differential equations,” arXiv preprint arXiv:2011.13456, 2020.
[50] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng, and S. Z. Li, “A survey on generative diffusion models,”
IEEE Transactions on Knowledge and Data Engineering, 2024.
[51] Y. Xu, M. Deng, X. Cheng, Y. Tian, Z. Liu, and T. Jaakkola, “Restart sampling for improving generative processes,”
Advances in Neural Information Processing Systems, vol. 36, pp. 76 806–76 838, 2023.
14
A PREPRINT - OCTOBER 24, 2024
PGDiffSeg model
6.1
Definition of the problem
The Diffusion Probabilistic Models (DPM) technique utilizes a Markov chain to convert a known distribution into a
desired distribution through the forward and reverse processes. Our model is based on the DDPM theory, which can be
found in detail in the works of [47] and [15].
The intermediate process of our model, illustrated in Fig. 1, entails progressively perturbing the segmentation label until
it transforms into a Gaussian distribution, followed by the restoration of the target segmentation from the Gaussian noise.
To precisely identify an image’s lesion area and get the desired outcomes, we incorporate the image as a condition into
the model, thereby generating a corresponding segmentation map.
In the forward process, we add Gaussian noise at each timestep, according to a variance schedule βt, t=1,..., T, as shown
in Equation 11.
q (xt | xt−1) := N

xt;
p
1 −βtxt−1, βtI

(11)
Then distribution in every timestep can be calculated given initial distribution xt, as Equation 12.
q (x1:T | x0) :=
T
Y
t=1
q (xt | xt−1)
(12)
It can be further simplified according to the property of the forward process and get a distribution of arbitrary timesteps
more conveniently. With αt := 1 −βt and ¯αt := Qt
s=1 αs , it can be defined as Equation 13.
q (xt | x0) = N
 xt; √¯αtx0, (1 −¯αt) I

(13)
Therefore we can skip the calculation of the intermediate distribution xt−1 . . . , x1 to obtain the arbitrary required
timestep distribution directly, as Equation 14. For easier understanding, xt can also be represented as[48] :
xt = √¯αtx0 +
√
1 −¯αtε
ε ∼N(0, I)
(14)
In the reverse process, we need to get xt−1 given xt for every timestep t, and this is also what the model needs to learn,
as Equation 15.
pθ (xt−1 | xt) := N
 xt−1; µθ (xt, t) , σ2
t

(15)
Thus every timestep of xt can be captured, so we can further obtain the target sample, as Equation 16.
pθ (x0) := p (xT )
T
Y
t=1
pθ (xt−1 | xt)
(16)
The variance here is still learnable. The relationship between the two extreme choices of variance βt and ˜βt =
1−¯α−
t−1
1−¯αt βt
had similar results. [15] also indicates that it is better to use a fixed variance because learning reverse process variance
can lead to instability in the training process and poor sample quality. So we refer to the DDPM setting that the
variation of βt can be linear, from β1=1×10-4 to βT =2×10-2. Following [15], we can also parameterize εθ instead of µt
according to Equation 17.
µθ (xt, t) =
1
√αt

xt −
βt
√1 −¯αt
εθ (xt, t)

(17)
and the sample xt−1 ∼pθ (xt−1 | xt) can be expressed as Equation 18.
xt−1 = 1
√αt

xt −
βt
√1 −¯αt
εθ (xt, t)

+ σtz,
where
z ∼N(0, I)
(18)
Our choice is consistent with DDPM that predicts the noise ε to produce µθ (xt, t) except for using MSE loss to
optimize the model parameters as Equation 19.
loss(x, y) = 1
n
X
(xi −yi)2
(19)
15
A PREPRINT - OCTOBER 24, 2024
(c)up unit
(a)denoising/condition unit
(b)Bottlenek
GroupNorm
SiLU
conv2D
Up Sample
Time 
Embedding
GroupNorm
c
conv2D
GroupNorm
SiLU
GroupNorm
SiLU
conv2D
GroupNorm
SiLU
conv2D
GroupNorm
SiLU
conv2D
GroupNorm
SiLU
conv2D
Attention
GroupNorm
SiLU
conv2D
GroupNorm
SiLU
conv2D
Down Sample
Time 
Embedding
Figure 12: Three sub-modules in the PGDiffSeg model. "(a) denoising/condition unit" is the details of the module
for denoising flow and condition flow in the feature encoder module, as shown in Fig. 2(b). The denoising unit and
condition unit have the same structure. "(b) Bottlenek" is a detailed presentation of Fig. 2(d). "(c) Up unit" is the
implementation of the component in Fig. 2(e).
6.2
Denoising and condition units
The condition flow uses the extracted image features to transmit spatial features, while the denoising flow uses the
extracted xt features to transmit denoised features. Due to the similar spatial structure between the image and the noisy
label, we designed the same unit for encoding both the image and the noised label, as shown in Fig. 12(a). Inspired
by [15] and 1, we added time embedding to capture the temporal order, allowing multiple time steps of the diffusion
process to share the same set of model parameters.
The denoising and condition units include time embedding, residual blocks, and downsampling. The residual block is
composed of two blocks with a residual connection. The definition of the block is as Equation 20.
H(·) = GroupNorm(SiLU(Conv(·)))
(20)
As shown on the left side of Fig. 2, the denoising flow performs convolution with xt to accomplish denoising. In contrast,
the condition flow performs convolution with the image and holds crucial information for denoising, significantly
impacting the ultimate denoising outcome. Conversely, the denoising flow also carries tumor-related information that
can emphasize the tumor’s location in the condition flow.
6.3
Bottleneck module
The Bottleneck module receives and fuses features from the feature encoding and prior knowledge-guided modules.
This fusion is performed to inject prior tumor information, which guides the learning process of the segmentation task.
The Bottleneck is a vital component in connecting the encoder and decoder, making it indispensable for the model.
As shown in Fig. 12(b), the Bottleneck comprises two residual blocks, as described in Section 6.2. Additionally, we
introduce a self-attention module between these two residual blocks. This inclusion enables improved integration of
high-level semantic features from the encoder and provides the decoder with richer contextual information.
6.4
Feature decoder
Feature decoder plays a critical role in reconstructing tumor details. It converts the high-level feature representations
extracted from the Bottleneck into the output corresponding to the original xt. By leveraging skip connections, it
integrates the abstract features extracted by the encoder to restore the details and structure of the initial input gradually.
1https://github.com/abarankab/DDPM
16
A PREPRINT - OCTOBER 24, 2024
Table 1: Details of the two datasets used in this experiment. Note: the Breast-MRI-NACT-Pilot (NACT for brief)
dataset consists of three-dimensional data, and we slice it into 2D slices to fit our model’s input.
Dataset
Modality
Image size (w×h)
Grayscale range
Total number/slices
Number/slices
(with tumor, no tumor)
Number/slices
(train, val, test)
NACT
MRI, 2D
256×256, 512×512
0-5196
3834
2873/961
(2634,420,780)
BUSI
Ultrasound, 2D
Different sizes
0-255
780
647/133
(545,78,157)
In this part, we refer to the previous work and design the Feature decoding using four up units and a convolutional
block. The structure of the up unit is described in Fig. 12(c). It first encodes the input with timesteps T, then increases
the size of the feature map through upsampling. Next, a skip connection is used with the outputs of the corresponding
units in feature encoding. Finally, it undergoes feature decoding using the residual blocks defined in the Denoising and
Condition Units section. After all the up units, we restore the channel dimensions of the feature map to the original size
through convolutional operations.
Efficient sampling
According to [49], the noise perturbations used in our diffusion model framework (DDPM) can be viewed as dis-
cretizations of a Stochastic Differential Equation (SDE). When DDPM is continuous, it can be formulated as a
Variance-Preserving SDE, meaning that as the number of steps approaches infinity, its variance remains bounded.
Additionally, [49] demonstrates that for all diffusion processes, a deterministic process exists whose trajectories have
the same marginal probability density as the SDE. Consequently, the SDE used for modeling also has an equivalent
Ordinary Differential Equation (ODE) form, known as the probability flow ODE.
Using this equivalent ODE form during sampling typically results in higher sampling efficiency and allows for precise
likelihood computation of the samples generated by the model. Compared to SDEs, probability flow ODEs can be solved
with larger step sizes because they have no randomness [50]. Consequently, many works have leveraged advanced
ODE solvers [43, 44, 45, 46] to achieve faster sampling speeds. These methods can reduce the number of function
evaluations by over 90% compared to the original DDPM sampler, significantly speeding up the sampling process
while still producing high-quality samples. Since ODE samplers introduce less discretization error than SDE samplers
[49, 51], most previous work on accelerating sampling has concentrated on ODEs. While ODE-based samplers are
generally faster, they have reached their performance limits. On the other hand, SDE-based samplers can provide better
sample quality, albeit at slower speeds [50].
Details of datasets
We present some details of the dataset used, including the modalities, image size, grayscale range, the total number of
images, and division of training, validation, and test sets for each dataset, as summarized in Table1.
Sampling process
The sampling process of the proposed GFDiffSeg model is illustrated in Fig13. It can be observed that the target region
gradually emerges from a noise-dominated distribution, indicating the model’s capability to iteratively denoise from a
state of pure Gaussian noise and accurately reconstruct the segmentation results.
17
A PREPRINT - OCTOBER 24, 2024
Image1
Image2
Image3
T=1000
T=850
T=700
T=550
T=400
T=250
T=100
T=1
Ground truth
Image
Image4
Figure 13: Sampling Process of the GFDiffSeg Model, visualized by viridis color mapping between 0 in deep purple
and 1 in bright yellow. The trained model starts sampling from Gaussian noise, and the corresponding labels gradually
emerge from the Gaussian noise under different images as conditions. We selected two representative images from each
dataset. The first two rows are from the Breast-MRI-NACT-Pilot dataset, and the last two are from the BUSI dataset.
18
