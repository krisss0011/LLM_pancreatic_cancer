CT to PET Translation:
A Large-scale Dataset and Domain-Knowledge-Guided Diffusion Approach
Dac Thai Nguyen1, Trung Thanh Nguyen2, Huu Tien Nguyen1, Thanh Trung Nguyen3,
Huy Hieu Pham4, Thanh Hung Nguyen1, Thao Nguyen Truong5, and Phi Le Nguyen1
1Hanoi University of Science and Technology, Vietnam; 2Nagoya Univeristy, Japan
3108 Military Central Hospital, Vietnam; 4VinUniversity, Vietnam
5National Institute of Advanced Industrial Science and Technology, Japan
Abstract
Positron Emission Tomography (PET) and Computed To-
mography (CT) are essential for diagnosing, staging, and
monitoring various diseases, particularly cancer. Despite
their importance, the use of PET/CT systems is limited by
the necessity for radioactive materials, the scarcity of PET
scanners, and the high cost associated with PET imag-
ing. In contrast, CT scanners are more widely available
and significantly less expensive. In response to these chal-
lenges, our study addresses the issue of generating PET im-
ages from CT images, aiming to reduce both the medical
examination cost and the associated health risks for pa-
tients. Our contributions are twofold: First, we introduce
a conditional diffusion model named CPDM, which, to our
knowledge, is one of the initial attempts to employ a diffu-
sion model for translating from CT to PET images. Second,
we provide the largest CT-PET dataset to date, compris-
ing 2,028,628 paired CT-PET images, which facilitates the
training and evaluation of CT-to-PET translation models.
For the CPDM model, we incorporate domain knowledge
to develop two conditional maps: the Attention map and the
Attenuation map. The former helps the diffusion process
focus on areas of interest, while the latter improves PET
data correction and ensures accurate diagnostic informa-
tion. Experimental evaluations across various benchmarks
demonstrate that CPDM surpasses existing methods in gen-
erating high-quality PET images in terms of multiple met-
rics. The source code and data samples are available at
https://github.com/thanhhff/CPDM.
1. Introduction
The combination of PET and CT has markedly enhanced
oncology imaging by integrating PET’s functional imaging
capabilities with the anatomical detail of CT scans. This
integration boosts accuracy in diagnosis, staging, and eval-
BBDM
DDIB
Palette
Pix2Pix
Ground Truth
Generated
Figure 1.
Examples of synthetic PET images produced by
natural image generative models (BBDM [18], DDIB [28],
Palette [26], and Pix2Pix [35]). The generated images do not
accurately replicate the ground truth.
uation of cancer treatments [33, 36]. However, the appli-
cation of PET in medical examination and treatment faces
several key challenges: (1) PET imaging requires the injec-
tion of a small amount of radioactive material, which may
pose health risks [4]; (2) PET scanners are limited in avail-
ability, particularly in underdeveloped regions [2, 6]; and
(3) the high cost of PET imaging imposes a financial bur-
den on patients [4]. In contrast, CT imaging mitigates many
of these issues, primarily because of its lower cost, typi-
cally around half that of PET imaging, thereby reducing the
financial burden on patients. Therefore, leveraging artificial
intelligence to generate PET images from CT scans offers
a viable solution to address the aforementioned challenges.
This approach also holds substantial practical significance
and high applicability for patient care.
The challenge of converting CT to PET images falls within
the domain of Image-to-Image (I2I) translation, a promi-
nent area of research in computer vision. Numerous meth-
ods have been proposed to address this task, with gen-
erative models such as Generative Adversarial Network
(GAN) [12] emerging as the leading approach. Based on
GAN, Pix2Pix [14] made significant strides in I2I transla-
tion tasks with its model consisting of a generator and a
discriminator. Since then, numerous GAN variants, such as
CycleGAN [38], UVCGAN [30], and SelectionGAN [29],
have been introduced. Despite their ability to generate syn-
thetic images that closely resemble real ones, GAN-based
models are often hindered by training instability and sen-
sitivity to hyperparameters. Recently, diffusion model [7]
1
arXiv:2410.21932v1  [eess.IV]  29 Oct 2024
has emerged as a promising alternative, noted for their sta-
bility during training and ability to generate highly realistic
images [8]. Diffusion models have been applied to address
the I2I translation problem in several studies [18, 21, 26].
However, as shown in Figure 1, their research primarily fo-
cuses on natural images, leading to suboptimal performance
when applied to medical images, particularly in the CT-to-
PET translation task. Several studies have concentrated on
addressing the I2I translation problem for medical images,
such as those in [3, 16, 20, 39]. However, current research
encounters certain challenges, including: (1) A consider-
able amount of current I2I translation research in medical
imaging is dedicated to transforming between two akin do-
mains, such as MRI T1 to MRI T2 [16, 34, 37], CBCT to
CT [20]. There is a noticeable shortage of research tack-
ling translation between entirely distinct modalities, espe-
cially, from CT images to PET; (2) The dominant approach
based on generative models entails generating a random im-
age that conforms to the distribution of the target domain.
While these models utilize the source image to guide the
generation process, thereby enhancing the resemblance of
the generated image to the ground truth, they fail to mitigate
the inherent stochasticity of the generated images; (3) While
the natural I2I translation benefits from large datasets [18],
the domain of medical I2I, and CT-to-PET in particular, re-
mains significantly data-constrained. This scarcity of data
has hindered advancement in these fields.
To tackle the aforementioned challenges, this study focuses
on a significant yet underexplored issue: CT-to-PET trans-
lation. Alongside providing an extensive dataset, we aim to
develop a CT-to-PET translation model with the following
objectives:
• Enhancing the deterministic nature of the gener-
ated PET image: Rather than relying on conventional
generative models that aim to generate random images,
we employ a diffusion model directly mapping from
the source domain to the target domain. In essence,
this diffusion model learns a process to directly con-
vert a CT image into a corresponding PET image.
• Improving the accuracy of the generated PET im-
age: We integrate domain-specific knowledge to guide
the diffusion process. Specifically, we utilize two con-
ditional maps, namely the Attention map and the Atten-
uation map, to steer the diffusion process. The former
essentially highlights pixels that are likely to be within
the PET region of interest. Meanwhile, the latter, vital
for accurate PET image quantification, computes pho-
ton attenuation at 511 keV from the CT scan, improv-
ing PET data correction and securing precise diagnos-
tic information.
The main contributions of this paper include:
• We propose CPDM (CT-to-PET Diffusion Model),
a novel approach that leverages a Brownian Bridge
process-based diffusion model to directly synthesize
PET images from CT scans. To refine the quality of the
generated PET images, we guide the diffusion process
with the domain knowledge from our additional con-
ditional maps, i.e., Attention and Attenuation maps.
• We offer a comprehensive large-scale PET/CT med-
ical dataset comprising 2,028,628 paired CT-PET im-
ages. To our knowledge, this represents the largest CT-
PET dataset available. This dataset is curated to sup-
port developing and evaluating advanced medical im-
age translation methods, providing a valuable resource
for the research community.
• We conduct comprehensive experiments to evalu-
ate the performance of CPDM against state-of-the-
art (SOTA) image translation methods.
The results
demonstrate that CPDM outperforms SOTA methods
across multiple metrics for PET image quality.
2. Related Work
In this section, we review existing CT and PET datasets in
Section 2.1, followed by a discussion of recent research on
I2I translation in Section 2.2.
2.1. CT and PET Datasets
The advancement of cancer imaging research has been sig-
nificantly enhanced by the availability of large-scale 3D
PET/CT datasets. The RIDER Lung PET-CT [23] (2015)
and Lung-PET-CT-Dx [19] (2020) datasets, both focus on
lung cancer, provide PET and CT scans that have been in-
strumental in developing multimodal imaging techniques
and machine learning models for lung cancer diagnosis. Ex-
panding the anatomical scope, the Head-Neck-PET-CT [32]
(2017) dataset offers specialized imaging data for head and
neck cancers, while the FDG-PET-CT-Lesions [11] (2022)
dataset includes comprehensive whole-body PET/CT scans
with manual annotations performed using specialized soft-
ware.
These datasets are pivotal in advancing research
on lesion detection and comprehensive cancer diagnostics
across different body regions.
Despite the significant contributions of these datasets, su-
pervised learning tasks such as I2I translation require pre-
cisely paired PET and CT images on a slice-by-slice ba-
sis, a criterion not typically met by existing datasets. Fur-
thermore, generative tasks, including image generation and
translation, generally require extensive datasets to develop
high-quality models, a need that is not adequately fulfilled
by current CT-PET datasets (refer to Table 1).
In this
context, our study contributes a large-scale paired CT-PET
dataset, which is intended not only to advance the CT2PET
2
Table 1. Overview of our dataset and existing CT and PET datasets
(sorted by their published year). Auto-Paired means that corre-
sponding CT and PET slices are automatically paired slice-by-
slice, a feature missing in existing datasets, which only provide
unpaired image series in DICOM format.
Name of Dataset
Auto
Paired Body Parts
Total
Slices
# of
Studies
RIDER Lung PET-CT [23]
✗
Lung
266K
274
Head-Neck-PET-CT [32]
✗
Head & Neck
123K
504
Lung-PET-CT-Dx [19]
✗
Lung
251K
430
FDG-PET-CT-Lesions [11]
✗
Whole body
917K
1,014
Our Dataset
✓
Whole body
2M
3,454
translation problem but also to support research involving
CT, PET, and medical imaging in general.
2.2. Image-to-Image Translation
Generative Models for Natural Images. Generative mod-
els have been crucial in advancing I2I translation tasks, es-
pecially for natural images. Pix2Pix [35] pioneers condi-
tional GANs to translate images from one domain to an-
other with controlled output generation. While this model
demonstrates significant progress, it struggles to produce di-
verse outputs due to its one-to-one mapping approach. Sub-
sequent models, CycleGAN [38] and DRIT++ [17] seek to
address these limitations by enabling unpaired image trans-
lation and generating diverse samples. However, these mod-
els face challenges in training stability and mode collapse,
restricting their performance in complex tasks. More re-
cently, diffusion-based models have emerged as a promis-
ing alternative for image synthesis, offering increased sta-
bility and high-quality outputs. Palette [26] and SDEdit [21]
demonstrate superior performance across various I2I tasks
without requiring task-specific tuning, making them flex-
ible solutions across domains.
Subsequently, LDM [24]
improves efficiency by conducting the diffusion process in
the latent space of pre-trained models. Building on these
advancements, BBDM [18] introduces a novel Brownian
Bridge diffusion process for direct image domain transla-
tion, enhancing stability. However, despite these improve-
ments, most of these models remain focused on natural im-
ages, with limited application to medical imaging.
Medical Image Translation Models. In the medical do-
main, several GAN-based models have been developed for
translating medical images, including PET-CT, MRI, and
CT-to-CT tasks. MedGAN [3] is one such framework that
applies GANs for medical image translation, particularly
in PET-CT translation, demonstrating the utility of GANs
in clinical applications.
Similarly, UP-GAN [38] adopts
an uncertainty-guided progressive learning approach for
translating between medical imaging modalities. Although
these models have made significant strides in medical image
Table 2. Comparison of the proposed CPDM with existing I2I
translation methods. S, R and F stand for the Sharpness, Re-
alism, and Faithfulness of the visual performance, estimated in
three levels: High (H), Medium (M) and Low (L).
Method
External
Knowledge
Medical
I2I
Visual Per.
S
R
F
GAN-based
Pix2Pix [35]
M
M
L
MedGAN [3]
✓
M
M
M
UP-GAN [38]
✓
M
M
M
FCN-cGAN [5]
✓
M
M
M
Diffusion-based
Palette [26]
H
H
L
DDIB [28]
H
H
L
SDEdit [21]
H
H
L
LDM [24]
H
H
L
BBDM [18]
H
H
L
CPDM (Ours)
✓
✓
H
H
H
translation, they are often limited by their reliance on paired
data or struggle with generating high-fidelity images useful
for diagnostic purposes. In contrast, diffusion-based mod-
els have shown promise for overcoming the limitations of
GANs in medical image translation. SynDiff [40] utilizes a
conditional diffusion process to generate high-quality medi-
cal images. However, its reliance on cycle-consistent archi-
tecture for training still poses inherent limitations, particu-
larly in generating diagnostically relevant outputs.
To address current challenges, we propose CPDM, a novel
diffusion-based method designed for CT2PET translation
tasks. Unlike existing methods, CPDM leverages a Brown-
ian Bridge process to directly synthesize PET images from
CT images by incorporating domain-specific knowledge to
guide the diffusion process, enhancing the quality and clin-
ical relevance of the results. We compare CPDM with ex-
isting I2I translation methods in Table 2.
3. Our Proposed PET/CT Dataset
Full Dataset.
We present a comprehensive large-scale
PET/CT dataset collected from hospitals. The dataset con-
sists of 2,028,628 paired CT-PET images from studies of
3,454 patients, designed to cover a wide range of anatom-
ical regions. To ensure compliance with privacy and ethi-
cal guidelines, we remove all pathological labels rigorously.
Each study includes approximately 250-500 paired CT and
PET slices from the top of the head to the upper thigh re-
gion, above the knees. The images are stored in DICOM
format, encapsulating pixel data and relevant metadata, in-
cluding patient age (in years), sex, body weight, injected
radiotracer activity, and other acquisition parameters. Im-
portantly, the PET images have undergone attenuation cor-
rection using the corresponding CT data. The images were
acquired using the GE Discovery 710 PET/CT and GE Dis-
3
Table 3. Detailed statistics of our large-scale CT-PET dataset.
General Information
CT Information
PET Information
Total CT-PET slices
2,028,628
Resolution (per image)
512 x 512 x 1
Resolution (per image)
256 x 256 x 1
Total studies
3,454
Tube voltage (kVp)
120 / 140
Radioactive tracer
18F-FDG
Total pairs per study
250-500
Slice thickness (mm)
3.75 / 5
Slice thickness (mm)
3.27
Body parts
Whole body
Slope coefficient
1.0
Attenuation correction
CT-based
PET/CT system
GE Discovery 710 / STE
Intercept coefficient
-1024.0
Uptake time (minutes)
60
‘
covery STE PET/CT systems, with acquisition parameters
defined by factors such as Kilovoltage peak (kVp), slope
coefficient, and intercept coefficient. The dataset contains
both diseased and non-diseased cases, providing a diverse
representation of clinical scenarios. Table 3 provides the
detailed statistics of this dataset.
Experimental Dataset. Due to computational constraints,
we limit our experiments (in Section 5) to a subset of the
complete dataset. Specifically, we derive a sub-dataset of
30000 CT-PET images (15000 pairs), corresponding to 598
studies, by randomly selecting 25-30 paired slices per study.
The experimental dataset is organized with sequential in-
dexing at the patient level, i.e., images with consecutive
indices correspond to contiguous anatomical sections from
the same individual. The dataset is subsequently stratified
into training, validation, and test sets using an 80:10:10
split. That is, we use 80% of the studies allocated to the
training set and the remaining 20% of data for the validation
and test sets. Consequently, the test set contains entirely un-
seen data during model training. All image pairs have been
standardized to a resolution of 256×256×1 and normalized
to a range of [−1; 1], based on the maximum pixel intensi-
ties of 211 −1 for CT images and 215 −1 for PET images.
Detailed statistics and analysis of the experiment dataset are
provided in Appendix 4.
4. CPDM: Domain-Knowledge-Guided CT-to-
PET Diffusion Model
4.1. Motivation and Architecture Overview
CT and PET images exhibit distinct characteristics com-
pared to natural images. In these medical images, the ma-
jority of pixels are black, with only a small portion of pix-
els containing the critical non-black pixels that convey the
most essential information. Specifically, in PET images, the
intensely bright areas (High-SUV regions) are particularly
significant, as they often signal regions with a high like-
lihood of abnormalities. Translating CT images into PET
images, thus, introduces unique challenges not encountered
in natural image translation. First, because the bright re-
gions are the most crucial, the translation process must pri-
oritize these areas rather than distributing focus across the
entire image. Second, the generated image must be suffi-
ciently sharp to emphasize the critical bright regions effec-
tively. Traditional generative models struggle to meet these
specific requirements, as they tend to produce images with
high variability, focusing more on fitting a general distribu-
tion than on achieving precise detail.
To address these challenges, we propose to use condition
maps to guide the diffusion process, facilitating the genera-
tion of images that align with the inherent characteristics of
PET images. For the first challenge, we introduce an atten-
tion map highlighting the regions likely to appear bright in
the PET image. This attention map is derived from the CT
image using a deep neural network trained through super-
vised learning. To tackle the second challenge, we based on
the fact that different body parts absorb radiation at varying
levels. We, thus, employ an attenuation map, a matrix that
represents the absorption levels of radiation (light intensity)
across different body parts The attenuation map can be ac-
curately calculated from the input CT image. Indeed, the
attenuation map is traditionally used to correct PET images,
making it an ideal tool for enhancing image clarity and fo-
cus. Additionally, inspired by [18], we integrate the Brow-
nian Bridge process into our diffusion model for CT2PET
translation. In contrast to existing diffusion methods, the
proposed approach directly establishes a mapping between
the input CT and output PET domains. This direct mapping
eliminates the stochasticity typically associated with gener-
ative models.
Figure 2 illustrates the architecture of the proposed method,
named CPDM. Starting with a CT image from the CT do-
main, we first extract its latent feature LCT. Following this,
the Brownian Bridge process maps LCT to its corresponding
latent representation LCT→PET in the PET domain. During
this process, domain knowledge in the form of Attention
map (Mσ) and Attenuation map (Mµ) are employed. Sub-
sequently, the synthesized PET image is generated by the
decoder. We leverage a pre-trained VQGAN [10] for the
Encoder/Decoder on our PET/CT dataset to enhance learn-
ing efficiency and model generalization.
4.2. Domain Knowledge-Guided Maps
Attention Map (Mσ).
The Attention map identifies
clinical regions of interest in CT images that show in-
4
Encoder
CT 
Latent
PET 
Latent
Pixel Space
Pixel Space
Latent Space (Brownian Bridge)
CT Image
PET Image
Attenuation Map 
Generation
Attentional 
Region Detection
𝑍!
𝑍!"#
𝑍$
𝑍#
⋯
Input/output data stream
Channel-wise concatenation
Decoder
Domain Knowledge
Forward process
Reverse process
𝒟
𝒟
Downsampling
Figure 2. Overview of CPDM. The medical knowledge Attention map and the Attenuation map are used to guide each stage of the
Brownian Bridge diffusion process.
creased 18F-Fluorodeoxyglucose (18F-FDG) and exhibit
higher SUVs uptake in PET images. To generate this map,
we employ the U-Net [25] segmentation model, which is
trained on CT images to distinguish these high-activity tis-
sues, using Dice Loss [22] function to measure accuracy.
PET images are employed to define the ground truth for the
U-Net model by creating segmented masks that highlight
areas of interest based on their brightness. In detail, we per-
form a qualitative assessment of PET images across differ-
ent anatomical regions and utilize a straightforward thresh-
olding approach to sample segmented masks, isolating pix-
els with intensity values that exceed a predefined threshold.
The loss function is defined as follows:
LMσ = 1 −2|Mpre ∩Mgt|
|Mpre| + |Mgt|,
where Mpre denotes the predicted mask from segmentation
model and Mgt represents the ground truth mask. The re-
sulting Attention map Mσ then guides the Brownian Bridge
diffusion process to focus on the highlighted regions when
generating the PET images.
Attenuation Map (Mµ).
In practical settings, PET/CT
scanners typically utilize CT images to correct attenua-
tion, thereby improving the quality of PET emission data.
This crucial process involves converting CT data, acquired
across a broad spectrum of photon energies (approximately
30 to 140 kVp), into attenuation coefficients for PET pho-
ton energies at 511 keV. Inspired by [1], we employ the
tri-linear scaling methodology to calculate the Attenuation
map.
The Linear Attenuation Coefficient (LAC) at 511
keV, denoted as µ, is calculated using the equation µ =
α × HU + β, where α and β are constants derived from
scanner calibration, which are listed in [1], and HU rep-
resents the Hounsfield Units calculated from CT data (i.e.
slope coefficient, intercept coefficient, and X-ray tube po-
tential). The generated Attenuation map Mµ = e−µ is sub-
sequently incorporated into the Brownian Bridge diffusion
process, providing a guidance informed by medical exper-
tise for model refinement.
4.3. Conditional Brownian Bridge Diffusion Pro-
cess
Drawing inspiration from [18], we adopt the Brownian
Bridge process to characterize the probability distribution
throughout the diffusion. This process aims to facilitate a
direct transformation from the source to the target domain.
To enhance the training process’s efficiency, we use an en-
coder and decoder to transform PET and CT images into
latent space vectors before the diffusion process. Denoting
the original PET and CT images as X and Y , and the en-
coder and decoder as E and D, the corresponding latent vec-
tors, x and y, are obtained as x := E(X) and y := E(Y ).
During the training phase, CPDM learns a mapping from y
to x in the latent space through a Brownian Bridge process.
At the end of the inference phase, the translated PET image
˜X is generated by the decoder of the pre-trained VQGAN
as ˜X := D(˜x), where ˜x is the final latent state of the re-
verse diffusion process.
Forward Process.
We utilize the formulation proposed
in [18]. Starting from an initial state x0 (i.e., the latent
vector of a PET image) and aiming for a destination state
y (i.e., the latent vector of the corresponding CT image),
the intermediate state xt at timestep t are determined in
discrete form as xt = (1 −mt) x0 + mty + √δtϵt, where
mt = t/T, T represents the total number of steps in the
5
Algorithm 1 CPDM Training Process
1: repeat
2:
Paired data: PET image x0 ∼q(x0), CT image y ∼
q(y); Attention map Mσ; Attenuation map Mµ
3:
Timestep t ∼Uniform(1, . . . , T)
4:
Gaussian noise ϵ ∼N(0, I)
5:
Forward diffusion xt = (1 −mt) x0 + mty + √δtϵ
6:
Take gradient descent:
∇θ
mt (y −x0) + √δtϵ −ϵθ (concat(xt, Mσ, Mµ), t)

1
7: until converged
diffusion process, δt indicates the variance of the Brownian
Bridge, and ϵ depicts a Gaussian noise, i.e., ϵ ∼N(0, I).
The forward process is defined as:
qBB (xt|x0, y) = N(xt; (1 −mt)x0 + mty, δtI) .
(1)
Throughout the training phase, we employ the following
formula to establish the transition probability between two
consecutive steps:
qBB(xt|xt−1, y) = N

xt; 1 −mt
1 −mt−1
xt−1
+

mt −
1 −mt
1 −mt−1
mt−1

y, δt|t−1I

,
with
δt|t−1 = δt −δt−1
(1 −mt)2
(1 −mt−1)2 .
(2)
Reverse Process.
The reverse initiates by conditionally
assigning xT = y. Furthermore, to incorporate the domain
knowledge, we employ the channel-wise concatenation to
integrate the Attention map M σ and the Attenuation map
M µ within the hidden state xt at every timestep t as:
pθ(xt−1|xt, M σ, M µ, y) = N(xt−1; µθ(xt, M σ, M µ, y, t), ˜δtI),
µθ(xt, M σ, M µ, y, t) = cxtxt + cyty
+cϵtϵθ(concat(xt, M σ, M µ), t),
(3)
where µθ(xt, M σ, M µ, y, t) represents the predicted
mean and ˜δt represents the variance of the distribution at
time step t, respectively; cxt, cyt and cϵt are non-trainable
factors derived from mt, mt−1, δt and δt−1:
cxt = δt−1
δt
1 −mt
1 −mt−1
+ δt|t−1
δt
(1 −mt−1) ,
cyt = mt−1 −mt
1 −mt
1 −mt−1
δt−1
δt
,
cϵt = (1 −mt−1) δt|t−1
δt
.
Training Objective. The training process entails optimiz-
ing the disparity between the predicted distribution of the
latent variables and the observed distribution in the forward
diffusion process (see Algorithm 1). This is achieved by
learning the mean value µθ (xt, M σ, M µ, y, t) through a
neural network parameterized by θ, employing maximum
likelihood estimation. In practice, this is accomplished by
Algorithm 2 CPDM Sampling Process
1: Sample conditional input: CT image xT = y ∼q(y);
Attention map Mσ; Attenuation map Mµ
2: for t = T, . . . , 1 do
3:
z ∼N(0, I) if t > 1, else z = 0
4:
xt−1 = cxtxt + cyty −cϵtϵθ (concat(xt, Mσ, Mµ), t) +
p˜δtz
5: end for
6: return x0
minimizing the following Evidence Lower Bound (ELBO):
ELBO = −Eq (DKL (qBB (xT | x0, y) ∥p (xT | M σ, M µ, y))
+
T
X
t=2
DKL (qBB (xt−1 | xt, x0, y) ∥pθ (xt−1 | xt, M σ, M µ, y))
−log pθ (x0 | x1, M σ, M µ, y)) .
By substituting the values of qBB(.) and pθ(.) from
Equations 2 and 3, respectively, we derive the final training
ELBO objective as follows:
Ex0,y,ϵ
h
cϵt
mt (y −x0) +
p
δtϵ −ϵθ (concat(xt, M σ, M µ), t)

1
i
.
Sampling Process. We adopt the basic idea of DDIM (De-
noising Diffusion Implicit Models) [27], where acceleration
is achieved through a non-Markovian process while main-
taining identical marginal distributions as Markovian infer-
ence processes (see Algorithm 2).
5. Performance Evaluation
In this section, we evaluate the performance of CPDM
using our PET/CT dataset.
We compare CPDM against
SOTA image translation methods, including GAN-based
models, i.e., Pix2Pix [14], MedGAN [3], and UP-GAN [31]
and diffusion-based models, i.e., DDIB [28], Palette [26],
SDEdit [21], LDM [24], and BBDM [18] (Section 5.2). We
also provide the ablation studies in Section 5.3.
5.1. Experimental Settings
Evaluation Metrics. Following [3,31], we utilize Learned
Perceptual Image Patch Similarity (LPIPS), Mean Absolute
Error (MAE), Structural Similarity Index Measure (SSIM),
and Peak Signal-to-Noise Ratio (PSNR) metrics to evaluate
the quality of generated PET images (detailed in Appendix
2.3). These metrics ensure that the generated PET images
are accurate and of high visual quality. LPIPS evaluates
perceptual similarity, which is crucial for capturing medical
details. MAE measures the average pixel-wise error, indi-
cating accuracy. SSIM reflects visual quality by assessing
changes in the image structure, whereas PSNR quantifies
pixel-level fidelity.
Models & Hyperparameters. We implement the CPDM
as detailed in Section 4.
The diffusion timesteps of the
6
Table 4. Comparison of CPDM against SOTA methods. The
best and second-based results are highlighted by red and blue,
respectively. Diff. shows the relative performance gaps of CPDM
compared to the nearest methods. ↓/↑means lower/higher is better.
Method
LPIPS ↓
MAE ↓
SSIM ↑
PSNR ↑
Pix2Pix [14]
0.0491
336.47
0.9298
27.52
MedGAN [3]
0.1322
336.97
0.9329
29.27
UP-GAN [31]
0.1082
373.49
0.9337
28.38
DDIB [28]
0.5556
4898.58
0.0343
16.10
Palette [26]
0.3827
4032.23
0.3054
20.92
SDEdit [21]
0.1577
813.65
0.8187
22.85
LDM [24]
0.0492
318.71
0.9317
28.30
BBDM [18]
0.0548
353.66
0.9232
27.56
CPDM (w/o Mσ)
0.0471
305.46
0.9353
28.76
CPDM (w/o Mµ)
0.0523
324.99
0.9277
28.47
CPDM (Ours)
0.0466
284.61
0.9396
29.68
Diff.
-5.1%
-10.7%
+0.6%
+1.4%
Brownian Bridge are set at 1000 for training and reduced to
200 for inference, aiming to balance quality and speed. The
Unet [25], utilizing a pre-trained ResNet50 [13] as its back-
bone, is employed for extracting the attention map. The hy-
perparameters of the baselines remain consistent with those
specified in the original papers (detailed in Appendix 2.2).
We adjust the input and output image channels to 1 and a
batch size to 16 to optimize hardware processing capabili-
ties. We utilize Adam [15] optimizer with an initial learning
rate of 10−4, with the step decay for learning rate schedul-
ing and a batch size of 16 across 200 epochs.
5.2. Comparison with Existing Methods
Table 4 presents a thorough comparison of PET image gen-
eration models, where CPDM stands out by achieving the
best results across all metrics: lowest LPIPS (0.0466), low-
est MAE (284.61), highest SSIM (0.9396), and highest
PSNR (29.68). It demonstrates that CPDM provides supe-
rior perceptual quality and structural fidelity. Other meth-
ods perform well on specific metrics but often underperform
on others. For instance, MedGAN achieves the second-best
PSNR (29.27) but ranks fourth-worst in LPIPS (0.1322),
highlighting its trade-off between pixel-level noise reduc-
tion and perceptual quality. Similarly, UP-GAN performs
well in SSIM but falls behind in LPIPS. Overall, CPDM
offers a more balanced and robust performance across all
metrics. The final row (Diff.) shows CPDM’s improve-
ment over the second-best models, with a 5.1% reduction
in LPIPS, a 10.7% decrease in MAE, a 0.6% increase in
SSIM, and a 1.4% rise in PSNR.
To further investigate the differences between CPDM and
other methods, we visualize several examples in Figure 3.
PET images generated by CPDM exhibit sharper details
than those produced by the other models. As error maps
show, most of CPDM’s pixels appear dark blue, indicat-
ing minimal reconstruction error. In contrast, GAN-based
models, such as MedGAN and UP-GAN, demonstrate rel-
atively good faithfulness to the input data but often suf-
fer from insufficient sharpness and lack of realism in the
generated images. Conversely, diffusion-based models like
LDM, which rely on conditional synthetic generation, pro-
duce images with high sharpness and detailed structures;
however, they exhibit lower faithfulness, struggling to pre-
serve anatomical integrity accurately. BBDM achieves a
more balanced performance, offering a reasonable trade-
off between sharpness, faithfulness, and realism.
How-
ever, it lacks the domain-specific knowledge necessary to
match the anatomical accuracy and structural faithfulness
that CPDM consistently delivers.
It highlights CPDM’s
ability to maintain a superior balance across crucial image
quality attributes, outperforming other methods.
5.3. Ablation Studies
5.3.1
Impacts of Domain Knowledge Guidance
We implement two variants of CPDM, namely CPDM (w/o
Mσ) and CPDM (w/o Mµ), to examine the impacts of the
two proposed conditional maps.
Quantitative Results.
The results in Table 4 show that
both variants of CPDM perform worse than the original
CPDM across all metrics, underscoring the effectiveness
of utilizing both conditional maps. Specifically, omitting
the Attention map Mσ leads to an increase of approxi-
mately 1.1% and 7.3% in LPIPS and MAE, along with
decreases of 0.46% and 3.0% in SSIM and PSNR, respec-
tively. In contrast, removing the Attenuation map Mµ re-
sults in significant increases of 12.2% and 14.2% in LPIPS
and MAE, accompanied by decreases of 1.3% and 4.1% in
SSIM and PSNR, respectively. The results demonstrate that
the Attenuation map Mµ is more critical than the Attention
map Mσ, as its absence causes more substantial degrada-
tion across all metrics.
Qualitative Results. To evaluate the effects of the Atten-
tion map Mσ and the Attenuation map Mµ on features
generated by the diffusion process, we use Principal Com-
ponent Analysis (PCA) [9] to visualize the intermediate
spatial features throughout the denoising network ϵθ. Fig-
ure 4 shows that the BBDM model spreads features into
the surrounding background rather than focusing on patient
anatomy. In contrast, the proposed CPDM effectively inte-
grates both two condition maps, achieving focused attention
on regions of interest and meaningful anatomical structures.
The CPDM without Mσ (second row) shows an enhanced
focus on anatomical features during the diffusion process,
particularly at intermediate timesteps (t ≈T/2), where it
maintains consistency even in high noise conditions. Mean-
while, CPDM without Mµ (third row) demonstrates that
7
Pix2Pix
MedGAN
UP-GAN
BBDM
CPDM
CT
Generated PET
Error Map
PET
CT
PET
Generated PET
Error Map
1
0
LDM
Figure 3. Visualization of PET images generated by the best-performing methods. Errors
induced by PET produced by CPDM are significantly more minor than other methods.
CPDM
⋯
⋯
CPDM
(w/o 𝑀!)
⋯
⋯
⋯
⋯
BBDM
⋯
⋯
CPDM
(w/o 𝑀")
CT
PET
Early step
Middle step
Final step
Figure 4. Visualization of spatial features
extracted from the denoising network ϵθ.
Table 5. Impact of the our conditional maps on existing methods.
Method
LPIPS ↓
MAE ↓
SSIM ↑
PSNR ↑
Pix2Pix
Original
0.0491
336.47
0.9298
27.52
w/ (Mσ, Mµ)
0.0470
305.33
0.9345
28.49
Diff.
-4.3%
-9.3%
+0.50%
+3.4%
MedGAN
Original
0.1322
336.97
0.9329
29.27
w/ (Mσ, Mµ)
0.1158
294.05
0.9325
29.73
Diff.
-12.4%
-12.7%
-0.04%
+1.5%
UP-GAN
Original
0.1082
373.49
0.9337
28.38
w/ (Mσ, Mµ)
0.1001
301.67
0.9296
29.99
Diff.
-7.5%
-19.2%
-0.44%
+5.7%
LDM
Original
0.0492
318.71
0.9317
28.30
w/ (Mσ, Mµ)
0.0468
302.70
0.9348
28.92
Diff.
-4.9%
-8.2%
+0.33%
+2.2%
CPDM (Ours)
0.0466
284.61
0.9396
29.68
the Attention map helps ϵθ focus on significant features re-
lated to regions of interest, maintaining this focus even at
intermediate timesteps with minimal information.
5.3.2
Robustness of the Condition Maps
In this section, we evaluate the robustness of the pro-
posed condition maps, Mσ, and Mµ, by integrating them
into various I2I models (i.e., Pix2Pix, MedGAN, UP-GAN,
and LDM) and analyzing their impact on performance en-
hancement. Specifically, we developed a conditional vari-
ant for each model that incorporates the proposed maps to
guide the generative process, embedding domain-specific
knowledge to refine model outputs. Table 5 presents quan-
titative results demonstrating that integrating our condi-
tional maps improves the performance of the original meth-
ods. The inclusion of Mσ and Mµ leads to significant en-
hancements in LPIPS and MAE, with UP-GAN achieving
gains of 7.5% and 19.2%, respectively. The slight reduc-
tion in MAE can be attributed to the conditional maps,
which provide additional guidance during generation and
effectively reduce pixel-wise discrepancies by embedding
domain-specific information. Despite these improvements,
UP-GAN and MedGAN still exhibit considerably higher
LPIPS than CPDM, indicating lower perceptual quality.
The conditional maps encourage these models to focus on
faithfulness, resulting in smoother images with fewer pixel-
level discrepancies but diminished sharpness and detail.
This trade-off likely explains the elevated PSNR and slight
decline in SSIM observed in UP-GAN and MedGAN, as
the generated images prioritize smoothness at the expense
of visual fidelity. In contrast, LDM, leveraging the grad-
ual refinement process typical of diffusion models, shows
more balanced improvements across all metrics by focus-
ing on local and global consistency. Nonetheless, CPDM
surpasses LDM by achieving a superior balance between
sharpness, realism, and faithfulness, preserving finer details
while delivering better overall performance.
6. Conclusion
This study proposed CPDM, a novel method for synthe-
sizing PET images from CT images using a diffusion pro-
cess.
CPDM utilized a Brownian Bridge diffusion pro-
cess to translate data directly from the CT domain to the
PET domain, mitigating the inherent stochastic character-
istics often present in generative models.
Furthermore,
CPDM integrated Attention and Attenuation maps, result-
ing in PET images with significantly improved quality and
reduced error compared to existing methods. The quantita-
8
tive and qualitative results demonstrated the superiority of
CPDM over existing GAN-based and diffusion-based meth-
ods across various evaluation criteria.
For future work, we plan to train and evaluate CPDM on
the full dataset and collaborate with medical professionals
to assess the clinical performance and diagnostic utility of
the PET images generated by our approach.
Acknowledgment
This work was funded by Vingroup Joint Stock Com-
pany (Vingroup JSC), Vingroup, and supported by Vin-
group Innovation Foundation (VINIF) under project code
VINIF.2021.DA00128.
References
[1] Monica Abella, Adam M Alessio, David A Mankoff,
Lawrence R MacDonald, Juan Jose Vaquero, Manuel De-
sco, and Paul E Kinahan. Accuracy of CT-based attenuation
correction in PET/CT bone imaging. Physics in Medicine &
Biology, 57(9):2477, 2012. 5
[2] Suhaib Alameen, Nissren Tamam, Sami Awadain, Abdel-
moneim Sulieman, Latifa Alkhaldi, and Amira Ben Hmed.
Radiobiological risks in terms of effective dose and organ
dose from 18f-fdg whole-body pet/ct procedures.
Saudi
Journal of Biological Sciences, 28:5947–5951, 2021. 1
[3] Karim Armanious, Chenming Jiang, Marc Fischer, Thomas
K¨ustner, Tobias Hepp, Konstantin Nikolaou, Sergios Ga-
tidis, and Bin Yang.
Medgan: Medical image translation
using GANs. Computerized Medical Imaging and Graphics,
79:101684, 2020. 2, 3, 6, 7
[4] Avi Ben-Cohen, Eyal Klang, Stephen P. Raskin, Michal Mar-
ianne Amitai, and Hayit Greenspan.
Virtual PET Images
from CT Data Using Deep Convolutional Networks: Ini-
tial Results, page 49–57. Springer International Publishing,
2017. 1
[5] Avi Ben-Cohen, Eyal Klang, Stephen P. Raskin, Shelly Sof-
fer, Simona Ben-Haim, Eli Konen, Michal Marianne Amitai,
and Hayit Greenspan. Cross-modality synthesis from ct to
pet using fcn and gan networks for improved automated le-
sion detection, 2018. 3
[6] Simon R Cherry, Terry Jones, Joel S Karp, Jinyi Qi,
William W Moses, and Ramsey D Badawi. Total-body PET:
maximizing sensitivity to create new opportunities for clini-
cal research and patient care. Journal of Nuclear Medicine,
59(1):3–12, 2018. 1
[7] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,
and Mubarak Shah. Diffusion models in vision: A survey.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2023. 1
[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat GANs on image synthesis. Advances in Neural Infor-
mation Processing Systems, 34:8780–8794, 2021. 2
[9] George H Dunteman. Principal components analysis, vol-
ume 69. Sage, 1989. 7
[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis.
In Pro-
ceedings of the 2021 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 12873–12883, 2021.
4
[11] S. Gatidis and T. Kuestner. A whole-body fdg-pet/ct dataset
with manually annotated tumor lesions (fdg-pet-ct-lesions),
2022. Dataset. 2, 3
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
Neural Information Processing Systems, 27, 2014. 1
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 7
[14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adversar-
ial networks. In Proceedings of the 2017 IEEE Conference
on Computer Vision and Pattern Recognition, pages 1125–
1134, 2017. 1, 6, 7
[15] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
Computing Research Repository
arXiv Preprints arXiv:1412.6980, 2014. 7
[16] Lingke Kong, Chenyu Lian, Detian Huang, Yanle Hu,
Qichao Zhou, et al. Breaking the dilemma of medical image-
to-image translation. Advances in Neural Information Pro-
cessing Systems, 34:1964–1978, 2021. 2
[17] Hsin-Ying Lee, Hung-Yu Tseng, Qi Mao, Jia-Bin Huang,
Yu-Ding Lu, Maneesh Singh, and Ming-Hsuan Yang.
Drit++: Diverse image-to-image translation via disentangled
representations, 2019. 3
[18] Bo Li, Kaitao Xue, Bin Liu, and Yu-Kun Lai.
BBDM:
Image-to-image translation with brownian bridge diffusion
models. In Proceedings of the 2023 IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 1952–
1961, 2023. 1, 2, 3, 4, 5, 6, 7
[19] P. Li, S. Wang, T. Li, J. Lu, Y. HuangFu, and D. Wang. A
large-scale ct and pet/ct dataset for lung cancer diagnosis
(lung-pet-ct-dx), 2020. Data set. 2, 3
[20] Zhiwen Liang, Hui Wei, Gang Liu, Mengjie Cheng, Jiaqi
Gao, Song Li, and Xin Tian. Leveraging GAN-based CBCT-
to-CT translation models for enhanced image quality and
accurate photon and proton dose calculation in adaptive ra-
diotherapy. Journal of Radiation Research and Applied Sci-
ences, 17(1):100809, 2024. 2
[21] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. arXiv preprint arXiv:2108.01073, 2021. 2, 3, 6, 7
[22] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In Proceedings of the 4th In-
ternational Conference on 3D Vision, pages 565–571, 2016.
5
[23] P. Muzi, M. Wanner, and P. Kinahan. Data from rider lung
pet-ct, 2015. Dataset. 2, 3
9
[24] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer.
High-resolution image
synthesis with latent diffusion models.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684–10695, 2022. 3, 6, 7
[25] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
Net: Convolutional networks for biomedical image segmen-
tation. In Proceedings of the 18th International Conference
on Medical Image Computing and Computer-Assisted Inter-
vention, pages 234–241, 2015. 5, 7
[26] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi.
Palette: Image-to-image diffusion models.
In
Proceedings of the 2022 ACM Special Interest Group on
Computer Graphics and Interactive Techniques Conference,
pages 1–10, 2022. 1, 2, 3, 6, 7
[27] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. Computing Research Reposi-
tory arXiv Preprints arXiv:2010.02502, 2020. 6
[28] Xuan Su, Jiaming Song, Chenlin Meng, and Stefano Er-
mon.
Dual diffusion implicit bridges for image-to-image
translation. Computing Research Repository arXiv Preprints
arXiv:2203.08382, 2022. 1, 3, 6, 7
[29] Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J Corso,
and Yan Yan. Multi-channel attention selection GAN with
cascaded semantic guidance for cross-view image transla-
tion.
In Proceedings of the 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 2417–
2426, 2019. 1
[30] Dmitrii Torbunov, Yi Huang, Haiwang Yu, Jin Huang, Shin-
jae Yoo, Meifeng Lin, Brett Viren, and Yihui Ren.
Uvc-
gan: Unet vision transformer cycle-consistent GAN for un-
paired image-to-image translation.
In Proceedings of the
2023 IEEE/CVF Winter Conference on Applications of Com-
puter Vision, pages 702–712, 2023. 1
[31] Uddeshya Upadhyay, Yanbei Chen, Tobias Hepp, Sergios
Gatidis, and Zeynep Akata. Uncertainty-guided progressive
GANs for medical image translation. In Proceedings of the
24th Internation Conference on Medical Image Computing
and Computer Assisted Intervention, pages 614–624, 2021.
6, 7
[32] Martin Valli`eres, Emily Kay-Rivest, L´eo Jean Perrin, Xavier
Liem, Christophe Furstoss, Nader Khaouam, Phuc F´elix
Nguyen-Tan, Chang-Shu Wang, and Khalil Sultanem. Data
from head-neck-pet-ct, 2017. Dataset. 2, 3
[33] Srinivasan Vijayakumar, Johnny Yang, Mary R Nittala,
Alexander E Velazquez, Brandon L Huddleston, Nickhil A
Rugnath, Neha Adari, Abhay K Yajurvedi, Abhinav Ko-
manduri, Claus Chunli Yang, N Duggar William, P Berlin
William, Duszak Richard, and Vijayakumar Vani. Changing
role of PET/CT in cancer care with a focus on radiotherapy.
Cureus, 14(12), 2022. 1
[34] Clinton J Wang, Natalia S Rost, and Polina Golland. Spatial-
intensity transforms for medical image-to-image translation.
IEEE Transactions on Medical Imaging, 2023. 2
[35] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro.
High-resolution image
synthesis and semantic manipulation with conditional gans,
2018. 1, 3
[36] Lise Wei and Issam El Naqa. Artificial intelligence for re-
sponse evaluation with PET/CT.
In Seminars in Nuclear
Medicine, volume 51, pages 157–169, 2021. 1
[37] Qianye Yang, Nannan Li, Zixu Zhao, Xingyu Fan, Eric I-
Chao Chang, and Yan Xu.
Mri cross-modality image-to-
image translation. Scientific reports, 10(1):3753, 2020. 2
[38] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros.
Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of the 2017
IEEE International Conference on Computer Vision, pages
2223–2232, 2017. 1, 3
[39] Yingying Zhu, Youbao Tang, Yuxing Tang, Daniel C El-
ton, Sungwon Lee, Perry J Pickhardt, and Ronald M Sum-
mers. Cross-domain medical image translation by shared la-
tent gaussian mixture model. In Medical Image Computing
and Computer Assisted Intervention–MICCAI 2020: 23rd
International Conference, Lima, Peru, October 4–8, 2020,
Proceedings, Part II 23, pages 379–389. Springer, 2020. 2
[40] Muzaffer ¨Ozbey, Onat Dalmaz, Salman UH Dar, Hasan A
Bedel, S¸aban ¨Ozturk, Alper G¨ung¨or, and Tolga C¸ ukur. Un-
supervised medical image translation with adversarial diffu-
sion models, 2023. 3
10
