Medical Image Analysis (2024)
Contents lists available at ScienceDirect
Medical Image Analysis
journal homepage: www.elsevier.com/locate/media
Diagnostic Text-guided Representation Learning in Hierarchical Classification for
Pathological Whole Slide Image
Jiawen Lia,c, Qiehe Suna,c, Renao Yana, Yizhi Wanga,c, Yuqiu Fua, Yani Weib, Tian Guana, Huijuan Shib,∗, Yonghong Hea,c,∗, Anjia
Hanb,∗
aShenzhen International Graduate School, Tsinghua University, Shenzhen 518055, China
bDepartment of Pathology, The First Affiliated Hospital of Sun Yat-sen University, Guangzhou 510080, China
cMedical Optical Technology R&D Center, Research Institute of Tsinghua, Pearl River Delta, Guangzhou 510700, China
A R T I C L E I N F O
Article history:
2000 MSC:
41A05,
41A10,
65D05,
65D17 Whole slide image, representa-
tion learning, hierarchical classification,
diagnostic text
A B S T R A C T
With the development of digital imaging in medical microscopy, artificial intelligent-
based analysis of pathological whole slide images (WSIs) provides a powerful tool for
cancer diagnosis. Limited by the expensive cost of pixel-level annotation, current re-
search primarily focuses on representation learning with slide-level labels, showing suc-
cess in various downstream tasks. However, given the diversity of lesion types and the
complex relationships between each other, these techniques still deserve further explo-
ration in addressing advanced pathology tasks. To this end, we introduce the concept
of hierarchical pathological image classification and propose a representation learning
called PathTree. PathTree considers the multi-classification of diseases as a binary tree
structure. Each category is represented as a professional pathological text description,
which messages information with a tree-like encoder. The interactive text features are
then used to guide the aggregation of hierarchical multiple representations. PathTree
uses slide-text similarity to obtain probability scores and introduces two extra tree-
specific losses to further constrain the association between texts and slides. Through
extensive experiments on three challenging hierarchical classification datasets: in-house
cryosectioned lung tissue lesion identification, public prostate cancer grade assessment,
and public breast cancer subtyping, our proposed PathTree is consistently competitive
compared to the state-of-the-art methods and provides a new perspective on the deep
learning-assisted solution for more complex WSI classification.
© 2024 Elsevier B. V. All rights reserved.
1. Introduction
Modern computational imaging technology has promoted the
transformation from the traditional microscope-based to the
digital imaging analysis paradigm in pathology (Niazi et al.
2019; Song et al. 2023). Through advanced optical imaging
systems, pathological tissue sections are rapidly converted into
∗Corresponding author
e-mail: shihj@mail.sysu.edu.cn, heyh@sz.tsinghua.edu.cn,
hananjia@mail.sysu.edu.cn (Anjia Han)
gigapixel-level WSIs, preserving the entire tissue structure and
lifting the limitations imposed by the fixed field of view during
the examination. The emergence of WSIs improves the work
efficiency of pathologists and promotes the application of com-
puter vision technology in diagnosing pathology (Li et al. 2022;
Perez-Lopez et al. 2024).
With the efficient utilization of clinical data and computa-
tional resources, extensive research has been conducted based
on deep learning for WSI analysis (Rodriguez et al. 2022; Song
et al. 2023). Due to the considerable size characteristic, existing
high-performance models designed for analyzing much smaller
arXiv:2411.10709v1  [cs.CV]  16 Nov 2024
2
Jiawen Li et al. / Medical Image Analysis (2024)
natural images cannot be directly transferred to high-resolution
pathological images.
Previous studies are mainly based on
patch-level learning (Wang et al. 2016; Coudray et al. 2018;
Yang et al. 2021; Li et al. 2023a), which follows the pipeline as
such. Starting by segmenting the WSI foreground into several
patches with appropriate sizes, each patch is assigned a manual
label to train an encoder and predictor. However, limited by the
challenge of acquiring large-scale pixel-level manual annota-
tions with domain-specific expertise, research has shifted from
original patch-level supervision toward slide-level representa-
tion learning (Campanella et al. 2019; Lu et al. 2021b; Song
et al. 2024; Jaume et al. 2024). These methods use a pre-trained
feature extractor to obtain local embeddings and then integrate
them through a learnable aggregation module to serve as the
WSI feature. Such a global representation can accomplish the
target optimization through different slide-level supervision sig-
nals.
Current WSI representation learning has demonstrated suc-
cess in common slide-level tasks.
For example, algorithms
based on multiple instance learning (Li et al. 2021; Zhang et al.
2022; Xiang and Zhang 2022), graph representation (Guan et al.
2022; Chen et al. 2021c; Li et al. 2024b), transformer (Chen
et al. 2022; Shao et al. 2021; Xiong et al. 2023) and their vari-
ants (Yang et al. 2024; Chu et al. 2024) show excellent per-
formance in tasks such as cancer detection for sentinel lymph
nodes (Ehteshami Bejnordi et al. 2017; Litjens et al. 2018;
B´andi et al. 2018) and non-small cell lung cancer subtyping.
However, there are three notable points regarding these tasks.
First, the number of categories involved in these classification
tasks is typically small, usually limited to two or three cate-
gories.
Second, there is a marked distinction between cate-
gories, such as non-cancerous versus cancerous tissues or well-
differentiated adenocarcinoma versus squamous cell carcinoma.
Third, the task difficulty is relatively low. These datasets repre-
sent fundamental issues in pathology, often encountered in the
routine and repetitive work of pathologists.
For challenging pathology tasks such as judging the inva-
siveness of cancer or detailed subtyping related to prognosis
(Echle et al. 2021), the high complexity of lesion morphol-
ogy and the variability in subjective judgments among different
pathologists make it struggle to extract meaningful representa-
tions from such highly heterogeneous data. To illustrate this
heterogeneity, pathologists use hierarchical structures to distin-
guish categories step by step (Kundra et al. 2021), but existing
models do not take this into account. In addition, the higher
the difficulty of the pathology task is, the more difficult it will
be for a simple text word or phrase to describe a category ac-
curately. Therefore, richer text information is needed to ex-
plain the observed diagnostic phenomenon (Zhang et al. 2019),
thereby guiding and optimizing the WSI representation with ad-
equate information. Although a few studies explore the impact
of pathological text on regions of interest or WSI diagnostic
analysis (Li et al. 2024a; Zhang et al. 2023; Shi et al. 2024;
Wang et al. 2023b), they have yet to conduct in-depth discus-
sions on the complex pathological context.
To overcome these challenges, inspired by the authorita-
tive diagnostic strategies of pathologists, we transform the
challenging pathological multi-classification issues into hierar-
chical classification tasks with binary tree structures and de-
sign a hierarchical WSI representation learning method called
PathTree. Specifically, we describe each category (including
coarse-grained and fine-grained nodes in the tree) with profes-
sional pathological terminology. Then, these texts are encoded
into high-dimensional embeddings and passed through a tree-
like graph to exchange messages. For image modality, PathTree
aggregates the patch embeddings into multiple slide-level em-
beddings. These embeddings are passed into the root node ac-
cording to the tree path, guided by the text embeddings. Unlike
the conventional linear classifier, PathTree obtains the predic-
tion scores based on slide-text similarity and introduces two ex-
tra tree-like aware losses to capture the semantic matching rela-
tionship between hierarchical text embeddings. To validate the
superiority of our proposed PathTree, we conduct extensive ex-
periments on three large-scale hierarchical datasets, including
internal cryosectioned lung tissue identification, public prostate
cancer grading, and public breast cancer subtyping. The re-
sults demonstrate the consistently outstanding performance of
PathTree. Our proposed hierarchical classification concept and
method provide more objective and precise assistance in chal-
lenging pathological tasks.
Our contributions can be summarized as follows.
1) We describe the challenging WSI multi-classification as
the hierarchical tasks with binary tree structure and pro-
pose PathTree to represent and analyze WSIs.
2) We utilize the textual modality to guide the formation of
slide features and measure the potential semantic relation-
ship between hierarchical texts.
3) We design and conduct experiments on three hierarchical
pathological WSI classification datasets. The results show
the effectiveness of our proposed method. Our code will
be available later.
2. Related Work
2.1. Representation learning of WSIs
The multi-level resolution and high heterogeneity make it
difficult for deep neural networks to extract WSI representa-
tions. Some works are denoted to designing directly trainable
vision models to obtain thumbnail representations. For exam-
ple, Pinckaers et al. (2020) combines forward and backward
propagation with gradient checkpointing, allowing WSIs of any
size into convolutional neural networks (CNNs). Chen et al.
(2021a) incorporates the unified memory mechanism and GPU
memory optimization techniques to facilitate the entire WSI
into CNNs. Xiang et al. (2022) utilizes multi-scale thumbnails
and neural-encoded embeddings to achieve dual-stream infor-
mation transmission of features. Wang et al. (2023a) designs
LongViT to capture both short-range and long-range dependen-
cies and generate representations of high-resolution thumbnails.
Beyond end-to-end methods, more researches focus on ob-
taining global representations from patch-level embeddings,
Jiawen Li et al. / Medical Image Analysis (2024)
3
and multiple instance learning (MIL) is one such widely dis-
cussed approach (Laleh et al. 2022). It treats cropped patches
as instances, the WSI as a bag, and generates bag-level fea-
tures by aggregating the instance-level scores or embeddings.
ABMIL proposed by Ilse et al. (2018) is one of the most com-
mon MIL approaches, where each instance generates an atten-
tion score based on its feature information, and the bag repre-
sentation is obtained by weighting the features of all instances.
Building on this, variants of ABMIL, such as CLAM (Lu et al.
2021b), which utilizes clustering constraints, DSMIL (Li et al.
2021), which employs the multi-scale pyramid for feature fu-
sion, TransMIL (Shao et al. (2021)), which uses the correlated
instance update strategy and so on (Zhang et al. 2022; Sun et al.
2024; Xiang and Zhang 2022), have recently been extensively
explored and seen numerous successful applications (Lu et al.
2021a; Fremond et al. 2023; Wagner et al. 2023).
MIL-based approaches predominantly focus on the instances
themselves but struggle to capture the interactions between
these instances. Conversely, graph representation methods treat
the WSI as a graph and consider cells, tissues, or patches as
nodes to convey local context between entities.
For exam-
ple, Chen et al. (2021c) introduces Patch-GCN and utilizes it
to model the significant morphological feature interactions be-
tween cell identities and tissue types to generate global WSI
representations. Li et al. (2023b); Di et al. (2022) obtains a
high-order global representation of WSIs via multilateral corre-
lation and a hypergraph convolution network. Chan et al. (2023)
designs a heterogeneous graph for WSI analysis by utilizing the
relations between different types of nuclei. Li et al. (2024b)
models the patch-level inter-entity correlations in WSI as di-
rected knowledge graphs for cancer classification and staging.
Finally, WSIs can be modeled and updated as sequence rep-
resentations with Transformer (Vaswani et al. 2017). Huang
et al. (2021) combines self-supervised learning with Trans-
formers to achieve slide-level survival prediction. Zheng et al.
(2022) brings the graph-transformer into the computational
pathology and successfully classifies WSIs of lung tissues.
Chen et al. (2022) designs a hierarchical image pyramid trans-
former to model representations from cellular spaces to tissue
microenvironment. Furthermore, Chen et al. (2021d) also joint-
learn WSIs and genomic modalities to design a co-attention
transformer that represents survival status.
Unlike the above methods, PathTree directly models the in-
terrelationships between pathological morphologies and utilizes
diagnostic text to guide and optimize the global representation.
2.2. Hierarchical classification of pathological WSIs
Previous WSI analysis studies have mainly been conducted
on foundational pathology classification tasks. For example, the
Camelyon16 dataset for detection of breast cancer lymph node
metastasis (Ehteshami Bejnordi et al. 2017; B´andi et al. 2018;
Litjens et al. 2018), the TCGA-NSCLC for non-small cell lung
cancer, and the TCGA-RCC dataset for renal cell carcinoma
(Linehan and Ricketts 2019). However, these binary or ternary
disease-state prediction tasks hardly reflect the broader detailed
lesions observed in real-world pathological practice (Chen et al.
2024). Inspired by the decision-making processes of pathol-
1
2
3
7
AIS
AAH
MIA
NOR
9
8
7
6
5
1
2
. . .
Neoplastic
Glandular
Non-neoplastic
SCC
AIS
AAH
Whole Slide Image (WSI)
Tree-like Hierarchical Classification
(Ours)
Planar Classification
(Existing Methods)
. . .
NOR
PNE
10
Label: Adenocarcinoma in Situ (AIS) 
Fig. 1. The existing WSI multi-classification method is described as a pla-
nar classification problem, which treats each category independently and
equally. However, the interrelations among various categories exhibit con-
siderable complexity in real-world pathological contexts. Pathologists typ-
ically analyze from coarse-grained to fine-grained levels, following a hier-
archical, tree-like relationship among classes. This approach enhances the
efficiency and precision of diagnostics and helps pathologists systematically
understand and interpret complex pathological information, making more
accurate medical decisions.
ogists during clinical practice and the OncoTree cancer clas-
sification system (Kundra et al. 2021), we consider challeng-
ing pathological diagnostic issues and model them as hierarchi-
cal classification tasks. Fig.1 displays the formal differences
between planar and hierarchical tree-like analysis structures.
Compared to planar methods, tree structures can describe cat-
egories from the coarse-grained macro to the fine-grained mi-
cro level, for example, from the primary type (such as glan-
dular or squamous) to the degree of cell infiltration (such as
non-infiltrated or thoroughly infiltrated). Our previous work
(Li et al. 2023a) uses tree-like deep learning architectures to
obtain the representation of pathological regions of interest. In
this paper, we focus on slide-level learning since it is closer
to clinical applications. In summary, hierarchical classification
ensures the comprehensiveness and precision of diagnoses. It
aids in detecting early signs or subtle differences in diseases
and is crucial for formulating prognostic plans.
3. Method
Our method consists of six stages. First, the WSI is cropped
into patches, and tree-like texts are designed to obtain the data
pair. Second, pre-trained encoders are used to extract high-
dimensional representations of patches and texts, and a tree-
like graph neural network is used as a text prompt encoder to
exchange text semantic information. Third, multiple slide-level
embeddings are generated through patch-level attention aggre-
gation, with each embedding representing slide features of one
node. Fourth, correlations between text prompts and slide-level
embeddings are calculated to guide the aggregation of multiple
slide-level embeddings to the root according to the branch path,
thereby obtaining global WSI features. Fifth, path alignment
and tree-aware matching learning loss functions are designed
to constrain the relationship of different text semantics. Finally,
prediction scores are obtained by calculating the slide-text sim-
ilarity. Fig.2 shows the overall process of PathTree.
3.1. Preparation of WSIs and Tree-like Texts
The gigapixel size of WSIs makes it difficult to process the
entire image directly. To preprocess WSIs, we use the Otsu
4
Jiawen Li et al. / Medical Image Analysis (2024)
⋯
No neoplasia lesions
are found.
⋯
⋯
⋯
The lesions are consistent with 
squamous cell carcinoma, 
characterized by … (SCC)
The lesion shows abnormal 
proliferation of glandular 
epithelium, indicative of ...
Patch
encoder
Attention
module
Whole slide image (WSI)
Cropped patches
Patch features
Multiple 
slide-level features
𝐼∈ℝଶேିଵ×ௗ
Text
encoder
𝑖ଵ
ୃ𝑡ଶ
𝑖ଵ
ୃ𝑡ଷ
…
𝑖ଶ
ୃ𝑡ଶ
𝑖ଶ
ୃ𝑡ଷ
…
𝑖ଷ
ୃ𝑡ଵ
𝑖ଷ
ୃ𝑡ଶ
𝑖ଷ
ୃ𝑡ଷ
…
…
…
…
…
…
…
𝑖ଶ
ୃ𝑡ଵ
𝑖ଵ
ୃ𝑡ଵ
𝑖ଶேିଵ
ୃ
𝑡ଷ
𝑖ଶேିଵ
ୃ
𝑡ଶ
𝑖ଶேିଵ
ୃ
𝑡ଵ
𝑖ଵ
ୃ𝑡ଶேିଵ
𝑖ଶ
ୃ𝑡ଶேିଶ
𝑖ଷ
ୃ𝑡ଶேିଷ
𝑖ଵ
𝑖ଶ
𝑖ଷ
…
𝑖ଶேିଵ
𝑡ଵ
𝑡ଶ
𝑡ଷ
…
𝑡ଶேିଵ
Softmax for each branch
𝑠ଵ
𝑠ଶேିଵ
𝑠ଶ
…
𝑠ଶேିଶ
…
Text prompt encoder
…
…
…
𝑠ଶேିଷ
𝑠ଶேିଵ
𝑠ଶேିଶ
𝑠ଶேି
𝑠ହ
𝑠଺
…
…
…
Text-guide tree-like aggregator
+
+
+
+
This is a pathological 
lung H&E slide.
AAH
AIS   
SCC   
…
…
NOR
Global WSI 
embedding
…
Path alignment loss
…
…
Target text prompts in the path 
Target slide-level 
embedding
…
…
Tree-aware matching loss
Fine-grained 
text prompt
Parent 
text prompt
Incorrect 
text prompt
…
Sibling 
text prompt
…
…
Slide-text similarity
Fine-grained 
text embeddings
Number of categories: 𝑵
Number of all nodes in the tree: 𝟐𝑵−𝟏
𝑇∈ℝଶேିଵ×ௗ
𝑠ଵ
𝑠ଶ
Multiple gated attention
Multi-head Nystrom
The lesions show neoplastic lesions and 
abnormal growths of …
The lesions show adenocarcinoma in situ, 
characterized by atypical glandular 
epithelial cells confined within ... (AIS)
Fig. 2. Overview of PathTree. The main idea is to convert challenging pathological multi-classification into hierarchical tree structures for analysis.
PathTree uses the WSI to generate multiple slide-level embeddings from patch-level features, allowing them to contrast text semantics. After aggregation
based on the tree path, the relationship between tree-like text semantics is measured by two structure-specific losses, and prediction scores are obtained by
calculating the cosine similarity between slide and fine-grained text embeddings.
method (Otsu 1975) to separate the foreground in the thumbnail
and then perform a sliding window operation at 20× magnifica-
tion to crop non-overlapping patches with 256 × 256 resolu-
tions. Recent contrastive learning (Radford et al. 2021) can ob-
tain well-performing feature extractors by training image-text
pairs, therefore, we use the image encoder of PLIP (Huang
et al. 2023), which is a model pre-trained on more than
200,000 pathological images paired with natural language de-
scriptions, to extract the high-dimensional embedding of each
patch. PathTree prepares text modalities based on specific tasks
in histopathology. Conventional text information only contains
category words or phrases. For the hierarchical tree structure,
there are additional categories on the root and branch nodes. We
provide a detailed description of the pathological morphology
for each node in the tree structure (including the root, branch,
and leaf nodes), which at least two pathology experts verify.
Fig.3 shows examples of the difference between the conven-
tional planar text, hierarchical text, and the text information we
design in lung tissues. Each pathology task has a dedicated
tree including hierarchical category descriptions from coarse-
grained to fine-grained. Coarse-grained categories (including
root nodes) represent the step-by-step judgment method used
by pathologists when making diagnoses, and fine-grained cat-
egories, i.e. leaf nodes, are the classification results ultimately
required for the pathology task. According to the properties
of the binary tree (Li et al. 2023a), we denote the number of
fine-grained categories as N and the number of coarse-grained
categories as N −1.
3.2. Text Prompt Encoding
PathTree uses textual semantic information as prompts to
guide forward propagation. To match the image encoding, the
text encoder of PLIP is utilized to generate tree-like text em-
beddings Xt ∈R(2N−1)×d, where d represents the dimension of
embeddings. The content of the prompt text has a significant
impact on the downstream performance of contrastive learning
models. To avoid excessive time costs on variations in word
phrasing, existing research has introduced learnable context to-
kens to enhance performance (Zhou et al. 2022b,a). However,
learnable prefixes and suffixes are often overgeneralized and
lack pathological specificity. Therefore, we further design a
tree-structure encoder to allow information transfer between
text prompts. Specifically, we represent the tree structure as
a directed graph G = {Xt, E1, E2}, where E1 represents the set
of directed edges from parent nodes to child nodes, E2 repre-
sents the set of directed edges from child nodes to parent nodes.
Their corresponding adjacency matrices are denoted as A1 and
A2 respectively. Then, graph attention network (GAT) layers
are used to transfer node information dynamically:
H(1)
j
= σ(GAT(1)
j (Xt, Aj)), H(2)
j
= σ(GAT(2)
j (H(1)
j , Aj)),
(1)
where j = 1, 2, σ is the activation function, such as ReLU.
We use a dual interaction mechanism to deliver the upper and
Jiawen Li et al. / Medical Image Analysis (2024)
5
Planar [CLASS]
→ Invasive (IC)
→ Ductal Carcinoma in Situ (DCIS)
→ Flat Epithelial Atypia (FEA)
→ Atypical Ductal Hyperplasia (ADH)
→ Usual Ductal Hyperplasia (UDH)
→ Benign (BE)
→ Normal (NOR)
Professional text prompts described by pathologists
Hierarchical [CLASS]
→ Invasive (IC)
→ Ductal Carcinoma in Situ (DC
→ Flat Epithelial Atypia (FEA)
→ Atypical Ductal Hyperplasi
……
→ Non-invasive
→ Non-atypical
→ Atypical of DCIS
→ Hyperplastic
→ Atypical
[ADH]: The lesions show atypical ductal hyperplasia, which mainly 
occurs in breast ducts. Its cell characteristics are similar to ductal 
carcinoma in situ, but the number and range of cells are not as large 
as ductal carcinoma in situ.
[Hyperplastic]: The lesions show hyperplasia, an excessive growth of 
cells, but are not cancerous.
[UDH]: The lesions show usual ductal hyperplasia, in which cells 
within the breast ducts proliferate but maintain normal arrangement and 
structure.
[Atypical]: The lesions show atypical, and their cells show a different 
morphology and structure from normal breast cells, but the degree of 
change is not enough to be diagnosed as cancer, which is a 
precancerous state.
......
(a) Planar label name
(b) Hierarchical label name
(c) Hierarchical pathological information description
Fig. 3. Three different text forms of pathological lung tissue categories. (a)
Planar text labels, using only fine-grained category names; (b) hierarchical
text labels, using both fine-grained and coarse-grained category names;
(c) hierarchical descriptive text labels, with each fine-grained and coarse-
grained category described in detail using pathology terminology.
lower-level messages of the tree nodes:
T = σ(W1(H(2)
1 + H(2)
2 ) + W2(H(2)
1 ⊙H(2)
2 )),
(2)
where W1 and W2 are learnable linear transformation matrices,
T represents the text representation with prior knowledge of hi-
erarchical paths.
3.3. Multiple Attention Module
Two attention modules are introduced to generate multiple
slide-level features, as shown in Fig.4. The first is multi-gated
attention:
I = A · Xp,
(3)
where I is regarded as a matrix composed of the slide-level rep-
resentation of each node in the tree, and A represents the score
matrix, which is composed of the attention score vectors of all
patches and is expressed as follows:
A = Softmax(W(tanh(U1 · Xp) ⊙sigm(U2 · Xp))),
(4)
where U1, U2 ∈R(d/2)×d, W ∈R(2N−1)×(d/2) are all learnable
parameters.
The second is multi-head Nystrom, where different heads can
capture distinct features and patterns of slides in parallel. We
set the number of heads to 2N −1, consistent with the num-
ber of nodes in the tree structure. The self-attention mechanism
achieves patch interaction by calculating similarity scores, but
its standard version leads to high memory and time complex-
ity in long sequence problems like WSIs (Chu et al. 2024).
Therefore, we use the Nystrom method (Xiong et al. 2021) to
approximate the self-attention computation to reduce computa-
tional overhead. The final embedding of each head is obtained
by using the average pooling operation. The form of multi-head
self-attention can be defined as follows:
I = [Avg(nystrom1(Xp)), . . . , Avg(nystrom2N−1(Xp))].
(5)
⋅
⋅
…
…
Linear proj 3
Linear proj 2
Linear proj 1
Non-linear 1
Non-linear 2
𝑋௣∈ℝெ×ௗ
𝐴∈ℝଶேିଵ×ெ
…
𝑋௣∈ℝெ×ௗ
…
Linear proj 2
Linear proj 3
× 2𝑁−1
(a) Multiple gated attention
(b) Multi-head Nystrom
+
Nystrom layer
Average Pooling
[{𝑛𝑦𝑠𝑡𝑟𝑜𝑚𝑋௣}] ∈ℝଶேିଵ×ெ×ௗ
Linear proj 1
Fig. 4. Schematic diagram of the two attention modules. (a) Multiple gated
attention assigns multiple attention scores to each patch, which are then
weighted with the patch embedding to obtain multiple slide representa-
tions; (b) multi-head Nystrom assigns 2N −1 heads, then uses the linear
Nystrom method to update the embedding in each head, and finally ob-
tains multiple slide representations through average pooling layer.
PathTree generates multiple slide embeddings based on the
above methods and then assigns them to the nodes correspond-
ing to the tree structure.
3.4. Text-guided Tree-like Aggregator
Text prompt embeddings provide crucial information that
helps guide the aggregation of multiple slide-level embeddings
along the path of the tree, which is beneficial to generating WSI
features that contain hierarchical morphological prior knowl-
edge. Starting from the leaves, we recursively aggregate em-
beddings of each node from bottom to top until the root. For
any node γ, its two embeddings of child nodes α and β are fused
as follows:
bγ = sαbα + sβbβ + iγ,
(6)
where
sα =
exp(bαt⊤
α )
exp(bαt⊤α ) + exp(bβt⊤
β ), sβ = 1 −sα,
(7)
where tα and tβ represent the text embeddings corresponding to
the two child nodes. bγ is the slide-level embedding of node γ
after fusion. Note that when a child node is a leaf, its original
embedding iγ is used as the input bγ. Algorithm 1 describes the
entire recursive aggregation process under programming logic.
When the aggregation is complete, the root integrates the infor-
mation of all nodes, and its slide-level embedding serves as the
global WSI feature.
6
Jiawen Li et al. / Medical Image Analysis (2024)
Algorithm 1 Text-guided Tree-like Aggregator
function SlideAggregator(tree, T, I)
1: Input tree: recursive tree node dictionary, each node in-
cludes id and child.
(if node is a leaf, node[′child′] = None).
2: Input T: [t1, t2, . . . , t2N−1]⊤, text prompt embeddings.
3: Input I: [i1, i2, . . . , i2N−1]⊤, multiple slide embeddings.
4: S ←I · T ⊤
 Compute a contrast matrix S .
5: D := [d1, d2, . . . , d2N−1] ←diag(S )
6:
 Get the diagonal of S .
7: F ←{}
 Create an empty dictionary to save embeddings.
8: F ←FuseBranch(tree, I, D, F)
9:
 Recursively fuse all slide embeddings.
10: g ←F[tree[′id′]]
 Global WSI feature.
11: return g
end function
12:
function FuseBranch(node, I, D, F)
13: if not child in node then
14:
F[node[′id′]] ←inode[′id′]
15:
 Assign the slide embedding of the leaf to F.
16: else
17:
for child in node[′child′] do
18:
F ←FuseBranch(child, I, D, F)
19:
 Recursively aggregate child information.
20:
end for
21:
sibling ←FindSibling(tree, child[′id′])
22:
 Find its sibling node.
23:
[sα, sβ] ←Softmax(dchild[′id′], dsibling[′id])
24:
 Information weight.
25:
[bα, bβ] ←[F[child[′id′]], F[sibling[′id′]]]
26:
bγ ←[sα, sβ] · [bα, bβ]⊤
 Fuse child embeddings.
27:
F[node[′id′]] ←bγ + inode[′id′]
28:
 Update embeddings.
29: end if
30: return F
end function
3.5. Joint Slide-text Constraints
Path Alignment Learning: The hierarchical classification
requires the WSI feature to align with the corresponding fine-
grained and coarse-grained categories. Inspired by research on
hierarchical text classification (Zhou et al. 2020; Chen et al.
2021b), our proposed PathTree aligns slide-level representation
with text prompt embeddings from the root to the corresponding
leaf to achieve this goal. Fig.5 demonstrates the schematic dia-
gram of path alignment learning. Specifically, for a global WSI
feature g and its fine-grained text prompt embedding, the mean
square error metric loss is designed to constrain their alignment
degree:
Lpath = 1
|P|
X
k∈P
∥g −tk∥2
2 ,
(8)
where P represents the set of all nodes in the path starting from
the root and ending at the corresponding leaf. By minimizing
between the slide-level representation and the embeddings of
Slide-level 
embedding
Target text prompt 
in the path
Fig. 5. Graphical demonstration of path alignment learning. Its goal is to
make the slide-level embedding of the target node close to all text embed-
dings in its path to the root node.
the fine and coarse-grained text prompts under the tree structure
path, Lpath helps the WSI feature to fit all hierarchical labels
better and prevent from being alienated from the tree structure
information driven by the entire problem, causing weak gen-
eralization performance. PathTree uses the depth-first search
(DFS) (Tarjan 1972) to find the path of the current node, and
Algorithm 2 shows its pseudocode.
Algorithm 2 Finding Paths with DFS Algorithm
function FindPath(node, target[′id′], path)
1: Input path: [ ], create it to save id.
2: path ←Append(path, node)
3: if node[′id′] = target[′id′] then
4:
return path
5: end if
6: if child in node then
7:
for child in node[′child′] do
8:
Path ←Copy(Path)
9:
result ←FindPath(child, target[′id′], Path)
10:
if result is not None then
11:
return results
12:
end if
13:
end for
14: end if
end function
Tree-aware Matching Learning: In addition to aligning
with the fine-grained text prompt, the WSI features extracted
by PathTree are expected to match three other text prompts in
the tree structure. Specifically, the parent node (containing the
coarse-grained text prompt) should produce strong correlations
with WSI features, sibling nodes should produce secondary cor-
relations, and other incorrect fine-grained text prompts should
stay away. We introduce the triplet loss Lneg to model the above
relationships. Lneg aims to use a margin λneg to penalize the
distance between the slide representation and the wrong text
semantics, which is expressed as follows:
Lneg = max(0, D(g, tpos) −D(g, tneg) + λneg),
(9)
where D represents the metric function based on the L2 norm,
tpos and tneg represent correct and incorrect prompt embeddings,
λ is a marginal constant, the larger it is, the greater the penalty.
Jiawen Li et al. / Medical Image Analysis (2024)
7
Slide-level 
embedding
Fine-grained text prompt
Parent text prompt
(Closer to the target)
Sibling text prompt
(Medium close to the targe)
Incorrect fine-grained text prompt
(Farthest from the target label)
(Target label)
Other incorrect 
fine-grained text prompt
𝜆௣௔௥௘௡௧
𝜆௦௜௕௟௜௡௚
𝜆௡௘௚భ
𝜆௡௘௚మ
Other incorrect 
fine-grained text prompt
𝜆௡௘௚య
𝜆௡௘௚೔> 𝜆௦௜௕௟௜௡௚> 𝜆௣௔௥௘௡௧ ,
𝑖= 1,2, … , 𝑁−1
Fig. 6. Graphical demonstration of tree-aware matching learning, where
the slide-level embedding of the target node is used as the anchor, its text
embedding as the positive sample, and other text embeddings as the nega-
tive sample. The goal is to make the distance between the anchor and the
negative sample larger than the distance to the positive sample.
We denote parent, sibling, and other leaf nodes as negative
nodes, and set λleaf > λsibling > λparent. We take the sum of
all the triplet losses as the final Lmatch, which is expressed as
follows:
Lmatch = Lparent + Lsibling +
1
N−1
P Lleaf .
(10)
Fig.6 demonstrates the schematic diagram of tree-aware match-
ing learning.
3.6. Slide Prediction with Text Prompts
Unlike conventional single-modality WSI classification
methods that directly utilize labels as integers for training,
PathTree obtains prediction scores by calculating the similar-
ity between WSI features and text prompt embeddings, which
improves flexibility in how the model predicts and increases
few-shot learning capabilities for downstream tasks. We use
text prompt embeddings T = [t1, t2, . . . , tN]⊤as the classifica-
tion weight vector to calculate predicted probabilities:
p(y = j|I) =
exp(sim(t j, g)/τ)
PN
n=1 exp(sim(tn, g)/τ)
,
(11)
where τ is a learnable temperature parameter, sim(·) represents
cosine similarity.
The probability is finally fed into cross-
entropy loss Lce to minimize the standard classification loss.
The entire loss of PathTree is as follows:
Lall = Lce + µmLmatch + µpLpath,
(12)
where µm and µp are the coefficients of the two joint constraint
losses. In summary, PathTree is trained through Lpath, Lmatch,
and the target domain loss corresponding to the downstream
task, allowing WSI features to perform constrained exploration
in the tree-like text feature space and contextual learning.
4. Experiments
4.1. Datasets
In-house SYSFL: We collect 1189 digital intraoperative
cryosections of lung tissue from the First Affiliated Hospital of
datasets
Fig. 7. The hierarchical forms of SYSFL, PANDA, and BRACS datasets.
They are all represented by binary trees.
Table 1. Comparison results of fine-grained classification
Datasets
The number of WSIs per fine-grained class
SYSFL
AAH
AIS
MIA
IAC
SCC
PNE
NOR
102
354
137
244
111
153
86
PANDA
ISUP 0
ISUP 1
ISUP 2
ISUP 3
ISUP 4
ISUP 5
-
2892
2666
1343
1242
1249
1224
-
BRACS
IC
DCIS
FEA
ADH
NOR
UDH
BE
132
61
41
48
44
74
147
Sun Yat-sen University to construct the SYSFL dataset, which
includes seven categories with atypical adenomatous hyperpla-
sia (AAH), adenocarcinoma in situ (AIS), minimally invasive
adenocarcinoma (MIA), invasive adenocarcinoma (IAC), squa-
mous cell carcinoma (SCC), pneumonia (PNE) and normal lung
tissue (NOR). They are all acquired using the Teksqray slide
system and scanned with 40× magnification. Note that AAH,
AIS, MIA, and IAC present progressive symptoms and are diffi-
cult to diagnose on cryosections. All slide-level labels are based
on the description of the frozen pathological report and veri-
fied by two professional pathologists. Due to the limitations
of preparation methods and diagnostic time, diagnosing lung
tissue cryosections is more difficult than conventional paraffin-
embedded pathological sections. Computer-aided diagnosis can
help pathologists reduce the burden of reviewing cryosections
of lung tissue.
Public PANDA (Bulten et al. 2022):
This large-scale
prostate dataset contains six categories, including 10618 H&E-
stained biopsy WSIs. Based on the ISUP grading index, the
cancer level can be divided into 0 (no cancer), 1 (Gleason score:
3 + 3 = 6), 2 (Gleason score: 3 + 4 = 7), 3 (Gleason score:
4+3 = 7), 4 (Gleason score: 4+4 = 8; 3+5 = 8; 5+3 = 8) and
5 (Gleason score: 4+5 = 9; 5+5 = 10). Due to the complexity
of the grading system, it is challenging to classify WSI directly
through slice-level labels.
8
Jiawen Li et al. / Medical Image Analysis (2024)
Table 2. ACC, AUC, and Weighted F1 results [%] on the fine-grained 7-category SYSFL, 6-category PANDA, and 7-category BRACS datasets. Best
performance in bold, second best underlined
Methods
SYSFL
PANDA
BRACS
ACC
AUC
Weighted F1
ACC
AUC
Weighted F1
ACC
AUC
Weighted F1
ABMIL (Ilse et al. 2018)
59.557.01
87.362.29
56.694.83
58.550.54
85.190.73
57.090.43
51.183.00
77.032.62
43.823.37
CLAM (Lu et al. 2021b)
58.467.19
85.523.06
57.585.83
60.290.88
87.480.51
59.510.57
53.573.20
78.224.86
51.552.17
DSMIL (Li et al. 2021)
59.555.28
87.482.10
58.393.70
59.960.45
87.080.42
58.820.54
53.945.19
80.333.71
52.474.54
TransMIL (Shao et al. 2021)
55.427.16
84.194.27
54.015.66
59.451.84
86.651.15
58.002.61
50.091.80
76.152.74
48.351.79
GTP (Zheng et al. 2022)
46.9310.60
74.778.07
44.989.30
50.452.38
80.931.13
48.862.32
41.338.14
71.299.01
39.979.43
DTFD-MIL (Zhang et al. 2022)
53.404.90
84.732.00
51.643.66
55.920.62
84.580.70
54.840.69
53.571.69
79.253.39
52.262.03
HIPT (Chen et al. 2022)
51.053.95
77.662.20
47.004.41
57.220.68
83.920.63
54.901.42
48.162.00
76.292.88
48.552.51
ILRA (Xiang and Zhang 2022)
60.395.51
86.322.16
58.663.17
60.411.05
86.430.99
59.730.76
51.743.82
77.336.55
50.452.66
PathTree (Ours)
61.754.32
88.492.21
59.483.52
61.140.96
87.390.47
60.271.23
55.224.62
81.812.43
54.644.11
PathTree-Ny (Ours)
63.684.87
88.930.67
62.043.74
62.350.65
88.260.33
61.540.51
51.711.72
80.342.41
52.611.91
Table 3. H-Precision, H-Recall, and H-F1 results [%] on the fine-grained 7-category SYSFL, 6-category PANDA, and 7-category BRACS datasets.
Methods
SYSFL
PANDA
BRACS
H-Precision
H-Recall
H-F1
H-Precision
H-Recall
H-F1
H-Precision
H-Recall
H-F1
ABMIL (Ilse et al. 2018)
76.524.78
80.365.61
78.374.97
70.910.74
71.220.68
71.060.61
65.332.21
66.512.42
65.902.06
CLAM (Lu et al. 2021b)
77.513.56
78.263.80
77.863.37
73.591.70
72.921.72
73.220.37
66.064.71
67.704.95
66.864.79
DSMIL (Li et al. 2021)
77.383.67
80.373.14
78.843.24
73.710.70
71.530.51
72.600.31
68.342.82
71.194.03
69.723.30
TransMIL (Shao et al. 2021)
73.546.94
76.664.27
74.985.16
73.061.30
71.561.92
72.301.43
66.511.28
65.972.43
66.221.37
GTP (Zheng et al. 2022)
71.836.50
70.854.96
71.255.11
63.593.37
66.740.69
65.091.79
57.989.22
59.749.04
58.758.78
DTFD-MIL (Li et al. 2021)
70.966.69
78.822.76
74.624.81
68.820.75
69.620.73
69.210.54
70.331.73
69.062.25
69.671.61
HIPT (Chen et al. 2022)
71.343.80
71.265.22
71.223.58
73.850.46
68.671.06
71.160.67
65.522.29
67.631.11
66.561.71
ILRA (Xiang and Zhang 2022)
77.315.27
81.235.08
79.225.14
73.121.67
72.490.69
72.790.85
67.152.92
70.361.95
68.712.37
PathTree (Ours)
79.213.79
80.913.12
80.033.09
73.241.63
73.381.07
73.290.50
70.204.64
72.033.47
71.093.97
PathTree-Ny (Ours)
80.623.55
80.792.75
80.692.96
74.310.78
74.161.89
74.220.56
66.852.36
70.421.77
68.581.86
Public BRACS (Brancati et al. 2022): This breast cancer
dataset contains 547 H&E-stained WSIs collected from 189
patients, including seven categories with normal breast tissue
(NOR), benign lesion (BE), usual ductal hyperplasia (UDH),
flat epithelial atypia (FEA), atypical ductal hyperplasia (ADH),
ductal carcinoma in situ (DCIS) and invasive ductal and lobular
carcinoma (IC). Due to the diversity of breast cancer subtypes,
pathologists can classify them in a tree-like progressive manner,
where the classification difficulty increases with each level.
According to actual diagnostic patterns, three datasets are
refined into hierarchical structures certified by authoritative
pathology experts to simulate real diagnostic scenarios. These
provide resources for evaluating PathTree and related WSI anal-
ysis methods. The hierarchical decision trees for coarse-to-fine
classification of the three datasets are shown in Figure 7. The
specific details are shown in Table 1.
4.2. Baselines
We consider eight major WSI analysis baselines for direct
comparison:
ABMIL (Ilse et al. 2018), CLAM (Lu et al.
2021b), DSMIL (Li et al. 2021), TransMIL (Shao et al. 2021),
GTP (Zheng et al. 2022), DTFD-MIL (Zhang et al. 2022), HIPT
(Chen et al. 2022), ILRA (Xiang and Zhang 2022), which are
the methods that have been widely discussed recently. In ad-
dition, three baselines are used for comparison to explore the
few-shot performance of our PathTree: (1) Linear-Probe (atten-
tion pooling) aggregates all patch features through a learnable
attention pooling layer and uses a linear layer to classify the
aggregated global features. (2) CoOp (Zhou et al. 2022b) con-
catenates additional learnable tokens to the text prompt based
on Linear-Probe, and then performs classification based on fea-
ture alignment between text and slide.
(3) TOP (Qu et al.
2024) generates text prototypes of multiple pathology prompt
groups through GPT-4 (OpenAI 2023), then designs a prompt-
guided pooling layer for aggregating patch features into bag
features, and finally performs classification through text-slide
feature alignment.
4.3. Evaluation Metrics
To comprehensively evaluate the pathological classification
task of the tree-like structure, planar and hierarchical metrics
are adopted. For planar metrics, we report accuracy (ACC),
AUC, and Weighted F1-score to evaluate the fine-grained per-
formance. For hierarchical metrics, each WSI has a multi-label
from coarse-grained to fine-grained levels, we additionally re-
port hierarchical precision (H-Precision), recall (H-Recall), and
F1-score (H-F1), which are expressed as follows:
H-Precision =
P
j |Ct( j) ∩Cp( j)|
P
j |Ct( j)|
,
(13)
H-Recall =
P
j |Ct( j) ∩Cp( j)|
P
j |Ct( j)|
,
(14)
Jiawen Li et al. / Medical Image Analysis (2024)
9
Table 4. ACC, AUC, and weighted F1 results [%] on the coarse-grained 4-category SYSFL, 2-category PANDA, and 3-category BRACS datasets.
Methods
SYSFL
PANDA
BRACS
ACC
AUC
Weighted F1
ACC
AUC
Weighted F1
ACC
AUC
Weighted F1
ABMIL (Ilse et al. 2018)
77.836.82
92.403.45
76.907.32
87.660.95
93.070.47
87.740.93
69.833.26
81.253.62
64.884.07
CLAM (Lu et al. 2021b)
76.994.53
91.442.60
76.884.61
87.810.60
94.160.48
88.020.45
70.584.68
83.285.86
70.315.13
DSMIL (Li et al. 2021)
78.935.05
92.492.01
78.245.82
87.030.90
93.680.53
87.181.03
75.324.08
84.853.75
74.273.62
TransMIL (Shao et al. 2021)
72.027.92
89.115.02
71.368.61
87.310.50
93.750.40
87.500.40
69.112.05
80.992.67
67.933.08
GTP (Zheng et al. 2022)
66.327.40
80.953.29
65.919.87
83.072.41
90.281.23
82.952.72
65.4610.57
76.639.33
63.1411.05
DTFD-MIL (Zhang et al. 2022)
71.686.89
88.363.77
69.659.01
84.900.71
91.520.28
84.890.72
72.763.53
83.495.69
72.133.65
HIPT (Chen et al. 2022)
67.162.88
85.322.62
62.596.88
86.240.83
91.610.58
86.620.87
70.263.34
80.903.45
70.082.83
ILRA (Xiang and Zhang 2022)
78.934.99
91.231.43
77.645.71
87.160.31
93.500.54
87.120.42
74.592.98
83.706.31
73.443.76
PathTree (Ours)
80.114.76
93.901.66
79.464.76
88.110.43
94.310.26
88.180.41
74.604.28
86.003.09
74.284.28
PathTree-Ny (Ours)
78.594.52
93.162.07
78.614.46
88.070.78
94.060.65
88.170.71
70.201.83
85.592.71
71.582.16
H-F1 = 2 H-Precision · H-Recall
H-Precision + H-Recall,
(15)
where Ct( j) and Cp( j) are the true and predicted hierarchical
label sets of the jth sample. Note that the root node is not in-
cluded in the label set. By calculating the number of intersec-
tions between the true and predicted node sets, three metrics
can be quantified to show the differences in hierarchical label
predictions between models. All results for the three datasets
are obtained using 5-fold cross-validation, and each metric is
reported as a percentage value and standard error.
4.4. Implementation
In the training phase, we employ the Adam (Kingma and Ba
2014) optimizer, with a learning rate of 3 × 10−4, betas of 0.9
to 0.98, and weight decay of 10−4. The epoch is set to 100, and
the batch size is set to 1, meaning only one WSI is processed
in each iteration. For the slide-text joint loss function, we set
λlea f , λsibling, λparent to 0.2, 0.1, and 0.002 respectively; for slide-
text similarity, we initialize τ to 0.07; for µm and µp, we set
them to 1 in the comparative experiments with other baselines,
and explore their impact in the ablation experiments. We use
the image encoder of PLIP for each baseline and PathTree, as
a fair comparison, and all experiments are conducted with the
same hyperparameters and data partitioning settings.
4.5. Results and Analysis
4.5.1. Fine-grained Classification Results
Table 2 presents the planar metric results for fine-grained
classification tasks on the SYSFL, PANDA, and BRACS
datasets. Overall, the PathTree based on gated attention outper-
forms other competitors in all three metrics, and the PathTree-
Ny based on multi-head Nystrom mechanisms achieves even
higher performance on the SYSFL and PANDA datasets com-
pared to PathTree.
For instance, in the weighted F1 metric
on SYSFL, PathTree-Ny surpasses PathTree by 2.56% and the
best alternative WSI analysis method by 3.38%. In the PANDA
dataset, the accuracy of PathTree-Ny exceeds that of PathTree
by approximately 1.21%, equating to a difference of nearly 128
WSIs. However, in the BRACS dataset, PathTree-Ny performs
less effectively than PathTree, possibly due to differences in the
classification tasks. In differentiating breast cancer subtypes, a
40
45
50
55
60
Average Weighted F1 over Three Datasets
Score (%)
ABMIL
CLAM
DSMIL TransMIL
GTP
DTFD-MIL
HIPT
ILRA
PathTree & SA
(Ours)
53.58
56.21
56.56
53.46
52.91
52.91
51.01
56.28
58.13
58.17
40
45
50
55
60
70
75
80
85
90
Average AUC over Three Datasets
Score (%)
ABMIL
CLAM
DSMIL TransMIL
GTP
DTFD-MIL
HIPT
ILRA
PathTree & SA
(Ours)
75
80
85
83.89
83.74
84.96
82.33
82.85
75.29
80.13
83.36
85.90
86.21
60
65
70
75
80
Average Hierarchical F1 over Three Datasets
Score (%)
65
70
75
ABMIL
CLAM
DSMIL TransMIL
GTP
DTFD-MIL
HIPT
ILRA
PathTree & SA
(Ours)
72.48
72.65
73.72
73.57
71.17
71.17
65.03
69.92
74.80
74.55
Fig. 8. Average results for the weighted F1, AUC, and H-F1 over three
datasets.
single slide may contain multiple regions related to other cat-
egories. Pathologists determine the category of a slide based
on the region with the highest malignancy. However, such re-
gions are often small, making it difficult for self-attention com-
putation to focus on these regions, leading to incorrect judg-
ments. Conversely, pathologists focus more on global regions
in tasks such as distinguishing lung tissue and grading prostate
cancer. For example, the infiltration degree of lung adenocarci-
noma is judged based on the morphological analysis of multiple
lesions. Therefore, self-attention-based methods more easily
capture information related to the global context, resulting in
correct judgments.
Table 3 shows the hierarchical metric results for the three
datasets. It should be noted that hierarchical metrics require
considering both the coarse-to-fine prediction results and the
ground truth labels for each slide. Our proposed PathTree meth-
ods achieve competitive levels in H-Precision and H-Recall,
10
Jiawen Li et al. / Medical Image Analysis (2024)
1
2
4
8
16
0.0
0.2
0.4
0.6
0.8
1.0
SYSFL Dataset
(7 fine-grained & 5 coarse-grained classes, 1189 slides, 5 runs)
Training Slides Per Class
Hierarchical F1 (%)
Linear-Probe
CoOp
TOP
PathTree (Ours)
1
2
4
8
16
0.0
0.2
0.4
0.6
0.8
1.0
SYSFL Dataset
(7 fine-grained & 5 coarse-grained classes, 1189 slides, 5 runs)
Training Slides Per Class
AUC (%)
Linear-Probe
CoOp
TOP
PathTree (Ours)
1
2
4
8
16
0.0
0.2
0.4
0.6
0.8
1.0
SYSFL Dataset
(7 fine-grained & 5 coarse-grained classes, 1189 slides, 5 runs)
Training Slides Per Class
Weighted F1 (%)
Linear-Probe
CoOp
TOP
PathTree (Ours)
1
2
4
8
16
0.0
0.2
0.4
0.6
0.8
1.0
PANDA Dataset
(6 fine-grained & 4 coarse-grained classes, 10618 slides, 5 runs)
Training Slides Per Class
Hierarchical F1 (%)
Linear-Probe
CoOp
TOP
PathTree (Ours)
1
2
4
8
16
0.0
0.2
0.4
0.6
0.8
1.0
PANDA Dataset
(6 fine-grained & 4 coarse-grained classes, 10618 slides, 5 runs)
Training Slides Per Class
AUC (%)
Linear-Probe
CoOp
TOP
PathTree (Ours)
1
2
4
8
16
0.0
0.2
0.4
0.6
0.8
1.0
PANDA Dataset
(6 fine-grained & 4 coarse-grained classes, 10618 slides, 5 runs)
Training Slides Per Class
Weighted F1 (%)
Linear-Probe
CoOp
TOP
PathTree (Ours)
1
2
4
8
16
0.0
0.2
0.4
0.6
0.8
1.0
BRACS Dataset
(7 fine-grained & 5 coarse-grained classes, 549 slides, 5 runs)
Training Slides Per Class
Hierarchical F1 (%)
Linear-Probe
CoOp
TOP
PathTree (Ours)
1
2
4
8
16
0.0
0.2
0.4
0.6
0.8
1.0
BRACS Dataset
(7 fine-grained & 5 coarse-grained classes, 549 slides, 5 runs)
Training Slides Per Class
AUC (%)
Linear-Probe
CoOp
TOP
PathTree (Ours)
1
2
4
8
16
0.0
0.2
0.4
0.6
0.8
1.0
BRACS Dataset
(7 fine-grained & 5 coarse-grained classes, 549 slides, 5 runs)
Training Slides Per Class
Weighted F1 (%)
Linear-Probe
CoOp
TOP
PathTree (Ours)
(a)                                                                      (b)                                                 
(c)
Fig. 9. AUC, Weighted F1, and H-F1 results [%] on the fine-grained 7-category SYSFL, 6-category PANDA, and 7-category BRACS datasets in few-shot
settings.
and outperform other methods by approximately 2% in the
more comprehensive H-F1 metric. This indicates that PathTree
can distinguish slides more precisely than other state-of-the-art
methods. Fig.8 shows the average results for the weighted F1,
AUC, and H-F1 metrics across the three datasets, which also
confirms the superiority of PathTree-based methods at both pla-
nar and hierarchical levels.
4.5.2. Coarse-grained Classification Results
Hierarchical WSI analysis can be designed to address coarse-
grained classification tasks based on pathological problems.
SYSFL involves the study of frozen lung tissue sections. Le-
sions with good prognosis (AIS, AAH, and MIA) have similar
properties and are low risk so that they can be clinically iden-
tified as one category; NOR and PNE are non-neoplastic and
can be grouped into one category. Consequently, the coarse-
grained SYSFL dataset is represented as a four-class classifica-
tion task. PANDA focuses on grading prostate cancer, where
an IUSP value greater than 0 indicates cancer. The distinction
between the presence and absence of cancer is crucial for ini-
tial biopsy screening. Thus, the coarse-grained PANDA can
be represented as a binary classification task.
The BRACS
dataset can be divided into three categories based on cancer
risk: non-cancerous (NOR+BE+UDH), precancerous lesions
(ADH+FEA), and cancerous (DCIS+IC). For each fold of fine-
grained classification, models are saved based on the optimal
weighted F1 score in the validation set and then directly inferred
for each coarse-grained classification task. The related results
are recorded in Table 4. It can be observed that although the ac-
curacy for the BRACS dataset is slightly lower than the optimal
result, the overall performance of the two PathTree methods re-
mains superior.
4.5.3. Comparison of Few-shot Learning
Contextual learning based on text prompts has been proven
to have good few-shot capabilities (Radford et al. 2021; Zhou
et al. 2022a,b). Fig.9 shows the comparison results of PathTree
with attention pooling-based Linear-Probe, CoOP, and TOP in
1, 2, 4, 8, and 16-shot settings. From the dataset perspective,
PathTree exhibits the best few-shot capabilities on the BRACS
and PANDA datasets, especially in BRACS, where it also has
the smallest standard deviation. On the SYSFL dataset, CoOp
also demonstrates strong performance. From the model capa-
bility perspective, Linear-Probe, which relies on fine-tuning a
linear layer, performs poorly and has the largest standard devi-
ation. However, CoOp and TOP, which incorporate text context
tokens, improve performance. From the shot settings perspec-
tive, although PathTree is not the best in the 1 and 2-shot, it
shows effectiveness in the 4, 8, and 16-shot, which indicates
that the fixed tree structure struggles to learn more contextual
information with extremely few samples. However, as the num-
ber of samples slightly increases, the hierarchical structure can
significantly learn the relationships within categories, thereby
improving model performance.
Jiawen Li et al. / Medical Image Analysis (2024)
11
Table 5. AUC results [%] of PathTree with different text prompts on three
fine-grained datasets.
Text prompt
SYSFL
PANDA
BRACS
’[CLASS]’
87.921.63
87.250.64
81.522.30
’this is [CLASS].’
87.693.13
87.260.50
80.660.89
’a photo of [CLASS].’
87.693.00
87.250.13
81.633.62
’an image of [CLASS].’
87.221.38
87.170.36
80.292.18
’[CLASS] is present.’
87.802.48
87.280.35
81.532.44
’a pathological image of [CLASS].’
88.301.94
87.240.52
80.502.47
’a whole slide image of [CLASS].’
88.242.59
87.330.38
80.992.54
Professional description (Ours)
88.492.21
87.390.47
81.812.43
GCN
GAT
GIN
Sage
BiTree
0.76
0.78
0.80
0.82
0.84
0.86
BRACS Dataset
AUC Score (%)
GCN
GAT
GIN
Sage
BiTree
0.75
0.80
0.85
0.90
0.95
Average over Three Datasets
AUC Score (%)
GCN
GAT
GIN
Sage
BiTree
0.860
0.865
0.870
0.875
0.880
0.885
0.890
PANDA Dataset
AUC Score (%)
GCN
GAT
GIN
Sage
BiTree
0.82
0.84
0.86
0.88
0.90
0.92
SYSFL Dataset
AUC Score (%)
Fig. 10. AUC results [%] with different graph-based text prompt encoders
on three fine-grained datasets.
4.6. Ablation Experiments
4.6.1. Effectiveness of Different Text Prompts
Our proposed PathTree uses professional descriptions as
prompt descriptions for learning and inference. To explore the
impact of different text prompts on PathTree, we conduct ex-
periments using different text prompts, including five conven-
tional and two general pathology-related text prompts. Their
comparative results are shown in Table 5.We find that the per-
formance of the two pathology-related texts is similar to that of
other general text prompts but slightly better in the few-sample
BRACS dataset, which suggests that specific semantic infor-
mation can help the model extract more useful information,
especially when the sample size is small. Moreover, our de-
signed professional descriptions consistently outperformed all
these methods.
This indicates a strong potential correlation
between the form of text prompts and model performance in
PathTree, suggesting that problem-specific language may en-
hance the ability of text-guided PathTree in WSI analysis.
4.6.2. Comparison of Different Graph-based Prompt Encoders
Text prompts pass through a tree structure to interact and
convey more semantic information. In this paper, we model
this tree structure as a graph representation and design a bidi-
rectional directed graph structure (BiTree). We conducted abla-
tion experiments comparing BiTree with several common graph
Table 6. AUC results [%] of PathTree (text encoder: PLIP Huang et al.
2023) with different patch encoders on three fine-grained datasets.
Patch encoder
SYSFL
PANDA
BRACS
ImageNet (Deng et al. 2009)
82.903.69
84.370.87
74.533.02
KimiaNet (Riasatian et al. 2021)
84.032.62
84.470.31
74.603.77
CLIP (Radford et al. 2021)
85.972.99
83.750.89
74.473.85
PathDino (Alfasly et al. 2024)
86.072.99
89.050.55
79.792.90
CONCH (Lu et al. 2024)
87.931.96
89.940.28
85.602.82
PLIP (Huang et al. 2023)
88.492.21
87.390.47
81.812.43
Table 7. AUC results [%] of PathTree (patch encoder: PLIP Huang et al.
2023) with different text encoders on three fine-grained datasets.
Text encoder
SYSFL
PANDA
BRACS
BERT (Kenton and Toutanova 2019)
87.871.97
87.300.73
81.342.80
BioBERT (Lee et al. 2020)
87.533.41
87.260.52
82.331.84
CLIP (Radford et al. 2021)
87.991.80
87.310.74
81.284.23
CONCH (Lu et al. 2024)
87.892.60
87.370.46
81.862.70
PLIP (Huang et al. 2023)
88.492.21
87.390.47
81.812.43
architectures: graph convolutional network (GCN) (Kipf and
Welling 2017), graph attention network (GAT) (Veliˇckovi´c et al.
2018), graph isomorphism network (GIN) (Xu et al. 2018), and
Sage (Hamilton et al. 2017) to investigate their impact on model
performance. The results are shown in Fig.10. For the SYSFL
dataset, GCN and BiTree perform better, with Sage showing
less variance; for the PANDA dataset, GCN and GIN have slight
advantages, but all methods show similar performance; for the
BRACS dataset, Sage and BiTree are more competitive, with
Sage showing less fluctuation. Overall, different graph-based
text prompt encoders have little impact on the final performance
of PathTree.
4.6.3. Effect of Different Patch and Text Encoders
Analyzing multimodal pathology vision-language data re-
quires encoders for both images and text. We conduct further
experiments to explore the impact of different patch and text
encoders on the performance of PathTree. Table 6 presents the
comparative results with various patch encoders while keep-
ing the text encoder unchanged. Several conclusions can be
drawn from these results.
Firstly, the performance is poor
when using patch encoders pre-trained on natural images, such
as the ImageNet-supervised ViT-small or the CLIP pre-trained
through image-text contrastive learning. This is likely due to
the significant difference between natural and pathological im-
age domains.
Secondly, KimiaNet, obtained through fully supervised
learning of pathological images, underperformed compared to
PathDino, which is obtained through self-supervised learning
based on the DINO (Caron et al. 2021). This suggests that
current fully supervised learning paradigms may be less effec-
tive than self-supervised methods as general patch encoders for
pathology-related downstream tasks, and the performance gap
may be attributed to the smaller and less diverse training dataset
KimiaNet uses.
Thirdly, patch encoders pre-trained with pathology image-
text contrastive learning, such as PLIP and CONCH, perform
better than those with purely visual supervision, such as Kimi-
12
Jiawen Li et al. / Medical Image Analysis (2024)
Coefficient of Alignment Loss
Coefficient of Alignment Loss
(a)
(b)
Fig. 11. The surface plots of PathTree under optimal average Weighted F1
and AUC score and different coefficient of Match Loss µm, Alignment Loss
µp in the BRACS dataset. The parameters µm, µp range from 0.0 to 1.0 with
step of 0.1.
0.0
0.2
0.4
0.6
0.8
1.0
0.4
0.5
0.6
0.7
0.8
0.9
Coefficient of Joint Loss
Score (%)
0.0
0.2
0.4
0.6
0.8
1.0
0.4
0.5
0.6
0.7
0.8
0.9
Coefficient
Score (%)
Weighted F1
AUC
Hierarchical F1
௣
௠
௣
௠
Coefficient
௣
௠
Coefficient
௣
Fig. 12. Comparison of PathTree with different coefficients in the BRACS
dataset. (a) Keep coefficients µp and µm equal. (b) Keep the sum of coeffi-
cients µp and µm equal to 1.
aNet and PathDino, which indicates that patch encoders pre-
trained on multimodal data have greater potential as general
image feature extractors for pathology tasks than purely visual
encoders.
Table 7 demonstrates the comparative results with various
text encoders while keeping the image encoder unchanged.
Encoders pre-trained on the biomedical text like BioBERT,
CONCH, and PLIP perform slightly better than encoders pre-
trained on natural language descriptions like BERT and CLIP.
However, this advantage is not as large as the image encoders
described above.
4.6.4. Ablation Study of Tree-like Constraint Loss
Two joint constraint losses based on tree structures can guide
the alignment of the slide with text embeddings, thereby af-
fecting the analysis. Fig.11 shows the comparative results of
PathTree on the BRACS dataset with different coefficients µm
and µp for Lmatch and Lpath. For the Weighted F1 score, when
neither Lmatch nor Lpath is used, the performance of PathTree
is the lowest, at only 50.06%, which is 5.68% lower than the
highest result of 55.74% obtained with these losses. For the
AUC score, the worst result is at a non-zero point, but only
0.62% lower than the zero point, and 4.61% lower than the best
result, demonstrating the effectiveness of the two multimodal
alignment losses.
To further explore the potential relationship between con-
straint losses and model performance, we analyze two scenar-
ios: when µm = µp and when µm + µp = 1, which are shown
in Fig.12. Notably, when the two coefficients are always equal,
PNE
SCC
IAC
AAH
MIA
AIS
Fig. 13. Visualization heatmap results of PathTree in the SYSFL dataset.
there is a positive correlation between the coefficients and the
three evaluation metrics. When the sum of the two coefficients
is always 1, the results show a certain peak, indicating that the
simultaneous use of both helps improve the performance. In
summary, setting both coefficients relatively large and close to
each other benefits PathTree. On the other hand, when these two
constraint losses are not used, PathTree loses the strong corre-
lation between slide and text embeddings, resulting in poorer
performance.
4.7. Visualization
By visualizing the scores of corresponding categories in the
multiple attention module on the patch regions, we can obtain
WSI heatmaps of PathTree to demonstrate the interpretabil-
ity. Fig.13 shows six visualized samples of non-normal cate-
gories in the SYSFL dataset. For the IAC and SCC categories,
PathTree can focus on extensive cancer regions. The visualiza-
tion results for the PNE and AAH categories show that PathTree
emphasizes lymphocytes and cell proliferation, which aligns
well with the areas of interest in actual pathological diagnoses.
For the MIA and AIS categories, the model highlights tumor
cells growing along the walls and local lesions, which is critical
for distinguishing these two in pathological morphology.
Jiawen Li et al. / Medical Image Analysis (2024)
13
5. Conclusion
In this paper, we transform conventional challenging patho-
logical tasks from planar into tree-like analysis methods, intro-
ducing the concept of hierarchical WSI classification. Mean-
while, we propose a representation learning called PathTree for
solving these hierarchical tasks. PathTree considers both text
and WSI modalities, using professional pathology descriptions
as coarse-grained and fine-grained prompts and introducing a
tree-like graph to exchange semantic information between cat-
egories. To let the text guide WSI representation, PathTree ag-
gregates multiple slide-level embeddings based on tree paths
and uses slide-text similarity and additional metric losses to op-
timize the objectives of downstream tasks. In three challenging
hierarchical classification tasks, we demonstrate that PathTree
outperforms other state-of-the-art methods and can provide a
more clinically relevant perspective for solving complex com-
putational pathology problems.
Declaration of competing interest
The authors declare that they have no known competing fi-
nancial interests or personal relationships that could have ap-
peared to influence the work reported in this paper.
CRediT authorship contribution statement
Jiawen Li: Conceptualization, Investigation, Methodology,
Data curation, Formal Analysis, Software, Validation, Visu-
alization, Writing - original draft, Writing - review & edit-
ing, Project administration.
Qiehe Sun: Conceptualization,
Methodology, Investigation, Software, Validation, Visualiza-
tion, Writing - review & editing. Renao Yan: Project admin-
istration, Software, Validation, Formal Analysis. Yizhi Wang:
Data curation, Investigation, Validation. Yuqiu Fu: Data cu-
ration, Validation. Yani Wei: Resources, Data curation. Tian
Guan: Project administration, Supervision. Huijuan Shi: Re-
sources, Data curation, Project administration, Investigation,
Supervision. Yonghong He: Project administration, Funding
acquisition, Supervision. Anjia Han: Resources, Data cura-
tion, Writing – review & editing, Supervision.
Acknowledgments
This
work
is
supported
by
the
Shenzhen
Engineer-
ing Research Centre (XMHT20230115004) and the Sci-
ence and Technology Research Program of Shenzhen City
(KCXFZ20201221173207022). We thank the support from the
Jilin Fuyuan Guan Food Group Co., Ltd. In.
References
Alfasly, S., Shafique, A., Nejat, P., Khan, J., Alsaafin, A., Alabtah, G.,
Tizhoosh, H., 2024.
Rotation-agnostic image representation learning for
digital pathology, in: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog-
nit., pp. 11683–11693.
Brancati, N., Anniciello, A.M., Pati, P., Riccio, D., Scognamiglio, G., Jaume,
G., De Pietro, G., Di Bonito, M., Foncubierta, A., Botti, G., Gabrani, M., Fe-
roce, F., Frucci, M., 2022. Bracs: A dataset for breast carcinoma subtyping
in h&e histology images. Database 2022, baac093.
Bulten, W., Kartasalo, K., Chen, P.H.C., Str¨om, P., Pinckaers, H., Nagpal, K.,
Cai, Y., Steiner, D.F., van Boven, H., Vink, R., Hulsbergen-van de Kaa, C.,
van der Laak, J., Amin, M.B., Evans, A.J., van der Kwast, T., Allan, R.,
Humphrey, P.A., Gr¨onberg, H., Samaratunga, H., Delahunt, B., Tsuzuki, T.,
H¨akkinen, T., Egevad, L., Demkin, M., Dane, S., Tan, F., Valkonen, M.,
Corrado, G.S., Peng, L., Mermel, C.H., Ruusuvuori, P., Litjens, G., Eklund,
M., Brilhante, A., C¸ akır, A., Farr´e, X., Geronatsiou, K., Molini´e, V., Pereira,
G., Roy, P., Saile, G., Salles, P.G.O., Schaafsma, E., Tschui, J., Billoch-
Lima, J., Pereira, E.M., Zhou, M., He, S., Song, S., Sun, Q., Yoshihara, H.,
Yamaguchi, T., Ono, K., Shen, T., Ji, J., Roussel, A., Zhou, K., Chai, T.,
Weng, N., Grechka, D., Shugaev, M.V., Kiminya, R., Kovalev, V., Voynov,
D., Malyshev, V., Lapo, E., Campos, M., Ota, N., Yamaoka, S., Fujimoto,
Y., Yoshioka, K., Juvonen, J., Tukiainen, M., Karlsson, A., Guo, R., Hsieh,
C.L., Zubarev, I., Bukhar, H.S.T., Li, W., Li, J., Speier, Williamand Arnold,
C., Kim, K., Bae, B., Kim, Y.W., Lee, H.S., Park, J., consortium, t.P.c., 2022.
Artificial intelligence for diagnosis and gleason grading of prostate cancer:
the panda challenge. Nat. Med. 28, 154–163.
B´andi, P., Geessink, O., Manson, Q., Van Dijk, M., Balkenhol, M., Hermsen,
M., Ehteshami Bejnordi, B., Lee, B., Paeng, K., Zhong, A., Li, Q., Zanjani,
F.G., Zinger, S., Fukuta, K., Komura, D., Ovtcharov, V., Cheng, S., Zeng,
S., Thagaard, J., Dahl, A.B., Lin, H., Chen, H., Jacobsson, L., Hedlund, M.,
C¸ etin, M., Halıcı, E., Jackson, H., Chen, R., Both, F., Franke, J., K¨usters-
Vandevelde, H., Vreuls, W., Bult, P., van Ginneken, B., van der Laak, J.,
Litjens, G., 2018. From detection of individual metastases to classification
of lymph node status at the patient level: the camelyon17 challenge. IEEE
Trans. Med. Imaging 38, 550–560.
Campanella,
G.,
Hanna,
M.G.,
Geneslaw,
L.,
Miraflor,
A.,
Werneck
Krauss Silva, V., Busam, K.J., Brogi, E., Reuter, V.E., Klimstra, D.S., Fuchs,
T.J., 2019. Clinical-grade computational pathology using weakly supervised
deep learning on whole slide images. Nat. Med. 25, 1301–1309.
Caron, M., Touvron, H., Misra, I., J´egou, H., Mairal, J., Bojanowski, P., Joulin,
A., 2021. Emerging properties in self-supervised vision transformers, in:
Proc. IEEE/CVF Int. Conf. Comput. Vis., pp. 9650–9660.
Chan, T.H., Cendra, F.J., Ma, L., Yin, G., Yu, L., 2023. Histopathology whole
slide image analysis with heterogeneous graph representation learning, in:
Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 15661–15670.
Chen, C.L., Chen, C.C., Yu, W.H., Chen, S.H., Chang, Y.C., Hsu, T.I., Hsiao,
M., Yeh, C.Y., Chen, C.Y., 2021a. An annotation-free whole-slide train-
ing approach to pathological classification of lung cancer types using deep
learning. Nat. Commun. 12, 1193.
Chen, H., Ma, Q., Lin, Z., Yan, J., 2021b. Hierarchy-aware label semantics
matching network for hierarchical text classification, in: Proc. 59th Annu.
Meeting Assoc. Comput. Linguistics & 11th Int. Joint Conf. Natural Lang.
Process., pp. 4370–4379.
Chen, R.J., Chen, C., Li, Y., Chen, T.Y., Trister, A.D., Krishnan, R.G., Mah-
mood, F., 2022. Scaling vision transformers to gigapixel images via hier-
archical self-supervised learning, in: Proc. IEEE/CVF Conf. Comput. Vis.
Pattern Recognit., pp. 16144–16155.
Chen, R.J., Ding, T., Lu, M.Y., Williamson, D.F.K., Jaume, G., Song, A.H.,
Chen, B., Zhang, A., Shao, D., Shaban, M., Williams, M., Oldenburg, L.,
Weishaupt, L.L., Wang, J.J., Vaidya, A., Le, L.P., Gerber, G., Sahai, S.,
Williams, W., Mahmood, F., 2024. Towards a general-purpose foundation
model for computational pathology. Nat. Med. 30, 850–862.
Chen, R.J., Lu, M.Y., Shaban, M., Chen, C., Chen, T.Y., Williamson, D.F.,
Mahmood, F., 2021c. Whole slide images are 2d point clouds: Context-
aware survival prediction using patch-based graph convolutional networks,
in: Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention,
Springer. pp. 339–349.
Chen, R.J., Lu, M.Y., Weng, W.H., Chen, T.Y., Williamson, D.F., Manz, T.,
Shady, M., Mahmood, F., 2021d. Multimodal co-attention transformer for
survival prediction in gigapixel whole slide images, in: Proc. IEEE/CVF Int.
Conf. Comput. Vis., pp. 4015–4025.
Chu, H., Sun, Q., Li, J., Chen, Y., Zhang, L., Guan, T., Han, A., He, Y.,
2024. Retmil: Retentive multiple instance learning for histopathological
whole slide image classification. arXiv preprint arXiv:2403.10858 .
Coudray, N., Ocampo, P.S., Sakellaropoulos, T., Narula, N., Snuderl, M.,
Feny¨o, D., Moreira, A.L., Razavian, N., Tsirigos, A., 2018. Classification
and mutation prediction from non–small cell lung cancer histopathology im-
14
Jiawen Li et al. / Medical Image Analysis (2024)
ages using deep learning. Nat. Med. 24, 1559–1567.
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. Imagenet: A
large-scale hierarchical image database, in: 2009 IEEE Conf. Comput. Vis.
Pattern Recognit., Ieee. pp. 248–255.
Di, D., Zou, C., Feng, Y., Zhou, H., Ji, R., Dai, Q., Gao, Y., 2022. Generating
hypergraph-based high-order representations of whole-slide histopathologi-
cal images for survival prediction. IEEE Trans. Pattern Anal. Mach. Intell.
45, 5800–5815.
Echle, A., Rindtorff, N.T., Brinker, T.J., Luedde, T., Pearson, A.T., Kather,
J.N., 2021. Deep learning in cancer pathology: a new generation of clinical
biomarkers. Br. J. Cancer 124, 686–696.
Ehteshami Bejnordi, B., Veta, M., Johannes van Diest, P., van Ginneken,
B., Karssemeijer, N., Litjens, G., van der Laak, J.A.W.M., the CAME-
LYON16 Consortium, 2017. Diagnostic assessment of deep learning algo-
rithms for detection of lymph node metastases in women with breast cancer.
JAMA 318, 2199–2210.
Fremond, S., Andani, S., Barkey Wolf, J., Dijkstra, J., Melsbach, S., Job-
sen, J.J., Brinkhuis, M., Roothaan, S., Jurgenliemk-Schulz, I., Lutgens,
L.C.H.W., Nout, R.A., van der Steen-Banasik, E.M., de Boer, S.M., Powell,
M.E., Singh, N., Mileshkin, L.R., Mackay, H.J., Leary, A., Nijman, H.W.,
Smit, V.T.H.B.M., Creutzberg, C.L., Horeweg, N., Koelzer, V.H., Bosse, T.,
2023. Interpretable deep learning model to predict the molecular classifica-
tion of endometrial cancer from haematoxylin and eosin-stained whole-slide
images: a combined analysis of the portec randomised trials and clinical co-
horts. Lancet Digital Health 5, e71–e82.
Guan, Y., Zhang, J., Tian, K., Yang, S., Dong, P., Xiang, J., Yang, W., Huang,
J., Zhang, Y., Han, X., 2022. Node-aligned graph convolutional network
for whole-slide image representation and classification, in: Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit., pp. 18813–18823.
Hamilton, W., Ying, Z., Leskovec, J., 2017. Inductive representation learning
on large graphs. Advances in neural information processing systems 30.
Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T.J., Zou, J., 2023.
A
visual–language foundation model for pathology image analysis using med-
ical twitter. Nat. Med. 29, 2307–2316.
Huang, Z., Chai, H., Wang, R., Wang, H., Yang, Y., Wu, H., 2021. Integration of
patch features through self-supervised learning and transformer for survival
analysis on whole slide images, in: Proc. Int. Conf. Med. Image Comput.
Comput.-Assisted Intervention, pp. 561–570.
Ilse, M., Tomczak, J., Welling, M., 2018. Attention-based deep multiple in-
stance learning, in: Proc. Int. Conf. Mach. Learn., PMLR. pp. 2127–2136.
Jaume, G., Oldenburg, L., Vaidya, A., Chen, R.J., Williamson, D.F., Peeters,
T., Song, A.H., Mahmood, F., 2024. Transcriptomics-guided slide repre-
sentation learning in computational pathology, in: Proc. IEEE/CVF Conf.
Comput. Vis. Pattern Recognit., pp. 9632–9644.
Kenton, J.D.M.W.C., Toutanova, L.K., 2019. Bert: Pre-training of deep bidi-
rectional transformers for language understanding, in: Proc. NAACL-HLT,
pp. 4171–4186.
Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 .
Kipf, T.N., Welling, M., 2017. Semi-supervised classification with graph con-
volutional networks, in: Proc. Int. Conf. Learn. Represent.
Kundra, R., Zhang, H., Sheridan, R., Sirintrapun, S.J., Wang, A., Ochoa, A.,
Wilson, M., Gross, B., Sun, Y., Madupuri, R., Satravada, B.A., Reales, D.,
Vakiani, E., Al-Ahmadie, H.A., Dogan, A., Arcila, M., Zehir, A., Maron,
S., Berger, M.F., Viaplana, C., Janeway, K., Ducar, M., Sholl, L., Dogan, S.,
Bedard, P., Surrey, L.F., Sanchez, I.H., Syed, A., Rema, A.B., Chakravarty,
D., Suehnholz, S., Nissan, M., Iyer, G.V., Murali, R., Bouvier, N., Soslow,
R.A., Hyman, D., Younes, A., Intlekofer, A., Harding, J.J., Carvajal, R.D.,
Sabbatini, P.J., Abou-Alfa, G.K., Morris, L., Janjigian, Y.Y., Gallagher,
M.M., Soumerai, T.A., Mellinghoff, I.K., Hakimi, A.A., Fury, M., Huse,
J.T., Bagrodia, A., Hameed, M., Thomas, S., Gardos, S., Cerami, E., Ma-
zor, T., Kumari, P., Raman, P., Shivdasani, P., MacFarland, S., Newman, S.,
Waanders, A., Gao, J., Solit, D., Schultz, N., 2021. Oncotree: a cancer clas-
sification system for precision oncology. JCO Clinical Cancer Informatics
5, 221–230.
Laleh, N.G., Muti, H.S., Loeffler, C.M.L., Echle, A., Saldanha, O.L., Mah-
mood, F., Lu, M.Y., Trautwein, C., Langer, R., Dislich, B., Buelow, D.R.,
Grabsch, I.H., Brenner, H., Chang-Claude, J., Alwers, E., Brinker, J.T.,
Khader, F., Truhn, D., Gaisa, T.N., Boor, P., Hoffmeister, M., Schulz,
V., Kather, N.J., 2022.
Benchmarking weakly-supervised deep learning
pipelines for whole slide classification in computational pathology. Med.
Image Anal. 79, 102474.
Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J., 2020. Biobert:
a pre-trained biomedical language representation model for biomedical text
mining. Bioinformatics 36, 1234–1240.
Li, B., Li, Y., Eliceiri, K.W., 2021. Dual-stream multiple instance learning
network for whole slide image classification with self-supervised contrastive
learning, in: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp.
14318–14328.
Li, H., Chen, Y., Chen, Y., Yu, R., Yang, W., Wang, L., Ding, B., Han, Y.,
2024a.
Generalizable whole slide image classification with fine-grained
visual-semantic interaction, in: Proc. IEEE/CVF Conf. Comput. Vis. Pat-
tern Recognit., pp. 11398–11407.
Li, J., Chen, Y., Chu, H., Sun, Q., Guan, T., Han, A., He, Y., 2024b. Dy-
namic graph representation with knowledge-aware attention for histopathol-
ogy whole slide image analysis, in: Proc. IEEE/CVF Conf. Comput. Vis.
Pattern Recognit., pp. 11323–11332.
Li, J., Cheng, J., Meng, L., Yan, H., He, Y., Shi, H., Guan, T., Han, A.,
2023a. Deeptree: Pathological image classification through imitating tree-
like strategies of pathologists. IEEE Trans. Med. Imaging .
Li, S., Zhao, Y., Zhang, J., Yu, T., Zhang, J., Gao, Y., 2023b.
High-order
correlation-guided slide-level histology retrieval with self-supervised hash-
ing. IEEE Trans. Pattern Anal. Mach. Intell. .
Li, X., Li, C., Rahaman, M.M., Sun, H., Li, X., Wu, J., Yao, Y., Grzegorzek,
M., 2022. A comprehensive review of computer-aided whole-slide image
analysis: from datasets to feature extraction, segmentation, classification and
detection approaches. Artif. Intell. Rev. 55, 4809–4878.
Linehan, W.M., Ricketts, C.J., 2019. The cancer genome atlas of renal cell
carcinoma: findings and clinical implications. Nat. Rev. Urol. 16, 539–552.
Litjens, G., Bandi, P., Ehteshami Bejnordi, B., Geessink, O., Balkenhol, M.,
Bult, P., Halilovic, A., Hermsen, M., van de Loo, R., Vogels, R., Manson,
Q.F., Stathonikos, N., Baidoshvili, A., van Diest, P., Wauters, C., van Dijk,
M., van der Laak, J., 2018. 1399 h&e-stained sentinel lymph node sections
of breast cancer patients: the camelyon dataset. GigaScience 7, giy065.
Lu, M.Y., Chen, B., Williamson, D.F.K., Chen, Richard J.and Liang, I., Ding,
T., Jaume, G., Odintsov, I., Le, L.P., Gerber, G., Parwani, A.V., Zhang, A.,
Mahmood, F., 2024. A visual-language foundation model for computational
pathology. Nat. Med. 30, 863–874.
Lu, M.Y., Chen, T.Y., Williamson, D.F., Zhao, M., Shady, M., Lipkova, J.,
Mahmood, F., 2021a. Ai-based pathology predicts origins for cancers of
unknown primary. Nature 594, 106–110.
Lu, M.Y., Williamson, D.F., Chen, T.Y., Chen, R.J., Barbieri, M., Mahmood,
F., 2021b. Data-efficient and weakly supervised computational pathology on
whole-slide images. Nat. Biomed. Eng. 5, 555–570.
Niazi, M.K.K., Parwani, A.V., Gurcan, M.N., 2019.
Digital pathology and
artificial intelligence. Lancet Oncol. 20, e253–e261.
OpenAI, 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 .
Otsu, N., 1975. A threshold selection method from gray-level histograms. Au-
tomatica 11, 23–27.
Perez-Lopez, R., Ghaffari Laleh, N., Mahmood, F., Kather, J.N., 2024. A guide
to artificial intelligence for cancer researchers. Nat. Rev. Cancer , 1–15.
Pinckaers, H., Van Ginneken, B., Litjens, G., 2020. Streaming convolutional
neural networks for end-to-end learning with multi-megapixel images. IEEE
Trans. Pattern Anal. Mach. Intell. 44, 1581–1590.
Qu, L., luo, x., Fu, K., Wang, M., Song, Z., 2024. The rise of ai language pathol-
ogists: Exploring two-level prompt learning for few-shot weakly-supervised
whole slide image classification. Proc. Adv. Neural Inf. Process. Syst. 36.
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sas-
try, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I., 2021.
Learning transferable visual models from natural language supervision, in:
Proc. Int. Conf. Mach. Learn., PMLR. pp. 8748–8763.
Riasatian, A., Babaie, M., Maleki, D., Kalra, S., Valipour, M., Hemati, S.,
Zaveri, M., Safarpoor, A., Shafiei, S., Afshari, M., Rasoolijaberi, M.,
Sikaroudi, M., Adnan, M., Shah, S., Choi, C., Damaskinos, S., Campbell,
J.C., Diamandis, P., Pantanowitz, L., Kashani, H., Ghodsi, A., Tizhoosh,
H., 2021. Fine-tuning and training of densenet for histopathology image
representation using tcga diagnostic slides. Med. Image Anal. 70, 102032.
Rodriguez, J.P.M., Rodriguez, R., Silva, V.W.K., Kitamura, F.C., Corradi,
G.C.A., de Marchi, A.C.B., Rieder, R., 2022. Artificial intelligence as a tool
for diagnosis in digital pathology whole slide images: A systematic review.
J. Pathol. Inform. 13, 100138.
Shao, Z., Bian, H., Chen, Y., Wang, Y., Zhang, J., Ji, X., zhang, y., 2021.
Transmil: Transformer based correlated multiple instance learning for whole
slide image classification. Proc. Adv. Neural Inf. Process. Syst. 34, 2136–
Jiawen Li et al. / Medical Image Analysis (2024)
15
2147.
Shi, J., Li, C., Gong, T., Zheng, Y., Fu, H., 2024. Vila-mil: Dual-scale vision-
language multiple instance learning for whole slide image classification, in:
Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 11248–11258.
Song, A.H., Chen, R.J., Ding, T., Williamson, D.F., Jaume, G., Mahmood,
F., 2024. Morphological prototyping for unsupervised slide representation
learning in computational pathology, in: Proc. IEEE/CVF Conf. Comput.
Vis. Pattern Recognit., pp. 11566–11578.
Song, A.H., Jaume, G., Williamson, D.F., Lu, M.Y., Vaidya, A., Miller, T.R.,
Mahmood, F., 2023. Artificial intelligence for digital and computational
pathology. Nat. Rev. Bioeng. 1, 930–949.
Sun, Q., Jiang, D., Li, J., Yan, R., He, Y., Guan, T., Cheng, Z., 2024. Nciemil:
Rethinking decoupled multiple instance learning framework for histopatho-
logical slide classification, in: Proc. Int. Conf. Medical Imaging Deep Learn.
Tarjan, R., 1972.
Depth-first search and linear graph algorithms.
SIAM J.
Comput. 1, 146–160.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. Proc. Adv. Neural
Inf. Process. Syst. 30.
Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y.,
2018. Graph attention networks, in: Proc. Int. Conf. Learn. Represent.
Wagner, S.J., Reisenb¨uchler, D., West, N.P., Niehues, J.M., Zhu, J., Foersch, S.,
Veldhuizen, G.P., Quirke, P., Grabsch, H.I., van den Brandt, P.A., Hutchins,
G.G., Richman, S.D., Yuan, T., Langer, R., Jenniskens, J.C., Offermans, K.,
Mueller, W., Gray, R., Gruber, S.B., Greenson, J.K., Rennert, G., Bonner,
J.D., Schmolze, D., Jonnagaddala, J., Hawkins, N.J., Ward, R.L., Morton,
D., Seymour, M., Magill, L., Nowak, M., Hay, J., Koelzer, V.H., Church,
D.N., Church, D., Domingo, E., Edwards, J., Glimelius, B., Gogenur, I.,
Harkin, A., Hay, J., Iveson, T., Jaeger, E., Kelly, C., Kerr, R., Maka, N.,
Morgan, H., Oien, K., Orange, C., Palles, C., Roxburgh, C., Sansom, O.,
Saunders, M., Tomlinson, I., Matek, C., Geppert, C., Peng, C., Zhi, C.,
Ouyang, X., James, J.A., Loughrey, M.B., Salto-Tellez, M., Brenner, H.,
Hoffmeister, M., Truhn, D., Schnabel, J.A., Boxberg, M., Peng, T., Kather,
J.N., 2023. Transformer-based biomarker prediction from colorectal cancer
histology: A large-scale multicentric study. Cancer Cell 41, 1650–1661.
Wang, D., Khosla, A., Gargeya, R., Irshad, H., Beck, A.H., 2016. Deep learning
for identifying metastatic breast cancer. arXiv preprint arXiv:1606.05718 .
Wang, W., Ma, S., Xu, H., Usuyama, N., Ding, J., Poon, H., Wei, F., 2023a.
When an image is worth 1,024 x 1,024 words: A case study in computational
pathology. arXiv preprint arXiv:2312.03558 .
Wang, Z., Yu, L., Ding, X., Liao, X., Wang, L., 2023b. Shared-specific feature
learning with bottleneck fusion transformer for multi-modal whole slide im-
age analysis. IEEE Trans. Med. Imaging 42, 3374–3383.
Xiang, J., Zhang, J., 2022. Exploring low-rank property in multiple instance
learning for whole slide image classification, in: Proc. Int. Conf. Learn. Rep-
resent.
Xiang, T., Song, Y., Zhang, C., Liu, D., Chen, M., Zhang, F., Huang, H.,
O’Donnell, L., Cai, W., 2022. Dsnet: A dual-stream framework for weakly-
supervised gigapixel pathology image analysis. IEEE Trans. Med. Imaging
41, 2180–2190.
Xiong, C., Chen, H., Sung, J.J., King, I., 2023. Diagnose like a pathologist:
transformer-enabled hierarchical attention-guided multiple instance learning
for whole slide image classification, in: Proc. Int. Joint Conf. Artif. Intell.,
pp. 1587–1595.
Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V.,
2021. Nystr¨omformer: A nystr¨om-based algorithm for approximating self-
attention, in: Proc. AAAI Conf. Artif. Intell., pp. 14138–14148.
Xu, K., Hu, W., Leskovec, J., Jegelka, S., 2018. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826 .
Yang, H., Chen, L., Cheng, Z., Yang, M., Wang, J., Lin, C., Wang, Y., Huang,
L., Chen, Y., Peng, S., Ke, Z., Li, W., 2021. Deep learning-based six-type
classifier for lung cancer and mimics from histopathological whole slide im-
ages: a retrospective study. BMC Med. 19, 1–14.
Yang, S., Wang, Y., Chen, H., 2024. Mambamil: Enhancing long sequence
modeling with sequence reordering in computational pathology.
arXiv
preprint arXiv:2403.06800 .
Zhang, H., Meng, Y., Zhao, Y., Qiao, Y., Yang, X., Coupland, S.E., Zheng, Y.,
2022. Dtfd-mil: Double-tier feature distillation multiple instance learning
for histopathology whole slide image classification, in: Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit., pp. 18802–18812.
Zhang, Y., Gao, J., Zhou, M., Wang, X., Qiao, Y., Zhang, S., Wang, D., 2023.
Text-guided foundation model adaptation for pathological image classifica-
tion, in: Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Interven-
tion, Springer. pp. 272–282.
Zhang, Z., Chen, P., Shi, X., Yang, L., 2019. Text-guided neural network train-
ing for image recognition in natural scenes and medicine. IEEE Trans. Pat-
tern Anal. Mach. Intell. 43, 1733–1745.
Zheng, Y., Gindra, R.H., Green, E.J., Burks, E.J., Betke, M., Beane, J.E., Ko-
lachalama, V.B., 2022. A graph-transformer for whole slide image classifi-
cation. IEEE Trans. Med. Imaging 41, 3003–3015.
Zhou, J., Ma, C., Long, D., Xu, G., Ding, N., Zhang, H., Xie, P., Liu, G., 2020.
Hierarchy-aware global model for hierarchical text classification, in: Proc.
58th Annu. Meeting Assoc. Comput. linguistics, pp. 1106–1117.
Zhou, K., Yang, J., Loy, C.C., Liu, Z., 2022a. Conditional prompt learning for
vision-language models, in: Proc. IEEE/CVF Conf. Comput. Vis. Pattern
Recognit., pp. 16816–16825.
Zhou, K., Yang, J., Loy, C.C., Liu, Z., 2022b. Learning to prompt for vision-
language models. Int. J. Comput. Vis. 130, 2337–2348.
