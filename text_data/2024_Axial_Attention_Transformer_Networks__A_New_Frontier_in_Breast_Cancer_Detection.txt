Axial Attention Transformer Networks: A New 
Frontier in Breast Cancer Detection 
1stWeijie He 
University of California, Los Angeles 
Los Angeles, USA 
 
 
2nd Runyuan Bao 
Johns Hopkins University 
Baltimore, USA 
 
 
3rdYiru Cang 
Northeastern University 
Boston, USA 
 
4thJianjun Wei 
Washington University in St. Louis 
St Louis, USA 
     
         
 
5thYang Zhang 
Boston University  
Boston, USA 
 
 
6thJiacheng Hu* 
Tulane University 
New Orleans, USA 
 
 
Abstractâ€”This paper delves into the challenges and 
advancements in the field of medical image segmentation, 
particularly focusing on breast cancer diagnosis. The authors 
propose a novel Transformer-based segmentation model that 
addresses the limitations of traditional convolutional neural 
networks (CNNs), such as U-Net, in accurately localizing and 
segmenting small lesions within breast cancer images. The model 
introduces an axial attention mechanism to enhance the 
computational efficiency and address the issue of global 
contextual information that is often overlooked by CNNs. 
Additionally, the paper discusses improvements tailored to the 
small dataset challenge, including the incorporation of relative 
position information and a gated axial attention mechanism to 
refine the model's focus on relevant features. The proposed 
model aims to significantly improve the segmentation accuracy 
of breast cancer images, offering a more efficient and effective 
tool for computer-aided diagnosis. 
Keywords-Medical Image Segmentation, Deep Learning, Breast 
Cancer Diagnosis, Axial Attention Mechanism 
I. 
INTRODUCTION 
The semantic segmentation of medical images is 
extremely important in the process of computer-aided 
diagnosis, as accurate localization and segmentation of lesions 
can provide strong assistance to doctors in clinical diagnosis. 
With the continuous improvement of computing power, a 
large number of researches related to deep learning have 
emerged in recent years, and many deep learning-based 
medical image segmentation algorithms have been successful. 
Although deep learning has made significant progress in the 
field of medical image analysis, it still faces many difficulties 
and challenges. 
Currently, there are excellent segmentation networks 
such as U-Net [1]. However, U-Net and other convolutional 
neural network-based segmentation models [2] have not fully 
met the strict requirements for segmentation accuracy in 
medical image analysis. In the task of breast cancer 
segmentation, the lesions usually account for a very small 
proportion of the entire breast cancer image, and most of the 
features extracted by the segmentation model are useless 
background information. In addition, considering the 
interference of occlusions between different tissues in the 
breast cancer segmentation task, how to accurately locate the 
lesions remains a challenging research work. 
The size of the convolutional kernel is fixed. Due to the 
inherent limitations of convolutional operations, each 
convolutional kernel only focuses on a local set of pixels in 
the entire image, forcing the network to pay more attention to 
local information rather than global contextual information, 
resulting in the lack of ability of convolutional neural 
networks to learn global information. Although subsequent 
research has proposed some methods [3, 4] to compensate for 
the shortcomings of convolutional neural networks, these 
methods are ultimately only a temporary solution and do not 
fundamentally solve the problems faced by convolutional 
neural networks. The summary of the second chapter has 
listed the advantages of Transformers [5] compared to 
convolutional neural networks, and Transformers are known 
for their ability to effectively model the global dependencies 
between inputs and outputs. However, most previous image 
segmentation methods have not been optimized for breast 
cancer segmentation tasks, and there is still a large room for 
improvement in lesion segmentation accuracy, so this paper 
focuses on the application of Transformers in breast cancer 
image segmentation. 
This paper first makes improvements to the problem of 
difficult localization of small lesions in breast cancer images, 
mainly including: 
(1) In response to the large size of medical images, the 
encoder structure of the Transformer is simplified, and the 
attention layer is optimized, replacing the traditional self-
attention mechanism with an axial attention mechanism. This 
reduces the computational complexity of the Transformer and 
improves the computational efficiency of the model on breast 
cancer images. 
(2) In response to the small size of medical image 
datasets, where the Transformer's segmentation performance 
is poor due to the lack of inductive bias on small datasets, this 
chapter adds relative position information during the 
calculation process of the axial attention mechanism. 
(3) In order to further enhance the key information in the 
relative position encoding and suppress the interference 
caused by inaccurate position information, this chapter 
proposes an axial attention mechanism with gated units. 
(4) By using the axial attention module with added 
gating units and relative position encoding to replace the 
traditional self-attention module, a Transformer segmentation 
model based on axial attention is constructed. 
II. 
RELATED WORK 
In clinical diagnosis, doctors also need to analyze the 
entire image from a global perspective to make judgments. 
Obviously, the Transformer is closer to the actual diagnostic 
process of doctors, so the potential of the Transformer-based 
segmentation network in medical image segmentation tasks is 
very large. However, the Transformer segmentation model 
based on the traditional attention mechanism is not without 
shortcomings. It can be seen that the huge computational cost 
of the traditional Transformer limits its potential in the 
analysis of large-sized images (such as medical images). In 
response to the above situation, Wang et al. [6] proposed 
Axial-Attention and probabilistic methods, which decomposes 
the two-dimensional self-attention mechanism into two one-
dimensional self-attention mechanisms. This can not only 
integrate global attention information and model long-distance 
spatial dependencies but also effectively reduce the 
computational cost of the attention mechanism, making it 
possible for the Transformer-based segmentation model to 
handle large-scale medical images. 
In recent years, deep learning has revolutionized the field 
of medical image analysis, particularly in cancer diagnosis 
and segmentation. The application of convolutional neural 
networks (CNNs) has been instrumental in many image 
classification and segmentation tasks. Xiao et al. [7] 
demonstrated the effectiveness of CNNs in classifying breast 
cancer cytopathology images, providing a foundational 
approach for image-based cancer detection. However, despite 
their success, CNN-based models like U-Net struggle with 
accurately segmenting small lesions due to their inability to 
capture global context efficiently. This limitation has 
motivated the exploration of alternative architectures, such as 
attention-based models and Transformers. 
The U-Net architecture has been a standard in medical 
image segmentation, yet it falls short in scenarios requiring 
high precision for small lesion detection, as in breast cancer 
images. Zhu et al. [8] proposed Attention-U-Net, which 
introduced an attention mechanism into the U-Net framework 
to improve segmentation accuracy by focusing on relevant 
regions of the image. Their work laid the groundwork for 
integrating attention mechanisms into segmentation networks, 
which directly inspired the axial attention mechanism 
introduced in this study. Zhu et al.'s work emphasized the 
importance of selectively enhancing the attention on critical 
areas within medical images, particularly where lesions are 
small and hard to distinguish. Xu et al. [9] highlighted the 
importance of data preprocessing and optimization techniques 
in improving deep learning performance in medical 
diagnostics, especially when dealing with limited datasets. 
Their emphasis on the preprocessing phase aligns with this 
study's strategy of using relative position encoding to enhance 
the performance of the Transformer model on small datasets. 
Yan et al. [10] extended the use of deep learning in 
cancer-related tasks beyond image segmentation by applying 
neural networks to survival prediction across diverse cancer 
types. Their work demonstrates the versatility of deep 
learning in medical applications, reinforcing the potential of 
neural networks to model complex medical data. Although 
survival prediction is not directly related to image 
segmentation, the optimization techniques and neural network 
structures 
they 
explored 
contribute 
to 
the 
broader 
understanding of how deep learning can be adapted to various 
medical challenges, including segmentation. Gao et al. [11] 
explored the optimization of neural networks through graph-
based techniques, which, while not specific to medical 
imaging, provide insights into enhancing network efficiency 
and accuracy. Their work on optimizing text classification 
models has parallels in this paper's use of the axial attention 
mechanism to optimize the efficiency of the Transformer 
model, particularly in high-resolution breast cancer image 
segmentation. Similarly, Yang et al. [12] developed dynamic 
neural network architectures for predicting sequential medical 
visits, showcasing how advanced neural architectures can be 
applied to complex medical data. This highlights the broader 
applicability of flexible neural network models in medical 
contexts, which informs the current work's approach to 
developing adaptable and efficient architectures for medical 
image segmentation. Additionally, Wang et al. [13] employed 
functional annotations to refine probabilistic models, 
highlighting methods that can inform more precise lesion 
segmentation in breast cancer imaging. Zheng et al. [14] 
proposed novel enhancements to deep learning optimizers, 
introducing adaptive friction mechanisms with sigmoid and 
tanh functions. Although their work focuses on optimizer 
improvements, 
the 
general 
principle 
of 
improving 
convergence and model performance has implications for the 
optimization strategies used in this study.  
III. 
BACKGROUND 
The gated axial attention mechanism introduced here 
similarly aims to refine the focus of the model on relevant 
features, enhancing segmentation accuracy and robustness in 
challenging datasets with limited size. 
 
Figure 1. Diagram of the Cross-Attention Module   
 
The principle of the axial attention approach is to 
repeatedly use the cross-attention module. As shown in Figure 
1, if you want to calculate the correlation between a pixel 
(ğœƒğ‘¥, ğœƒğ‘¦) and any pixel (ğ‘¢ğ‘¥, ğ‘¢ğ‘¦) in the figure, you can obtain it 
through two cross-attentions. Specifically, in the first cross-
attention, the blue pixel (ğœƒğ‘¥,ğœƒğ‘¦) interacts with the surrounding 
light green pixels that have a cross relationship, such as the  
points along the vertical axis (ğœƒğ‘¥, ğ‘¢ğ‘¦) and the points along the  
horizontal axis (ğ‘¢ğ‘¥, ğœƒğ‘¦). The second cross-attention can be 
based on the first calculation, and through the cross-
relationship of the light green features, establish a relationship 
with the deep green target pixel (ğ‘¢ğ‘¥, ğ‘¢ğ‘¦) . In the above 
example, the light green pixels (ğ‘¢ğ‘¥, ğœƒğ‘¦)  and (ğœƒğ‘¥,ğ‘¢ğ‘¦)  have 
already included the dependency relationship with the blue 
pixel(ğœƒğ‘¥, ğœƒğ‘¦) in the first information interaction, so any two 
points in the two-dimensional image only need two cross-
attentions to establish information interaction. Compared with 
the traditional attention mechanism, the axial attention 
mechanism can greatly reduce the network's computational 
load to O(hw), while still effectively achieving the interaction 
of global contextual information. 
IV. 
METHOD 
Traditional convolutional neural networks lack the ability 
to model long-range dependencies in images, while 
Transformers are effective in modeling global dependencies. 
This chapter focuses on exploring the application of 
Transformers in the field of medical image segmentation and 
proposes a solution to the problem of locating small lesions in 
breast cancer image segmentation tasks. This section mainly 
introduces 
the 
overall 
structure 
of 
the 
Transformer 
segmentation model based on axial attention and the 
important axial attention module. 
A. Relative Position Information and Axial Attention 
The current situation of small data volume in medical 
image datasets limits the performance of the Transformer 
from another aspect. In response to the characteristic of small 
data volume in medical image datasets, the solution of this 
paper is to add relative position information to the axial 
attention mechanism. Relative position information plays a 
role in supplementing position information in the process of 
axial attention computation, supplementing information from 
the perspective of the query, and supplementing information 
from the perspective of the key. The more information related 
to the segmentation task is gathered, the more useful 
information is obtained, and the more accurate the 
segmentation result is. 
Taking Figure 3 as an example, assuming the input 
feature map is xx, the height of the feature map is HH, the 
width is WW, and the number of channels is CC. The 
traditional self-attention mechanism can be expressed as 
follows: 
 
Figure 3.  The schematic diagram of the attention mechanism with the 
addition of relative position encoding.    
ğ‘¦ğ‘–ğ‘—= âˆ‘â€Š
ğ»
â„=1
âˆ‘â€Š
ğ‘Š
ğ‘¤=1
softmax (ğ‘ğ‘–ğ‘—
Tğ‘˜â„ğ‘¤)ğ‘£â„ğ‘¤ 
The self-attention mechanism with added relative 
position encoding can be expressed as follows: 
 
 
Figure 2. Structural diagram of the Transformer segmentation model based on axial attention. 
ğ‘¦ğ‘–ğ‘—= âˆ‘â€Š
ğ»
â„=1
âˆ‘â€Š
ğ‘Š
ğ‘¤=1
softmax (ğ‘ğ‘–ğ‘—
Tğ‘˜â„ğ‘¤+ ğ‘ğ‘–ğ‘—
Tğ‘…ğ‘„+ ğ‘˜â„ğ‘¤
T ğ‘…ğ¾)(ğ‘£â„ğ‘¤
+ ğ‘…ğ‘‰) 
Equation adds three learnable relative position encodings 
compared to the traditional attention mechanism. In 
experiments, the relative position encodings are a set of 
randomly initialized parameters, that is, learnable positional 
information. Although it increases the computational cost to 
some extent, the relative position information compensates for 
the weak inductive bias modeling ability of the Transformer 
when the data volume is small, which can be said to further 
reduce the obstacles of the Transformer in the breast cancer 
segmentation task by adding relative position encoding. 
Assuming it is decomposed, it can be obtained along the 
height axis attention and along the width axis attention, that is, 
a two-dimensional attention is decomposed into two one-
dimensional axial attentions. 
The one-dimensional axial attention along the height can be 
expressed as follows: 
ğ‘¦ğ‘–ğ‘—= âˆ‘â€Š
ğ»
â„=1
softmax (ğ‘ğ‘–ğ‘—
Tğ‘˜â„ğ‘—+ ğ‘ğ‘–ğ‘—
Tğ‘…ğ‘„+ ğ‘˜â„ğ‘—
T ğ‘…ğ¾)(ğ‘£â„ğ‘—+ ğ‘…ğ‘‰) 
The one-dimensional axial attention along the width can be 
expressed as follows: 
ğ‘¦ğ‘–ğ‘—= âˆ‘â€Š
ğ‘Š
ğ‘¤=1
softmax (ğ‘ğ‘–ğ‘—
Tğ‘˜ğ‘–ğ‘¤+ ğ‘ğ‘–ğ‘—
Tğ‘…ğ‘„+ ğ‘˜ğ‘–ğ‘¤
T ğ‘…ğ¾)(ğ‘£ğ‘–ğ‘¤+ ğ‘…ğ‘‰) 
B. Gated Axial Attention Module 
Inspired by Attention U-Net, this paper adds a gating 
mechanism to the axial attention to further amplify key 
information and suppress interference items. The motivation 
for this improvement is that on small datasets such as breast 
cancer images, positional biases are difficult to learn, and 
therefore the long-distance information interaction in the 
Transformer is not always accurate. Although the introduction 
of relative positional encoding in the previous section has 
alleviated this issue to some extent, it has overlooked the fact 
that inaccurate positional encoding may reduce the model's 
segmentation accuracy  
Therefore, based on Equations, this paper adds gate units 
to form an improved axial attention mechanism, as shown in 
Figure 4. Gate units can control the influence of positional 
biases on non-local context encoding and regulate the learning 
process of relative positional encoding to some extent. The 
modified axial attention mechanism calculation process can 
be expressed as follows: 
ğ‘¦ğ‘–ğ‘—= âˆ‘â€Š
ğ»
â„=1
softmax (ğ‘ğ‘–ğ‘—
Tğ‘˜â„ğ‘—+ ğºğ‘„ğ‘ğ‘–ğ‘—
Tğ‘…ğ‘„+ ğºğ¾ğ‘˜â„ğ‘—
T ğ‘…ğ¾)(ğ‘£â„ğ‘—
+ ğºğ‘‰ğ‘…ğ‘‰) 
ğ‘¦ğ‘–ğ‘—= âˆ‘â€Š
ğ‘Š
ğ‘¤=1
softmax (ğ‘ğ‘–ğ‘—
Tğ‘˜ğ‘–ğ‘¤+ ğºğ‘„ğ‘ğ‘–ğ‘—
Tğ‘…ğ‘„+ ğºğ¾ğ‘˜ğ‘–ğ‘¤
T ğ‘…ğ¾)(ğ‘£ğ‘–ğ‘¤
+ ğºğ‘‰ğ‘…ğ‘‰) 
In Equations, the three gate units, like relative positional 
encodings, are learnable parameters. When the data volume of 
the breast cancer image dataset is not sufficient for the 
network to learn accurate positional biases, the gate units will 
give the relative positional encoding a smaller weight to 
suppress inaccurate relative positional information and avoid 
interference with the model's segmentation accuracy. 
Conversely, the gate units give the relative positional 
encoding a larger weight to assist the model in highlighting 
important positional pixels in the axial attention calculation, 
achieving accurate positioning of subtle lesions in the breast 
cancer segmentation task. 
 
Figure 4. Schematic Diagram of the Gated Axial Attention Mechanism 
V. 
EXPERIMENT 
A. Dataset 
The Breast Ultrasound Images Dataset (BUSI) [15] is a 
public dataset that was collected in 2018. It includes breast 
ultrasound images from women aged between 25 and 75 years 
old. The BUSI dataset comprises 780 breast ultrasound 
images from 600 patients, captured using the LOGIQ E9 
ultrasound system. The average size of the images in BUSI is 
500 Ã— 500 pixels, and the format of the images is NG.  
B. Experiment Results Analysis 
Table 1. Experiment Results 
Model 
IoU 
Precision 
Recall 
F1 
Segnet[16] 
0.7402 
0.8061 
0.8112 
0.8086 
U-Net 
0.7731 
0.8278 
0.8371 
0.8324 
Res-UNet[17] 
0.7795 
0.8369 
0.8554 
0.8460 
Axial U-
Net[18] 
0.7691 
0.8577 
0.8311 
0.8441 
Axial 
Transformer 
0.7634 
0.8813 
0.8217 
0.8504 
 
As indicated in Table 1, the experiments quantitatively 
analyzed different models using precision, recall, intersection-
over-union (IoU), and F1 score. In the BMRI dataset, the 
challenge of locating small breast cancer lesions is more 
pronounced than in the breast ultrasound image dataset due to 
the small size of the lesions. Analysis of the contents of Table 
1 reveals that the proposed method surpasses mainstream 
convolutional neural network methods in the F1 score metric.  
This suggests that the Axial Transformer can better 
capture 
long-term 
dependencies 
compared 
to 
other 
convolution-based segmentation models, mainly thanks to the 
multi-head attention mechanism of the Transformer. With 
better modeling of long-range dependencies, even when the 
background is much larger than the target, the Axial 
Transformer is less likely to misjudge the background as a 
lesion. This indicates that the Axial Transformer has indeed 
addressed the issue of difficult small lesion localization in 
breast cancer segmentation tasks. 
In summary, the proposed Axial Transformer leverages 
the advantages of the Transformer model in modeling long-
range dependencies for breast cancer segmentation tasks, 
combining axial attention and relative position encoding to 
achieve accurate localization of small breast cancer lesions. In 
performance comparisons, the segmentation precision and F1 
score are higher than those of CNN segmentation models. 
However, the Axial Transformer has lower recall and IoU 
evaluation metrics, indicating a weaker ability to find all 
lesions. Radiologists have pointed out that, in addition to 
accurately locating small lesions, breast cancer segmentation 
models also need to accurately segment the blurred edges of 
lesions. 
VI. CONCLUSIONS 
In this paper, we introduced a transformative approach to 
breast cancer image segmentation by developing a novel 
Transformer-based model that employs an axial attention 
mechanism to overcome the limitations of traditional 
convolutional neural networks, such as U-Net, in accurately 
localizing small lesions. The proposed model decomposes the 
conventional self-attention mechanism into two one-
dimensional axial attention along the horizontal and vertical 
axes, significantly enhancing the model's ability to capture 
critical features across the entire image while simultaneously 
reducing 
computational 
complexity. 
Recognizing 
the 
challenges posed by small datasets in medical imaging, we 
further incorporated a gated position-sensitive axial attention 
mechanism that leverages relative position information, 
allowing the model to focus more precisely on relevant 
lesions. This gated mechanism enhances the model's 
robustness and adaptability, particularly in scenarios with 
limited data, by regulating the attention focus, thereby 
reducing the impact of irrelevant background noise and 
occlusions. Our work not only advances the accuracy of breast 
cancer 
lesion 
segmentation 
but 
also 
provides 
a 
computationally efficient solution that can be seamlessly 
integrated into clinical workflows, offering a significant leap 
forward in computer-aided diagnostic tools and potentially 
improving early detection and treatment outcomes for breast 
cancer patients. 
REFERENCES 
[1] O. Ronneberger, P. Fischer, and T. Brox, "U-Net: Convolutional 
Networks for Biomedical Image Segmentation," in Proceedings 
of the International Conference on Medical Image Computing 
and Computer-Assisted Intervention (MICCAI), Springer, 2015, 
pp. 234-241. 
[2] V. K. G, "Convolutional neural network based segmentation," 
in Proceedings of the International Conference on Information 
Processing, Springer, Berlin, Heidelberg, 2011, pp. 190-197. 
[3] J. Yao, C. Li, K. Sun, Y. Cai, H. Li, W. Ouyang, and H. Li, 
"Ndc-scene: Boost monocular 3d semantic scene completion in 
normalized device coordinates space," in Proceedings of the 
2023 IEEE/CVF International Conference on Computer Vision 
(ICCV), IEEE Computer Society, 2023, pp. 9421-9431. 
[4] S. Mehta, M. Rastegari, A. Caspi, et al., "Espnet: Efficient 
spatial 
pyramid 
of 
dilated 
convolutions 
for 
semantic 
segmentation," in Proceedings of the European Conference on 
Computer Vision (ECCV), 2018, pp. 552-568. 
[5] A. Vaswani, "Attention is all you need," arXiv preprint 
arXiv:1706.03762, 2017. 
[6] M. Wang, Y. Xie, J. Liu, A. Li, L. Chen, A. Stromberg, and C. 
Wang, "A Probabilistic Approach to Estimate the Temporal 
Order of Pathway Mutations Accounting for Intra-Tumor 
Heterogeneity," Cancers, vol. 16, no. 13, pp. 2488, 2024. 
[7] M. Xiao, Y. Li, X. Yan, M. Gao, and W. Wang, "Convolutional 
neural network classification of cancer cytopathology images: 
taking breast cancer as an example," Proceedings of the 2024 
7th 
International 
Conference 
on Machine 
Vision 
and 
Applications, pp. 145-149, 2024. 
[8] Z. Zhu, Y. Yan, R. Xu, Y. Zi, and J. Wang, "Attention-Unet: A 
Deep Learning Approach for Fast and Accurate Segmentation in 
Medical Imaging," Journal of Computer Science and Software 
Applications, vol. 2, no. 4, pp. 24-31, 2022. 
[9] R. Xu, Y. Zi, L. Dai, H. Yu, and M. Zhu, "Advancing Medical 
Diagnostics with Deep Learning and Data Preprocessing," 
International Journal of Innovative Research in Computer 
Science & Technology, vol. 12, no. 3, pp. 143-147, 2024. 
[10] X. Yan, W. Wang, M. Xiao, Y. Li, and M. Gao, "Survival 
prediction across diverse cancer types using neural networks," 
Proceedings of the 2024 7th International Conference on 
Machine Vision and Applications, pp. 134-138, 2024. 
[11] E. Gao, H. Yang, D. Sun, H. Xia, Y. Ma, and Y. Zhu, "Text 
classification optimization algorithm based on graph neural 
network," arXiv preprint arXiv:2408.15257, 2024. 
[12] W. Yang, Z. Wu, Z. Zheng, B. Zhang, S. Bo, and Y. Yang, 
"Dynamic Hypergraph-Enhanced Prediction of Sequential 
Medical Visits," arXiv preprint arXiv:2408.07084, 2024. 
[13] M. Wang, T. Yu, J. Liu, L. Chen, A. J. Stromberg, J. L. Villano, 
and C. Wang, "A probabilistic method for leveraging functional 
annotations to enhance estimation of the temporal order of 
pathway 
mutations 
during 
carcinogenesis," 
BMC 
Bioinformatics, vol. 20, pp. 1-12, 2019. 
[14] H. Zheng, B. Wang, M. Xiao, H. Qin, Z. Wu, and L. Tan, 
"Adaptive Friction in Deep Learning: Enhancing Optimizers 
with 
Sigmoid 
and 
Tanh 
Function," 
arXiv 
preprint 
arXiv:2408.11839, 2024. 
[15] W. Al-Dhabyani, M. Gomaa, and H. Khaled, "Dataset of breast 
ultrasound images," Data in Brief, vol. 28, pp. 104863, 2020. 
[16] V. Badrinarayanan, A. Kendall, and R. Cipolla, "Segnet: A deep 
convolutional 
encoder-decoder 
architecture 
for 
image 
segmentation," IEEE Transactions on Pattern Analysis and 
Machine Intelligence, vol. 39, no. 12, pp. 2481-2495, 2017. 
[17] X. Xiao, S. Lian, Z. Luo, et al., "Weighted res-unet for high-
quality retina vessel segmentation," Proceedings of the 2018 9th 
International Conference on Information Technology in 
Medicine and Education (ITME), IEEE, 2018, pp. 327-331. 
[18] H. Wang, Y. Zhu, B. Green, et al., "Axial-deeplab: Stand-alone 
axial-attention for panoptic segmentation," in Proceedings of the 
2020 16th European Conference on Computer Vision (ECCV), 
Springer, 2020, pp. 108-126. 
