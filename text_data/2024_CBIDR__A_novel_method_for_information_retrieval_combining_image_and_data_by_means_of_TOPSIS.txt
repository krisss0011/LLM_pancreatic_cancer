CBIDR: A novel method for information retrieval
combining image and data by means of TOPSIS applied
to medical diagnosis
Humberto Giuria,b,∗, Renato A. Krohlinga,b
aLabcin - Nature-inspired computing Lab. - Federal University of Esp´ırito Santo -
Vit´oria, ES, Brazil
bGraduate Program in Computer Science, PPGI, UFES - Federal University of Esp´ırito
Santo - Vit´oria, ES, Brazil
Abstract
Content-Based Image Retrieval (CBIR) have shown promising results in the
field of medical diagnosis, which aims to provide support to medical pro-
fessionals (doctor or pathologist). However, the ultimate decision regarding
the diagnosis is made by the medical professional, drawing upon their ac-
cumulated experience. In this context, we believe that artificial intelligence
can play a pivotal role in addressing the challenges in medical diagnosis not
by making the final decision but by assisting in the diagnosis process with
the most relevant information. The CBIR methods use similarity metrics to
compare feature vectors generated from images using Convolutional Neural
Networks (CNNs). In addition to the information contained in medical im-
ages, clinical data about the patient is often available and is also relevant in
the final decision-making process by medical professionals. In this paper, we
propose a novel method named CBIDR, which leverage both medical images
and clinical data of patient, combining them through the ranking algorithm
TOPSIS. The goal is to aid medical professionals in their final diagnosis by
retrieving images and clinical data of patient that are most similar to query
data from the database. As a case study, we illustrate our CBIDR for diag-
nostic of oral cancer including histopathological images and clinical data of
patient. Experimental results in terms of accuracy achieved 97.44% in Top-1
∗Corresponding author
Email addresses: krohling.renato@gmail.com (Humberto Giuri),
humbertogiuri@gmail.com (Renato A. Krohling)
Preprint submitted to Journal
October 10, 2024
arXiv:2410.06180v1  [cs.IR]  26 Sep 2024
and 100% in Top-5 showing the effectiveness of the proposed approach.
Keywords:
Content-Based Image Retrieval. Convolutional Neural
Networks. Ranking Algorithms. TOPSIS. Histopathological Images.
Clinical data
1. Introduction
In recent years, Convolutional Neural Networks (CNN) for image classi-
fication have provided promising results (Rawat and Wang, 2017), (Li et al.,
2020), (Wang and Deng, 2021), (Nawaz et al., 2022), among others. Applica-
tions focused on the medical field (Yang et al., 2021) have also been explored
achieving good results such as the classification of skin lesion images (Wu
et al., 2022), classification of histopathological images of oral cancer using
different neural network architectures (Maia et al., 2023), among others. Be-
jnordi et al. (2017) used 7 different types of CNNs to classify metastases
in breast cancer histopathological images, achieving results comparable to
11 experienced pathologists. However, many challenges still need to be ad-
dressed when using CNNs for medical image classification. The lack of pub-
licly labeled databases by experts, class imbalance, and the high resolution of
histopathological images are some of the challenges faced and opportunities
that arise in this area of study (Tizhoosh and Pantanowitz, 2018), (Goyal
et al., 2020).
Image retrieval aims to provide support to medical professionals during
diagnosis. However, the ultimate decision regarding the diagnosis is made
by the doctor or pathologist, drawing upon their accumulated experience.
In this context, we believe that deep learning can play an important role
in addressing the challenges in medical diagnosis—not by making the final
decision but by assisting in the diagnosis process by suggesting the most rele-
vant information to the doctors. To overcome the challenges while leveraging
the expertise of pathologists, applications and frameworks designed to assist
pathologists are starting to emerge, showing promising results. Advances
have been made in both accuracy and speed when applied to areas such as
Image Retrieval (Kuo et al., 2016), especially for applications that rely on
the visual characteristics of the image. This field of study is called Content-
Based Image Retrieval (CBIR). Barhoumi and Khelifa (2021) propose a tool
to retrieve similar images related to an input image by measuring the dis-
tance to the feature map generated by the convolution and pooling processes
2
of a CNN.
There is an extensive body of literature in the field of image retrieval,
with the majority of works focusing on content-based image retrieval using
neural networks as feature extractors. For example, Alzu'bi et al. (2017)
achieved an accuracy of nearly 95.7% on the Oxford 5K database (Philbin
et al., 2007) and 88.6% on the Oxford 105K database (Philbin et al., 2007)
using two parallel CNNs as feature extractors.
When applied to the medical field, CBIR systems have proven to be a
valuable tool in diagnostic support. For instance, Choe et al. (2022) improved
the diagnostic accuracy of interstitial lung disease and the agreement among
experts with varying levels of experience using CBIR for chest computed
tomography images with deep learning.
Komura et al. (2018) developed a web application called Luigi, which
retrieves histopathological images similar to various types of cancer using
a pretrained convolutional neural network to extract features. Hegde et al.
(2019) proposed the SMILY framework (Similar Medical Images Like Yours).
SMILY is a tool for searching for similar histopathological images trained
without the use of labeled histopathological images in training. SMILY was
trained and tested on The Cancer Genome Atlas Program (TCGA) database,
achieving high accuracy and efficiency in image retrieval. The tool achieved
an average precision and recall of 96% and 98%, respectively. It was able
to search a database containing tens of thousands of samples in less than
1 second, outperforming various state-of-the-art algorithms in both accu-
racy and time efficiency.
Due to the high dimensionality of this type of
image, as the number of captured and usable images in the system increases,
image retrieval operations become increasingly computationally expensive.
Chen et al. (2022), in addition to discussing this point, they propose a fast
and scalable image retrieval system for slides, which are glass slides contain-
ing samples of biological tissue prepared for microscopic analysis, achieved
through deep self-supervised learning. The algorithm, named SISH (Self-
Supervised Image Search for Histology), achieved nearly constant retrieval
time regardless of the image database size.
In order to assist pathologists in their decision-making process regarding
the diagnosis, Defraire (2021) proposed a distributed approach to histopatho-
logical image retrieval to handle millions or even billions of images. This
approach utilizes convolutional neural network architectures as feature ex-
tractors and the Faiss library (Johnson et al., 2017) for fast approximate
nearest neighbor search, which is commonly used in image retrieval systems.
3
Hashimoto et al. (2023) introduced a new method for similar image retrieval
in histopathological images stained with hematoxylin and eosin for malig-
nant lymphoma. When using a whole-slide image as the input query, which
can be of high dimensionality, it is desirable to retrieve similar cases while
focusing on areas of pathological importance, such as tumor cells.
In an effort to incorporate the information contained in clinical data into
computer-aided diagnosis (CAD), de Lima et al. (2023) assessed the impor-
tance of supplementary data in the analysis of histopathological images of
oral leukoplakia and carcinoma. The study concluded that clinical and de-
mographic data positively influenced the accuracy of the models, resulting
in a 30% improvement in balanced accuracy.
In the majority of cases, image retrieval work relies on content-based ap-
proaches using convolutional neural networks. In cases of histopathological
images, these images are often collected along with clinical data of patient.
This information is quite rich and has significant potential for providing in-
sights into the disease. Therefore, this study will investigate the combination
of images and clinical data in the process of assisting pathologists to achieve
a more accurate diagnosis.
In this work, a novel approach for image retrieval is proposed combining
convolutional neural networks used for image feature extraction with clin-
ical data of patient. As a case study, a dataset NDB-UFES consisting of
histopathological images of oral cancer and patient data (de Assis et al.,
2023), is used. This dataset contains three classes considered for diagnosis:
oral cavity squamous cell carcinoma, leukoplakia with dysplasia, and leuko-
plakia without dysplasia. Additionally, clinical and sociodemographic patient
information is included, such as gender, lesion location, smoking habits, al-
cohol consumption, age, and sun exposure (de Assis et al., 2023).
The main contributions of this work are twofold:
• we propose for the first time, as far as we know, a new method for
Content Based Image and Data Retrieval (CBIDR) by means of the
ranking algorithm TOPSIS (Technique for Order Preference by Simi-
larity to Ideal Solution).
• we apply the method CBIDR to a case study involving diagnostic of oral
cancer using NDB-UFES dataset, which consists of histopathological
images and clinical data of patient in order to illustrate the approach
and show its feasibility.
4
In the remaining sections of the paper, we follow this structure: Section
2 briefly describes Convolutional Neural Networks, which are used in this
work as feature extractors. Section 3 explains the basic concepts of Content-
Based Image Retrieval (CBIR) and proposes a new methodology (CBIDR)
to combine images and data through the ranking algorithm TOPSIS. Section
4 presents and discuss the obtained results and Section 5 ends up the paper
with conclusions as well as directions for future work.
2. Convolutional Neural Networks
2.1. Background
Convolutional Neural Networks (CNNs), proposed by LeCun et al. (1989),
were originally used for handwritten postal code recognition. However, they
have found applications in various fields, especially in dealing with high-
dimensional data. Krizhevsky et al. (2017) demonstrated the potential of
CNNs by winning the ”ImageNet Large Scale Visual Recognition Challenge”
competition (Russakovsky et al., 2015) using a multi-layered CNN known
as AlexNet.
This result showcased the viability and versatility of CNNs.
CNNs architecture consist of three types of layers: convolutional, pooling,
and fully connected layers (Goodfellow et al., 2016). Figure 1 illustrates a
CNN architecture.
Figure 1: Illustration of an architecture of convolutional neural network with convolutional
layers, pooling, and fully connected layers.
The convolutional layer consists of filters, often referred to as kernels.
These trainable filters traverse the entire input data, applying the convolution
process. In most cases, the input data is an image. These input filters are
trainable, meaning they seek to learn the best configuration so that the
output of this layer is a feature map that effectively describes the input.
5
Typically, towards the end of the convolutional layer, an activation function
is applied to the resulting feature map. This activation function introduces
non-linearity to the model, and enhances important features, allowing the
CNN to learn and model complex relationships within the input data. One
commonly used activation function in CNNs is the Rectified Linear Unit
(ReLU) (Goodfellow et al., 2016). It is often chosen for its simplicity and
effectiveness, helping to identify activation regions in the feature maps of the
convolutional layer.
The pooling operation aims to reduce the dimensionality of the feature
map resulting from the convolutional layer (Goodfellow et al., 2016) by se-
lecting the most important features. During the pooling operation, a sliding
window, for example, 2x2, is applied across the output of the previous layer.
This window’s purpose is to summarize the information within that area
into a single value, thus achieving dimensionality reduction and highlighting
relevant features. This technique helps simplify and compress information,
enabling more efficient processing and a more compact representation of the
data. The two most well-known pooling techniques are Max Pooling (Max-
Pool) and Average Pooling (AvgPool).
The final part of a CNN, consists of a feedforward neural network, also
known as a Multilayer Perceptron (MLP) (Goodfellow et al., 2016). Typi-
cally, it acts as the classification block in the CNN, taking the feature map
as input and producing an output for a given input. A feedforward neural
network consists of three main types of layers: the input layer, hidden lay-
ers, and the output layer (Goodfellow et al., 2016). The input layer, as the
name suggests, receives the input data. In a CNN, this input data represents
the feature map. The hidden layers, situated between the input and output
layers, are responsible for learning and storing abstract representations of
the input data. In these hidden layers, each neuron receives outputs from
neurons in the previous layer, performs calculations based on its weights and
activation function, and passes the result to the next layer. Ultimately, in
the output layer, the model’s output is computed.
In convolutional neural networks, before feeding the fully connected (FC)
layer with the resulting feature map, it is necessary to perform the flatten
operation. Typically, this feature map is a three-dimensional vector, with
depth representing the number of filters used. The FC layer needs to receive a
one-dimensional vector, so the feature map is flattened into a one-dimensional
vector, allowing the feedforward neural network to classify the input data.
Most convolutional neural networks, such as ResNet (He et al., 2015),
6
MobileNetV2 (Sandler et al., 2018), and DenseNet-121 (Huang et al., 2017),
sequentially employ blocks of convolution and pooling layers multiple times.
This architecture allows the model to capture information at different scales.
Since convolutional neural networks can be used for dimensionality reduction
of input data, providing a one-dimensional vector, removing the fully con-
nected layers (FC) from the network yields the feature vector. This task is
known as feature extraction (Atasever et al., 2023) and involves representing
the original input data in a compact form without losing information qual-
ity, capable of capturing essential features while discarding deemed irrelevant
information. Feature extraction enables high-dimensional data, such as im-
ages, to have their key characteristics mapped to reduced dimensionality,
improving efficiency.
2.2. Training Convolutional Neural Networks
For training a neural network, the backpropagation algorithm, as intro-
duced by Rumelhart et al. (1986), is commonly used. The goal of this al-
gorithm is to adjust the network’s weights to minimize the associated cost
function. Initially, the network’s weights and bias are randomly initialized.
The network is then fed with the training data, and the output of each
neuron is calculated until reaching the final layer. After calculating the fi-
nal network outputs, the error can be computed by comparing the obtained
output with the expected output. Next, the error is propagated backward
through the network, determining the contribution of error from each neuron.
This is accomplished using gradient descent, which involves computing the
partial derivative of the error with respect to the synaptic weights of each
connection. The weights are updated for each connection by an amount pro-
portional to the gradient descent, multiplied by a learning rate. This update
is aimed at reducing the error. These steps are repeated for all training data,
constituting one epoch. The training of a neural network typically involves
multiple epochs, often predetermined, or until convergence is achieved. In
summary, backpropagation is a key algorithm for training neural networks,
and it iteratively adjusts the weights to minimize the error between predicted
and actual outputs during training.
The goal of training a neural network is to find the weights and bias
configurations for a given problem, and for a CNN, this includes finding
the optimal filter configurations for the convolutional layers. In the training
process of a convolutional neural network, labeled data must be provided to
enable the network to make predictions. By using a loss function (Goodfellow
7
et al., 2016), one can measure the comparison between predicted classes and
true classes. The objective is to minimize this resulting error to obtain the
correct predictions. The Cross-Entropy loss function is commonly used and
it is described by:
Loss(x, y) = −
N
X
i
xi log yi
(1)
where xi represents the true probability of image i belonging to the correct
class, yi is the predicted probability by the neural network that image i
belongs to the correct class and N is the total number of training examples.
In feature extraction tasks, loss functions based on ranking are commonly
used. They aim to predict relative distances between inputs. One such loss
function is the Margin Loss (Wu et al., 2017), defined as:
Lmargin =
X
(i,j)∈P
γ + Iyi=yj (d (ϕi, ϕj) −β) −Iyi̸=yj (d (ϕi, ϕj) −β)
(2)
where β is an adjustable parameter, regulated during training.
The goal
here is to learn a function ϕ such that dϕ(xa, xn) −dϕ(xa, xp) < γ, assuming
xa is an anchor image from a specific class, xn is an image from a different
class than xa, and xp is an image from the same class as xa. Thus, this
loss function encourages images of the same class to be closer while pushing
images of different classes farther apart.
To minimize the training error, optimization algorithms like Stochastic
Gradient Descent (SGD) (Ruder, 2016) and Adam (Kingma and Ba, 2017)
are commonly used. This class of algorithms aims to minimize the loss func-
tion to its optimal value by updating the weights and biases of the network. In
real-world applications, convolutional neural networks often leverage transfer
learning. So, a pre-trained CNN, trained on a large dataset like ImageNet
(Deng et al., 2009), is used. This allows to save the previously learned net-
work parameters and fine-tune them on your specific target dataset.
3. Image Retrieval
3.1. Preliminaries
The main methods for image retrieval (Riad et al., 2012) are text-based
image retrieval (TBIR), content-based image retrieval (CBIR), and hybrid
8
approaches. In TBIR, the text associated with the image is considered to
determine its content, making it possible to retrieve similar elements based
on the text present. In the case of CBIR, features such as color, shape, and
texture are used to index and compare the similarity of input images with
those in the database. The third method, a hybrid retrieval, aims to combine
the two previously mentioned approaches to achieve better results.
To determine the similarity between the input image and those in the
database, a similarity metric is computed based on the extracted features
(Riad et al., 2012). Therefore, research in this area focuses on how to ex-
tract features that best describe the images, aiming for a detailed compari-
son. With the advancement of CNNs for feature extraction tasks, they have
been introduced into CBIR systems for this purpose.
For instance, Rian
et al. (2019) investigated the use of convolutional neural networks as feature
extractors for image retrieval in the iNaturalist database (Van Horn et al.,
2017), achieving accuracy of 89%.
The main advantage of using CNNs for image retrieval lies in their ability
to extract features with a richness of information that describes the image.
Gkelios et al. (2021) conclude that when CNNs are employed for feature
extraction (or descriptors), they produce descriptors with high discriminative
power in a computationally efficient manner. These feature vectors enable
the comparison of two images using similarity metrics like the Euclidean
distance, which quantifies how similar two images are.
Datta et al. (2008) address the challenges and trends in the field of image
retrieval, presenting and reviewing the main similarity metrics used in this
area. Consider two feature vectors, a and b. The Euclidean distance between
them is defined as:
D(a, b) =
v
u
u
t
p
X
i=1
(ai −bi)2
(3)
where the sum of the square root of the difference between a and b in their
respective dimensions. There are several metrics that can be used to calculate
similarity between feature vectors but this is not the focus of this work and
may be explored elsewhere.
3.2. Standard CBIR Method
The image retrieval task is divided into 4 steps. The first one is to extract
the feature vectors from all images in the database using a CNN as a feature
9
extraction mechanism to create the descriptor database. The second step is
to receive a query image that will serve as the input image for the search.
After extracting the feature vector from the query image using the same CNN
as in the first step, distances are calculated between the input image and the
descriptors in the database. This allows ranking the compared descriptors
based on the metric and selecting the top images with the highest similarity
value. Figure 2 illustrates the CBIR method using CNN.
Figure 2: Illustration of an image retrieval process using only images.
The first step
represents the creation of the descriptor database. In this step, the images in the database
are passed through a CNN to extract features. Step 2 describes the process of feature
extraction from the query image. In step 3, the feature vector resulting from step 2 is
compared to those stored in the descriptor database, resulting in similarities between the
images. Step 4 concludes the process by sorting in ascending order, and displaying the
first images being the most similar.
The Faiss 1 library (Johnson et al., 2017) is used to store the descriptor
database. It performs the actual search for the k nearest neighbors through
an exhaustive search. The distance |xj −yi|2 is expanded to |xj|2 + |yi|2 −
2 ⟨xj, yi⟩, where ⟨.,.⟩is the dot product. The first two terms are precomputed,
so the bottleneck lies in evaluating ⟨xj, yi⟩.
1https://faiss.ai/
10
3.3. Method Combining Image and Data
As mentioned earlier, an individual’s lifestyle is directly related to the oc-
currence of various diseases. Therefore, clinical and sociodemographic data,
such as smoking habits, alcohol consumption, gender, and others, are usually
collected along with the images and provide relevant information for diagno-
sis. Since it is possible to numerically define how similar two images are by
extracting their respective feature vectors and measuring, using a similarity
metric, the distance between them, it is also possible to quantify how close
two sets of clinical information are. By organizing the information into a
binary vector, where 0 corresponds to the absence of a particular feature -
such as whether or not alcohol is consumed - and 1 corresponds to its pres-
ence, it is possible to measure the distance between two binary vectors using
distance metrics.
One of the most commonly used metrics to calculate similarity between
two binary vectors is the Hamming distance. As highlighted by Bookstein
(2002), the Hamming distance is commonly used to quantify the extent to
which two bit strings of the same dimension differ. An early application was
in the theory of error-correcting codes (Hamming, 1980), where the Ham-
ming distance measured the error introduced by noise over a channel when a
message, typically a sequence of bits, is transmitted between the source and
destination. To calculate the Hamming distance, a logical XOR operation is
performed between the positions of the two input vectors. The number of 1
bits in the resulting vector determines the value of the distance.
By using CNNs in the image retrieval process, distances between the ex-
tracted features of each image are ranked. By incorporating clinical data into
the analysis, two distance metrics are considered: one for images and another
for clinical data. Therefore, when performing an operation with image and
data retrieval, it is possible to compare an input query image and data with
those image features and data stored in the database, resulting in two dis-
tances for each element in the database. Thus, the system will generate an
Mx2 matrix, where each column represents one of the obtained distances. To
identify the option most similar to the query image and data, it is necessary
to rank an Mx2 matrix rather than a one-dimensional vector of size M. For
this purpose, the TOPSIS algorithm for ranking is employed, which has been
applied in different knowledge domains (Krohling and Pacheco, 2015).
11
3.3.1. TOPSIS
TOPSIS (Technique for Order Preference by Similarity to Ideal Solution)
proposed by Hwang and Yoon (1981) is a decision-making technique, which
works by evaluating the performance of alternatives based on their similarity
to the ideal solution. Multi-criteria decision-making problems are character-
ized by a decision matrix D, where each row represents an alternative and
each column represents a criterion. This matrix is used to organize the rel-
evant information for the decision problem, allowing for the comparison of
alternatives with respect to the established criteria. Based on this matrix,
the steps of the TOPSIS method are applied to determine the preferred al-
ternative based on a combination of criteria and their weights. TOPSIS is a
valuable technique for multi-criteria decision-making and is used in various
fields to support decision processes when there are multiple alternatives and
criteria to consider. Multi-criteria decision-making problems are character-
ized by a decision matrix D:
C1
. . .
Cn
D =
A1
...
Am



x11
. . .
x1n
...
...
...
xm1
· · ·
xmn



(4)
where the rows of this matrix represent the alternatives (A1, A2, ..., Am). The
columns are the criteria analyzed in the problem (C1, C2, ..., Cn). The ele-
ments of the matrix, xij determine the performance of alternative Ai evalu-
ated on criterion Cj. Each criterion has a weight that affects the decision-
making process, defined by W = (w1, w2, ..., wn), where wj is the weight for
criterion Cj. It is important to note that the sum of all weights must equal
1. There are two types of criteria: cost and benefit. The difference between
them is that for the cost criterion, the focus is on minimizing, meaning that a
lower value is better, while for the benefit criterion, it is exactly the opposite.
In this work, both criteria are cost criteria, as we use the distances between
images and between clinical data as the criteria, aiming for the smallest dis-
tance values possible.
Before using TOPSIS, it is necessary to normalize the decision matrix
D, transforming it into the matrix R = [rij]m×n, allowing for comparisons
between all criteria. To normalize the decision matrix, the following formula
is applied:
12
rij =
xij
qPm
i=1 x2
ij
(5)
Once normalized, the resulting matrix R has its values weighted by the weight
vector W, generating a weighted matrix P = [pij]m×n, calculated as follows:
pij = wj × rij
(6)
Upon obtaining matrix P, the TOPSIS algorithm begins. First, the positive
ideal solution (A+) and the negative ideal solution (A−), respectively, for the
benefit and cost criteria are identified as follows:
A+ = (p+
1 , p+
2 , . . . , p+
n )
A−= (p−
1 , p−
2 , . . . , p−
n )
(7)
with:
p+
j =
maxi(pij)
mini(pij)
p−
j =
mini(pij)
maxi(pij)
(8)
where the maximum operator is used for benefit criteria, and the minimum
operator is used for cost criteria. This step aims to select the best perfor-
mance for each criterion, whether it is the cost criterion (where the best
performance is the lowest value) or the benefit criterion (where the best per-
formance is the highest value). Next, two vectors, A+ and A−, are formed
to represent the ideal performance. Then, for each alternative Ai, the Eu-
clidean distance from each element to the positive solution vector A+ and
the negative solution vector A−is calculated as follows:
d+
i =
v
u
u
t
n
X
j=1
(d+
ij)2
d−
i =
v
u
u
t
n
X
j=1
(d−
ij)2
(9)
13
where, d+
i = p+
j −pij and d−
i = p−
j −pij. The next step is to determine the
relative closeness ξi based on the obtained distances, calculated as follows:
ξi =
d−
i
d+
i + d−
i
(10)
this allows the selection of the alternative closest to the positive ideal solution
and farthest from the negative ideal solution.
Finally, the alternatives in the ξ vector are ranked, with the best solutions
having higher values of ξi, indicating that they are closer to the ideal solution.
3.3.2. The Proposed Method CBIDR
The novel proposed Content-Based Image and Data Retrieval, for short,
CBIDR is described in the following. The image retrieval process using con-
volutional neural networks combined with patient clinical data through the
TOPSIS algorithm begins by creating a descriptor database using a CNN
as the feature extraction mechanism. Next, an input image is passed to the
system, and its features are extracted using the same CNN as in the previous
step. Subsequently, the distance between the input feature vector and the
descriptor database is measured, generating the first vector X1, with a size of
m, where m is the number of descriptors present in the database. Continuing,
the clinical information linked to the input image is compared, utilizing the
Hamming distance, with the information in the database, generating the sec-
ond distance vector X2, also of size m. Combining the two distance vectors
results in a decision matrix D with dimensions m x 2. This matrix serves as
input for the TOPSIS technique, which, along with its weights and criteria,
calculates and ranks the best alternatives. The complete process of image
retrieval using convolutional neural networks combined with clinical data of
patient and the TOPSIS algorithm is shown in Figura 3.
4. Experimental Results
In this section, the results obtained for retrieval of histopathological im-
ages of oral cancer, which combines convolutional neural networks (CNNs)
to extract image features with clinical data of patient, is presented. Three
CNNs were employed to evaluate the quality of feature extraction. Firstly,
the dataset used in the experiments is introduced. Next, the configurations
used in the experiments are presented and discussed. Finally, the results are
discussed.
14
Figure 3: Illustration of the image retrieval process using CNN for feature extraction and
clinical data using the TOPSIS technique. The two distance vectors are generated and
passed as input to TOPSIS, producing ranked results that will be presented to the medical
professionals for the final diagnostic.
4.1. Dataset
In this work, the dataset NDB-UFES (de Assis et al., 2023) resulted
from an extension project carried out at the Federal University of Esp´ırito
Santo, collected histopathological images of patients diagnosed with oral cav-
ity squamous cell carcinoma and oral leukoplakia between January 2010 and
December 2021. Figure 4 shows an example of histopathological images for
each class in the NDB-UFES dataset. It contains originally, 237 images (2048
x 1536 pixels) and clinical and socio-demographic data were also collected
from the respective patients, including gender, lesion location, cigarette us-
age, alcohol consumption, age, and sun exposure (de Assis et al., 2023).
Next, the PatchExtractor from the Python library Scikit-Learn (Pedregosa
et al., 2011) was used to generate patches for each image, increasing the
dataset to 3753 images (512 x 512 pixels).
These 3753 images were la-
beled by pathologists, resulting in a database called P-NDB-UFES, which
contains 1930 images (51.29%) for the class leukoplakia with dysplasia, 707
images (18.79%) for the class leukoplakia without dysplasia, and 1126 images
(29.92%) for the class oral squamous cell carcinoma (OSCC).
15
Figure 4: Examples of histopathological images used in this study.
Table 1 presents the distribution of images by class in the dataset and
Figure 5 shows examples of histopathological patches images in the database.
Figure 5: Generation process of histopathological patches images.
4.2. Experiments Setting
The experiments conducted were divided into two parts: Experiment I
and II. Experiment I represents the configuration using only image features
(CBIR) as baseline, while Experiment II utilizes the TOPSIS algorithm to
combine images and clinical data of patient.
In both experiments, three convolutional neural network architectures
were used to perform the task of extracting image features. These archi-
tectures include ResNet50 (He et al., 2015), DenseNet-121 (Huang et al.,
16
NDB-UFES
P-NDB-UFES
Class
Number of images
Class
Number of images
Leucoplakia
with Dysplasia
89
Leucoplakia
with Dysplasia
1930
Leucoplakia
without Dysplasia
57
Leucoplakia
without Dysplasia
707
OSCC
91
OSCC
1126
Table 1: Division of Images by Class in the NDB-UFES and P-NDB-UFES Databases.
2017) and MobileNetV2 (Sandler et al., 2018).
Table 2 lists the number
of parameters for each architecture. Both ResNet50 and DenseNet-121 are
widely used for feature extraction tasks and have been tested and validated
in various studies. On the other hand, MobileNetV2 offers a lightweight con-
figuration, making it computationally less demanding. This is an important
feature when running the application on computers with limited processing
capabilities.
Architecture
Parameters (Millions)
ResNet50
25.56
DenseNet-121
7.97
MobileNetV2
3.4
Table 2: Number of parameters (millions) in each model.
In both experiments, two training phases were conducted, each consisting
of 50 epochs, a batch size of 32, and the Adam optimizer (Kingma and Ba,
2017) with an initial learning rate of 0.0001, which decreased by 30% (gamma
rate) per step, along with a weight decay of 0.0004. The MarginLoss (Wu
et al., 2017) was used as the loss function. All three architectures used in
this work were already imported and pre-trained using the ImageNet dataset
(Deng et al., 2009).
First, the conventional training of the convolutional
neural network was performed using the P-NDB-UFES dataset, saving its
weights and biases at the end of training. In the second stage, the NDB-
UFES dataset containing the original images with dimensions of 2048x1536
was utilized. Both the NDB-UFES and P-NDB-UFES datasets were divided
as follows: 5/6 for training and 1/6 for testing, with images resized to 224x224
pixels.
For evaluation, the Top-k accuracy was used, which is defined as follows:
17
Top-k =
P
xi∈Xtest I (∃xi ∈F s.t. yi = yq)
| Xtest |
, F = argmin
|F|=k
d (ϕ (xi) , ϕ (xq))
(11)
The Top-k accuracy counts as correct when the class of the input query image
is among the top k most similar images returned by the proposed method.
When k is one, this task becomes a classification one, where the class of the
returned image needs to be the same as the input image. In this paper, Top-1
and Top-5 accuracies are computed.
The experiments were conducted using an Intel i9-7900X CPU (20 cores)
with a clock speed of 4.3GHz, equipped with 128 GB of RAM, and an
NVIDIA TITAN Xp graphics card with 12GB of memory. The entire code
was written in Python, using various open-source libraries such as PyTorch,
NumPy, Matplotlib, and Faiss.
4.3. Results and Discussions
Next, the results obtained are presented and discussed.
4.3.1. Experiment I - CBIR using only Images
In Experiment I, only the features extracted from the histopathological
images were used. Table 3 presents the accuracy obtained for each CNN.
Experiment I - Results
Architecture
Top-1 (%)
Top-5 (%)
ResNet50
74.36
87.18
MobileNetV2
56.41
89.74
DenseNet-121
48.72
84.62
Table 3: Results obtained using only the images.
By analyzing Table 3, it can be observed that ResNet50 achieved a Top-
1 accuracy of 74.36%, which is the best result.
The MobileNetV2 and
DenseNet-121 architectures achieved Top-1 accuracies of 56.41% and 48.72%,
respectively. In terms of the Top-5, MobileNetV2 achieved 89.74%, followed
by ResNet50 with 87.18% and DenseNet-121 with 84.62%. The confusion ma-
trix obtained in Experiment I for the ResNet50 architecture, which overall
achieved the best results for Top-1, is presented in Figure 6. The significant
18
improvement from Top-1 to Top-5 accuracy is expected since only one cor-
rect image among the top 5 is required to be considered correct by the Top-5
metric.
Next, we show in Figure 7 the 5 most similar images to an input query
image using the ResNet50 configuration that achieved a Top-5 accuracy of
87.18%. In this test, the input query image was previously labeled by the
pathologist as OSCC (oral squamous cell carcinoma), and among the re-
turned images, 3 were from the OSCC class, with the top two ranked as the
best matches, and two images from the leukoplakia with dysplasia class.
Figure 6: Confusion matrix obtained by ResNet50 in Experiment I.
4.3.2. Experiment II - Results obtained with the proposed method CBIDR
In Experiment II, clinical data of patient was used in the image retrieval
process. The TOPSIS algorithm considers two criteria: the L2 between the
feature vector extracted from the input query image with those stored in
the Faiss library (Johnson et al., 2017), and the Hamming distance between
the corresponding clinical data of the query image with those saved in the
database. Therefore, it is necessary to specify the weights that each criterion
assume in the final decision, with values between 0 and 1. The following
weight sequence was used, respecting the minimum value of 0.5 for the dis-
tances obtained by Faiss: W = [0.5, 0.5], [0.6, 0.4], [0.7, 0.3], [0.8, 0.2], [0.9, 0.1].
The weight combination [1.0, 0.0] would be equivalent to Experiment I and
was therefore not considered.
19
Figure 7: Example of response when receiving an input query image. The first image in
the figure illustrates the input query image (above), while the others (below) are arranged
as the most similar to the input query image. In this test, the query image was from the
OSCC class, and the most similar ones were: OSCC, OSCC, leukoplakia with dysplasia,
OSCC, and leukoplakia with dysplasia.
Table 4 presents the accuracy for each CNN. One notes in Table 4, that
the best result was achieved by MobileNetV2, obtaining 97.44% for Top-1
and 100% for Top-5 accuracy with the weights [0.5, 0.5], indicating equal
importance for both sources of information.
It is worth noting that the
ResNet50 and DenseNet-121 architectures also achieved their best results
using the weights [0.5, 0.5]. Figure 8 depicts the confusion matrix obtained
using the MobileNetV2 architecture with weights [0.5, 0.5].
We show also in Figure 9 the 5 most similar images to an input query
image and their respective clinical data, using the MobileNetV2 architecture
with weights [0.5, 0.5], which achieved 100% Top-5 accuracy. It is worth
mentioning that in this test, the input query image was previously labeled
by the pathologist as leukoplakia with dysplasia, and among the returned
images, all 5 were also from the class leukoplakia with dysplasia.
20
Experiment II - Results
Architecture
Top-1 (%)
Top-5 (%)
TOPSIS
ResNet50
92.31
100.00
[0.5, 0.5]
ResNet50
89.74
94.87
[0.6, 0.4]
ResNet50
84.62
92.31
[0.7, 0.3]
ResNet50
79.49
92.31
[0.8, 0.2]
ResNet50
76.92
87.18
[0.9, 0.1]
MobileNetV2
97.44
100.00
[0.5, 0.5]
MobileNetV2
89.74
97.44
[0.6, 0.4]
MobileNetV2
84.62
94.87
[0.7, 0.3]
MobileNetV2
84.62
94.87
[0.8, 0.2]
MobileNetV2
64.10
97.44
[0.9, 0.1]
DenseNet-121
89.74
100.00
[0.5, 0.5]
DenseNet-121
82.05
92.31
[0.6, 0.4]
DenseNet-121
79.49
94.87
[0.7, 0.3]
DenseNet-121
74.36
92.31
[0.8, 0.2]
DenseNet-121
71.79
92.31
[0.9, 0.1]
Table 4: Results obtained using images and clinical data.
Figure 8: Confusion matrix obtained by MobileNetV2 with weights [0.5, 0.5] in Experiment
II.
21
Figure 9: The first image in the figure illustrates the input query image, while the others
are arranged as the most similar to the input image. In this test, the query image was
from the leukoplakia with dysplasia class, and all the returned alternatives also belong to
this class.
4.3.3. Discussions
In general, a significant improvement in terms of accuracy is observed by
using the proposed approach combining images and clinical data of patient.
For Top-1 accuracy, the best configuration was the ResNet50 with value of
74.36%, which was surpassed by 23.08% by MobileNetV2 and TOPSIS with
weights [0.5, 0.5].
In terms of Top-5 accuracy, all three models achieved
100% when using clinical data, surpassing the first approach, which provided
a maximum valaue for accuracy of 89.74% with MobileNetV2.
Defraire (2021), achieved 82% and 94%, respectively, in the Top-1 and
Top-5 metrics using MarginLoss as the loss function on the the TCGA
database. de Lima et al. (2023) achieved 83% balanced accuracy when us-
ing ResNetV2 to classify images from the NDB-UFES database along with
clinical patient data. In both cases, as we know, it is not possible to directly
compare the results obtained in this work with the ones mentioned above.
The experiment II indicates promising results with the proposed method
CBIDR for histopathological image combined with clinical data of patient.
22
5. Conclusion
This paper proposed a novel method to support medical professionals in
their diagnostic tasks. The methodology combines feature extraction from
images using convolutional neural networks with the clinical data obtained
from patients during their consultations. We illustrate the method by using
the NDB-UFES dataset collected and curated by pathologists to diagnostic
of oral cancer. Two experiments were conducted. In Experiment I using
standard CBIR considered only the features extracted from the images as
baseline method, the ResNet50 achieved the Top-1 accuracy, with value of
74.36% and the MobileNetV2 a value of 89.74% for the Top-5 accuracy. In
Experiment II using the proposed method CBIDR, a significant increase in
accuracy was observed, reaching 97.44% in Top-1 and 100% in Top-5 using
MobileNetV2. The results obtained from the Experiment II, show that the
proposed method CBIDR improved accuracy in retrieving histopathological
images with clinical data of patients. This work opens up a new research
avenue for multimodal information retrieval using several information sources
in an easy and effective way. For future work, we aim to make the approach
usable to medical professionals (doctors/pathologists) by creating a friendly
graphical interface to allow for more intuitive use.
Additionally, we also
intend to apply the proposed approach to other medical datasets.
6. Acknowledgments
R.A. Krohling thanks the Brazilian research agency Conselho Nacional
de Desenvolvimento Cient´ıfico e Tecnol´ogico (CNPq), Brazil - grant no.
304688/2021 −5 and the Funda¸c˜ao de Amparo `a Pesquisa e Inova¸c˜ao do
Esp´ırito Santo (FAPES), Brazil – grant no. 21/2022.
7. Declaration of Competing Interest
The authors declare that they have no known competing financial inter-
ests or personal relationships that could have appeared to influence the work
reported in this paper.
8. Role of the Funding Source
The funders had no role in study design, data collection and analysis,
decision to publish, or preparation of the manuscript.
23
9. CRediT authorship contribution statement
Renato Antonio Krohling: Conceptualization, Methodologies, Inves-
tigation, Validation, Writing - Review & Editing, Supervision. Humberto
Giuri Calente: Software, Validation, Writing - Original Draft, Review &
Editing.
References
Alzu'bi, A., Amira, A., and Ramzan, N. (2017). Content-based image re-
trieval with compact deep convolutional features. Neurocomputing, 249:95–
105.
Atasever, S., AZGINOGLU, N., TERZI, D. S., and TERZI, R. (2023). A
comprehensive survey of deep learning research on medical image analysis
with focus on transfer learning. Clinical Imaging, 94:18–41.
Barhoumi, W. and Khelifa, A. (2021).
Skin lesion image retrieval using
transfer learning-based approach for query-driven distance recommenda-
tion. Computers in Biology and Medicine, 137:104825.
Bejnordi, B. E., Veta, M., van Diest, P. J., van Ginneken, B., Karssemeijer,
N., Litjens, G., van der Laak, J. A. W. M., and Consortium, C. (2017).
Assessment of Deep Learning Algorithms for Detection of Lymph Node
Metastases in Women With Breast Cancer. JAMA, 318:2199–2210.
Chen, C., Lu, M. Y., Williamson, D. F., Chen, T. Y., Schaumberg, A. J., and
Mahmood, F. (2022). Fast and scalable search of whole-slide images via
self-supervised deep learning. Nature Biomedical Engineering, 6(12):1420–
1434.
Choe, J., Hwang, H. J., Seo, J. B., Lee, S. M., Yun, J., Kim, M.-J., Jeong, J.,
Lee, Y., Jin, K., Park, R., Kim, J., Jeon, H., Kim, N., Yi, J., Yu, D., and
Kim, B. (2022). Content-based image retrieval by using deep learning for
interstitial lung disease diagnosis with chest CT. Radiology, 302(1):187–
197.
Datta, R., Joshi, D., Li, J., and Wang, J. Z. (2008). Image retrieval. ACM
Computing Surveys, 40(2):1–60.
24
de Assis, M. C. F. R., Soares, J. P., de Lima, L. M., de Barros, L. A. P., Gr˜ao-
Velloso, T. R., Krohling, R. A., and Camisasca, D. R. (2023). NDB-UFES:
An oral cancer and leukoplakia dataset composed of histopathological im-
ages and patient data. Data in Brief, 48:109128.
de Lima, L. M., de Assis, M. C. F. R., Soares, J. P., Gr˜ao-Velloso, T. R.,
de Barros, L. A. P., Camisasca, D. R., and Krohling, R. A. (2023). On
the importance of complementary data to histopathological image analysis
of oral leukoplakia and carcinoma using deep neural networks. Intelligent
Medicine.
Defraire, S. (2021). A distributed deep learning approach for histopathology
image retrieval. Master’s thesis, l’Universit´e de Li`ege.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).
ImageNet: A large-scale hierarchical image database. In 2009 IEEE Con-
ference on Computer Vision and Pattern Recognition.
Gkelios, S., Sophokleous, A., Plakias, S., Boutalis, Y., and Chatzichristofis,
S. A. (2021). Deep convolutional features for image retrieval. Expert Sys-
tems with Applications, 177:114940.
Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT
Press.
Goyal, M., Knackstedt, T., Yan, S., and Hassanpour, S. (2020). Artificial
intelligence-based image classification methods for diagnosis of skin can-
cer: Challenges and opportunities. Computers in Biology and Medicine,
127:104065.
Hashimoto, N., Takagi, Y., Masuda, H., Miyoshi, H., Kohno, K., Nagaishi,
M., Sato, K., Takeuchi, M., Furuta, T., Kawamoto, K., Yamada, K., Morit-
subo, M., Inoue, K., Shimasaki, Y., Ogura, Y., Imamoto, T., Mishina, T.,
Tanaka, K., Kawaguchi, Y., Nakamura, S., Ohshima, K., Hontani, H., and
Takeuchi, I. (2023). Case-based similar image retrieval for weakly anno-
tated large histopathological images of malignant lymphoma using deep
metric learning. Medical Image Analysis, 85:102752.
He, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep residual learning for
image recognition.
25
Hegde, N., Hipp, J. D., Liu, Y., Emmert-Buck, M., Reif, E., Smilkov, D.,
Terry, M., Cai, C. J., Amin, M. B., Mermel, C. H., Nelson, P. Q., Peng,
L. H., Corrado, G. S., and Stumpe, M. C. (2019). Similar image search for
histopathology: SMILY. Digital Medicine, 2(1).
Huang, G., Liu, Z., Maaten, L. V. D., and Weinberger, K. Q. (2017). Densely
connected convolutional networks. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR).
Hwang, C.-L. and Yoon, K. (1981).
Multiple Attribute Decision Making.
Springer Berlin Heidelberg.
Johnson, J., Douze, M., and J´egou, H. (2017). Billion-scale similarity search
with gpus.
Kingma, D. P. and Ba, J. (2017). Adam: A method for stochastic optimiza-
tion.
Komura, D., Fukuta, K., Tominaga, K., Kawabe, A., Koda, H., Suzuki, R.,
Konishi, H., Umezaki, T., Harada, T., and Ishikawa, S. (2018). LUIGE:
Large-scale histopathological image retrieval system using deep texture
representations.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). ImageNet classifi-
cation with deep convolutional neural networks. Communications of the
ACM, 60(6):84–90.
Krohling, R. A. and Pacheco, A. G. (2015). A-TOPSIS: An approach based
on TOPSIS for ranking evolutionary algorithms. Procedia Computer Sci-
ence, 55:308–317.
Kuo, C.-H., Chou, Y.-H., and Chang, P.-C. (2016). Using deep convolutional
neural networks for image retrieval. Electronic Imaging, 28(2):1–6.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,
W., and Jackel, L. D. (1989). Backpropagation applied to handwritten zip
code recognition. Neural Computation, 1(4):541–551.
Li, Y., Zhou, R.-G., Xu, R., Luo, J., and Hu, W. (2020). A quantum deep
convolutional neural network for image recognition. Quantum Science and
Technology, 5(4):044003.
26
Maia, B. M. S., de Assis, M. C. F. R., de Lima, L. M., Rocha, M. B.,
Calente, H. G., Correa, M. L. A., Barroso, D. R. C., and Krohling, R. A.
(2023). Transformers, convolutional neural networks, and few shot learning
for classification of histopathological images of oral cancer. Submitted to
Publication, Expert Systems with Applications.
Nawaz, M., Nazir, T., Javed, A., Masood, M., Rashid, J., Kim, J., and
Hussain, A. (2022). A robust deep learning approach for tomato plant leaf
disease localization and classification. Scientific Reports, 12(1).
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas,
J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay,
E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830.
Philbin, J., Chum, O., Isard, M., Sivic, J., and Zisserman, A. (2007). Object
retrieval with large vocabularies and fast spatial matching. In 2007 IEEE
Conference on Computer Vision and Pattern Recognition.
Rawat, W. and Wang, Z. (2017).
Deep convolutional neural networks
for image classification: A comprehensive review. Neural Computation,
29(9):2352–2449.
Riad, A. M., Elminir, H. K., and Abd-Elghany, S. (2012). A literature review
of image retrieval based on semantic concept. International Journal of
Computer Applications, 40(11):12–19.
Rian, Z., Christanti, V., and Hendryli, J. (2019). Content-based image re-
trieval using convolutional neural networks. In 2019 IEEE International
Conference on Signals and Systems (ICSigSys).
Ruder, S. (2016). An overview of gradient descent optimization algorithms.
arXiv preprint arXiv:1609.04747.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986).
Learning
representations by back-propagating errors. Nature, 323(6088):533–536.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang,
Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L.
27
(2015). ImageNet Large Scale Visual Recognition Challenge. International
Journal of Computer Vision (IJCV), 115(3):211–252.
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. (2018).
Mobilenetv2: Inverted residuals and linear bottlenecks.
Tizhoosh, H. R. and Pantanowitz, L. (2018). Artificial intelligence and digital
pathology: Challenges and opportunities. Journal of Pathology Informat-
ics, 9(1):38.
Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A.,
Adam, H., Perona, P., and Belongie, S. (2017). The inaturalist species
classification and detection dataset.
Wang, M. and Deng, W. (2021). Deep face recognition: A survey. Neuro-
computing, 429:215–244.
Wu, C.-Y., Manmatha, R., Smola, A. J., and Kr¨ahenb¨uhl, P. (2017). Sam-
pling matters in deep embedding learning.
Wu, Y., Chen, B., Zeng, A., Pan, D., Wang, R., and Zhao, S. (2022). Skin
cancer classification with deep learning: A systematic review. Frontiers in
Oncology, 12.
Yang, S., Zhu, F., Ling, X., Liu, Q., and Zhao, P. (2021). Intelligent health
care: Applications of deep learning in computational medicine. Frontiers
in Genetics, 12.
28
