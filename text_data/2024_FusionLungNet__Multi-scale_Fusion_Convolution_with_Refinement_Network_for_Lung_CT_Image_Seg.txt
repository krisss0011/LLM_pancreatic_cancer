FusionLungNet: Multi-scale Fusion Convolution with Refinement Network for Lung
CT Image Segmentation
Sadjad Rezvania, Mansoor Fateha,∗, Yeganeh Jalalia, Amirreza Fatehb,∗
aFaculty of Computer Engineering, Shahrood University of Technology, Shahrood, Iran.
bSchool of Computer Engineering, Iran University of Science and Technology (IUST), Tehran, Iran
Abstract
Early detection of lung cancer is crucial as it increases the chances of successful treatment. Automatic lung image
segmentation assists doctors in identifying diseases such as lung cancer, COVID-19, and respiratory disorders. However,
lung segmentation is challenging due to overlapping features like vascular and bronchial structures, along with pixel-
level fusion of brightness, color, and texture. New lung segmentation methods face difficulties in identifying long-range
relationships between image components, reliance on convolution operations that may not capture all critical features,
and the complex structures of the lungs. Furthermore, semantic gaps between feature maps can hinder the integration
of relevant information, reducing model accuracy.
Skip connections can also limit the decoder’s access to complete
information, resulting in partial information loss during encoding. To overcome these challenges, we propose a hybrid
approach using the FusionLungNet network, which has a multi-level structure with key components, including the
ResNet-50 encoder, Channel-wise Aggregation Attention (CAA) module, Multi-scale Feature Fusion (MFF) block, self
refinement (SR) module, and multiple decoders. The refinement sub-network uses convolutional neural networks for
image post-processing to improve quality. Our method employs a combination of loss functions, including SSIM, IOU,
and focal loss, to optimize image reconstruction quality.
We created and publicly released a new dataset for lung
segmentation called LungSegDB, including 1800 CT images from the LIDC-IDRI dataset (dataset version 1) and 700
images from the Chest CT Cancer Images from Kaggle dataset (dataset version 2). Our method achieved an IOU score
of 98.04, outperforming existing methods and demonstrating significant improvements in segmentation accuracy. Both
the dataset and code are publicly available (Dataset link, Code link).
Keywords:
Lung CT image, Segmentation, self refinement, Hybrid loss function, Feature Fusion Module
1. Introduction
Lung cancer stand as a significant health concern that
is on the rise. Early detection of lung cancer is essential
to increase the chance of treatment [1]. To address this
need, advancements in medical imaging have led to the
development of critical techniques, including lung image
segmentation.
Lung image segmentation is essential for
accurately detecting and outlining lung structures in med-
ical images, such as computed tomography (CT) scans and
X-rays. The method has been highly popular for research
purposes over the years because of its potential in diagnos-
ing various lung-related or other diseases, such as covid-19
[2], pneumonia [3], and lung cancer [4].
In the process of lung segmentation, there are inher-
ent challenges related to the characteristics of lung tis-
sue. These include overlaps between tissue features such
∗Corresponding authors.
Email addresses: sadjadrezvani@shahroodut.ac.ir (Sadjad
Rezvani), Mansoor_fateh@shahroodut.ac.ir (Mansoor Fateh),
jalali.yeganeh@shahroodut.ac.ir (Yeganeh Jalali),
amirreza_fateh@comp.iust.ac.ir (Amirreza Fateh)
as blood vessels and bronchial structures with other el-
ements like airways and the chest wall.
This complex-
ity makes precise delineation of tissue boundaries difficult,
thereby reducing the accuracy and reliability of lung tis-
sue segmentation [5].
During manual segmentation, ra-
diologists rely on their experience to accurately outline
tissue boundaries. However, this method has challenges
such as depending heavily on individual skill, being time-
consuming, having differences among operators, and the
potential for human errors. These difficulties can affect
the accuracy of segmentation and disease diagnosis, high-
lighting the importance of advanced automated methods
and improvements [6]. Therefore, the exploration and ad-
vancement of automated segmentation techniques are im-
perative to overcome the limitations posed by manual seg-
mentation and to ensure more accurate and efficient lung
analysis in medical imaging.
In recent years, deep learning methods and their variants
in medical imaging have got widespread attention [7, 8, 9].
The rise of deep convolutional neural networks (DCNNs)
marked a notable advancement in overcoming the men-
tioned challenges [10]. Following this, fully convolutional
Preprint submitted to Elsevier
October 22, 2024
arXiv:2410.15812v1  [eess.IV]  21 Oct 2024
neural networks (FCNs) were suggested for the purpose
of image segmentation [11]. These networks accomplish
pixel-level segmentation by reducing the resolution of in-
put feature maps. Subsequently, the U-Net [12] model was
introduced as the foremost segmentation network among
encoder-decoder models.
U-Net is distinguished by its
unique encoder and decoder layers, linked through skip
connections. These connections effectively blend charac-
teristics from both high-level and low-level image features,
resulting in enhanced model accuracy and addressing the
issue of vanishing gradients.
Nowadays, researchers have proposed various versions
of the U-Net network including NAUNet [13], IterMiUnet
[14], EU-Net [15], MSIU-Net [16], IBA-U-Net [17], Wavelet
U-Net++ [18] and etc. While U-Net family networks have
made substantial strides in medical segmentation, but they
face several unique challenges. U-Net-based networks in-
herently struggle to capture long-range dependencies due
to the local nature of convolutional operations. This can
limit their ability to fully capture intricate details, espe-
cially in smaller structures. Additionally, the semantic gap
between encoder and decoder feature maps, along with
noise sensitivity, further impacts segmentation accuracy.
Skip connections address this by transferring information
from deeper network layers (encoding lower-resolution but
larger contextual features) to shallower layers (encoding
higher-resolution features). This helps the network con-
sider both local details and contextual information. How-
ever, skip connections cannot fully recover lost or com-
pressed information during the encoding process, leading
to a semantic gap that may hinder the decoder from hav-
ing all the necessary information for accurate segmenta-
tion [17, 19, 20].
These challenges collectively influence
the practicality of these models in clinical applications.
Additionally, while some state-of-the-art models [9, 21]
utilize single loss functions to address segmentation tasks,
they may not effectively capture the multifaceted nature
of lung segmentation, leading to suboptimal performance.
These challenges collectively influence the practicality of
these models in clinical applications.
Given the mentioned challenges in lung segmentation,
we have introduced an approach for lung CT segmen-
tation known as FusionLungNet.
FusionLungNet is an
end-to-end, multi-level model that comprises two sub-
networks: segmentation and refinement. The segmenta-
tion sub-network itself consists of several key components,
including an encoder based on ResNet-50, a Channel-wise
Aggregation Attention (CAA) module, a Multi-scale Fea-
ture Fusion (MFF) block, a self refinement (SR) module,
and a decoder. First, the ResNet-50 encoder extracts im-
portant information from input images.
These features
are then structured into four blocks, with those beyond
Block 4 forwarded to the CAA module. Here, the CAA
module optimizes feature representation while mitigating
computational complexity, ensuring the fidelity of target
feature extraction. Following CAA, the MFF block oper-
ates strategically across decoder layers, fostering coherence
between samples and merging information across diverse
feature levels. Subsequently, the SR module resolves chal-
lenges related to multi-scale feature fusion modules by op-
timally transferring features to the decoder. This module
can be used to correct minor defects in the original image.
It can also be utilized to enhance the clarity and contrast
of the original image.
Thus, the image is processed by
the self-correction module to produce a higher quality ini-
tial reconstruction image. During the training process, a
hybrid loss function supervises the training stages to en-
sure accurate predictions. Our network’s hybrid loss func-
tion consists of three metrics: Structural Similarity Index
(SSIM) [22], Intersection over Union (IOU) [23], and fo-
cal loss [24]. The combination of these metrics improves
image reconstruction quality through several mechanisms.
SSIM evaluates the perceived quality by comparing struc-
tural information between the original and reconstructed
images, while IOU measures the overlap accuracy of pre-
dicted segments with the ground truth, ensuring spatial
precision, and focal loss focuses on hard-to-classify pixels,
reducing the impact of class imbalance. This comprehen-
sive evaluation ensures that the reconstructed image re-
tains structural integrity, spatial accuracy, and balanced
classification. Using multiple metrics helps identify and
correct different types of errors, enhancing detail preser-
vation and making the model more robust to variations
and noise.
This balanced focus ensures that the recon-
structed image is not only accurate in specific areas but
also maintains high quality overall, leading to detailed,
accurate, and robust results critical for medical imaging
applications.
The refinement sub-network,
embedded within the
residual enhancement module, further improves the qual-
ity of image reconstruction. Leveraging convolutional neu-
ral networks, this module conducts image reconstruction
post-processing. It adeptly rectifies minor imperfections
and enhancing clarity and contrast in reconstructed im-
ages.
By seamlessly integrating both segmentation and
refinement sub-networks, FusionLungNet presents a holis-
tic solution to the challenges of lung CT segmentation,
promising substantial advancements in image reconstruc-
tion quality and segmentation accuracy.
The contributions of this paper can be summarized as
follows.
• Introducing a novel dataset: One of the main in-
novation of our work is the development and release
of new labels for an existing lung CT image datasetS.
We introduce LungSegDB, which features precise la-
beling of lung CT images from the LIDC-IDRI dataset
(Version 1) and the Chest CT Cancer Images Dataset
(Version 2). This is the first time these labels have
been created, and we are publicly sharing them along
with the images.
• Channel-wise Aggregation Attention (CAA):
Our proposed approach introduces the CAA module,
2
which enhances feature representation by emphasiz-
ing critical channels in feature maps. This allows the
model to capture important details in lung CT images
more effectively, enabling more accurate segmentation
and reconstruction.
• Multi-scale Feature Fusion module (MFF): A
noteworthy advancement in our approach involves en-
hancing the Feature Fusion Module. This module sig-
nificantly contributes to elevating the quality of the
reconstructed image by effectively combining features
extracted from diverse sources.
• Hybrid loss function: Our network introduces a
Hybrid Loss Function, incorporating three essential
metrics including SSIM, IOU and Focal. This inno-
vative combination of metrics is carefully designed to
optimize the reconstruction quality.
• Refinement sub-network: Another distinctive fea-
ture is the refinement network [25], which acts as an
additional network for further refinement of the re-
constructed image.
The remainder of this paper follows a structured outline.
Section 2 provides an overview of related works in medical
image segmentation, while section 3 provides a descrip-
tion of the dataset and explain the labeling process. Sec-
tion 4 details the proposed method. Subsequently, Section
5 presents comprehensive experiments and visualization
analyses. The conclusion and future works for this work
are summarized in Section 6.
2. Related work
In general, medical image segmentation methods can be
categorized into two main groups [26]: traditional methods
and deep learning-based methods. Essentially, traditional
methods rely on low-level visual features of images, while
deep learning-based methods use deep neural networks to
extract more intricate features. Next, we conduct an anal-
ysis of these two categories, highlighting key approaches
within traditional methods and exploring the diverse land-
scape of deep learning-based methods. Also, we provide
illustrative examples from each category to better eluci-
date their respective strengths and limitations.
2.1. Traditional segmentation methods
Traditional techniques for lung CT image segmenta-
tion encompass several methods. One of these methods
is threshold-based segmentation, where different compo-
nents of the image are identified by setting a threshold
based on pixel intensities [27]. This approach is straight-
forward but may not be robust in cases of varying inten-
sities or complex structures. Another common technique
is the region-growing method, which detects homogeneous
regions within the image [28]. This method starts with
a seed pixel and progressively expands regions with simi-
lar intensities, but it may struggle with noise or irregular
shapes. Watershed segmentation is another approach, em-
ploying watershed basins to identify sub-threshold points
and transform them into basins [29]. While effective in
certain scenarios, it can be sensitive to noise and may
over-segment the image. Active contour models, including
snake models, are also used [30]. These models combine
image information and specific energies to automatically
detect and move contours [31]. While they are suitable for
analyzing images with complex structures and lower lev-
els of noise, they may struggle with highly variable organ
shapes and contours.
Despite their utility, traditional medical image segmen-
tation methods face challenges with substantial variations
in organ shapes and contours [32]. This often results in
segmentation results that fluctuate or decline in accuracy,
especially as the complexity of segmented objects increases
in practical applications. Consequently, ensuring satisfac-
tory accuracy becomes increasingly difficult. The limita-
tions of traditional medical image segmentation methods
underscore the need for automated techniques with reli-
able algorithms, particularly in computer-aided diagnosis
(CAD) applications for lung CT segmentation. Develop-
ing such techniques is crucial for enhancing segmentation
accuracy and improving the reliability of diagnostic pro-
cesses in medical imaging.
Techniques based on deep learning in the segmentation
of lung CT images have been highly considered due to their
superior ability to learn complex features as well as their
ability to interact with big data [30]. One of the popu-
lar models in medical image segmentation, including lung
CT images, is the U-Net [10] model. This model uses a
convolutional neural network structure with encoder and
decoder layers, which is specifically designed for segmenta-
tion applications. So far, many researchers have attempted
to segment the lung with U-Net.
2.2. Deep learning-based segmentation methods
Techniques based on deep learning in the segmentation
of lung CT images have been highly considered due to their
superior ability to learn complex features as well as their
ability to interact with big data [33]. One of the popu-
lar models in medical image segmentation, including lung
CT images, is the U-Net [34] model. This model uses a
convolutional neural network structure with encoder and
decoder layers, which is specifically designed for segmenta-
tion applications. So far, many researchers have attempted
to segment the lung with U-Net.
2.2.1. U-Net based Methods
The good performance of U-Net has led to the devel-
opment of various derivative architectures. For example,
Liu et al. introduces a new method called MD-UNet for
accurately extracting lesion areas in medical images [35].
3
They utilized some modules like Mixed depth-wise convo-
lution attention block (MDAB) and Mixed depth-wise con-
volution residual module (MDRM) to mitigate intra-class
differences and enhance segmentation accuracy.
Kumar
et al. introduced IterMiUnet, a lightweight convolution-
based segmentation model for automatic blood vessel seg-
mentation in fundus images [14]. IterMiUnet reduces pa-
rameters significantly while maintaining performance com-
parable to existing models by combining the segmentation
capabilities of Iternet with the encoder-decoder structure
of MiUnet. However, this may lead to a slight decrease
in accuracy and other evaluation metrics.
Additionally,
this method may limit the network’s ability to identify
complex structures in images.
Azad et al.
have intro-
duced the BDCU-Net architecture, combining a bidirec-
tional ConvLSTM with U-Net for medical image segmen-
tation [36].
This design optimizes model efficiency and
complexity by utilizing dense convolutional layers in the
final encoding path, thereby reducing the number of train-
ing parameters. In another paper, Zhang et al. proposed
a novel model called BCU-Net for medical image segmen-
tation [37]. BCU-Net fully analyzes both local details and
overall context by combining the strengths of ConvNeXt
and U-Net models. Also, this method tackles the issue of
imbalanced detection of rare abnormalities by employing
MRL loss, significantly improving diagnostic accuracy. Yu
et al. introduced EU-Net, Automatic U-Net Neural Ar-
chitecture Search for Medical Image Segmentation Using
Differential Evolutionary Algorithm [15]. EU-Net employs
the DE algorithm to automatically optimize the U-Net
neural network architecture. It leverages various strate-
gies to effectively explore the search space and identify
the optimal architecture for each specific dataset. Zheng
et al.
introduced an end-to-end saliency detection net-
work named HLU2-Net [38]. This network utilizes an in-
novative end-to-end structure, a hybrid loss function, and
a coordinate attention module to combine and enhance
features. In [39], a neural network called DRU-Net is pre-
sented for the segmentation of pulmonary arteries, incor-
porating a uniquely crafted hybrid loss function. DRU-Net
has shown enhanced capabilities in dealing with larger vol-
umes of computed tomography images, effectively improv-
ing performance and mitigating overfitting. Furthermore,
the efficacy of the devised hybrid loss function has been
successfully implemented to boost the overall efficiency of
the network in this study.
Despite remarkable advancements in medical image seg-
mentation, challenges still exist in deep learning for med-
ical image segmentation. Main limitations are semantic
gap problem, low learning efficiency, and insufficient infor-
mation integration in skip connections. Attention mech-
anisms have emerged as a promising solution to address
these challenges and enhance the performance of deep su-
pervised learning methods for medical image segmentation
[40].
2.2.2. Attention guided models
Attention-based approaches enable models to selectively
focus on specific regions or features of the input data that
are most relevant for the task at hand. This can help to
improve information integration in skip connections, re-
duce the problem of semantic gap, and improve learning
efficiency. Using the attention mechanism in U-Net im-
proves the ability of the network to focus on the critical
areas of the image and increase the accuracy in the seg-
mentation of the desired objects [41]. Many researchers
have used the attention mechanism in their segmentation
network to improve model sensitivity and prediction accu-
racy in segmentation. Oktay et al. [42], proposed the At-
tention U-Net, which utilizes attention gates to highlight
significant features in medical images. This approach con-
tributes to more precise predictions in the segmentation
of objects of interest. Yang et al. introduced NAUNet,
a lightweight encoder-decoder network for retinal vessel
segmentation in fundus images [13]. It utilizes techniques
such as DropBlock regularization, efficient attention mod-
ules, nested skip connections, data augmentation, and a
mixed loss function to improve accuracy and address is-
sues related to vascular images. Chen et al. proposed the
Dual Attention Network (DANet) for multivariate time se-
ries classification [43]. DANet utilizes two dual-attention
layers to capture both local and global information in
multivariate time series data.
These layers enable the
extraction of key features, thereby improving the classi-
fication performance.
Chen et al.
introduced an inno-
vative method for medical image segmentation using the
IBA-U-Net network [17]. This method incorporates three
main ideas: integrating ConvLSTM and attention blocks
to reduce semantic gaps, factoring convolutions using Re-
designed Inception, and developing a deep convolutional
neural network with multiscale features and incorporat-
ing an Attentive BConvLSTM mechanism. This approach
not only improves segmentation accuracy but also achieves
significant performance enhancements with lower compu-
tational costs compared to previous models. Wang et al.
presented PCRTAM-Net, a deep learning model for auto-
matically segmenting retinal vessels in color fundus images
[44]. The method incorporates three key elements: pre-
activated dropout convolution residual for better feature
learning, residual atrous convolution spatial pyramid for
multiscale information extraction, and a triple attention
mechanism for structural information capture.
Evalua-
tion on multiple datasets showed PCRTAM-Net achieving
state-of-the-art results in retinal vessel segmentation, af-
firming its efficacy and advancement in this domain. Al-
though the mentioned studies have achieved good results
in medical image segmentation, they still face challenges
such as low resistance to noise, low contrast in images, and
complex structures.
2.2.3. Feature fusion modules
In the medical field, the distinction and detection of sen-
sitive areas and subtle changes in medical images are ex-
4
tremely importance. Feature fusion modules facilitate the
extraction of crucial and fundamental information from
different layers of a neural network, and by merging them,
a unified and rich representation of image features is ob-
tained [45]. This action facilitates the improved recogni-
tion of subtle differences in images and enhances detection
accuracy. In scenarios where the identification and visu-
alization of small and critical regions in medical images
are imperative, the use of feature fusion modules in U-Net
proves to be impactful and essential.
So far, some studies in this domain have focused on the
development of models and architectures based on U-Net
with feature fusion modules to enhance performance in
lung segmentation.
For example, Xie et al.
[46] intro-
duced a novel method for lung image segmentation. Their
proposed approach utilizes the Multi-Scale Dilated Con-
volution (MSDC) feature fusion module to address chal-
lenges arising from low contrast and complex appearances
in medical images. By incorporating this module into the
network, the capability to analyze a broader range of infor-
mation from images has improved, leading to a substantial
increase in segmentation accuracy.
The results demon-
strate a significant improvement of this method compared
to previous approaches in lung image segmentation. Jiang
et al. [47] have introduced a model called IU-Net for medi-
cal image segmentation. This model combines Swin Trans-
former and CNN, utilizing a feature fusion module based
on wave function representation to enhance global contex-
tual information and improve interaction with features.
The feature fusion module, employing the wave function
representation, integrates feature information from mul-
tiple sources, resulting in improved performance in skin
lesion and lung field segmentation tasks. Xu et al. [48]
introduced a novel method for diagnosing breast and lung
parenchyma radiological images using the Kiu-Net multi-
interaction network model.
This approach significantly
improves feature fusion compared to existing methods and
shows superior detection performance on the Montgomery
County and Shenzhen datasets. The Feature Integration
Module collects and merges features from U-Net and Ki-
Net networks, enhancing accuracy and efficiency in radi-
ological image detection by utilizing more comprehensive
information.
Sun et al.
introduced a novel gland seg-
mentation network called DARMF-UNet [49], which im-
proves gland segmentation accuracy compared to existing
methods by utilizing deep supervision techniques and com-
bining multi-scale features.
DARMF-UNet incorporates
modules such as Coordinate Parallel Attention (CPA) and
Dense Atrous Convolution (DAC) to enhance semantic in-
formation in each layer and uses a hybrid loss function
to improve segmentation accuracy. Experimental results
demonstrate that this model outperforms existing meth-
ods in terms of segmentation accuracy and quality.
According to the proposed studies and inspired by the
idea of Res-UNet, attention mechanism and feature fu-
sion module, we considered a network based on u-net by
integrating resnet blocks as encoder.
We also designed
a channel attention module and a feature fusion module.
Our network also uses a self refinement module. Finally,
we decoded the modified features and then created a data
modification network to improve. Also, our model uses a
hybrid loss function. Therefore, our network is able to im-
prove the accuracy and efficiency in detecting features in
lung CT images. Also, the self refinement module allows
the network to perform automatic feature refinement, and
the combination of compression layers provides an over-
all improvement in model performance. In addition, the
use of channel attention modules and feature fusion, to-
gether with the integration of resnet blocks, has helped to
increase the accuracy and sensitivity in distinguishing im-
portant features and has increased the speed and efficiency
of the network. The details of the proposed method are
explained in detail in Section 4.
3. Proposed Dataset
3.1. Workflow Overview
In this paper, we introduce LungSegDB, a comprehen-
sive dataset for lung segmentation.
We followed a sys-
tematic workflow to prepare the datasets, which included
downloading the datasets, creating lung masks using any
labeling software, conducting an expert review for accurate
validation, and finally evaluating the results of the anno-
tation process.
Figure 1 illustrates the sequential steps
of label generation.
LungSegDB is our dataset, which
LIDC-IDRI
Chest CT Cancer Images 
2
Lung Mask Creation by 
Anylabeling Tool
1         Download Datasets
3
Validate the Labeling Process 
by Experts
4
Annotation Results
Figure 1: Labeling lung CT images process.
is defined from two datasets: Version 1, which includes
the LIDC-IDRI (Lung Image Database Consortium Image
Collection) dataset [50], from which we randomly selected
1800 DICOM images (referred to as Dataset Version 1),
and Version 2, which includes the Chest CT-Scan Cancer
Images Dataset from Kaggle which we randomly chose 700
PNG/JPG images (referred to as Dataset Version 2).
5
Together, these datasets provided us with the raw CT
images. Next, we created lung masks (segmentation la-
bels) using the AnyLabeling software, which allowed us to
accurately outline the lung regions in each CT image. Af-
ter generating the masks, we conducted an expert review of
the images to validate the labeling and ensure high-quality
annotations for the lung segmentation task.
3.2. Description of Datasets
In this paper, we utilized two datasets for training and
testing our lung segmentation model, collectively referred
to as LungSegDB: Dataset Version 1, which is the LIDC-
IDRI dataset, and Dataset Version 2, the Chest CT Cancer
Images Dataset from Kaggle1. Below, we provide details
on each dataset and the process of labeling the images.
Dataset Version 1 (LIDC-IDRI) consists of a collection
of medical images used for the detection and analysis of
lung nodules in CT scans. This dataset was created by the
LIDC consortium and the National Institute of Biomedical
Imaging and Bioengineering (IDRI). It includes 1018 cases
from 1010 patients, with each case consisting of images
from a clinical thoracic CT scan, accompanied by an asso-
ciated XML file documenting the results from a two-phase
image annotation process conducted by four experienced
thoracic radiologists.
The CT images are stored in DI-
COM (Digital Imaging and Communications in Medicine)
format, featuring three channels and a resolution of 512
× 512. This dataset is publicly available, and researchers
can access it through reputable medical imaging websites.
Since the images in the LIDC-IDRI dataset lack labels
for lung tissue segmentation, which are crucial for training
a segmentation model, we manually labeled these images
and, for the first time, publicly released them along with
their labels. Initially, we labeled 1800 images from Dataset
Version 1.
To further enhance our training data, we incorporated
an additional dataset, referred to as Dataset Version 2,
the Chest CT Cancer Images Dataset from Kaggle. This
dataset contains CT-scan images of patients with various
types of chest cancer, including adenocarcinoma, large cell
carcinoma, squamous cell carcinoma, as well as normal
lung images.
The images are stored in JPG and PNG
formats and were initially collected for a chest cancer de-
tection project. Since these images were also unlabeled for
lung segmentation tasks, we manually labeled 700 images
from this dataset.
In total, LungSegDB comprises 2,500 labeled images,
combining 1800 images from Dataset Version 1 and 700
images from Dataset Version 2. For the training process,
we selected 2,350 images, while the remaining 150 images
were used for testing. This combined dataset enabled us
to enhance the performance and generalization of our lung
segmentation model.
1https://www.kaggle.com/datasets/dishantrathi20/
ct-scan-images-for-lung-cancer
Figure 2: Examples of the labeling extraction process from various
images.
3.3. Lung Mask Creation with AnyLabeling Tool
In this stage, we used the AnyLabeling software to seg-
ment lung lobes and label the lung CT images. AnyLabel-
ing is an advanced data labeling tool powered by artificial
intelligence, designed for fast and efficient labeling. This
software supports various types of labeling, including poly-
gons, rectangles, circles, lines, points and etc. A standout
feature of AnyLabeling is its use of advanced AI models
such as YOLO and Segment Anything for automatic label-
ing. These models enable the software to perform initial
labeling with high accuracy, which users can then manu-
ally refine and complete. Figure 2 shows several examples
of the labeling extraction process from different images.
These images illustrate how the AnyLabeling.
3.4. Expert Review and Generating Results
After generating the initial labels using the AnyLabel-
ing tool for all CT images, specialists in medical imag-
ing and radiology thoroughly reviewed and approved them
to ensure accuracy and reliability. These labels are now
available through a Dataset link, making them available
for public use and future research.
This approach not
only speeds up the labeling process but also provides re-
searchers and physicians with high-quality, precise data
that can help improve disease diagnosis and treatment.
3.5. challenges in Label generation
Labeling lung regions in CT images presents numerous
challenges that can significantly affect the accuracy of di-
agnoses.
One of the most prominent issues is the pres-
ence of regions with varying density.
High-density tis-
sues, such as tumors, infections, or abscesses, may ap-
pear prominently white in CT images, complicating the
decision-making process regarding their classification. It is
often challenging to determine whether these high-density
areas should be considered part of the lung or excluded
from labeling. Conversely, lower-density regions, like air
spaces within the lung, are typically classified as lung tis-
sue. However, as diseases progress and increase tissue den-
sity, the boundaries become less distinct, making accurate
labeling more difficult.
The primary criterion for labeling the lungs in CT im-
ages is the accurate identification of the anatomical bound-
aries of the lungs. These boundaries are typically distin-
guishable from other structures (such as the chest wall,
diaphragm, and heart).
The lungs appear as relatively
6
organized, air-filled structures in CT images, showing up
in dark color (low density). Additionally, lung tissue in-
cludes blood vessels and bronchi, which appear in CT im-
ages as thin and brighter structures (white or light gray
dots). These areas represent the airways and blood ves-
sels within the lungs and are considered part of the normal
lung tissue. Examples of these areas are depicted in Fig-
ure 3 with green circles. Areas with higher density, which
usually appear bright (white) in CT images, may include
tumors, infections, or other abnormal structures that do
not appear darker than normal lung tissue due to their
high density. These areas are typically identified as abnor-
mal or pathological, with examples highlighted in Figure
3 using red arrows in each image.
The challenge arises in labeling lung CT images, as even
if high-density areas (such as cancerous masses or inflam-
mation) exist within the lung, these areas are still consid-
ered part of the lung structure. If a cancerous mass or any
lesion with higher density is present in the lung (appear-
ing as large white spots in CT images), this area may be
recognized by the system as non-lung and ignored during
labeling.
This highlights the importance of accuracy in
labeling and the need for advanced methods to correctly
identify lung regions.
In this regard, we have been able to identify these chal-
lenges and accurately label the lung areas with precision
and attention to detail.
Our work is valuable as this
accuracy in labeling can help improve the diagnosis and
treatment of lung diseases, positively impacting the overall
quality of healthcare services. By addressing these chal-
Figure 3: Examples of the labeling extraction process from
various images Images (a) and (b) correspond to Dataset
Version 1 (LIDC-IDRI), while images (c) and (d) corre-
spond to Dataset Version 2 (Chest CT Cancer Images
Dataset).
lenges in our labeling process, we emphasize the signifi-
cance and value of our work in accurately labeling lung
regions in CT images, ultimately contributing to better
disease diagnosis and treatment.
4. Methodology
Our
suggested
approach
comprises
five
integrated
phases within the general pipeline for lung segmentation,
illustrated in Figure 4. The process begins with the initial
preparation of the dataset, where data labeling is essential
for accurate image analysis. Following this, the images en-
ter the preprocessing stage, which includes several critical
steps to standardize and enhance the input data. Initially,
each image is standardized through resizing to ensure uni-
formity across the dataset. Image normalization is applied
to scale pixel values, followed by noise reduction through
median filtering, which preserves important edges.
Dy-
namic thresholding enhances the contrast to highlight im-
portant features while suppressing irrelevant background.
Additionally, artifact removal is performed to eliminate
unwanted components from the images.
Following pre-
processing, the images are fed into the proposed network,
FusionLungNet. The final stage involves a thorough eval-
uation of the segmented images using a set of accuracy
metrics and model performance assessments.
This section begins by providing a comprehensive
overview of FusionLungNet. Then, we present in detail the
CAA module, which is used to enhance the feature repre-
sentation in the network.
Next, we introduce the MFF
block strategically operating on different layers of the de-
coder to enhance the information correlation between sam-
ples. Subsequently, our focus turns to the design of the
Residual Refinement Module, dedicated to refining image
boundaries. In addition, we use self refinement block, en-
hancing features through self-modality considerations, fa-
cilitating an efficient transfer into the decoder.
Finally,
we explain the network training loss. The integration of
these modules enables FusionLungNet to achieve excellent
performance in lung segmentation tasks.
In the final part of this section, we detail the preprocess-
ing stage, essential for preparing the dataset for effective
segmentation. This stage encompasses critical techniques
such as image resizing, median filtering, dynamic thresh-
olding and artifact removal.
4.1. Overview of the Proposed Network
FusionLungNet is a multi-level hybrid end-to-end model,
taking images as input and producing images as output
to predict binary maps for segmentation task. The over-
all architecture of our FusionLungNet is shown in Figure
5.
In the first step, images are fed into the ResNet-50
backbone network to capture contextual information es-
sential for extracting target features. The obtained fea-
tures through this process are organized into four layers,
{Block
i | i = 1, 2, 3, 4}. Then the highest-level features
(output of Block 4) are input into CAA module to improve
the accuracy of the extracted target features.
Low-level features capture fine details such as texture
and boundaries but may include extra background noise.
7
Figure 4: Overall structure of pipeline of proposed framework.
On the other hand, high-level features provide abstract in-
formation, helping identify important objects and reduce
noise. Additionally, extracting more global features can
be advantageous for various tasks, as these features offer
a broader context and contribute to better overall perfor-
mance.
As a result, it’s common to combine these two
levels of features to achieve a more comprehensive repre-
sentation. However, this combination suffers from some
defects due to the contradictory response of different lay-
ers. To address this, we employ a self refinement mod-
ule and integrate it with the two-level features generated
by CAA and different encoding parts of the model. This
integration helps alleviate the challenges associated with
multi-scale feature fusion modules.
Finally, through the up-sampling process, output maps
with the same resolution as the original image are ob-
tained. Throughout the training process at various stages,
we employ a hybrid loss to supervise the training for accu-
rate predictions. Additionally, to enhance the final binary
map produced by the prediction module, we incorporate
a Residual Refinement Module that employs a residual
encoder-decoder architecture.
4.2. Channel-Wise Aggregation Attention Module (CAA)
In this study, we introduce the Channel-Wise Aggre-
gation Attention (CAA) module as a crucial element for
enhancing the discriminative power of high-level features,
specifically Fblock4, within our deep learning architecture
for lung segmentation. The CAA module is meticulously
designed to refine feature representations by emphasizing
important channels while suppressing less relevant ones,
thereby improving the network’s focus on critical spatial
information.
4.2.1. Architecture and Operations
As depicted in Figure 6, the CAA module integrates
several key operations aimed at optimizing feature maps
with minimal computational overhead:
• Initial Convolutional Processing:
The module
begins with a sequence of convolutional layers—first a
1×1 convolution followed by a 3×3 convolution. The
1 × 1 convolution serves to reduce the dimensionality
of the feature maps, facilitating efficient computation,
while the subsequent 3 × 3 convolution captures spa-
tial dependencies and extracts common information
across channels.
This dual convolutional approach
generates an intermediate feature map I4, as defined
in Equation (1):
I4 = C3×3 (C1×1 (Fblock4))
(1)
Where, Fblock4 represents the input feature map, and
C denotes the convolution operation.
• Asymmetric Convolutions for Enhanced Con-
textual Encoding:
To further enrich the feature
representation, the CAA module employs asymmet-
ric convolutions—specifically 1 × 3 and 3 × 1 convo-
lutions. Inspired by the findings in [51], asymmetric
convolutions are advantageous for capturing contex-
tual information from varying receptive fields, espe-
cially effective for small-sized feature maps typical in
8
Figure 5: Architecture of our proposed FusionLungNet.
high-level layers. These convolutions enable the net-
work to assimilate multi-scale contextual cues without
significantly increasing computational complexity.
• Global Average Pooling (GAP) for Channel
Weight Calculation:
Following the asymmetric
convolutions, GAP is applied to aggregate spatial in-
formation, producing channel-wise statistics.
This
operation calculates the average activation for each
channel, yielding the channel weights Iw
4 as per Equa-
tion (2):
Iw
4 = GAP (C3×1 (C1×3 (I4)))
(2)
These weights effectively serve as attention scores,
quantifying the importance of each channel in the con-
text of the entire feature map.
• Channel-Wise Attention Mechanism: The final
step involves modulating the original feature map I4
with the computed attention weights Iw
4 .
This is
achieved through element-wise multiplication, result-
ing in the refined feature map FCAA as described in
Equation (3):
FCAA = I4 ⊙Iw
4
(3)
Where, ⊙denotes element-wise multiplication. This
operation ensures that channels with higher attention
scores contribute more significantly to the subsequent
layers, thereby enhancing the network’s ability to fo-
cus on salient features relevant for accurate lung seg-
mentation.
4.2.2. Rationale and Benefits
The incorporation of the CAA module addresses several
critical aspects in deep learning-based segmentation:
• Enhanced Feature Discrimination: By assigning
adaptive weights to each channel, the CAA module
allows the network to prioritize features that are more
pertinent to lung structures, thereby improving the
overall segmentation accuracy.
• Efficient Computational Utilization: The strate-
gic use of 1 × 1 and asymmetric convolutions en-
sures that the module remains computationally ef-
ficient, making it suitable for real-time or resource-
constrained applications.
• Contextual Awareness: The combination of asym-
metric convolutions and GAP facilitates a comprehen-
sive understanding of the feature maps’ spatial and
channel-wise contexts, enabling more nuanced feature
refinement.
In summary, the CAA module plays a crucial role in
our network by effectively harnessing channel-wise atten-
tion to optimize high-level feature representations. This
9
multiplication
G
G
Global Average Pooling
𝐹𝑏𝑙𝑜𝑐𝑘 4
BatchNorm
Relu
Convolution 3 x 1
Convolution 3 x 3
Convolution 1 x 3
𝐼4
𝐼4
𝑤
𝐹𝐶𝐴𝐴
Convolution 1 x 1
Figure 6: Schematic of the Channel-Wise Aggregation Attention module.
leads to more precise and computationally efficient lung
segmentation, addressing both the accuracy and efficiency
requirements of medical imaging applications.
4.3. Multi-scale Feature Fusion block (MFF)
Effective image segmentation requires the integration of
features from multiple scales to capture both the high-
level semantic context and the low-level spatial details.
High-level features provide extensive semantic information
but often lack precise spatial resolution, while low-level
features offer detailed spatial information but may include
noise and lack contextual understanding. To harness the
strengths of both, we propose the MFF block, designed
to seamlessly integrate features across different scales and
levels of abstraction.
As illustrated in Figure 7, the MFF block fuses three
input feature maps: I1, I2, and I3. Specifically:
• I1 is the output from the CAA module, capturing
global contextual information through channel atten-
tion mechanisms.
• I2 is the feature map from the encoder block, contain-
ing high-level semantic features with reduced spatial
resolution.
• I3 is derived from the self refinement block (detailed
in subsection 4.4), which enhances feature represen-
tations by refining and reinforcing important details.
To prepare for fusion, each input feature map undergoes
a 3 × 3 convolution operation, denoted as C3×3, to extract
salient features and unify the number of channels across
all inputs. To ensure spatial alignment, an up-sampling
operation, represented by R, is applied to I1 and I3 so
that all feature maps match the spatial resolution of I2
from the encoder.
The core fusion process involves pairwise element-wise
multiplication of the adjusted feature maps, capturing the
complementary information between different scales:
Fab = C3×3(R(I1)) ⊙C3×3(I2)
(4)
Fac = C3×3(R(I1)) ⊙C3×3(R(I3))
(5)
Fbc = C3×3(I2) ⊙C3×3(R(I3))
(6)
FMF F = C3×3(concatenation(Fab, Fac, Fbc))
(7)
Where, ⊙denotes element-wise multiplication, which ef-
fectively emphasizes the shared important features be-
tween the paired inputs while suppressing less relevant in-
formation.
The resulting feature maps Fab, Fac, and Fbc are then
concatenated to aggregate the multi-scale information:
FMF F = C3×3(concatenate(Fab, Fac, Fbc))
(8)
A final 3 × 3 convolutional layer further refines the fused
features, enhancing the model’s ability to focus on criti-
cal aspects of the input data by combining rich semantic
context with detailed spatial information.
By integrating features from multiple scales through
these operations, the MFF block effectively captures and
consolidates information across different levels of the net-
work. This multi-scale fusion enhances the segmentation
performance by providing a more comprehensive under-
standing of the image content, allowing the model to make
more accurate predictions.
4.4. Self Refinement Block
Drawing inspiration from [52], the integration of the
Self Refinement (SR) Block successive to each MFF block
is crucial for improving and enhancing feature mapping,
mitigating the defects associated with the direct fusion of
different layers, and ensuring the generation of more dis-
criminative features for more accurate saliency maps. As
10
I1
I2
I3
R
R
C
R
C
Reshape
Concatenate
Element-wise 
multiplication
a
b
c
𝐹𝑎𝑏
𝐹𝑎c
𝐹bc
Figure 7: Detailed network architecture diagram of The multi-scale feature fusion block.
shown in Figure 8, Initially, we employ a 3 × 3 convolu-
tional layer to compress the input features, denoted as Fin,
into a feature vector F1 with a channel dimension of 256,
while retaining valuable information. In other direction,
256 channels expanded to 512 by convolution 3 × 3 (F2).
Following this, the 512 channels are split into two seg-
ments, each containing 256 channels. This division results
in the generation of Fm for the multiplication operation
and Fc for the addition operation.
4.5. Residual Refinement module
The output from the decoder part fails to accurately seg-
ment the boundary between the lung and non-lung areas.
To address this limitation and enhance boundary delin-
eation, we introduce a Residual Refinement module. The
Residual Refinement Module adopts a residual encoder-
decoder architecture to mitigate the boundary identifica-
tion challenges in the segmentation process.
As shown in Figure 9, our approach initiates with a 3 ×
3 convolution. Subsequently, a sequence of five convolu-
tional layers and four pooling layers is employed to encode
the feature map. In the decoder part, resolution recovery
is achieved through bi-linear interpolation, and essential
high-resolution features are complemented by leveraging
shortcut connections that bridge the contracting and ex-
pansive paths. At the final layer, we employ a 1 × 1 convo-
lution to generate the ultimate binary segmentation result.
4.6. Hybrid Loss
We propose a hybrid loss function, comprising focal loss,
SSIM loss, and IoU loss, to enhance regional segmentation
and boundary accuracy(Eq.9).
Loss = λ1Lossfocal + λ2Lossssim + λ3Lossiou
(9)
where λ denotes the weight of different loss, and According
to the quantitative comparison in Table 1, we set λ1 = 0.3,
λ2 = 0.4, λ3 = 0.3.
The total loss comprises two components: the primary
loss linked to the Residual Refinement Module output and
the supplementary loss from each decoder.
Losstotal = Lossprim +
4
X
n=1
Lossn
sup
(10)
Lossprim and Losssup indicate the primary and supple-
mentary losses, respectively. The supplementary losses are
only present during the training stage and are discarded
during inference.
In proposed dataset, the segmented lung regions within
certain images constitute a relatively small proportion of
the entire image, as illustrated in Figure 10. This class
imbalance highlights the need for specialized techniques,
such as the application of focal loss, to address the inher-
ent challenges associated with binary lung segmentation.
Specifically, focal loss is employed to mitigate the class im-
balance resulting from uneven distributions of foreground
and background pixels. By assigning greater importance
to the lung region during training, focal loss enhances the
model’s capability to accurately converge on all pixels. It
is defined as:
Lbinary−focal =

−α(1 −y′)γ log(y′),
y = 1
−(1 −α)(y′)γ log(1 −y′),
y = 0
(11)
Utilizing the ssim loss [53] as defined in Eq.
12, we
focus on the patch level to capture structural data and
enhance detailed boundary predictions. This approach in-
volves considering adjacent neighbourhoods for individual
pixels, assigning increased weight to edges, thereby opti-
mizing the segmentation of boundaries.
ssim = 1 −(2µxµy + C1) + (2σxy + C2)
(µ2x + µ2y + C1)(σ2x + σ2y + C2)
(12)
The covariance of x and y is represented by σxy. The mean
and standard deviation of x and y are represented by µx,
11
(512, ℎ, 𝑤)
(256, ℎ, 𝑤)
(𝑐, ℎ, 𝑤)
(256, ℎ, 𝑤)
(256, ℎ, 𝑤)
(256, ℎ, 𝑤)
Add
Element-wise 
multiplication
(256, ℎ, 𝑤)
𝐹𝑖𝑛
𝐹1
𝐹2
𝐹𝑜𝑢𝑡
𝐹𝑚
𝐹𝑐
Figure 8: Details of the proposed self refinement block.
Table 1: Quantitative comparison of λ for our hybrid loss.
λ1 →lossfocal
λ2 →lossSSIM
λ3 →lossIoU
IoU
F1-Score
Accuracy
0.3
0.4
0.3
98.04
99.02
99.73
0.4
0.3
0.3
97.86
98.6
99.65
0.3
0.3
0.4
97.73
98.53
99.63
µy, σx, and σy, respectively. In order to prevent division
by zero, we empirically fixed C1 = 0.012 and C2 = 0.032
from this paper [54].
4.7. Preprocessing
Preprocessing is the first and essential step in automatic
lung segmentation. A common limitation in lung imaging
is the frequent suboptimal quality of the CT scans due
to noise, low contrast, and irrelevant background struc-
tures. To achieve segmentation with high accuracy, our
objectives are twofold: firstly, to enhance the quality of
the CT images, and secondly, to focus on the lung regions
by eliminating unnecessary background information. Our
preprocessing pipeline aims to improve the clarity and de-
tail of the lung images through several techniques. The
preprocessing consists of image resizing, median filtering,
dynamic thresholding and lung region segmentation. The
initial step involves resizing all CT images to a standard-
ized dimension of 320 × 320 pixels. This standardization
is crucial for maintaining uniformity across the dataset.
After resizing, we apply median filtering to each image
to reduce noise while preserving important anatomical de-
tails. CT images often contain artifacts that can obscure
fine structures within the lungs.
Median filtering effec-
tively mitigates this issue by replacing each pixel’s value
with the median value of its neighboring pixels. This pro-
cess smooths the image without blurring critical edges and
features essential for accurate segmentation.
To enhance the contrast of the images and emphasize
the lung tissues, we employ contrast enhancement tech-
niques based on dynamic thresholding. Contrast enhance-
ment is important in improving the visibility of structures
within medical images, which can significantly impact the
performance of segmentation algorithms.
Recent stud-
ies have demonstrated the effectiveness of adaptive con-
trast enhancement methods in medical imaging applica-
tions [55, 56, 21, 57]. We adopt a dynamic thresholding
approach inspired by these recent advancements. Unlike
global thresholding methods that apply a single threshold
value to the entire image, dynamic thresholding calculates
threshold values based on local image characteristics. This
adaptive approach allows for better differentiation between
lung tissues and surrounding structures, especially in im-
ages with uneven illumination or varying contrast levels.
The dynamic threshold T(x, y) at each pixel location
(x, y) is computed using local statistics within a neighbor-
hood window. The enhanced image Ienh(x, y) is then ob-
tained by applying a transformation function that adjusts
the pixel intensities based on the local threshold:
Ienh(x, y) =
(
α · Imed(x, y) + β,
if Imed(x, y) > T(x, y)
Imed(x, y),
otherwise
(13)
where Imed(x, y) is the median-filtered image, and α
and β are parameters controlling the level of enhancement.
12
Conv 1 x 1
Conv 3 x 3
BatchNorm
Max pooling
ReLU
Bi-Linear Upsampling
BatchNorm
Final output
Add
Figure 9: Architecture of our proposed Residual refinement module.
Figure 10: Demonstrates the challenge of class imbalance in our dataset, with segmented lung regions forming a small proportion of the
image.
This method enhances the contrast of lung tissues while
suppressing irrelevant background, improving the subse-
quent segmentation process.
The final step in the preprocessing pipeline is artifact re-
moval. After enhancing the images and suppressing back-
ground noise, we eliminate unwanted artifacts that may
interfere with accurate lung segmentation. Artifacts, such
as scanner-induced distortions or irrelevant structures, are
removed to ensure that only meaningful information re-
mains in the image. This process helps reduce interference
from non-lung regions, allowing the segmentation model to
focus solely on relevant anatomical structures. By effec-
tively removing artifacts, we improve the quality of the
input data, enhancing the efficiency and accuracy of the
segmentation model.
To demonstrate the effectiveness of our preprocessing
steps, we visually present the transformation of a sample
CT image through each stage in Figure 11. This figure
illustrates how each preprocessing technique contributes to
enhancing the image quality and isolating the lung region
for accurate segmentation.
5. Experiments
In this section, we performed experiments to com-
pare the performance of the proposed lung segmentation
dataset with state-of-the-art deep learning models for seg-
mentation. In the training process, we implemented our
model in two scenarios.
Secondly, we trained the model on both LungSegDB-V1
and LungSegDB-V2 after applying the preprocessing tech-
niques discussed in Section 4.7. We expanded the dataset
to include 2,500 labeled images, which were divided into a
training set (2,350 images) and a test set (150 images).
5.1. Experimental Settings
For both scenarios, we used a ResNet-50 encoder for
all segmentation models. This choice was motivated by
the balance between model complexity and performance,
leveraging the pre-trained weights from ImageNet [34] to
facilitate transfer learning.
The experiments were con-
ducted on Google Colab, utilizing the Tesla T4 GPU. We
employed the PyTorch framework for model training. The
training batch size was set to 32, and we utilized the Adam
optimizer with a learning rate of 10−4 for parameter op-
timization. The input images were resized to 320 × 320
pixels.
13
Original Image
Median Filtered Image
Dynamic Thresholded Image
Artifact Removal
Figure 11: Preprocessing steps in the proposed lung segmentation method.
Table 2: A quantitative comparison of the results obtained on the first version of proposed lung dataset test set using a variety of techniques
for image segmentation. The best scores are highlighted with bold.
Model
IoU
F1-score
Precision
Recall
Acc
MCC
Unet [12]
95.02
96.22
96.11
96.34
95.18
94.68
RU-Net [58]
94.93
96.35
95.52
97.21
97.15
94.80
ResNet34-Unet [59]
95.28
97.83
97.32
98.35
96.73
96.87
BCDU-Net [36]
96.32
98.52
99.02
98.03
97.21
97.61
ResBCDUnet [60]
97.15
98.05
99.12
97.01
97.58
96.60
NasNet [49]
97.56
98.71
98.93
98.45
98.76
97.90
DABT-U-Net [40]
97.31
98.79
98.24
98.86
98.45
97.90
ABANet [25]
97.57
98.55
98.70
98.30
99.34
97.63
Ours
98.04
99.02
98.65
99.41
99.73
98.67
5.2. Evaluation Metrics
We employed several metrics to comprehensively evalu-
ate the performance of our network. These metrics include
measures for semantic segmentation tasks. Here, we pro-
vide details on the five metrics used for evaluation: Inter-
section over Union (IoU) serves as the primary evaluation
measure for semantic segmentation. Additionally, we cal-
culate the following standard metrics [61]:
Precision =
TP
TP + FP
(14)
Recall =
TP
TP + FN
(15)
F1-score =
2 · TP
2 · TP + FP + FN
(16)
Accuracy =
TP + TN
TP + TN + FP + FN
(17)
IoU =
TP
TP + FP + FN
(18)
MCC =
(TP × TN) −(FP × FN)
p
(TP + FP)(TP + FN)(TN + FP)(TN + FN)
(19)
5.3. Comparison with state-of-the-art models
On our lung segmentation dataset,
we conduct a
comparison
of
our
network
with
other
state-of-the-
art approaches using the same training configuration
of 1600 training images.
Segmentation models are
Unet[12], RU-Net[58], ResNet34-Unet[59], BCDU-Net[36],
ResBCDUnet[60], ABANet[25], NasNet [49] and DABT-
U-Net [40].
Quantitative Evaluation: as shown in Table 2, the
evaluation metrics were calculated and summarized for
quantitative comparisons on first version of proposed
dataset. As can be observed from Table 2, our proposed
FusionLungNet achieved 98.04, 99.02, 98.43, 99.73 and
98.67 in terms of IoU, F1-score, recall, accuracy and MCC,
respectively, which were superior to those of other meth-
ods. The IoU and F1-Score of the proposed network were
approximately 3% higher than those of the classical net-
works like Unet, RU-Net, and ResNet34-Unet, which im-
plied that the hybrid loss can help efficiently. Compared to
newer methods like ABANet and DABT-U-Net, the pro-
posed network showed marginal improvements in the re-
sult, which indicates that various proposed modules are
suitable for medical image segmentation.
To further validate the effectiveness of our approach, we
14
Table 3: A quantitative comparison of the results obtained on the second version of proposed lung dataset test set using a variety of techniques
for image segmentation. The best scores are highlighted with bold.
Model
IoU
F1-score
Precision
Recall
Acc
MCC
Unet [12]
95.62
96.93
97.28
96.74
98.24
95.93
BCDU-Net [36]
97.56
98.85
98.49
98.95
98.67
98
ABANet [25]
97.57
98.74
98.83
98.65
99.48
98.15
Ours
98.12
99.01
98.83
99.2
99.79
98.81
conducted additional experiments using the first and sec-
ond version of our dataset, which comprises 2,350 labeled
images with applied preprocessing techniques as detailed
in Section 4.7. The results of these experiments are pre-
sented in Table 3.
As illustrated in Table 3, our FusionLungNet model con-
sistently outperforms the baseline models across all evalu-
ation metrics when trained on the enhanced dataset with
preprocessing.
Specifically, FusionLungNet achieved an
IoU of 98.12%, F1-score of 99.01%, Precision of 98.83%,
Recall of 99.2%, Accuracy of 99.79%, and MCC of 98.81%.
These improvements underscore the benefits of increasing
the dataset size and applying preprocessing techniques,
which collectively contribute to more robust and accurate
lung segmentation performance.
Comparing the results from Version 1 (Table 2) and Ver-
sion 2 (Table 3), it is evident that the expanded dataset
and preprocessing steps lead to significant enhancements
in model performance. The higher Recall and Accuracy
values indicate better detection capabilities and overall
correctness, while the increased IoU and MCC reflect im-
proved segmentation overlap and correlation with ground
truth labels.
Qualitative Evaluation:
In Figure 12, we present
a comparison of experiments conducted on our proposed
dataset, showcasing typical samples from the test set.
Among the evaluated networks Unet, NasNet, DABT-U-
Net, and ABANet, our network exhibits strong perfor-
mance in predicting the fuzzy edges seen on the right side
of sample D in figure 12, closely resembling the ground
truth. However, DABT-U-Net overly emphasizes bound-
ary information, neglecting to predict the overall area of
the sample, which impacts its accuracy. Conversely, Unet
and NasNet incorrectly identify the background as lung
tissue in the small region on the center of sample A in fig-
ure 12. In contrast, our approach demonstrates minimal
deviation from the ground truth, highlighting its superior
performance in handling challenging cases.
Overall, while state-of-the-art methods often suffer from
over-segmentation or under-segmentation when dealing
with complex scenes, our approach excels at precisely pre-
serving lung boundaries and structures. Notably, our net-
work proves more robust in describing boundaries and
edges compared to existing approaches.
5.4. Ablation study
In this section, to further evaluate the effectiveness of
FusionLungNet, we conducted ablation studies using the
proposed dataset as examples.
To verify the effectiveness of each component, we take
the encoder with the CAA module and the decoder as the
baseline and compare its results with four modalities of
our lung segmentation: Baseline + MFF, baseline + resid-
ual refinement module, baseline + MFF + self refinement
module, and baseline + MFF + self refinement module +
residual refinement module. The implicit parameters of all
models, such as batch size and learning rate, are set the
same to ensure the fairness of the experiment. The experi-
ment is carried out on our lung segmentation dataset. The
F1-score, IoU, precision, recall, and accuracy are selected
as evaluation metrics.
The experimental results are shown in Table 4. We first
introduce the MFF block into the baseline.
Compared
with the baseline, the introduction of the MFF block im-
proves the IoU, F1-score, and accuracy by 0.7%, 0.36%,
and 1%, respectively.
Other components improved the
baseline to a certain extent. The scores of the baseline with
all components in the five evaluation metrics are higher
than those of other network models. Compared with the
baseline, the segmentation performance of the proposed
method is improved by 3.4% in IoU, 2.65% in F1-score,
1.13% in precision, 3.7% in recall, and 2.6% in accuracy.
Figure 13 shows a graphical visualization of the results
of the ablation experiment, displaying four examples of
images and the segmentation masks corresponding to the
different component modules. From the segmentation re-
sults of the first test instance, it can be seen that there
are varying degrees of under-segmentation compared to
the base version of our entire model, while the proposed
network model had better feature representation. It is ev-
ident from the segmentation results shown in Figure 13
that our entire framework provided clearer segmentation
15
Table 4: The performance of different configurations of FusionLungNet. The best scores are highlighted with bold.
Method
IoU
F1-score
Precision
Recall
Acc
baseline
95.57
96.67
97.54
95.82
97.15
baseline + MFF
96.64
97.03
96.94
97.14
98.1
baseline + residual refinement module
95.91
96.88
97.28
96.37
97.256
baseline + MFF + self refinement block
97.18
98.54
98.3
98.83
99.52
baseline + MFF + self refinement block +
residual refinement module
98.04
99.02
98.65
99.41
99.73
Table 5: FusionLungNet performance with various input picture resolutions.
Input size
160 × 160
320 × 320
640 × 640
IoU
97.93
98.04
97.81
Table 6: The performance of different optimizer.
Optimizer
IoU
F1-score
Acc
SGD
96.8
97.58
98.65
Adamax
97.5
98.71
99.77
RMSprop
97.63
99.05
99.79
Adam
98.04
99.02
99.73
Table 7: Effect of various loss functions on FusionLungNet performance.
Losses
IoU
F1-score
Acc
Focal
95.42
95.18
96.4
IoU
95.9
95.81
96.64
Focal + IoU
96.27
97.5
97.74
Focal + SSIM + IoU
98.04
99.02
99.73
Table 8: effectiveness of preprocessing stage.
Model
Preprocessing
IoU
F1-score
Acc
FusionLungNet
✓
98.12
99.01
99.79
FusionLungNet
×
98.07
99.02
99.75
16
A
B
C
D
Input
Ground Truth
Unet
NasNet
DABT-U-Net
ABANet
Ours
Figure 12: Qualitative comparison of lung dataset results with cutting-edge image segmentation algorithms.
The first two rows display
the original photos and the matching ground truth. Rows 3 to 8 illustrate the segmentation results respectively derived from the proposed
method, Unet, RU-Net, ResNet34-Unet, BCDU-Net, and ResBCDUnet.
of the boundary information than either a single or a com-
bination of any component.
To evaluate the stability and performance of the net-
work, we trained FusionLungNet with three different input
sizes: 160 × 160, 320 × 320 and 640 × 640 pixels. The
selection of these various input dimensions was aimed at
assessing the model’s performance under different condi-
tions that reflect practical scenarios in medical imaging.
Specifically, smaller input sizes, such as 160 × 160 pix-
els, allow for faster processing and reduced computational
17
A
B
C
D
Input
Ground Truth
base
base + residual
refinement module
base + MFF
base + MFF + self
refinement block
base+MFF+ self
refinement block +
residual refinement
module
Figure 13: Qualitative comparison of lung dataset results with cutting-edge image segmentation algorithms.
The first two rows display
the original photos and the matching ground truth. Rows 3 to 8 illustrate the segmentation results respectively derived from the proposed
method, Unet, RU-Net, ResNet34-Unet, BCDU-Net, and ResBCDUnet.
demands. Conversely, larger input sizes, like 640 × 640
pixels, capture more detailed features, which may enhance
segmentation accuracy but require more computational re-
sources. Testing these input sizes helps ensure the model’s
robustness and generalizability across varying image qual-
ities and resolutions.
Based on the data presented in Table 5, the input size
of 320 × 320 pixels yielded the best IoU score of 98.04%,
indicating an optimal balance between computational effi-
ciency and segmentation accuracy.
18
In this article, we use Adam as our optimizer.
We
also comprise different major optimizers: Adam, Adamax,
RMSprop, and SGD in Table 6. the Adam optimizer out-
performed other optimizers.
In order to illustrate the efficacy of our suggested hy-
brid loss, we perform an ablation study on losses with the
same experimental setup. Table 7 provides the comparing
results. It can be observed that both focal loss and IoU
loss Have almost similar performance in terms of F1-score
and accuracy metrics. However, when it comes to the IoU
measure, IoU loss outperforms focal loss. When we mix
focal loss and IoU loss, all metrics increased slightly. To
propose a final hybrid loss, we use SSIM with two other
losses. Table 7 indicates that by equipping hybrid loss on
FusionLungNet performance improve greatly. It is due to
utilizing SSIM loss and obtaining more detailed boundary
prediction.
To evaluate the impact of the preprocessing stage on the
performance of our proposed model, FusionLungNet, an
ablation study was conducted. From the results in Table
8, it can be observed that the inclusion of preprocessing
steps led to a slight improvement in IoU and Accuracy.
The ablation study confirms that the preprocessing step,
although not dramatically altering performance, adds a
beneficial enhancement to the overall segmentation accu-
racy, supporting its inclusion in the pipeline.
6. Conclusion
This paper introduces a novel framework Fusion-
LungNet for lung segmentation. The proposed approach
is based on an encoder-decoder architecture that utilizes
various modules for the segmentation process in lung CT
images, incorporating an innovative hybrid loss. To effec-
tively mitigate the defects associated with the direct fu-
sion of different layers and ensure the generation of more
discriminative features, we propose a self refinement mod-
ule. Additionally, we introduce the MFF (Multi-Feature
Fusion) block designed to integrate features from various
levels. Finally, for further refinement of the reconstructed
image, we propose the residual refinement module. Our
experiments on the proposed datasets demonstrate that
FusionLungNet outperforms eight state-of-the-art meth-
ods across different evaluation metrics.
Advantages of FusionLungNet include its ability to cap-
ture detailed features at multiple scales, enhancing seg-
mentation accuracy while maintaining computational ef-
ficiency.
The incorporation of a hybrid loss function,
which combines Focal Loss, Structural Similarity Index
(SSIM), and IoU Loss, contributes significantly to the
model’s robustness in handling class imbalance and im-
proving boundary delineation.
While the FusionLungNet framework has shown promis-
ing results, there are several limitations to our approach.
First, the model’s performance might be constrained by
the class imbalance present in the dataset, where lung re-
gions constitute a small proportion of the entire image.
This imbalance can lead to less accurate predictions for
minority classes. Second, although our method integrates
multiple innovative modules, the increased complexity can
result in higher computational costs and longer training
times. Finally, the proposed method’s effectiveness may
vary with different datasets, and its generalizability needs
further validation across a wider range of clinical scenarios
and imaging modalities.
For future work, several directions can be pursued to
address these limitations and enhance the FusionLungNet
framework. One potential improvement is to implement
advanced data augmentation techniques to alleviate the
class imbalance issue. Additionally, exploring more effi-
cient network architectures could help reduce computa-
tional costs while maintaining high performance. Incorpo-
rating domain adaptation techniques might also improve
the model’s generalizability to diverse datasets.
CRediT author statement
S. Rezvani:
Conceptualization, Methodology, Soft-
ware, Writing – Original Draft, Formal Analysis, Visual-
ization, resources. M. Fateh: Validation, Conceptualiza-
tion, Supervision, Review & Editing, Methodology, Inves-
tigation, Project administration. Y. Jalali: Data Cura-
tion, Writing – Original Draft, Formal Analysis, resources.
A. Fateh: Supervision, Writing – Review & Editing, Val-
idation, Visualization, Investigation.
19
References
[1] J. D. Minna, J. A. Roth, and A. F. Gazdar, “Focus on lung
cancer,” Cancer cell, vol. 1, no. 1, pp. 49–52, 2002.
[2] B. Udugama, P. Kadhiresan, H. N. Kozlowski, A. Malekjahani,
M. Osborne, V. Y. Li, H. Chen, S. Mubareka, J. B. Gubbay,
and W. C. Chan, “Diagnosing covid-19: the disease and tools
for detection,” ACS nano, vol. 14, no. 4, pp. 3822–3835, 2020.
[3] S. F. van Vugt, T. J. Verheij, P. A. de Jong, C. C. But-
ler, K. Hood, S. Coenen, H. Goossens, P. Little, and B. D.
Broekhuizen, “Diagnosing pneumonia in patients with acute
cough: clinical judgment compared to chest radiography,” Eu-
ropean Respiratory Journal, vol. 42, no. 4, pp. 1076–1082, 2013.
[4] A. E. McLean, D. J. Barnes, and L. K. Troy, “Diagnosing lung
cancer: the complexities of obtaining a tissue diagnosis in the
era of minimally invasive and personalised medicine,” Journal
of Clinical Medicine, vol. 7, no. 7, p. 163, 2018.
[5] A. Mansoor, U. Bagci, B. Foster, Z. Xu, G. Z. Papadakis, L. R.
Folio, J. K. Udupa, and D. J. Mollura, “Segmentation and image
analysis of abnormal lungs at ct: current approaches, challenges,
and future trends,” Radiographics, vol. 35, no. 4, pp. 1056–1076,
2015.
[6] D. D. Patil and S. G. Deore, “Medical image segmentation: a
review,” International Journal of Computer Science and Mobile
Computing, vol. 2, no. 1, pp. 22–27, 2013.
[7] P. A. C. de Almeida and D. L. Borges, “A deep unsupervised
saliency model for lung segmentation in chest x-ray images,”
Biomedical Signal Processing and Control, vol. 86, p. 105334,
2023.
[8] A. Halder and D. Dey, “Atrous convolution aided integrated
framework for lung nodule segmentation and classification,”
Biomedical Signal Processing and Control, vol. 82, p. 104527,
2023.
[9] V. Nitha and V. C. SS, “Novel cefnet framework for lung disease
detection and infection region identification,” Biomedical Signal
Processing and Control, vol. 96, p. 106624, 2024.
[10] H.-J. Yoo, “Deep convolution neural networks in computer vi-
sion: a review,” IEIE Transactions on Smart Processing and
Computing, vol. 4, no. 1, pp. 35–43, 2015.
[11] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional net-
works for semantic segmentation,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2015,
pp. 3431–3440.
[12] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional
networks for biomedical image segmentation,” in International
Conference on Medical image computing and computer-assisted
intervention.
Springer, 2015, pp. 234–241.
[13] D. Yang, H. Zhao, K. Yu, and L. Geng, “Naunet: lightweight
retinal vessel segmentation network with nested connections and
efficient attention,” Multimedia Tools and Applications, vol. 82,
no. 16, pp. 25 357–25 379, 2023.
[14] A. Kumar,
R. Agrawal,
and L. Joseph,
“Itermiunet:
A
lightweight architecture for automatic blood vessel segmenta-
tion,” Multimedia Tools and Applications, vol. 82, no. 28, pp.
43 207–43 231, 2023.
[15] C. Yu, Y. Wang, C. Tang, W. Feng, and J. Lv, “Eu-net: Au-
tomatic u-net neural architecture search with differential evolu-
tionary algorithm for medical image segmentation,” Computers
in Biology and Medicine, vol. 167, p. 107579, 2023.
[16] R. Wu, Y. Xin, J. Qian, and Y. Dong, “A multi-scale interac-
tive u-net for pulmonary vessel segmentation method based on
transfer learning,” Biomedical Signal Processing and Control,
vol. 80, p. 104407, 2023.
[17] S. Chen, Y. Zou, and P. X. Liu, “Iba-u-net: Attentive bcon-
vlstm u-net with redesigned inception for medical image seg-
mentation,” Computers in Biology and Medicine, vol. 135, p.
104551, 2021.
[18] S. A. Agnes, A. A. Solomon, and K. Karthick, “Wavelet u-
net++ for accurate lung nodule segmentation in ct scans: Im-
proving early detection and diagnosis of lung cancer,” Biomed-
ical Signal Processing and Control, vol. 87, p. 105509, 2024.
[19] S. Zhang, H. Yuan, H. Cao, M. Yang, and C. Zhang, “Sclmnet:
A dual-branch guided network for lung and lung lobe segmen-
tation,” Biomedical Signal Processing and Control, vol. 86, p.
105211, 2023.
[20] Y. Wu, R. Du, J. Feng, S. Qi, H. Pang, S. Xia, and W. Qian,
“Deep cnn for copd identification by multi-view snapshot in-
tegration of 3d airway tree and lung field,” Biomedical Signal
Processing and Control, vol. 79, p. 104162, 2023.
[21] S. Vijayakumar, S. Aarthy, D. Deepa, and P. Suresh, “Sustain-
able framework for automated segmentation and prediction of
lung cancer in ct image using capsnet with u-net segmentation,”
Biomedical Signal Processing and Control, vol. 99, p. 106873,
2025.
[22] I. Bakurov, M. Buzzelli, R. Schettini, M. Castelli, and L. Van-
neschi, “Structural similarity index (ssim) revisited: A data-
driven approach,” Expert Systems with Applications, vol. 189,
p. 116087, 2022.
[23] D. Zhou, J. Fang, X. Song, C. Guan, J. Yin, Y. Dai, and
R. Yang, “Iou loss for 2d/3d object detection,” in 2019 in-
ternational conference on 3D vision (3DV).
IEEE, 2019, pp.
85–94.
[24] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, “Focal
loss for dense object detection,” in Proceedings of the IEEE
international conference on computer vision, 2017, pp. 2980–
2988.
[25] S. Rezvani, M. Fateh, and H. Khosravi, “Abanet: Attention
boundary-aware network for image segmentation,” Expert Sys-
tems, p. e13625, 2024.
[26] A. Saber, P. Parhami, A. Siahkarzadeh, and A. Fateh, “Effi-
cient and accurate pneumonia detection using a novel multi-
scale transformer approach,” arXiv preprint arXiv:2408.04290,
2024.
[27] N. Senthilkumaran and S. Vaithegi, “Image segmentation by
using thresholding techniques for medical images,” Computer
Science & Engineering: An International Journal, vol. 6, no. 1,
pp. 1–13, 2016.
[28] M. Mancas, B. Gosselin, and B. Macq, “Segmentation using a
region-growing thresholding,” in Image Processing: Algorithms
and Systems IV, vol. 5672.
SPIE, 2005, pp. 388–398.
[29] G. Hamarneh and X. Li, “Watershed segmentation using prior
shape and appearance knowledge,” Image and Vision Comput-
ing, vol. 27, no. 1-2, pp. 59–68, 2009.
[30] X. Chen, B. M. Williams, S. R. Vallabhaneni, G. Czanner,
R. Williams, and Y. Zheng, “Learning active contour models for
medical image segmentation,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2019,
pp. 11 632–11 640.
[31] P. Radeva, J. Serrat, and E. Marti, “A snake for model-based
segmentation,” in Proceedings of IEEE International Confer-
ence on Computer Vision.
IEEE, 1995, pp. 816–821.
[32] T. Blaschke, C. Burnett, and A. Pekkarinen, “Image segmen-
tation methods for object-based analysis and classification,” in
Remote sensing image analysis: Including the spatial domain.
Springer, 2004, pp. 211–236.
[33] X. Liu, L. Song, S. Liu, and Y. Zhang, “A review of deep-
learning-based medical image segmentation methods,” Sustain-
ability, vol. 13, no. 3, p. 1224, 2021.
[34] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Im-
agenet large scale visual recognition challenge,” International
journal of computer vision, vol. 115, no. 3, pp. 211–252, 2015.
[35] Y. Liu, S. Yao, X. Wang, J. Chen, and X. Li, “Md-unet: a
medical image segmentation network based on mixed depthwise
convolution,” Medical & Biological Engineering & Computing,
vol. 62, no. 4, pp. 1201–1212, 2024.
[36] R. Azad, M. Asadi-Aghbolaghi, M. Fathy, and S. Escalera,
“Bi-directional convlstm u-net with densley connected convo-
lutions,” in Proceedings of the IEEE/CVF international con-
ference on computer vision workshops, 2019, pp. 0–0.
[37] H. Zhang, X. Zhong, G. Li, W. Liu, J. Liu, D. Ji, X. Li, and
J. Wu, “Bcu-net: Bridging convnext and u-net for medical im-
20
age segmentation,” Computers in Biology and Medicine, vol.
159, p. 106960, 2023.
[38] Z. Zheng, H. Yang, L. Zhou, B. Yu, and Y. Zhang, “Hlu 2-net:
a residual u-structure embedded u-net with hybrid loss for tire
defect inspection,” IEEE Transactions on Instrumentation and
Measurement, vol. 70, pp. 1–11, 2021.
[39] M. Zulfiqar, M. Stanuch, M. Wodzinski, and A. Skalski, “Dru-
net:
Pulmonary artery segmentation via dense residual u-
network with hybrid loss function,” Sensors, vol. 23, no. 12,
p. 5427, 2023.
[40] Y. Jalali, M. Fateh, and M. Rezvani, “Dabt-u-net: Dual atten-
tive bconvlstm u-net with transformers and collaborative patch-
based approach for accurate retinal vessel segmentation,” Inter-
national Journal of Engineering, 2024.
[41] A. Al Qurri and M. Almekkawy, “Improved unet with attention
for medical image segmentation,” Sensors, vol. 23, no. 20, p.
8589, 2023.
[42] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich,
K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz
et al., “Attention u-net: Learning where to look for the pan-
creas,” arXiv preprint arXiv:1804.03999, 2018.
[43] R. Chen, X. Yan, S. Wang, and G. Xiao, “Da-net:
Dual-
attention network for multivariate time series classification,”
Information Sciences, vol. 610, pp. 472–487, 2022.
[44] H.-D. Wang, Z.-Z. Li, I. P. Okuwobi, B.-B. Li, X.-P. Pan, Z.-
B. Liu, R.-S. Lan, and X.-N. Luo, “Pcrtam-net: A novel pre-
activated convolution residual and triple attention mechanism
network for retinal vessel segmentation,” Journal of Computer
Science and Technology, vol. 38, no. 3, pp. 567–581, 2023.
[45] Y. Zhang, Y. Liu, P. Sun, H. Yan, X. Zhao, and L. Zhang,
“Ifcnn: A general image fusion framework based on convolu-
tional neural network,” Information Fusion, vol. 54, pp. 99–118,
2020.
[46] Y. Xie, Y. Zhou, C. Wang, Y. Ma, and M. Yang, “Multi-scale
feature fusion network with local attention for lung segmenta-
tion,” Signal Processing: Image Communication, vol. 119, p.
117042, 2023.
[47] Y. Jiang, J. Dong, T. Cheng, Y. Zhang, X. Lin, and J. Liang,
“iu-net: a hybrid structured network with a novel feature fusion
approach for medical image segmentation,” BioData Mining,
vol. 16, no. 1, p. 5, 2023.
[48] X. Xu, M. Lei, D. Liu, M. Wang, and L. Lu, “Lung segmenta-
tion in chest x-ray image using multi-interaction feature fusion
network,” IET Image Processing, vol. 17, no. 14, pp. 4129–4141,
2023.
[49] Y. Zhang, B. D. Davison, V. W. Talghader, Z. Chen, Z. Xiao,
and G. J. Kunkel, “Automatic head overcoat thickness measure
with nasnet-large-decoder net,” in Proceedings of the Future
Technologies Conference (FTC) 2021, Volume 2.
Springer,
2022, pp. 159–176.
[50] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray,
C. R. Meyer, A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Hen-
schke, E. A. Hoffman et al., “The lung image database consor-
tium (lidc) and image database resource initiative (idri): a com-
pleted reference database of lung nodules on ct scans,” Medical
physics, vol. 38, no. 2, pp. 915–931, 2011.
[51] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,
“Rethinking the inception architecture for computer vision,” in
Proceedings of the IEEE conference on computer vision and
pattern recognition, 2016, pp. 2818–2826.
[52] Z. Chen, Q. Xu, R. Cong, and Q. Huang, “Global context-aware
progressive aggregation network for salient object detection,” in
Proceedings of the AAAI conference on artificial intelligence,
vol. 34, 2020, pp. 10 599–10 606.
[53] X. Cai, Y. Cao, Y. Ren, Z. Cui, and W. Zhang, “Multi-objective
evolutionary 3d face reconstruction based on improved encoder–
decoder network,” Information Sciences, vol. 581, pp. 233–248,
2021.
[54] Z. Wang, E. P. Simoncelli, and A. C. Bovik, “Multiscale struc-
tural similarity for image quality assessment,” in The Thrity-
Seventh Asilomar Conference on Signals, Systems & Comput-
ers, 2003, vol. 2.
Ieee, 2003, pp. 1398–1402.
[55] D. Vijayalakshmi and M. K. Nath, “A novel multilevel frame-
work based contrast enhancement for uniform and non-uniform
background images using a suitable histogram equalization,”
Digital Signal Processing, vol. 127, p. 103532, 2022.
[56] ——, “A systematic approach for enhancement of homogeneous
background images using structural information,” Graphical
Models, vol. 130, p. 101206, 2023.
[57] S. Rezvani, F. S. Siahkar, Y. Rezvani, A. A. Gharahbagh, and
V. Abolghasemi, “Single image denoising via a new lightweight
learning-based model,” IEEE Access, 2024.
[58] M. Z. Alom, C. Yakopcic, T. M. Taha, and V. K. Asari, “Nu-
clei segmentation with recurrent residual convolutional neural
networks based u-net (r2u-net),” in NAECON 2018-IEEE Na-
tional Aerospace and Electronics Conference.
IEEE, 2018, pp.
228–233.
[59] S. L. Lau, E. K. Chong, X. Yang, and X. Wang, “Automated
pavement crack segmentation using u-net-based convolutional
neural network,” Ieee Access, vol. 8, pp. 114 892–114 899, 2020.
[60] Y. Jalali, M. Fateh, M. Rezvani, V. Abolghasemi, and M. H.
Anisi, “Resbcdu-net:
a deep learning framework for lung ct
image segmentation,” Sensors, vol. 21, no. 1, p. 268, 2021.
[61] A. Fateh, M. Fateh, and V. Abolghasemi, “Multilingual hand-
written numeral recognition using a robust deep network joint
with transfer learning,” Information Sciences, vol. 581, pp. 479–
494, 2021.
21
