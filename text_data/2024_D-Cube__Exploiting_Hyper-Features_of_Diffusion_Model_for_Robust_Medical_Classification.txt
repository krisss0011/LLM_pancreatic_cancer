D-Cube: Exploiting Hyper-Features of Diffusion
Model for Robust Medical Classification
Minhee Jang∗, Juheon Son∗, Thanaporn Viriyasaranon‡, Junho Kim§†, Jang-Hwan Choi‡†
∗Artificial Intelligence Convergence, Departments of Artificial Intelligence and Software, Ewha Womans University, Seoul, Korea
‡Department of Artificial Intelligence, Ewha Womans University, Seoul, Korea
§NAVER AI Lab
{minhee_jang, thswngjs77, choij}@ewha.ac.kr, thanaporn.v09@gmail.com, jhkim.ai@navercorp.com
Abstract—The integration of deep learning technologies in
medical imaging aims to enhance the efficiency and accuracy of
cancer diagnosis, particularly for pancreatic and breast cancers,
which present significant diagnostic challenges due to their high
mortality rates and complex imaging characteristics. This paper
introduces Diffusion-Driven Diagnosis (D-Cube), a novel approach
that leverages hyper-features from a diffusion model combined
with contrastive learning to improve cancer diagnosis. D-Cube
employs advanced feature selection techniques that utilize the
robust representational capabilities of diffusion models, enhancing
classification performance on medical datasets under challenging
conditions such as data imbalance and limited sample availability.
The feature selection process optimizes the extraction of clinically
relevant features, significantly improving classification accuracy
and demonstrating resilience in imbalanced and limited data
scenarios. Experimental results validate the effectiveness of D-
Cube across multiple medical imaging modalities, including CT,
MRI, and X-ray, showing superior performance compared to
existing baseline models. D-Cube represents a new strategy in
cancer detection, employing advanced deep learning techniques
to achieve state-of-the-art diagnostic accuracy and efficiency. The
code is available at the provided link1.
Index Terms—Diffusion Models, Medical Image Classification,
Feature Selection, Contrastive Learning, Synthetic Data Genera-
tion.
I. INTRODUCTION
Deep learning has become a crucial component in computer-
aided decision support systems (CADs) for medical image
analysis [61], [62]. It has significantly improved the efficiency
of medical diagnoses, supported treatment processes, and
mitigated diagnostic discrepancies among radiologists and
physicians. However, despite these advancements, deep learning
for medical image diagnosis still faces several challenges.
The development of deep learning for medical image
diagnosis faces several obstacles. One major challenge is the
need for large annotated datasets to train and validate deep
learning models, which is often limited by the time-consuming
and costly nature of the annotation process. Additionally,
medical image datasets frequently exhibit class imbalances,
introducing biases into the models. Each imaging modality, such
as computed tomography (CT), magnetic resonance imaging
∗Minhee Jang and Juheon Son are co-first authors.
†Corresponding author: Junho Kim, Jang-Hwan Choi
1https://github.com/medical-ai-cv/D-Cube.git
TABLE I
PERFORMANCE COMPARISON BASED ON ADAPTATION METHODS IN
FOUNDATION MODELS ON THE PANCREAS CANCER CT DATASET.
Model
Acc
F1
RADIO (full-tuning)
91.00
85.10
RADIO + LoRA (fine-tuning)
87.20
78.50
RADIO + Linear probing
76.30
58.10
Med-CLIP + Linear probing
77.96
63.59
D-Cube
93.61
89.69
(MRI) [2], [24], and X-ray, presents unique characteristics and
complexities, further complicating model development.
Standard approaches to address these challenges include
data sampling techniques like undersampling to balance class
representation. However, these methods have shown limited
success in improving accuracy or enhancing feature extraction
capabilities [63]. Another approach involves using models
pretrained on large-scale natural image datasets like ImageNet
[54], [55]. Recently, multimodal foundation models such as
RADIO [53], CLIP [49], and DINO [50] have shown promising
results on downstream tasks using techniques like linear probing
or Low-Rank Adaptation (LoRA) [56].
Despite these advancements, models pretrained on natural
images often perform suboptimally when applied to medical
imaging tasks, as illustrated in Table I. Our initial studies
fine-tuning the multimodal foundation model RADIO on
a pancreatic cancer CT dataset showed that these models
could not outperform those pretrained specifically on medical
images. This performance gap highlights the importance of
tailoring models to the specific characteristics of medical image
modalities.
To further investigate, we evaluated the Med-CLIP model
[57], pretrained on X-ray images, for a CT pancreatic can-
cer classification task. The results demonstrated suboptimal
performance, confirming that image modality significantly
impacts model effectiveness. This underscores the necessity of
developing models pretrained on medical images tailored to
specific modalities to enhance classification performance.
Improving feature extraction quality is critical in addressing
medical image analysis challenges. Diffusion models have
recently shown promising performance in generative tasks
arXiv:2411.11087v1  [cs.CV]  17 Nov 2024
due to their robust mathematical foundations, which prevent
convergence to local minima and allow continuous evolution
by systematically adding and removing noise. This process
helps capture complex structures and inherent data variability,
leading to high-quality feature representations. Models like
Sora [60] and DALL-E [10] have demonstrated significant
advancements in this area.
This study introduces D-Cube, a novel approach combining
contrastive learning with diffusion models. By leveraging high-
quality feature representations from diffusion models pretrained
on specific medical image modalities, D-Cube aims to address
data imbalance and indistinct class boundaries. Our feature
selection methodology isolates the most discriminative features,
significantly improving classification performance. D-Cube not
only performs consistently well across various modalities but
also enhances predictive accuracy in medical applications.
We evaluated D-Cube’s performance on multiple medical
image modalities, including CT pancreatic cancer classifica-
tion, MRI breast cancer classification, and X-ray COVID-19
classification. The results show that D-Cube outperforms state-
of-the-art models across all image modalities.
Our contributions can be summarized as follows:
• We introduce a novel feature selection technique based
on the Gaussianity metric for diffusion models, leading
to significant performance improvements across various
medical imaging datasets.
• Our method demonstrates robust performance in scenar-
ios with imbalanced and limited datasets, showcasing
strong adaptability and achieving state-of-the-art perfor-
mance compared to 12 baseline models.
• We validate our approach across multiple medical imag-
ing domains, including CT from an Asian population,
MRI from a Western population, and X-ray, ensuring its
broad applicability.
• We present a comprehensive ablation study that high-
lights the impact of our feature selection method, the use
of sub-features from pretrained models, and the integration
of tailored loss functions, illustrating the key components
contributing to performance gains.
II. RELATED WORK
In this section, we introduce research related to our study,
focusing on the adaptation of advanced vision techniques to
the medical domain, particularly through the use of diffusion
features. We discuss the challenges of integrating traditional
vision models into medical imaging and highlight how diffusion
can enhance diagnostic accuracy.
A. Medical Domain Adaptation
As computer vision technology advances, its application
in the medical domain has expanded significantly. However,
applying well-known general models like ResNet [12], ViT
[16], and PiT [19] to medical domains presents substantial
challenges, particularly in distinguishing sensitive features
such as fine blood vessels or ambiguous tumors. Extensive
research has been conducted on performing segmentation,
detection, and classification tasks in the medical image domain
[39], [40]. Yet, diagnostic models often depend on precisely
segmented masks to identify cancer or tumor locations—a
process that is both costly and labor-intensive [4], [5], [7],
[26]–[28]. Consequently, there has been a surge in research
focused on effectively learning from the abundant unlabeled
data in medical domains [33]. With the recent emergence of
generative models, numerous attempts have been made to apply
diffusion algorithms to the medical domain for tasks such as
tumor segmentation and prediction [29], [34], [36], [37], [58].
Efforts have also been made to address data imbalance by
generating synthetic data and incorporating them into models
[8], [38]. However, there are situations where it is necessary to
use diffusion models in an ensemble for training or to enhance
performance further [35]. Due to the nature of medical data,
where small samples often contain significant variability and
heterogeneity, training performance can be inconsistently robust,
heavily influenced by these data distributions [41].
B. Exploiting Diffusion Features
Since the advent of Denoising Diffusion Probabilistic Models
(DDPM) [3], the field of generative diffusion models has
witnessed significant progress [11], [25]. This advancement
has spurred research on applying diffusion algorithms to
downstream vision tasks. For instance, DFormer [44] effectively
employs diffusion models for image segmentation, and recent
developments include zero-shot classification, leveraging the
generation of random noise from diffusion models [45].
Additionally, recent studies have analyzed the internal rep-
resentations of diffusion model features, highlighting their
utility in tasks like medical segmentation [42], [43]. Drawing
parallels with DINO, known for its high-quality features
used in segmentation and classification, extensive research
is underway. This research explores the semantic content of
features generated by diffusion algorithms in vision tasks and
the use of features from high-performing generative diffusion
models [46], [48]. However, some studies focus on aggregating
features from a hand-selected subset of layers and specific
timesteps t for enhanced semantic understanding [47].
In contrast to these approaches, the DiffMIC algorithm
[29] uses a diffusion model to generate classification logits
instead of images. It extracts global and local features using a
pretrained ResNet and aggregates them using a top-k method
to produce logit vectors from three distinct sets: global features,
local features, and a fusion of both. These logits are used as
conditioning inputs for the diffusion model, which predicts
them during training using mean squared error (MSE) and
maximum mean discrepancy (MMD) loss functions. During
testing, the model generates logits through sampling, which
are then used for classification.
Our model differs significantly from DiffMIC. While
DiffMIC focuses on generating classification logits, our model
generates images to ensure the semantic richness of features.
We extract global features from ResNet, aggregate them via
summation to obtain logit vectors, and multiply these logits
with single-channel diffusion features. This approach allows
2
Fig. 1.
Overall Architecture: Step 1 involves the process of generating diffusion features that enhance the performance of D-Cube, while step 2 entails
training a D-Cube for cancer diagnosis using the features generated from frozen diffusion model. xt represents an original image x0 with noise at random time
step t. For more detailed information, refer to the method in Section III and the analysis in Fig. 2. The dashed line indicates the optional use of sub features.
us to leverage the diffusion process more integrally, bypassing
the need for direct logit-based classification. Furthermore,
our study introduces a robust feature selection method for
diffusion models, ensuring the selection of semantically rich
and meaningful features. This strategy enhances the overall
effectiveness of diffusion models in various downstream tasks,
especially in complex domains such as medical imaging. By
focusing on effective feature selection and integration, our
approach aims to improve the performance and applicability
of generative diffusion models.
III. D-CUBE
Our training framework is structured around a dual-stage
process, as illustrated in Fig. 1.
A. Preliminary: Diffusion Models
In this study, we employ the DDPM framework, renowned
for its exceptional ability to comprehend training data, to
extract significant features from medical images. We explain
the fundamental operations of the DDPM here.
A diffusion model is probabilistic generative model designed
to learn data distributions. The DDPM refines noisy data
through a two-step process: forward and reverse. According to
Markov chain theory, each step depends solely on the previous
step’s state. In the forward process, noise is incrementally
added at each timestep t to the original data x0, transforming
it into xt, until reaching a predefined large number of steps T,
ultimately converting xT into Gaussian noise. Conversely, the
reverse process aims to reconstruct x0 from xt, wherein the
diffusion model learns to denoise the data added during the
forward process.
The forward process can be represented as a Markov chain,
as shown in Eq. (1):
q(xt|xt−1) = N(xt; √αtxt−1, (1 −αt)I),
(1)
where αt is a fixed variance schedule. Our objective is to
employ a class-conditional diffusion model to generate data
more representative of specific classes. In reverse process, xt is
accompanied by a class condition, y. With learned functions µθ
and Σθ representing mean and covariance, the reverse process
is expressed as shown in Eq. (2):
p(xt−1|xt, y) = N(xt−1; µθ(xt, t, y), Σθ(xt, t, y))
(2)
The diffusion loss in our study is designed following
the principles outlined for class-conditional diffusion models.
Specifically, it is calculated by optimizing the evidence lower
bound (ELBO). For a given timestep t, input x, and class
condition y, the loss is defined as the MSE between the model’s
output noise ϵθ given y and the random Gaussian noise ϵ used
3
to generate xt at timestep t as expressed in the following
Eq. (3):
LDiff(xt, y) = Et,ϵ [∥ϵ −ϵθ(xt, y)∥2]
(3)
This formulation ensures that the diffusion model learns to
minimize the difference between the predicted and true noise,
thereby effectively capturing the underlying data distribution.
B. Dual-Stage Training
Step 1 involves training the diffusion model to effectively
address challenges such as class indistinguishability and data
imbalance prevalent in medical datasets, optimizing the model
for robust feature representation. In step 2, we freeze the
pretrained diffusion model and then focus on selecting the
most effective features from it for classification. Additionally,
step 2 introduces advanced techniques like cycle loss and con-
sistency regularization, aimed at further refining the classifier’s
performance and ensuring its stability.
1) Step 1, Training a Diffusion Model:
a) Objective: Medical datasets often face challenges
like class indistinguishability and data imbalance. In such
environments, the diffusion model may struggle to learn
tail distributions effectively, leading to suboptimal feature
representations [64]. Moreover, when the data volume is limited,
the distribution learning process of the model can be time-
consuming. We introduce a contrastive loss function for the
diffusion model, making it possible to utilize the label-aligned
representations of the diffusion model to enhance feature
representation for the downstream tasks, thereby promoting the
embedding of critical semantic information. Our contrastive
loss in Eq. (4) is designed to minimize feature distances within
the same class and maximize them for different classes, thereby
fostering robust feature representations for classification.
LCont(x1
t, x2
t) =
N
X 
(1 −target) ·
fmid(x1
t) −fmid(x2
t)

2
+target ·
 max(margin −
fmid(x1
t) −fmid(x2
t)

2 , 0)
 
(4)
In Eq. (4), N denotes the batch size, and t indicates a specific
timestep, where x1
t and x2
t are input data with added noise at
timestep t. fmid represents the middle layer of the diffusion
model. fmid(x1
t) is the output from the middle layer when input
x1
t is provided. fmid(x2
t) is defined similarly. When the labels of
the two input data are the same, the target value is 0, and it is
1 otherwise. Hence, LCont minimizes the L2 distance between
the two feature embeddings when the labels are identical and
ensures that the L2 distance exceeds a specific margin when
they are different.
The final loss LGen for training the diffusion model is formed
as Eq. (5). x1
t and x2
t represent noisy images at the same
timestep t with labels y1 and y2. The labels y1 and y2 can be
identical or different. ϵ1,ϵ2 represent the predicted noise for x1
t
and x2
t, respectively. The differences between ϵ1 and ϵ2 and
the ground truth ϵ are calculated as LDiff(x1
t) and LDiff(x2
t).
This leads to a lower Fréchet Inception Distance (FID) [22],
indicating reduced discrepancy between real and generated data
across all classes.
LGen = LCont + LDiff(x1
t) + LDiff(x2
t)
(5)
2) Step 2, Training a Classifier Model: We freeze all layers
of the pretrained diffusion model and extract features from
specific layers for classification. To enhance this process, we
propose a feature selection method based on a gaussianity met-
ric to identify the most discriminative features for classification.
Additionally, to further improve performance, we introduce a
cycle loss and incorporate a consistency regularization loss to
stabilize the training process.
a) Feature Selection with a Gaussianity Metric: Diffu-
sion models are designed to denoise xt by accurately predicting
the true noise present in xt. This leads to a situation where
certain layers in the model produce feature maps that closely
resemble a Gaussian distribution, while other layers capture
the semantic features of xt. To quantify this phenomenon, we
employ the Kolmogorov-Smirnov (KS) test as a Gaussianity
metric, which measures the degree to which a feature map
approximates a Gaussian distribution. Consequently, if the
feature map of a particular layer is found to be close to
a Gaussian distribution, it can be inferred that this layer is
primarily involved in noise prediction. Conversely, if the feature
map deviates from a Gaussian distribution, it indicates that the
layer is preserving the semantic features of the image. Based
on this understanding, we propose a feature selection method
to appropriately choose features from the diffusion model for
classification, presenting criteria for their use.
Specifically, we employ the KS test to identify and select
feature maps that deviate from a Gaussian distribution, as these
are more likely to preserve semantic information useful for
classification. The KS test compares the cumulative distribution
function (CDF) of the feature map with that of a Gaussian
distribution. As stated in Eq. (6), The test statistic D is defined
as the maximum distance between these two CDFs:
D = sup
x0
|Fn,i(x0) −F(x0)|
(6)
where Fn,i(x0) is the empirical CDF of the feature map from
layer i based on the batch size n, and F(x0) is the CDF
of the Gaussian distribution. Here, x0 represents the feature
map from a diffusion layer. The p-value obtained from the
KS test indicates the likelihood that the feature map follows
a Gaussian distribution. If the p-value is greater than 0.05,
we consider the feature map to be Gaussian. This detailed
analysis enables us to discern which layers are more likely to
preserve semantic features versus those primarily engaged in
noise prediction, thereby guiding the feature selection process
for optimal classification performance.
b) Objective: We propose a cycle loss to improve per-
formance of the classification model. D-Cube predicts ˜y, and
its ground truth y passes through a diffusion model with the
same input, generating each output noise. To minimize the
difference in above each noise, the learning process should
be directed towards making ˜y and y same. This contributes to
4
the D-Cube module producing a more accurate ˜y. We utilize
cycle loss only to update the weights of the D-Cube so that
the pretrained diffusion model from step 1 is kept in a frozen
state.
LCycle(xt, y, ˜y) = E [∥ϵθ(xt, ˜y) −ϵθ(xt, y)∥2]
(7)
Eq. (7) represents our proposed cycle loss, where θ denotes
pretrained diffusion model. It quantifies L2 distance of the ϵ
predicted by the model (θ) when input xt and predicted class
˜y from step 2 of the training process are fed along with the
ground truth class y at a specific timestep t.
By incorporating consistency regularization loss, we aimed
to stabilize the training process by ensuring that the classifi-
cation output remains consistent even when the input images
undergo transformations. Specifically, we trained the D-Cube
using both the original images and their horizontally flipped
versions, matching the logits produced by these variations. The
consistency regularization loss, LCR, is calculated as shown in
Eq. (8). This equation represents the MSE loss between the
logits:
LCR = 1
N
N
X
i=1
∥g(xi) −g(T(xi))∥2
2
(8)
Here, N is the batch size, xi represents the original image, and
T(xi) denotes the horizontally flipped image. The function g(x)
is the logit output of the model for the input x. Cross-entropy
loss LCE is calculated between predicted class and ground truth
classes ˜y and y as shown in Eq. (9). We scale these losses LCR
and LCE with scaling parameter λ1 and λ2. Ultimately, LCLS
becomes the final classification loss as shown in Eq. (10):
LCE(y, ˜y) = −E [y log(˜y) + (1 −y) log(1 −˜y)]
(9)
LCLS = LCE + λ1LCycle + λ2LCR
(10)
c) Additional Technique: For classification tasks, we
leverage features extracted from a pretrained diffusion model.
Features selected based on the Gaussianity metric are fed
forward into our classifier, D-Cube. To further enhance perfor-
mance efficiently, we utilize a ResNet pretrained on ImageNet
[51] to extract sub features for performance improvement, as
pretrained ResNet models have been proven to be effective in
various downstream tasks [54], [55]. We obtain a sub feature
fsub ∈RC×H×W , corresponding to the arbitrary channel.
Features fsub and fmain are concatenated and subjected to global
sum pooling to derive sub scores for each channel. Multiplying
the single-channel diffusion feature by the ResNet sub features,
which we arbitrarily set to three channels, effectively expands
the single-channel feature into three channels. This process
reflects the important elements identified by the ResNet, thereby
achieving the effect of channel expansion. ResNet, along with
D-Cube, is fine-tuned to our data, producing fsub that is tailored
to our specific dataset. These scores are then used to weight
the concatenated features. D-Cube incorporates convolutional
layers equipped with multiple kernel sizes of 3 × 3 and 5 ×
5, repeated three times. We avoid pooling to preserve spatial
information, focusing on a gradual reduction in the channel
dimensions of the output. The final prediction is made through
two fully connected layers.
IV. EXPERIMENTS
A. Datasets
To facilitate validation across different races and modalities,
we used the Duke MRI dataset, representing Western popu-
lations, and the National Information Society Agency (NIA)
CT dataset for an Asian populations. This strategy aimed to
broaden the diversity and global applicability of our research.
1) NIA Pancreas Cancer CT: We utilized a rigorously
labeled dataset from the NIA’s Medical Big Data Construction
Project, comprising CT images of pancreatic cancer and normal
tissues from eight South Korean hospitals. Additional data were
sourced from the Medical Segmentation Decathlon and the
Cancer Imaging Archive. The dataset, encompassing records
from 6,063 pancreatic cancer patients, was divided into training
and testing sets at a 7:3 ratio. Initially containing 11,775 benign,
11,486 malignant, and 41,033 normal images, we refined the
dataset by removing non-pancreatic slices and cropping images
to 256 × 256 pixels, centering on the pancreas. After processing,
the dataset consisted of 5,513 benign, 6,744 malignant, and
24,626 normal images.
2) Duke Breast Cancer MRI [32]: This dataset comprises
scans from 922 patients, with a focus on classifying lym-
phadenopathy and suspicious lymph nodes in MRI scans from
127 patients. These patients were categorized into training
(100 patients) and testing (27 patients) groups based on their
IDs. The training dataset included 2,477 images, consisting
of 1,071 images classified as lymphadenopathy and 1,406
images classified as suspicious lymph nodes. The testing
dataset consisted of 707 images, with 272 images in the
lymphadenopathy class and 434 images in the suspicious lymph
node class.
3) COVID-19 Radiography Dataset [30], [31]: We used
a chest X-ray dataset that was developed by a team of
researchers from Qatar University, Doha, Qatar, and the
University of Dhaka, Bangladesh, in collaboration with their
counterparts from Pakistan and Malaysia and medical doctors.
This comprehensive dataset includes images of COVID-19
positive cases, normal cases, and cases of viral pneumonia.
Specifically, the dataset comprises 3,616 COVID-19 positive
cases, 10,192 normal cases, 6,012 lung opacity cases (non-
COVID lung infections), and 1,345 viral pneumonia cases. The
dataset is split into training and testing sets in an 8:2 ratio.
B. Implementation Details
We developed our framework using PyTorch and executed
it on an NVIDIA RTX A6000 GPU.
1) Details Regarding Step 1: We set the contrastive
loss margin to 0.1 and did not perform any additional data
augmentation during the first step of training. For the pancreas,
breast, and COVID datasets, we set the learning rate at 2×10−4
and trained the models for approximately 280, 950, and
730 epochs, respectively, until the FID scores stabilized. We
employed the AdamW optimizer. Considering both training
5
TABLE II
QUANTITATIVE ANALYSIS ON PANCREAS CANCER CT, BREAST CANCER MRI, AND COVID CHEST X-RAY DATASETS. THE HIGHEST AND
SECOND-HIGHEST SCORES ARE HIGHLIGHTED IN BOLD AND UNDERLINED.
Model
Pancreas Cancer (CT)
Breast cancer (MRI)
COVID Chest (X-ray)
Acc
Precis
Recall
F1
Acc
Precis
Recall
F1
Acc
Precis
Recall
F1
CNN-Based
ResNet-101 [12]
85.23
77.47
74.51
75.72
64.50
64.32
55.93
52.44
90.83
93.20
92.37
92.71
ResNeXt-101 [13]
87.55
74.58
67.46
69.37
61.53
56.67
38.53
28.28
89.20
90.89
89.12
89.97
ResNeSt [14]
87.20
81.27
76.65
78.36
66.76
43.35
40.74
40.59
92.01
93.90
92.60
93.23
ShuffleNet V2 [15]
89.25
84.52
81.11
82.47
68.32
66.68
63.59
63.83
90.34
92.19
91.40
91.76
Transformer-Based
T2T-ViT [17]
82.14
74.58
67.46
69.37
62.18
82.58
57.51
52.51
87.07
90.07
85.63
87.64
ViT-B/16 [16]
78.55
68.15
60.68
63.31
66.90
82.48
57.14
51.88
85.21
89.14
84.33
86.50
CVT [18]
78.32
65.82
59.09
60.34
68.60
79.10
59.68
56.49
85.85
87.14
84.67
85.85
MiT-b0 (SegFormer) [23]
88.77
83.88
79.87
81.63
66.62
77.18
57.12
52.34
91.49
93.62
90.07
91.71
PiT-B [19]
88.24
83.21
79.18
80.71
65.91
67.98
57.42
54.33
92.60
94.29
92.72
93.48
PVT V2-b0 [20]
89.59
85.21
81.33
83.09
65.49
50.47
37.10
33.22
92.41
94.04
92.32
93.15
PVT-Tiny [21]
89.52
85.96
80.31
82.47
67.47
73.16
58.83
55.82
92.93
94.84
92.84
93.80
CPT [6]
92.78
90.70
86.48
88.22
74.54
73.37
71.79
72.30
96.08
97.18
96.50
96.83
Diffusion-Based
DiffMIC [29]
92.68
90.74
86.90
88.53
75.01
72.85
68.09
68.70
96.40
97.72
96.68
97.20
D-Cube (ours)
93.61
92.05
88.05
89.69
77.98
77.87
74.64
75.52
96.28
97.49
96.28
96.87
speed and memory efficiency, we adopted U-ViT [9] as our
backbone model.
2) Details Regarding Step 2: In step 2, the model uses the
original image x0 without any noise at a random time step t as
input. During the classifier module training, we applied random
horizontal flip augmentation with a 50% probability. For the
pancreas, breast, and COVID datasets, we set the learning rate
at 2 × 10−4. Due to the relatively small size of the breast
dataset, we stabilized training by setting a weight decay of 0.1.
We used the AdamW optimizer and enhanced training stability
by applying the Exponential Moving Average (EMA) with an
α value of 0.999. The pancreas cancer CT and COVID chest X-
ray datasets achieved consistent performance around 70 epochs,
while the breast cancer MRI dataset continued experiments up
to 50 epochs. We scaled the cycle loss by applying weights
of 10 for the pancreas and 100 for the breast. When applying
consistency regularization loss, we empirically set the λ value
to 0.1, achieving the highest performance across all datasets.
C. Benchmarking
We performed experiments to compare our D-Cube model
with existing CNN-based, Transformer-based, and diffusion-
based models. In the diffusion-based medical classification
models, due to the limited number of studies and the lack of
publicly available code, reproducing the results was challenging.
Therefore, we only compared DiffMIC with open source.
In the medical domain, metrics like recall and F1 score
are essential. High recall ensures that diseases are identified
with minimal misses, while the F1 score provides a balanced
measure of a model’s accuracy, especially important in medical
settings to avoid both false positives and false negatives.
As shown in Table II, D-Cube surpasses previous mod-
els in almost metrics across datasets. This achievement is
attributed to the features D-Cube learns, effectively capturing
the training data distribution and embedding essential semantic
TABLE III
AUGMENTATION WITH SYNTHETIC DATA OF D-CUBE ON PANCREAS
CANCER CT. SYN INDICATES GENERATED DATA FROM DIFFUSION MODEL
OF STEP 1.
Model
Acc
Precis
Recall
F1
ResNeXt-101
87.55
81.87
77.94
78.92
+ Syn
88.53+0.98
83.18+1.31
79.86+1.92
80.99+2.07
ViT-B/16
78.55
68.15
60.68
63.31
+ Syn
80.59+2.04
70.83+2.68
64.90+4.22
67.14+3.83
information to boost classification performance. Furthermore,
the experimental results illustrate that D-Cube outperforms
CPT, a transformer-based architecture model that utilized self-
supervised pretraining on medical images domain, achieving the
best accuracy among transformer-based models. The DiffMIC,
which employs a diffusion algorithm that effectively leverages
the training data, demonstrated performance similar to ours;
however, D-Cube achieves accuracies of 93.61% and 77.98%
for pancreas cancer CT and breast cancer MRI datasets
respectively, compared to DiffMIC with accuracies of 92.68%
and 75.01% on the same datasets. Notably, D-Cube outperforms
DiffMIC by approximately 6% on the smaller Breast dataset.
Therefore, our D-Cube has demonstrated robust performance
even in situations with limited data, regardless of the domain.
Our model has the advantage of generating images, which
were used to address class imbalance issues in training lower-
performing CNN and transformer models, as demonstrated in
Table II. For the pancreas data, we added 30,000 synthetic
images, adjusting the dataset’s ratio from 1:1:4 (benign,
malignant, and normal) to a more balanced 2:2:3. As a result, as
shown in Table III, we observed improvements in all evaluation
metrics with notable increases of approximately 2-4% in recall
and F1 score, demonstrating our approach’s efficacy.
6
TABLE IV
ABLATION ANALYSIS OF D-CUBE ON PANCREAS CANCER CT. FS
INDICATES USING THE PROPOSED FEATURE SELECTION, AND fSUB
INDICATES THE USE OF SUB FEATURES EXTRACTED FROM THE RESNET.
Model
Acc
Precis
Recall
F1
Baseline
85.30
77.82
74.81
75.82
+ LGen
85.97+0.67
78.95+1.13
75.91+1.1
77.10+1.28
+ Fs
87.86+1.89
82.13+3.18
78.64+2.73
79.95+2.85
+ LCls
88.26+0.4
82.77+0.64
79.20+0.56
80.61+0.66
+ fsub
93.61+5.35
92.05+9.28
88.05+8.85
89.69+9.08
Fig. 2.
Gaussianity Test of Each Datasets This graph presents the p-value
analysis for pancreas cacner CT, breast cancer MRI, and COVID chest X-ray.
Yellow star denote the best combination used, while red cross marks represent
the worst combination. The p-value is calculated based on the average of
features with a batch size of 256 when t=100.
D. Ablation Study
1) Model Ablation: Our quantitative results confirm that
features from the diffusion model, which deeply understand
the training data, contain crucial semantic information for
classification tasks. We explored this by conducting an ablation
study, assessing the benefits of features derived from the
diffusion model and the impact of various loss functions on the
D-Cube’s performance, as shown in Table IV. For the baseline,
in training step 1, we applied only Ldiff, and in step 2 within the
classifier, we applied only LCE. Moreover, we used the features
from the final output layer before applying feature selection.
Additionally, the efficacy of a diffusion model is indicated
by lower FID scores. Our LGen method led to reduced FID
scores, as shown in Table V, thereby enhancing performance.
Enhancements in the diffusion model through LGen are linked to
improved D-Cube performance. Moreover, our feature selection
strategies (Fs) helped identify crucial features for classification,
enabling high performance even in scenarios with imbalanced
and scarce data. In Fig. 2, features from layers marked with
yellow star—corresponding to p-values less than 0.05—were
considered the best selections and applied. This approach
resulted in significant performance improvements. As observed
in Table IV, the classification performance using features from
the diffusion model, without the aid of sub-features, matches
that of CNN and transformer based models. This implies that
the diffusion model, although not specifically designed for
classification tasks, is capable of learning valuable feature
representations that enhance classification effectiveness. The
integration of our LCls allowed for more refined learning, and
the use of fsub positively influenced D-Cube’s effectiveness.
Through strategic feature selection and diverse loss functions, D-
Cube’s performance, detailed in Table II, matches other models
except for DiffMIC. By utilizing sub-features, we outperformed
DiffMIC.
2) Feature Selection: We utilized a Gaussianity metric to
determine which layer feature maps in diffusion models for
pancreas cancer CT, breast cancer MRI, and COVID chest
X-ray approximate a Gaussian distribution, as shown in Fig. 2.
The diffusion models are designed to accurately predict true
noise present in xt. Therefore, if a feature map of a specific
layer closely resembles a Gaussian distribution, it implies
that this feature map is predominantly noise with minimal
semantic information. Conversely, if a feature map deviates
from a Gaussian distribution, it indicates that the layer retains
significant semantic information in xt. Based on these findings,
layers that contain the most semantic information were marked
with yellow stars, while those that did not were marked with
red crosses.
We concatenated the feature maps of the layers marked with
yellow stars, referred to as the Best Selection, and those marked
with red crosses, referred to as the Worst Selection. We then
conducted a comparative analysis for each dataset, the results
of which are presented in Table VI. Our analysis reveals that
the Best Selection retains more semantic information, leading to
superior performance, thereby validating our feature selection.
3) Input Ablation: Given the random sampling nature of
diffusion-based methods, we analyzed the robustness of our
proposed method according to the input. As demonstrated in
Table VII through input ablation studies, our model showed
remarkably robust performance, unaffected by the random Gaus-
sian noise added to the input. Notably, the best performance
was observed using the original images x0 without added noise
and at a randomly generated timestep t.
4) Compatibility of Pretrained Models with Diffusion
Features: In contrast to general natural images, medical images
require the capture of fine details due to the often small
size of lesions. To achieve this, networks need to function
7
TABLE V
COMPARISON OF FID SCORES SYNTHETIC IMAGES GENERATED BY A DIFFUSION MODEL WITH AND WITHOUT LGEN ON PANCREAS CANCER CT, BREAST
CANCER MRI, AND COVID CHEST X-RAY DATASETS.
FID ↓
Pancreas Cancer (CT)
Breast Cancer (MRI)
COVID Chest (X-ray)
Benign
Malignant
Normal
Lymphadenopathy
Suspicious
COVID
Normal
Opactiy
Viral
U-ViT (step 1)
24.8
29.3
27.3
79.7
71.1
70.03
58.53
74.60
112.70
U-ViT (step 1) w/ LGen
22.2
24.0
21.0
66.2
66.9
66.86
59.14
74.81
101.02
TABLE VI
FEATURE SELECTION ANALYSIS ON PANCREAS CANCER CT, BREAST CANCER MRI, AND COVID CHEST X-RAY DATASETS. WE SELECTED FEATURES
BASED ON FIG. 2.
Pancreas Cancer (CT)
Breast Cancer (MRI)
COVID Chest (X-ray)
Acc
Precis
Recall
F1
Acc
Precis
Recall
F1
Acc
Precis
Recall
F1
Worst Selection
93.23
91.25
87.38
88.94
76.84
75.63
74.78
75.12
95.85
97.28
95.59
96.41
Best Selection
93.61
91.97
87.92
89.52
77.98
77.87
74.64
75.52
96.28
97.49
96.28
96.87
TABLE VII
ABLATION STUDY OF D-CUBE ON PANCREATIC CANCER CT. THE
RESULTS ARE BASED ON VARIATIONS IN INPUT IMAGE NOISE AND RANDOM
TIMESTEPS t DURING THE CLASSIFICATION PROCESS IN STEP 2
Input
Acc
Precis
Recall
F1
x0, t
93.61
92.05
88.05
89.69
xt, t
93.40
91.81
87.77
89.33
x0, 0
93.10
91.41
87.03
88.79
TABLE VIII
SUB FEATURES ANALYSIS OF D-CUBE ON PANCREAS CANCER CT.
Model of Sub Features
Acc
Precis
Recall
F1
CNN-Based
ResNeXt-101
93.61
92.05
88.05
89.6
DenseNet
93.40
91.36
87.76
89.21
Transformer-Based
ViT
80.94
69.70
65.83
66.75
RADIO
78.28
61.56
65.97
62.86
like high-pass filters, amplifying high-frequency components.
Moreover, the medical domain typically has a limited amount
of data, making inductive biases such as locality and translation
invariance advantageous for learning from limited data. Vision
Transformers (ViTs) tend to reduce high-frequency components,
resulting in a shape bias, whereas CNNs amplify high-frequency
components, leading to a texture bias [52]. The inductive
bias inherent in CNNs makes them particularly effective
in data-constrained environments such as the medical field.
Our experiments in Table VIII confirm that CNN-based
models, such as ResNet and DenseNet, are more adept at
capturing useful fsub; however, when using sub-features from
the transformer-based models, like ViT and RADIO series,
it was observed that the diffusion features did not synergize
well.
V. CONCLUSION
In conclusion, our research convincingly demonstrates the
advantages of diffusion models in the medical domain, sig-
nificantly outperforming traditional foundation models. Dif-
fusion models show exceptional capability in addressing the
specific challenges of limited data and class imbalance often
encountered in medical imaging tasks. By leveraging feature
selection via the Gaussianity metric, particularly focusing
on non-Gaussian features, we have notably enhanced the
performance of D-Cube.
Our approach utilizes the generative capacity of diffusion
models to improve the accuracy of networks by incorporating
synthetic data and employing tailored loss functions and feature
selection techniques. This strategy has resulted in state-of-the-
art performance across various medical imaging modalities.
Additionally, our findings reveal that diffusion features, when
combined with sub-features from CNN architectures, effectively
capture fine details essential for accurately detecting subtle
anatomical structures in medical images.
For future work, we aim to extend our insights and
methodologies to general datasets beyond medical diagnostics,
exploring the potential of diffusion models in other fields
characterized by data scarcity and complexity. By continuing
to refine feature selection criteria and integrating advanced
deep learning techniques, we anticipate further advancements
in the performance and applicability of diffusion models in
diverse domains.
VI. ACKNOWLEDGEMENT
This work was supported by Institute of Information &
communications Technology Planning & Evaluati on (IITP)
grant funded by the Korea government(MSIT). (No. RS-
2022-00155966, Artificial Intelligence Convergence Innovation
Human Resources Development (Ewha Womans University))
8
REFERENCES
[1] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in Proc. MICCAI, 2015.
[2] L. Chen, X. Liang, C. Shen, S. Jiang, and J. Wang, “Synthetic CT
generation from CBCT images via deep learning,” Med. Phys., 2020.
[3] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
NeurIPS, 2020.
[4] P.-T. Chen, D. Chang, T. Wu, M.-S. Wu, W. Wang, and W.-C. Liao,
“Applications of artificial intelligence in pancreatic and biliary diseases,”
J. Gastroenterol. Hepatol., 2021.
[5] P.-T. Chen, T. Wu, P. Wang, D. Chang, K.-L. Liu, M.-S. Wu, H. R.
Roth, P.-C. Lee, W.-C. Liao, and W. Wang, “Pancreatic cancer detection
on CT scans with deep learning: a nationwide population-based study,”
Radiology, 2023.
[6] T. Viriyasaranon, S. M. Woo, and J.-H. Choi, “Unsupervised Visual
Representation Learning Based on Segmentation of Geometric Pseudo-
Shapes for Transformer-Based Medical Tasks,” IEEE J. Biomed. Health
Inform., 2023.
[7] Zhang, Jiadong, Zhiming Cui, Zhenwei Shi, Yingjia Jiang, Zhiliang
Zhang, Xiaoting Dai, Zhenlu Yang, Yuning Gu, Lei Zhou, Chu Han, et
al. "A robust and efficient AI assistant for breast tumor segmentation
from DCE-MRI via a spatial
[8] S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet, “Synthetic
data from diffusion models improves imagenet classification,” arXiv
preprint arXiv:2304.08466, 2023.
[9] F. Bao, S. Nie, K. Xue, Y. Cao, C. Li, H. Su, and J. Zhu, “All are worth
words: A vit backbone for diffusion models,” in Proc. CVPR, 2023.
[10] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,
and I. Sutskever, “Zero-shot text-to-image generation,” in Proc. ICML,
2021.
[11] W. Chai, X. Guo, G. Wang, and Y. Lu, “Stablevideo: Text-driven
consistency-aware diffusion video editing,” in Proc. ICCV, 2023.
[12] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proc. CVPR, 2016.
[13] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” in Proc. CVPR, 2017.
[14] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun, T. He, J.
Mueller, R. Manmatha, and others, “ResNeSt: Split-Attention Networks”
in Proc. CVPR, 2022.
[15] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “Shufflenet v2: Practical
guidelines for efficient cnn architecture design,” in Proc. ECCV, 2018.
[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An Image is Worth 16x16 Words:
Transformers for Image Recognition at Scale,” in Proc. ICLR, 2021.
[17] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z.-H. Jiang, F. E. Tay, J.
Feng, and S. Yan, “Tokens-to-token vit: Training vision transformers
from scratch on imagenet,” in Proc. ICCV, 2021.
[18] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,
“Cvt: Introducing convolutions to vision transformers,” in Proc. ICCV,
2021.
[19] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, “Rethinking
spatial dimensions of vision transformers,” in Proc. ICCV, 2021.
[20] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
L. Shao, “Pvt v2: Improved baselines with pyramid vision transformer,”
Computational Visual Media, 2022.
[21] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and
L. Shao, “Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions,” in Proc. ICCV, 2021.
[22] G. Parmar, R. Zhang, and J.-Y. Zhu, “On Aliased Resizing and Surprising
Subtleties in GAN Evaluation,” in Proc. CVPR, 2022.
[23] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,
“SegFormer: Simple and efficient design for semantic segmentation with
transformers,” NeurIPS, 2021.
[24] A. S. Lundervold and A. Lundervold, “An overview of deep learning in
medical imaging focusing on MRI,” Zeitschrift für Medizinische Physik,
2019.
[25] J. Kim, M. Kim, H. Kang, and K. H. Lee, “U-GAT-IT: Unsupervised Gen-
erative Attentional Networks with Adaptive Layer-Instance Normalization
for Image-to-Image Translation,” in Proc. ICLR, 2019.
[26] N. M. Braman, M. Etesami, P. Prasanna, C. Dubchuk, H. Gilmore, P.
Tiwari, D. Plecha, and A. Madabhushi, “Intratumoral and peritumoral
radiomics for the pretreatment prediction of pathological complete
response to neoadjuvant chemotherapy based on breast DCE-MRI,” Breast
Cancer Res., 2017.
[27] T. Ren, S. Lin, P. Huang, and T. Q. Duong, “Convolutional neural
network of multiparametric MRI accurately detects axillary lymph node
metastasis in breast cancer patients with pre neoadjuvant chemotherapy,”
Clin. Breast Cancer, 2022.
[28] Z. Zhu, E. Albadawy, A. Saha, J. Zhang, M. R. Harowicz, and M. A.
Mazurowski, “Deep learning for identifying radiogenomic associations
in breast cancer,” Comput. Biol. Med., 2019.
[29] Yang, Y., Fu, H., Aviles-Rivero, A. I., Schönlieb, C. B., & Zhu, L. (2023,
October). Diffmic: Dual-guidance diffusion network for medical image
classification. In International Conference on Medical Image Computing
and Computer-Assisted Intervention (pp. 95-105). Cham: Springer Nature
Switzerland.
[30] M.E.H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M.A. Kadir,
Z.B. Mahbub, K.R. Islam, M.S. Khan, A. Iqbal, N. Al-Emadi, M.B.I.
Reaz, M. T. Islam, “Can AI help in screening Viral and COVID-19
pneumonia?” IEEE Access, Vol. 8, 2020, pp. 132665 - 132676.
[31] Rahman, T., Khandakar, A., Qiblawey, Y., Tahir, A., Kiranyaz, S., Kashem,
S.B.A., Islam, M.T., Maadeed, S.A., Zughaier, S.M., Khan, M.S. and
Chowdhury, M.E., 2020. Exploring the Effect of Image Enhancement
Techniques on COVID-19 Detection using Chest X-ray Images. arXiv
preprint arXiv:2012.02238.
[32] Saha, A., Harowicz, M.R., Grimm, L.J., Kim, C.E., Ghate, S.V., Walsh,
R. and Mazurowski, M.A., 2018. A machine learning approach to
radiogenomics of breast cancer: a study of 922 subjects and 529 DCE-
MRI features. British journal of cancer, 119(4), pp.508-516.
[33] Huang, S. C., Pareek, A., Jensen, M., Lungren, M. P., Yeung, S., &
Chaudhari, A. S. (2023). Self-supervised learning for medical image
classification: a systematic review and implementation guidelines. NPJ
Digital Medicine, 6(1), 74.
[34] Feng, Z., Wen, L., Xiao, J., Xu, Y., Wu, X., Zhou, J., ... & Wang,
Y. (2023). Diffusion-based Radiotherapy Dose Prediction Guided by
Inter-slice Aware Structure Encoding. arXiv preprint arXiv:2311.02991.
[35] Zapaishchykova, Anna, et al. "Diffusion deep learning for brain age
prediction and longitudinal tracking in children through adulthood."
Imaging Neuroscience 2 (2024): 1-14.
[36] Fu, L., Li, X., Cai, X., Wang, Y., Wang, X., Yao, Y., & Shen, Y.
(2023). SP-DiffDose: A Conditional Diffusion Model for Radiation Dose
Prediction Based on Multi-Scale Fusion of Anatomical Structures, Guided
by SwinTransformer and Projector. arXiv preprint arXiv:2312.06187.
[37] Lv, T., Liu, Y., Miao, K., Li, L., & Pan, X. (2023, October). Diffusion
Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI.
MICCAI (pp. 100-109). Cham: Springer Nature Switzerland.
[38] Hardy, R., Klepich, J., Mitchell, R., Hall, S., Villareal, J., & Ilin, C. (2023).
Improving nonalcoholic fatty liver disease classification performance
with latent diffusion models. Scientific Reports, 13(1), 21619.
[39] Ling, Y., Wang, Y., Dai, W., Yu, J., Liang, P., & Kong, D. (2023).
MTANet: Multi-Task Attention Network for Automatic Medical Image
Segmentation and Classification. IEEE Transactions on Medical Imaging.
[40] Ashurov, A., Chelloug, S. A., Tselykh, A., Muthanna, M. S. A., Muthanna,
A., & Al-Gaashani, M. S. (2023). Improved breast Cancer classification
through combining transfer learning and attention mechanism. Life, 13(9),
1945.
[41] Dushatskiy, Arkadiy, et al. "Data variation-aware medical image segmen-
tation." Medical Imaging 2022: Image Processing. Vol. 12032. SPIE,
2022.
[42] Wu, Junde, et al. "Medsegdiff: Medical image segmentation with diffusion
probabilistic model." Medical Imaging with Deep Learning. PMLR, 2024.
[43] Chen, Shoufa, et al. "Diffusiondet: Diffusion model for object detection."
Proceedings of the IEEE/CVF ICCV. 2023.
[44] Wang, H., Cao, J., Anwer, R. M., Xie, J., Khan, F. S., & Pang, Y. (2023).
Dformer: Diffusion-guided transformer for universal image segmentation.
arXiv preprint arXiv:2306.03437.
[45] Li, Alexander C., et al. "Your diffusion model is secretly a zero-shot
classifier." Proceedings of the IEEE/CVF International Conference on
Computer Vision. 2023.
[46] Luo, Grace, et al. "Diffusion hyperfeatures: Searching through time and
space for semantic correspondence." Advances in Neural Information
Processing Systems 36 (2024).
[47] Baranchuk, D., Rubachev, I., Voynov, A., Khrulkov, V.,& Babenko,
A. "Label-Efficient Semantic Segmentation with Diffusion Models." In
ICCV, 2022.
9
[48] Yu, Jiwen, et al. "Freedom: Training-free energy-guided conditional
diffusion model." Proceedings of the IEEE/CVF ICCV. 2023.
[49] Radford, Alec, et al. "Learning transferable visual models from natural
language supervision." ICML. PMLR, 2021.
[50] Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski,
P., & Joulin, A. (2021). Emerging properties in self-supervised vision
transformers. In Proceedings of the IEEE/CVF ICCV.
[51] Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009,
June). Imagenet: A large-scale hierarchical image database. In 2009 IEEE
conference on CVPR (pp. 248-255). Ieee.
[52] Park, Namuk, and Songkuk Kim. "How do vision transformers work?."
arXiv preprint arXiv:2202.06709 (2022).
[53] Ranzinger, Mike, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. "AM-
RADIO: Agglomerative Visual Foundation Model – Reduce All Domains
Into One." In CVPR, 2024.
[54] Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S.,
& Houlsby, N. (2020). Big transfer (bit): General visual representation
learning. In Computer Vision–ECCV 2020: 16th European Conference,
2020.
[55] Yamada, Yutaro, and Mayu Otani. "Does robustness on imagenet transfer
to downstream tasks?." Proceedings of CVPR. 2022.
[56] Hu, Edward J., et al. "LoRA: Low-Rank Adaptation of Large Language
Models." ICLR. 2022.
[57] Wang, Z., Wu, Z., Agarwal, D., & Sun, J. (2022). Medclip: Con-
trastive learning from unpaired medical images and text. arXiv preprint
arXiv:2210.10163.
[58] Kazerouni, A., Aghdam, E. K., Heidari, M., Azad, R., Fayyaz, M.,
Hacihaliloglu, I., & Merhof, D. (2023). Diffusion models in medical
imaging: A comprehensive survey. Medical Image Analysis, 102846.
[59] Jia, C., Yang, Y., Xia, Y., Chen, Y. T., Parekh, Z., Pham, H., ... & Duerig,
T. (2021, July). Scaling up visual and vision-language representation
learning with noisy text supervision. ICML. PMLR.
[60] OpenAI. “Sora: Creating video from text.” Accessed 2024. https://openai.
com/sora
[61] Litjens, G., Kooi, T., Bejnordi, B. E., Setio, A. A. A., Ciompi, F.,
Ghafoorian, M., ... & Sánchez, C. I. (2017). A survey on deep learning
in medical image analysis. Medical image analysis.
[62] Shin, H. C., Roth, H. R., Gao, M., Lu, L., Xu, Z., Nogues, I., ...
& Summers, R. M. (2016). Deep convolutional neural networks for
computer-aided detection: CNN architectures, dataset characteristics and
transfer learning. IEEE transactions on medical imaging, 35(5), 1285-
1298.
[63] Wongvorachan, T., He, S., & Bulut, O. (2023). A comparison of
undersampling, oversampling, and SMOTE methods for dealing with
imbalanced classification in educational data mining. Information, 14(1),
54.
[64] Qin, Y., Zheng, H., Yao, J., Zhou, M., & Zhang, Y. (2023). Class-
balancing diffusion models. CVPR.
TABLE IX
THE NETWORK CONFIGURATION FOR BENCHMARKING
Method
Learning Rate
Weight Decay
Epochs (#)
Optimizer
ResNet-101
1e-3
1e-4
50
SGD
ResNeXt-101
1e-3
1e-4
50
SGD
ShuffleNet V2
1e-3
5e-5
50
SGD
ViT-Base/16
6e-6
3e-1
200
AdamW
MiT-b0 (SegFormer)
8e-6
1e-2
100
AdamW
CVT
6.5e-5
1e-2
100
AdamW
PVT v2 - b0
1e-5
1e-2
50
AdamW
PVT - tiny
8e-6
1e-2
50
AdamW
CPT (ours)
5e-5
1e-2
50
AdamW
APPENDIX
A. Implementation details
Table IX displays the hyperparameters for benchmarking
models applied to three datasets: pancreas cancer CT, breast
cancer MRI, and covid chest X-ray. The same hyperparameters
were used across all datasets, and the number of epochs was
determined based on the convergence of accuracy.
10
