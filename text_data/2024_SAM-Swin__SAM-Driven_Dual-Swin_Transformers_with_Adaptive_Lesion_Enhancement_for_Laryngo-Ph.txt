Medical Image Analysis (2024)
Contents lists available at ScienceDirect
Medical Image Analysis
journal homepage: www.elsevier.com/locate/media
SAM-Swin: SAM-Driven Dual-Swin Transformers with Adaptive Lesion Enhancement
for Laryngo-Pharyngeal Tumor Detection
Jia Weia,1, Yun Lib,1, Xiaomao Fana,∗, Wenjun Mac, Meiyu Qiua, Hongyu Chena, Wenbin Leib,∗
aCollege of Big Data and Internet, Shenzhen Technology University, Shenzhen, China
bThe First Affiliated Hospital, Sun Yat-Sen University, Guangzhou, China
cAberdeen Institute of Data Science and Artificial Intelligence, South China Normal University, Foshan, China
A R T I C L E I N F O
Article history:
Keywords: Dual-branch network, SAM,
Swin transformer,
laryngo-pharyngeal
tumor detection
A B S T R A C T
Laryngo-pharyngeal cancer (LPC) is a highly lethal malignancy in the head and neck
region.
Recent advancements in tumor detection, particularly through dual-branch
network architectures, have significantly improved diagnostic accuracy by integrating
global and local feature extraction. However, challenges remain in accurately localiz-
ing lesions and fully capitalizing on the complementary nature of features within these
branches. To address these issues, we propose SAM-Swin, an innovative SAM-driven
Dual-Swin Transformer for laryngo-pharyngeal tumor detection. This model lever-
ages the robust segmentation capabilities of the Segment Anything Model 2 (SAM2) to
achieve precise lesion segmentation. Meanwhile, we present a multi-scale lesion-aware
enhancement module (MS-LAEM) designed to adaptively enhance the learning of nu-
anced complementary features across various scales, improving the quality of feature
extraction and representation. Furthermore, we implement a multi-scale class-aware
guidance (CAG) loss that delivers multi-scale targeted supervision, thereby enhanc-
ing the model’s capacity to extract class-specific features. To validate our approach,
we compiled three LPC datasets from the First Affiliated Hospital (FAHSYSU), the
Sixth Affiliated Hospital (SAHSYSU) of Sun Yat-sen University, and Nanfang Hos-
pital of Southern Medical University (NHSMU). The FAHSYSU dataset is utilized
for internal training, while the SAHSYSU and NHSMU datasets serve for external
evaluation. Extensive experiments demonstrate that SAM-Swin outperforms state-of-
the-art methods, showcasing its potential for advancing LPC detection and improv-
ing patient outcomes.
The source code of SAM-Swin is available at the URL of
https://github.com/VVJia/SAM-Swin.
© 2024 Elsevier B. V. All rights reserved.
1. Introduction
Laryngo-pharyngeal cancer (LPC), encompassing both la-
ryngeal and hypopharyngeal cancers, represents 65% to 70% of
all upper respiratory tract cancers. Reports from 2020 indicate
∗Xiaomao Fan and Wenbin Lei are co-corresponding authors.
1These authors contributed equally to this work.
that LPC was responsible for 130,000 deaths [16]. Laryngeal
cancer, in particular, shows limited responsiveness to radiother-
apy and chemotherapy, often necessitating total laryngectomy
in advanced stages [17, 34]. This procedure significantly di-
minishes patients’ quality of life and contributes to their high
mortality rates.
In contrast, early-stage LPC can be treated
with minimally invasive procedures, achieving a 5-year survival
rate of up to 90%, with minimal impact on the voice function
[27]. Thus, early and accurate diagnosis is vital for effective
arXiv:2410.21813v1  [cs.CV]  29 Oct 2024
2
Wei et al. / Medical Image Analysis (2024)
treatment, reducing mortality, and improving patient progno-
sis [29]. Diagnosis of LPC relies on the use of an electronic
laryngoscope. Biopsies under an electronic laryngoscope are
currently the gold standard for diagnosing LPC [26]. However,
due to the limitations of the endoscopist’s skill and experience,
there are still instances of missed diagnoses and unnecessary re-
peated biopsies. Consequently, developing accurate automatic
detection methods to assist endoscopists in detecting LPC from
laryngoscope images is of great significance.
Recent investigations into deep learning methodologies for
tumor detection have categorized these approaches into two pri-
mary frameworks: single-branch networks [31, 4, 33, 7, 23, 20,
2] and dual-branch networks [38, 35, 1, 8, 40]. Single-branch
networks, including architectures such as U-Net [31], DeepLab
[4], EfficientNet [33], UC-DenseNet [23], and MTANet [20],
primarily focus on the extraction of global features from endo-
scopic images, which are critical for various diagnostic appli-
cations. While these networks are proficient at capturing over-
arching patterns and structures within the images, they often
overlook the vital local information present in lesion regions.
This local context is essential for accurate tumor detection and
characterization, as it can provide nuanced details that are piv-
otal for distinguishing between benign and malignant tissues.
To address the limitations associated with single-branch net-
works, researchers have developed dual-branch networks that
integrate both global and local feature representations. Archi-
tectures such as ELNet [38], DLGNet [35], RadFormer [1], IP-
Net [8], and cVAN [40], exemplify this approach, aiming to en-
hance the detection capabilities by combining the strengths of
both feature types. However, despite the advancements offered
by dual-branch networks, significant challenges remain. One of
the primary issues is the precise localization of lesions within
endoscopic images, where the inherent similarity between the
foreground (lesion) and background can complicate the model’s
ability to differentiate between the two. Additionally, many ex-
isting dual-branch networks employ a straightforward concate-
nation method to merge global and local features before they
are processed by fusion modules. This direct integration may
fail to exploit the complementary nature of these features, po-
tentially limiting the effectiveness of the feature fusion process.
As a consequence, these limitations can adversely impact the
overall performance of tumor detection, particularly in com-
plex scenarios such as identifying laryngo-pharyngeal tumors,
where accurate localization and characterization are crucial for
effective clinical intervention.
To address the aforementioned issues, we introduce SAM-
Swin, an innovative Dual-Swin Transformer framework en-
hanced by the Segment Anything Model 2 (SAM2) for the de-
tection of laryngo-pharyngeal tumors. The SAM-Swin mainly
consists of four pivotal components:
SAM2-guided lesion
location(SAM2-GLLM), whole image branch (WIB), lesion
region branch (LRB), and multi-scale lesion-aware enhance-
ment module (MS-LEAM). The SAM-Swin capitalizes on the
exceptional object segmentation capabilities of SAM2, facil-
itating precise delineation of tumor regions. To further aug-
ment the model’s performance, we incorporate an MS-LAEM,
which adaptively strengthens the learning of nuanced comple-
mentary features across various scales. Additionally, we pro-
pose a multi-scale class-aware guidance (CAG) loss function,
designed to provide targeted supervision for the extraction of
class-specific features, thereby improving the model’s discrim-
inative power across different tumor categories. Extensive ex-
periment results on the First Affiliated Hospital (FAHSYSU),
the Sixth Affiliated Hospital (SAHSYSU) of Sun Yat-sen Uni-
versity, and Nanfang Hospital of Southern Medical University
(NHSMU) demonstrate that the SAM-Swin is superior to the
existing state-of-the-art baselines. To sum up, our contributions
can be summarized as follows:
• We propose a novel SAM-driven Dual-Swin transformer
network specifically designed for the detection of laryngo-
pharyngeal tumors. This innovative approach represents
the first application of a dual-branch network architecture
tailored for the LPC medical imaging challenge.
• By leveraging the advanced object segmentation capabili-
ties of the SAM2, we pioneerly integrate SAM2 into the
SAM-Swin framework, enabling SAM-Swin to achieve
highly precise segmentation of the lesion region.
• We propose MS-LAEM designed to adaptively enhance
the learning of nuanced complementary features across
various scales, improving the quality of feature extraction
and representation.
• We introduce the multi-scale CAG loss, a novel approach
that employs targeted supervision to facilitate the extrac-
tion of class-specific features within the model. This loss
function is designed to enhance the model’s capacity to
differentiate between various tumor categories by provid-
ing explicit guidance during training.
• Extensive experiments conducted across three datasets of
FAHSYSU, SAHSYSU, and NHSMU demonstrate that
SAM-Swin achieves competitive performance, consis-
tently outperforming state-of-the-art counterparts.
The remainder of this paper is structured as follows. Sec-
tion 2 provides a concise overview of related work concerning
SAM and dual-branch frameworks in medical imaging analy-
sis. Section 3 presents the details of the proposed SAM-Swin
methodology. Section 4 outlines the experimental setup, dis-
cusses the results and their visualization, and addresses the tun-
ing of hyperparameters. Finally, Section 5 offers a summary of
this paper.
2. Related works
2.1. Segment anything model in medical imaging analysis
The SAM is a foundation model known for its impressive
zero-shot segmentation performance across diverse natural im-
age datasets [18]. Building upon SAM, the SAM2 was recently
introduced, offering improved segmentation accuracy and ef-
ficiency compared to its predecessor [30].
Given SAM and
SAM2’s outstanding performance in natural images, several
Wei et al. / Medical Image Analysis (2024)
3
studies have explored their applicability in medical imaging
[28, 15].
Experimental results in these studies show that while SAM
performs well on specific medical tasks, particularly those in-
volving well-defined anatomical structures, it struggles in more
complex or ill-defined cases. Furthermore, it has been observed
that fine-tuning SAM for domain-specific medical tasks can sig-
nificantly enhance its segmentation capabilities. Consequently,
the potential of SAM for medical image analysis has sparked
interest in various fine-tuning strategies tailored for medical ap-
plications.
Fine-tuning methods can generally be divided into two cat-
egories: parameter-efficient fine-tuning (PEFT) and non-PEFT
strategies. PEFT approaches, such as LoRA [13], have been
adopted to reduce the number of trainable parameters while
maintaining strong performance [41, 5, 10, 3]. However, non-
PEFT strategies, which involve fine-tuning more extensive por-
tions of the model, have typically shown better performance.
For instance, studies such as [24], [39], [42], and [25] applied
non-PEFT strategies, fine-tuning either key modules like the
Image Encoder and Mask Decoder, or, in some cases, the en-
tire model. While this requires more computational resources,
it often leads to greater domain-specific adaptability and im-
proved segmentation accuracy.
Despite the success of fine-
tuning strategies across various medical applications, SAM2
has yet to be specifically adapted for laryngo-pharyngeal tumor
segmentation. Given the performance advantages of non-PEFT
approaches, we chose to apply a non-PEFT fine-tuning strategy
for SAM2, aiming to achieve better domain-specific adaptabil-
ity and enhanced segmentation accuracy.
2.2. Dual-branch frameworks in medical imaging analysis
Dual-branch frameworks are designed to integrate both
global and local features, significantly enhancing lesion classi-
fication accuracy across various medical domains. For instance,
Wu et al. [38] proposed a dual-stream network that combines
global and local contextual information for esophageal lesion
classification, with Fast R-CNN [9] used to accurately locate
lesion regions. Similarly, Basu et al. [1] utilized activation
heatmaps derived from global feature maps to crop salient re-
gions, and then employed a transformer-based architecture to
fuse global and local features for Gallbladder Cancer detection.
Wang et al. [35] introduced a dual-branch neural network for
intestinal lesion classification, using Mask R-CNN [11] to de-
tect regions of interest and combine local and global feature
representations. Additionally, Fu et al. [8] proposed a dual-
branch network with hierarchical intersection blocks to extract
both global and local features for whole-stage colorectal lesion
classification. However, despite the success of these methods,
several challenges remain: i) The accuracy and precision of le-
sion localization are critical. Inaccurate or imprecise localiza-
tion can severely limit the classification performance of sub-
sequent network stages by introducing irrelevant or misleading
information. ii) Moreover, simple concatenation is commonly
employed to combine global and local features. However, this
approach may not fully leverage the complementary aspects of
these features, potentially undermining the effectiveness of the
fusion process. As a result, these limitations can negatively
impact the overall performance of tumor detection, especially
in complex situations like laryngo-pharyngeal tumor identifica-
tion.
3. Methods
3.1. Overview
In this paper, we present SAM-Swin, a novel architecture
known as SAM-driven Dual-Swin Transformer with adaptive
lesion enhancement, specifically engineered for the detection of
laryngo-pharyngeal tumors, as illustrated in Fig. 1. The SAM-
Swin framework comprises four essential components: SAM2-
GLLM (discussed in Section 3.2), WIB (explained in Section
3.3), LRB (described in Section 3.4), and MS-LEAM (covered
in Section 3.5). To further enhance the performance of SAM-
Swin, we incorporate multi-scale CAG losses within both the
WIB and LRB modules. Additionally, a cross-entropy loss is
applied to the final output, as detailed in Section 3.6.
Formally, we define the laryngoscopic image dataset D =
(x(i)
w , y(i))
N
i=1, which consists of N laryngoscopic images. In this
context, x(i)
w represents the holistic image, while y(i) ∈{0, 1, 2}
indicates the classification label, corresponding to normal, be-
nign, and malignant conditions, respectively. The lesion region
image x(i)
l
is generated through the SAM2-GLLM and shares
the identical label y(i) as x(i)
l . The SAM-Swin framework em-
ploys a multi-task learning approach, integrating multi-scale
CAG losses alongside a classification loss. This results in the
final outputs denoted Lw, Ll, and Lcls for global CAG loss,
local CAG loss, and final classification loss, respectively. The
overall objective loss for the SAM-Swin framework, denoted as
FS AM−S win, is formulated as follows:
Ltotal = Lcls + Lw + Ll,
(1)
This comprehensive loss function enables the SAM-Swin to ef-
fectively balance the contributions from different aspects of the
learning process, ensuring robust detection and classification of
laryngo-pharyngeal tumors.
3.2. SAM2-GLLM
Fig. 2 illustrates the workflow of the SAM2-GLLM frame-
work. In the context of medical imaging, accurately extract-
ing lesion-specific regions from holistic laryngoscopic images
is essential for reliable tumor identification. This is typically
achieved through object detection or segmentation techniques
that localize lesions using bounding boxes or segmentation
masks. These localized regions are subsequently cropped for
further analysis.
In our approach, we leverage the robust segmentation capa-
bilities of SAM2 to produce high-quality lesion masks. To en-
hance SAM2’s performance in the medical imaging domain, we
employ a non-PEFT fine-tuning approach by holistically fine-
tuning both the image encoder and mask decoder. This tailored
adjustment aims to optimize their functionality for the LPC im-
age segmentation task. Notably, we maintain the prompt en-
coder in a frozen state to preserve its inherent capacity for ef-
fectively processing input prompts. This strategic configuration
4
Wei et al. / Medical Image Analysis (2024)
Fig. 1. The overall architecture of SAM-Swin. SAM-Swin consists of four key parts: a SAM2-guided lesion location module (SAM2-GLLM), a whole image
branch (WIB), a lesion region branch (LRB), and a multi-scale lesion-aware enhancement module (MS-LAEM).
Fig. 2. Illustration of the workflow of SAM2-Guided Lesion Location Mod-
ule (SAM2-GLLM). The whole image xw is processed by SAM2, generating
the corresponding lesion mask mw. Points P1 and P2 are selected based on
the foreground, defined as the region where mw(x, y) = 255. The lesion re-
gion image xl is then cropped from the xw using the coordinates of these
two points.
ensures that SAM2-GLLM is well-equipped for accurate and
efficient lesion identification and segmentation.
Specifically, we employ the SAM2, denoted as FS AM2, with
the automatic segmentation option (segment-everything) on the
whole image xw, yielding the lesion mask mw as follows:
mw = FS AM2(xw),
(2)
Subsequently, points P1 and P2 are generated based on the
foreground defined by the mask:
P1 = (xmin, ymin) = (min(x), min(y)),
(3)
P2 = (xmax, ymax) = (max(x), max(y)),
(4)
where (x, y) ∈{(x, y) | mw(x, y) = 255}. Finally, the lesion re-
gion image xl is cropped from the whole image xw using the
coordinates of these two points, and then upsampled to match
the size of xw:
xl = Fup(Fcrop(xw, [P1; P2])),
(5)
where Fcrop denotes the image cropping function, and Fup rep-
resents the upsampling function.
3.3. WIB
Differed from the work [37] with the ResNet [12] as the back-
bone, we adopt a much more powerful Swin Transformer V2
[21] in this study. This architecture captures long-range de-
pendencies and extracts multi-scale features, thereby enhanc-
ing the model’s ability to detect and identify lesions. Following
the original Swin Transformer V2 architecture, the whole im-
age xw ∈RH×W×3 is first reshaped into a sequence of flattened
parches xwp ∈RN×3P2, where H and W denote the height and
width of the input image, respectively, P× P is the resolution of
each patch, and N = HW/P2 is the total number of patches. In
this study, we set the P to 4, following the original Swin Trans-
former V2 configuration. These patch tokens are then projected
Wei et al. / Medical Image Analysis (2024)
5
into a C-dimensional space via a linear projection, resulting in
xs0
w ∈R( H
4 × W
4 )×C.
As illustrated in Fig. 1, the encoder in the WIB consists
of four stages, where each stage comprises Swin Transformer
Blocks (STB) followed by a Patch Merging (PM) layer, except
for the final stage, which omits the PM layer. The PM layer
reduces the resolution of the feature maps by a factor of 2, en-
abling the encoder to produce hierarchical feature representa-
tions. The input tokens xs0
w are processed through four stages to
generate multi-scale features, as follows:
xsiw =

F si
w (xsi−1
w ),
if i = 1,
F si
w (x′si−1
w ),
if i ∈{2, 3, 4},
(6)
where i ∈{1, 2, 3, 4} represents the stage index, x′si−1
w
is the out-
put from MS-LAEM (described in Section 3.5), and Fsi(·) de-
notes the transformation at each stage in the WIB. The resolu-
tion of each stage’s outputs xsiw are ( H
8 × W
8 )×2C, ( H
16 × W
16)×4C,
( H
32 × W
32) × 8C, and ( H
32 × W
32) × 8C, respectively. This hierarchi-
cal structure allows the models to capture both fine-grained de-
tails and high-level semantic information, which are crucial for
accurate lesion classification. Additionally, a fully connected
(FC) layer is employed as a projection header following the PM
layer at each stage. This layer generates the prediction proba-
bility ˆysiw, which is used for optimizing the CAG loss.
3.4. LRB
In this section, we adopt the same architecture as the WIB to
ensure consistency in lesion feature extraction. Notably, they
do not share the weight parameters, allowing each branch to
learn complementary lesion-specific information from different
perspectives. Given a lesion region image xl extracted from a
whole image xw through SAM2-GLLM (described in Section
3.2), the image is first split into patches. These patches are
then reshaped into patch tokens. These path tokens are passed
through a linear projection, resulting in xs0
l
∈R( H
4 × W
4 )×C. Fur-
ther details are provided in Section 3.3. Subsequently, similar
to Eq. 6, the tokens xs0
l are processed through four stages, pro-
ducing multi-scale features as follows:
xsi
l = F si
l (xsi−1
l
),
(7)
where i ∈{1, 2, 3, 4} denotes the stage index, and F si
l (·) de-
notes the transformation at each stage in the LRB. Given the
rich lesion-specific information present in the lesion region im-
age, the extraction of multi-scale features enables the model to
capture more discriminative features, enhancing its ability to
distinguish between different categories of lesions. To optimize
the CAG loss, an FC layer is utilized as a projection header after
the PM layer at each stage. This layer produces the prediction
probability ˆysi
l .
3.5. MS-LAEM
As shown in Fig. 3, the whole image tokens xsiw (see Eq. 6)
are passed through linear projections to generate the Ksiw and
V siw , while the lesion region tokens xsi
l (see Eq. 7) are processed
to produce Qsi
l .
We then perform the Multi-Head Attention
Fig. 3.
The illustration of lesion-aware enhancement module (LAEM).
Query tokens are generated from the lesion region tokens, while key and
value tokens are produced from the whole image tokens. These query, key,
and value tokens are then processed through Multi-Head Attention (MHA)
to derive enhanced feature tokens, which contain richer lesion-specific fea-
ture representations. Subsequently, the learnable, zero-initialized gating
factor is applied to multiply the enhanced feature tokens, adaptively ad-
justing the importance of the lesion features. Lastly, these enhanced fea-
ture tokens are combined with the original whole image tokens to produce
the final tokens.
(MHA). Specifically, the attention mechanism is applied inde-
pendently across h heads, and for each head j, the attention is
computed as:
zsi
j = S oftmax(
Qsi
l,j(Ksi
l,j)T
√dk
)V si
w,j,
(8)
where Qsi
l, j, Ksi
l,j, and V si
w,j are the projections for head j, dk is
the dimensionality of each head (so that the scaling is done per
head), and S oftmax(·) is the softmax function. The outputs are
obtained by concatenating attention output for all heads,
zsi = Concat(zsi
1 , zsi
2 , . . . , zsi
h ).
(9)
The enhanced feature tokens zsi represent the lesion-guided fea-
tures, which probably cause disturbance at the beginning of
training. To this end, we adopt a learnable gating factor, de-
noted as gsi, to adaptively control the importance of zsi. The
gsi is initialized by zero, eliminating the influence of noisy or
poorly learned features at the early stages of training. As train-
ing progresses, it allows the model to gradually learn the ap-
propriate contribution of lesion-guided features. Thus, the en-
hanced feature representation is given by:
ˆzsi = gsi · zsi.
(10)
Finally, inspired by residual connection mechanism [12], we
integrate the enhanced feature tokens ˆzsi back into the whole
image tokens xsiw. This ensures the model retains essential in-
formation from the original features while incorporating the re-
fined lesion-aware details for more accurate lesion feature ex-
traction. This integration can be formulated as:
x′si
w = xsiw + ˆzsi,
(11)
where x′si
w denotes the enhanced tokens for the next stage of
processing.
3.6. Objective loss function
3.6.1. Multi-scale CAG loss
In this section, the stage-specific prediction probabilities ˆysiw
and ˆysi
l generated by both WIB and LRB lead to the formulation
6
Wei et al. / Medical Image Analysis (2024)
of the overall multi-scale CAG losses. These losses are defined
as follows:
Lw =
4
X
i=1
(2i−1α)Lsiw(ˆysiw, y),
(12)
Ll =
4
X
i=1
(2i−1α)Lsi
l (ˆysi
l , y),
(13)
Here, Lsiw(·) and Lsi
l (·) represent the cross-entropy losses at the
i-th stage on both WIB and LRB, while α serves as a critical
trade-off hyperparameter. The exponential weighting 2i−1 in-
creases with each stage, underscoring the importance of deeper
feature representations that capture richer semantic information
vital for accurate classification. By allocating greater weight to
these deeper features, we enhance the model’s ability to focus
on significant patterns, thereby mitigating the risk of overfitting
to shallower features that may merely capture noise. A detailed
discussion on the selection of the hyperparameter α will be pre-
sented in Section 4.5.
3.6.2. Classification loss
At the final stage, we obtain the enhanced whole image to-
kens x′s4
w from MS-LAEM (see Eq. 11), and lesion region to-
kens xs4
l
from LRB. These two tokens are concatenated and
subsequently passed through the MLP layer to produce the final
prediction probabilities, denoted as ˆycls. The final classification
loss Lcls is calculated using cross-entropy as follows:
Lcls = CrossEntropyLoss(ˆycls, y),
(14)
4. Experiment
4.1. Experiment settings
4.1.1. Datasets
All laryngoscopic images in this study are provided by three
hospitals, including the First Affiliated Hospital of Sun Yat-
sen University (i.e., FAHSYSU), the Sixth Affiliated Hospital
of Sun Yat-sen University (i.e., SAHSYSU), and the Nanfang
Hospital of Southern Medical University (i.e., NHSMU).
All laryngoscopic images were collected during routine clin-
ical practice using standard laryngoscopes (ENF-VT2, ENF-
VT3, or ENF-V3; Olympus Medical Systems, Tokyo, Japan)
and imaging systems (VISERA ELITE OTV-S190, EVIS EX-
ERA III CV-190, Olympus Medical Systems) at an original res-
olution of 512 × 512 pixels. Our dataset includes laryngoscopic
images captured in narrow-band imaging (NBI) mode, com-
pensating for the limitations of white light imaging (WLI) by
enhancing the clarity and recognizability of microvasculature
[19]. The detailed data descriptions of three datasets are shown
in Table 1.
• FAHSYSU: The FAHSYSU as the internal dataset con-
tains 25,256 images, with 8,137 in NBI mode and 17,119
in WLI mode. It was used for model training, validation,
and internal testing.
• SAHSYSU: The SAHSYSU as the external dataset con-
tains 2,788 images, with 135 in NBI mode and 2,653 in
WLI mode. It was only used for external testing.
• NHSMU: The NHSMU as the external dataset contains
6,684 images, only in WLI mode. It was also used for ex-
ternal testing without any data leakage on model training.
4.1.2. Evaluation metrics
To comprehensively evaluate the performance of our pro-
posed SAM-Swin, we utilized several metrics, including Accu-
racy, Precision, Recall, and F1-score. These metrics are defined
as follows:
Accuracy =
TP + TN
TP + TN + FP + FN,
(15)
Precision =
TP
TP + FP,
(16)
Recall =
TP
TP + FN,
(17)
F1-score = 2 × Precision × Recall
Precision + Recall.
(18)
where the true positive (TP), false positive (FP), false negative
(FN), and true negative (TN) values are derived from the confu-
sion matrix. Moreover, Intersection over Union (IoU) and Dice
coefficient (Dice) were employed to measure the quality of seg-
mentation produced by SAM2.
4.1.3. Implementation details
The experiments were performed on a computing server
equipped with 4 NVIDIA RTX A6000 GPUs and an AMD
EPYC 7763 64-Core Processor running at 1.50 GHz. The im-
plementation was carried out using the PyTorch framework,
with Python version 3.10.14, and CUDA version 12.1.
The test data from the FAHSYSU dataset were randomly
selected by laryngologists, ensuring patient-level partitioning.
The rest of the data were divided into training and validation
sets in a 90%-10% ratio for hyperparameter tuning, resulting in
16,222 images for training, 1,806 for validation, and 7,229 for
testing (refer to Table 2).
At first, we fine-tuned SAM2 following the fine-tuning strat-
egy outlined in [25]. To adapt SAM2 for the LPC datasets,
we empirically applied the fine-tuning configuration shown in
Table 3. After this process, SAM2 was used to automatically
segment the lesions. In cases where no lesions were detected
(i.e. when the predicted mask had zero pixels), a 128×128 cen-
ter region was cropped from the entire image. All segmented
lesion regions were then used as inputs for the lesion region
in subsequent stages. After fine-tuning SAM2, we proceeded
to train the SAM2-Swin network, keeping the SAM2’s param-
eters frozen. Our training configuration closely followed the
methodologies described in [22, 21], with detailed parameters
outlined in Table 4. In Stage 1, we fine-tuned the SAM2-Swin
network using SwinV2-B as backbones, utilizing pre-trained
weights from ImageNet-1K [6]. Additionally, we intentionally
omitted the CAG loss to prevent potential disruptions, particu-
larly in the shallow layers. In Stage 2, we further refined the
last checkpoint for 10 epochs, incorporating CAG loss as an
Wei et al. / Medical Image Analysis (2024)
7
Table 1. The statistic description of the FAHSYSU, SAHSYSU, and NHSMU datasets.
FAHSYSU (Total=25,256)
SAHSYSU (Total=2,788)
NHSMU (Total=6,684)
NBI
WLI
NBI
WLI
NBI
WLI
Normal
695
7,310
9
2,202
0
5,772
Benign
1,488
3,332
27
218
0
536
Malignant
5,954
6,477
99
233
0
376
Total
8,137
17,119
135
2,653
0
6,684
Table 2. Data distribution for training, validation, and test sets of the FAH-
SYSU dataset.
Training
Validation
Test
Normal
5,134
571
2,301
Benign
3,276
366
1,178
Malignant
7,812
869
3,750
Total
16,222
1,806
7,229
Table 3. The configurations of SAM2 fine-tuning.
Parameter
Details
Version
sam2 hiera tiny
Number of Epochs
40
Batch Size
96
Warm-up Steps
100
Learning Rate Decay Type
Cosine
Weight Decay
0.1
Initial Learning Rate
2e-5
Automatic Mixed Precision
Enabled
Optimizer
AdamW
Image Size
1024 × 1024
Loss Functions
Dice Loss + Cross-entropy Loss
Data Augmentations
Normalized only
Trainable Components
Image Encoder, Mask Decoder
Frozen Components
Prompt Encoder
Prompt Input
No prompts used as input
Mask Prediction
Single mask predicted for simplicity
additional class-specific supervision signal, while also decreas-
ing the values for the learning rate, weight decay, and warm-up
epochs. Furthermore, to ensure a fair comparison with other
baseline models, all input images were resized to 256×256, and
the hyperparameter settings were based on their default config-
urations [21, 1, 35, 37].
4.2. Experiment results
4.2.1. Baselines
To comprehensively demonstrate the superiority of our pro-
posed SAM-Swin, we evaluate its performance compared with
nine state-of-the-art classification methods across three cate-
gories on the internal dataset (FAHSYSU) and external datasets
(SAHSYSU and NHSMU). The comparison includes CNN-
based methods: VGGNet [32], ResNet [12], DenseNet [14],
and EfficientNet [33]; Transformer-based methods: Vision
Transformer (ViT) [7], and Swin Transformer V2 (SwinV2)
[21]; Dual-branch methods: RadFormer [1], DLGNet [35],
and SAM-FNet [37].
Table 4. Configurations of SAM-Swin in training Stage 1 and Stage 2.
Parameter
Stage 1
Stage 2
Version
SwinV2-B
SwinV2-B
Number of Epochs
50
10
Batch Size
256
256
Warm-up Epochs
5
2
Learning Rate Decay Type
Cosine
Cosine
Weight Decay
0.05
1e-8
Initial Learning Rate
3e-4
3e-5
Automatic Mixed Precision
Enabled
Enabled
Optimizer
AdamW
AdamW
Image Size
256 × 256
256 × 256
Loss Functions
Cross-entropy Loss
Cross-entropy Loss
Data Augmentations
RandAugment
RandAugment
• VGGNet: A CNN model known for its use of small con-
volutional filters to capture spatial hierarchies.
• ResNet: A CNN that introduces residual learning to solve
the vanishing gradient problem, enabling the training of
much deeper networks.
• DenseNet: A CNN where each layer connects to every
other layer, prompting feature reuse and efficient gradient
flow.
• EfficientNet:
A family of CNNs that scales network
width, depth, and resolution using a compound scaling
method to improve efficiency.
• ViT: A pure transformer model that applied self-attention
mechanisms to patches of an image, capturing long-range
dependencies.
• SwinV2: A hierarchical transformer model that processes
images with shifted windows for efficient computation and
better performance across scales.
• RadFormer: A dual-branch network that leverages trans-
former architecture to fuse global and local features, de-
signed for precise gallbladder cancer detection in ultra-
sound images.
• DLGNet: A dual-branch network that combines global
and local features to incorporate contextual lesion infor-
mation, specifically for colon lesion classification.
• SAM-FNet: A dual-branch network that fuses global fea-
tures and lesion-specific features, with lesion regions lo-
calized by SAM, for detecting laryngo-pharyngeal tumors.
8
Wei et al. / Medical Image Analysis (2024)
4.2.2. Classification performance on internal dataset
Table 5 compares the performance of various state-of-the-art
methods, including VGGNet, ResNet, DenseNet, EfficientNet,
ViT, SwintV2, RadFormer, DLGNet, and SAM-FNet, with the
proposed SAM-Swin on the internal dataset FAHSYSU. Re-
garding the overall results, the SAM-Swin achieves the highest
performance across all the evaluated metrics. It attains an over-
all accuracy of 94.52%, precision of 93.75%, recall of 91.21%,
and F1-score of 92.30%. This represents a significant improve-
ment over the other techniques, with the next best performer
being SAM-FNet, which has an accuracy of 92.14%, preci-
sion of 89.57%, recall of 88.68%, and F1-score of 89.08%.
When examining the recall for different classes, the SAM-Swin
again demonstrates superior performance. It achieves a recall of
96.54% for the normal class, 78.73% for the benign class, and
98.35% for the malignant class. This suggests that the SAM-
Swin is highly effective in accurately detecting and classify-
ing the various types of cases in the datasets. Compared to the
other state-of-the-art methods, the SAM-Swin consistently out-
performs them across all the evaluation metrics. For instance,
it surpasses the VGGNet, ResNet, DenseNet, and EfficientNet
methods by a margin of several percentage points in terms of
accuracy, precision, recall, and F1-score, respectively. Simi-
larly, it exhibits superior performance compared to the more
recent methods like ViT, SwintV2, RadFormer, DLGNet, and
SAM-FNet. These results demonstrate the effectiveness and
robustness of the SAM-Swin in the context of the FAHSYSU
dataset. The substantial improvements over the state-of-the-art
techniques highlight the potential of the proposed SAM-Swin
for practical applications in laryngo-pharyngeal tumor detec-
tion.
Moreover, Fig. 4 presents the confusion matrices obtained by
applying the proposed SAM-Swin method and other compara-
tive methods on the FAHSYSU dataset. The confusion matri-
ces provide a detailed breakdown of the model’s performance
in classifying the endoscopic images into three categories: nor-
mal, benign, and malignant. Starting with the VGGNet model,
the confusion matrix (a) shows a significant misclassification
between the benign and malignant classes, with a high number
of malignant cases being classified as benign. This suggests
that the VGGNet model struggles to accurately distinguish be-
tween these two clinically relevant classes. The ResNet model
(b) exhibits a more balanced performance, with a better separa-
tion between the normal, benign, and malignant classes. How-
ever, there are still some instances of misclassification, par-
ticularly between benign and malignant cases. The DenseNet
model (c) demonstrates an improved classification, with clearer
boundaries between the three classes. The number of misclas-
sifications between the benign and malignant cases is reduced
compared to the previous models. As we move to the more
recent models, the confusion matrices show further enhance-
ments in performance. The EfficientNet (d) and ViT (e) models
exhibit a higher degree of accuracy in separating the normal,
benign, and malignant cases, with a more distinct and compact
representation of the class boundaries. The SwintV2 (f), Rad-
Former (g), and DLGNet (h) models continue to demonstrate
improvements, with the confusion matrices displaying sharper
Fig. 4. Confusion matrices obtained by our proposed SAM-Swin and other
comparative methods on the FAHSYSU dataset. (a) VGGNet, (b) ResNet,
(c) DenseNet, (d) EfficientNet, (e) ViT, (f) SwinV2, (g) RadFormer, (h) DL-
GNet, (i) SAM-FNet, (j) SAM-Swin.
and more defined class separations. These models appear to be
better equipped to distinguish clinically relevant cases, which
is crucial for accurate diagnosis and treatment planning. Fi-
nally, the confusion matrices for the SAM-FNet (i) and the
proposed SAM-Swin (j) methods show the most distinct and
well-defined class boundaries among all the evaluated models.
The SAM-Swin method, in particular, exhibits an exception-
ally clear separation between the normal, benign, and malignant
cases, indicating its superior ability to accurately classify the
different types of cases in the FAHSYSU dataset. These results
align with the quantitative performance metrics reported in the
previous Table 5, where the SAM-Swin method outperformed
the other state-of-the-art techniques across various evaluation
measures.
The detailed confusion matrices provide a visual
representation of the model’s classification capabilities, further
corroborating the effectiveness of the proposed SAM-Swin ap-
proach in the context of the FAHSYSU dataset.
4.2.3. Classification performance on external datasets
As shown in Table 5, the SAM-Swin shows similar domi-
nance results on external datasets of SAHSYSU and NHSMU
as the internal dataset of FAHSYSU. As for the NHSMU
dataset, the SAM-Swin achieves an overall accuracy of 93.06%,
which is the highest among the compared baselines. The pre-
cision of 80.25% and recall of 88.20% contribute to an im-
pressive F1-score of 83.83%. Regarding the recall for differ-
ent classes on the NHSMU dataset, the SAM-Swin again ex-
hibits its strengths. It attains a recall of 94.94% for the nor-
mal class, 71.79% for the benign class, and 97.87% for the
malignant class. This showcases the model’s ability to effec-
tively identify and classify the various types of cases in the
external NHSMU dataset. Compared to the other methods on
the NHSMU dataset, the SAM-Swin outperforms its counter-
parts across the board. It surpasses the performance of VG-
GNet, ResNet, DenseNet, EfficientNet, ViT, SwintV2, Rad-
Former, DLGNet, and SAM-FNet in terms of overall accuracy,
precision, recall, and F1-score. The consistent and exceptional
performance of the SAM-Swin method on the SAHSYSU and
NHSMU datasets, in addition to the previously discussed FAH-
SYSU dataset, demonstrates the robustness and generalizabil-
ity of the proposed SAM-Swin. This highlights its potential
for practical applications in laryngo-pharyngeal tumor detec-
Wei et al. / Medical Image Analysis (2024)
9
Table 5. Experiment results of the SAM-Swin and other baselines on the FAHSYSU, SAHSYSU, and NHSMU datasets (Unit: %).
Dataset
Method
Overall results
Recall for different classes
Accuracy
Precision
Recall
F1-score
Normal
Benign
Malignant
FAHSYSU
VGGNet [32]
84.37
78.87
79.51
79.14
87.11
61.38
90.05
ResNet [12]
89.45
86.14
85.13
85.52
93.82
67.89
93.68
DenseNet [14]
85.88
80.93
81.27
81.03
88.51
64.05
91.25
EfficientNet [33]
89.31
85.76
85.71
85.69
93.16
71.23
92.75
ViT [7]
87.74
82.93
84.57
83.65
90.96
71.89
90.85
SwinV2 [21]
91.54
90.82
86.30
88.04
94.83
66.56
97.52
RadFormer [1]
87.01
82.87
82.42
82.63
89.08
65.55
92.61
DLGNet [35]
88.95
85.04
84.60
84.80
91.18
68.47
94.13
SAM-FNet [37]
92.14
89.57
88.68
89.08
93.95
75.81
96.27
SAM-Swin (Ours)
94.52
93.75
91.21
92.30
96.54
78.73
98.35
SAHSYSU
VGGNet [32]
82.42
64.42
70.71
66.98
87.15
42.15
82.83
ResNet [12]
91.07
80.18
82.37
81.22
94.81
67.05
85.24
DenseNet [14]
84.58
68.62
72.69
70.47
89.43
45.21
83.43
EfficientNet [33]
87.88
74.69
81.58
77.50
90.52
68.97
85.24
ViT [7]
89.67
78.14
79.95
78.68
94.03
67.82
78.01
SwinV2 [21]
91.75
81.05
86.02
83.35
94.08
72.41
91.57
RadFormer [1]
86.80
71.57
78.64
74.61
90.30
63.98
81.63
DLGNet [35]
86.76
72.89
80.80
75.96
89.20
67.05
86.14
SAM-FNet [37]
92.29
82.71
84.52
83.59
95.58
69.73
88.25
SAM-Swin (Ours)
91.28
80.14
88.39
83.63
92.39
79.69
93.07
NHSMU
VGGNet [32]
85.37
61.20
72.90
64.52
90.08
33.95
94.68
ResNet [12]
92.35
77.88
85.69
81.21
94.72
68.75
93.62
DenseNet [14]
86.83
63.98
74.20
66.67
91.57
35.81
95.21
EfficientNet [33]
91.37
76.93
82.21
79.34
94.52
61.15
90.96
ViT [7]
92.47
81.38
76.20
78.40
97.73
47.64
83.24
SwinV2 [21]
92.41
77.99
87.33
82.06
94.42
69.43
98.14
RadFormer [1]
87.94
68.37
83.50
73.25
89.77
66.05
94.68
DLGNet [35]
88.63
68.83
82.65
73.95
90.87
64.53
92.55
SAM-FNet [37]
92.34
77.62
85.99
81.17
94.63
68.92
94.41
SAM-Swin (Ours)
93.06
80.25
88.20
83.83
94.94
71.79
97.87
1The best performance is in bold.
tion, where accurate and reliable classification is crucial.
4.2.4. Visualization analysis of Grad-CAM
The Grad-CAM visualizations shown in Fig. 5 provide an
in-depth analysis for the FAHSYSU, SAHSYSU, and NHSMU
datasets. Starting with the FAHSYSU dataset, the Grad-CAM
maps reveal some interesting observations.
The VGGNet,
ResNet, and DenseNet models tend to focus on broader, more
diffuse regions across the input images, often failing to pre-
cisely capture the key features when the benign tumor is small.
Similarly, CNN-based methods like EfficientNet, DLGNet, and
SAM-FNet tend to concentrate on limited parts of the tumors,
potentially leading to misclassification between tumor types
due to the lack of contextual information. Transformer-based
architectures such as ViT and SwinV2 benefit from a broader
receptive field, allowing for greater attention coverage. How-
ever, they often struggle to emphasize lesion regions effectively,
lacking the capacity to capture detailed, lesion-specific features.
In contrast, the SAM-Swin model hones in on the most discrim-
inative regions within the input images, even for small and su-
perficial tumors. It also captures the complete tumor region, in-
cluding boundary details and surrounding tissue, enabling it to
harness contextual information essential for fine-grained tumor
differentiation [36]. A similar pattern emerges for the SAH-
SYSU and NHSMU datasets. While the other state-of-the-art
models show varying degrees of attention to different parts of
the images, the SAM-Swin consistently demonstrates a more
targeted and comprehensive focus on the most informative vi-
sual cues. This precise and focused attention exhibited by the
SAM-Swin aligns with its superior performance reported in the
previous table. By effectively capturing and utilizing the most
relevant features in the input images, the SAM-Swin is able to
achieve higher accuracy, precision, recall, and F1-score com-
pared to the other deep learning techniques. The Grad-CAM
visualizations offer a visual interpretation of the inner workings
of these models, providing insights into their decision-making
processes. The clear and focused attention of the SAM-Swin
suggests its ability to learn and leverage the most discriminative
features in a more efficient and effective manner, contributing to
its overall superior performance on the FAHSYSU, SAHSYSU,
and NHSMU datasets.
4.2.5. Ablation experiments
Table 6 presents the results of the ablation experiments con-
ducted on the FAHSYSU dataset for the SAM-Swin and its
variants. These experiments aim to evaluate the impact of dif-
10
Wei et al. / Medical Image Analysis (2024)
Fig. 5. Illustrations of the Grad-CAM visualization on the FAHSYSU, SAHSYSU, and NHSMU datasets.
ferent components and architectural choices on the overall per-
formance of the model. The variants investigated include M1,
M2, M3, M4, and the final proposed method M5 (SAM-Swin).
Specifically:
• M1 (WIB): This configuration utilizes solely the WIB
module. It serves as a baseline for assessing the efficacy of
the full image processing capabilities in isolation.
• M2 (LRB): In this model, only the LRB model is em-
ployed.
This setup allows for an in-depth analysis of
the region-based processing mechanisms without the in-
fluence of the whole image context.
• M3 (WIB + LRB): This variant integrates both the WIB
and the LRB modules. By combining these two modules,
we aim to evaluate the synergistic effects of processing
both whole images and localized regions.
• M4 (WIB + LRB + MS-LAEM): Building on the M3
configuration, this model incorporates the MS-LEAM
module. This integration is hypothesized to further en-
hance the model’s ability to adaptively learn from multi-
scale features.
• M5 (SAM-Swin): The final configuration encompasses all
previous components: WIB, LRB, MS-LEAM), and in-
corporates CAG loss. This comprehensive model aims to
leverage the strengths of all preceding variants for optimal
performance in complex tasks.
Table 6 summarizes the performance metrics of the various
model variants, including accuracy, precision, recall, and F1-
score, as well as the recall values for the three classes: normal,
benign, and malignant. The M1 variant achieves a commend-
able overall accuracy of 91.54%, with a precision of 90.82%,
recall of 86.30%, and an F1-score of 88.04%. The M2 vari-
ant achieves an overall accuracy of 89.78%, with a precision of
87.70%, a recall of 83.45%, and an F1-score of 84.87%. It can
be observed that the M2 variant experiences a decrease across
all metrics, suggesting that the absence of comprehensive con-
Wei et al. / Medical Image Analysis (2024)
11
Table 6. Ablation experiments performance on the FAHSYSU dataset (Unit: %).
Variant
WIB
LRB
MS-LAEM
CAG loss
Overall results
Recall for different classes
Accuracy
Precision
Recall
F1-score
Normal
Benign
Malignant
M1
✓
91.54
90.82
86.30
88.04
94.83
66.56
97.52
M2
✓
89.78
87.70
83.45
84.87
94.08
59.38
96.88
M3
✓
✓
93.53
92.73
89.57
90.87
96.45
74.40
97.87
M4
✓
✓
✓
94.05
93.11
90.52
91.63
96.19
77.23
98.13
M5 (SAM-Swin)
✓
✓
✓
✓
94.52
93.75
91.21
92.30
96.54
78.73
98.35
1The best performance is in bold.
Fig. 6.
The t-SNE visualization of the ablation experiment on the FAH-
SYSU dataset. Among them, (a) M1 (WIB only), (b) M2 (LRB only), (c)
M3 (WIB+LRB), (d) M4 (WIB+LRB+MS-LAEM), (e) M5 (SAM-Swin, all
components).
textual information may hinder its ability to distinguish lesions
effectively, leading to a decline in overall performance. As we
progress through the ablation study with subsequent variants
(M3 and M4), we observe incremental enhancements in perfor-
mance metrics. Notably, the M4 variant demonstrates signifi-
cant improvements, achieving an overall accuracy of 94.05%,
precision of 93.11%, recall of 90.52%, and an F1-score of
91.63%.
The final model, SAM-Swin (M5), showcases the
highest performance metrics, with an accuracy of 94.52%, pre-
cision of 93.75%, recall of 91.21%, and an F1-score of 92.30%.
The recall for the normal, benign, and malignant classes is par-
ticularly noteworthy, at 96.54%, 78.73%, and 98.35%, respec-
tively. The consistent improvements observed across the differ-
ent variants, especially in the recall for benign and malignant
classes, underscore the effectiveness of the architectural choices
and components integrated within the SAM-Swin model.
Furthermore, Fig. 6 illustrates the t-SNE visualizations
derived from the ablation experiment results on the FAH-
SYSU dataset. The t-SNE technique effectively projects high-
dimensional feature representations onto a two-dimensional
space, facilitating a visual exploration of cluster structures and
class separability across different model variants: M1, M2, M3,
M4, and the final SAM-Swin approach (M5). In the t-SNE plot
for the M1 variant (a) and the M2 variant (b), a degree of class
separation is evident; however, notable overlap persist, particu-
Table 7. Average IoU and Dice scores of different segmentation methods on
the FAHSYSU (Unit: %).
Model
IoU (Average)
Dice (Average)
UNet
47.27
59.22
Mask-RCNN
60.34
69.97
SAM
60.96
72.07
SAM2
63.63
74.55
larly with the benign class. This suggests that further refinement
is necessary for improved discrimination among case types. As
the ablation progresses, the M3 variant (c) exhibits a more pro-
nounced separation between classes, with clearer distinctions
for normal and malignant samples, as well as improved differ-
entiation of benign samples. The M4 variant (d), which incor-
porates additional architectural components, further enhances
class separability, resulting in more distinct and tighter clus-
ters for each class. The t-SNE visualization of the SAM-Swin
method (M5) (e) achieves the most remarkable class separation
among all variants, with minimal overlap between normal, be-
nign, and malignant samples. This clear delineation of class
boundaries indicates that the SAM-Swin approach has effec-
tively learned highly discriminative features.
4.3. Analysis of SAM2-GLLM
As shown in Table 7, SAM2 achieves the highest segmen-
tation performance among the segmentation methods, with an
average IoU of 63.63% and an average Dice score of 74.55%.
Compared to traditional models like UNet (IoU: 47.27%, Dice:
59.22%) and Mask-RCNN (IoU: 60.34%, Dice:
69.97%),
SAM2 demonstrates significant improvements. Furthermore, it
surpasses the SAM by 2.67% in IoU and 2.48% in Dice, high-
lighting its enhanced capability for accurate lesion localization.
These results validate the effectiveness of the SAM2-GLLM in
improving segmentation precision in the laryngo-pharyngeal tu-
mor context.
As illustrated in Fig. 7, SAM2 demonstrates superior seg-
mentation performance across different tumor categories. No-
tably, SAM2 achieves a substantial improvement in segment-
ing benign lesions, achieving a Dice score of 64.01% compared
to 35.88% for UNet, 57.71% for Mask R-CNN, and 59.18%
for SAM. Additionally, SAM2 attains an average Dice score
of 80.76%, surpassing other methods such as UNet (72.96%),
Mask R-CNN (77.18%), and SAM (79.66%). This significant
advantage in lesion segmentation highlights SAM2’s enhanced
12
Wei et al. / Medical Image Analysis (2024)
Fig. 7. Average Dice scores of different segmentation methods across be-
nign, malignant, and combined (All) categories on the FAHSYSU dataset.
capability to accurately locate less-defined lesion boundaries,
underscoring its potential as a reliable lesion localization mod-
ule in challenging cases.
4.4. Analysis of MS-LAEM
Influence of the number of LAEMs. As mentioned in Sec-
tion 3.5, the MS-LAEM contains several LAEMs applied to
different stages. To evaluate their impact on classification per-
formance, we varied the number of inserted LAEMs from 0 to
4. A setting of ”0” represents no enhancement, while ”1” to
”4” indicates the incremental inclusion of LAEMs from later
to earlier stages, with ”4” utilizing all stages, and the results
are shown in Fig. 8. It is observed that from 0 to 1 LAEM,
there is a noticeable improvement in both recall and F1-score,
demonstrating the effectiveness of introducing the LAEM for
feature enhancement. Additionally, from 1 to 3 LAEMs, accu-
racy steadily increases, suggesting that incorporating additional
LAEMs at multiple stages continues to refine the model’s per-
formance. At 4 LAEMs, the best results are achieved across
all metrics, indicating that fully leveraging multi-scale features
provides the most comprehensive enhancement.
Visualization of lesion-aware attention maps. To verify
the effectiveness of LAEM, we plotted the multi-head attention
heatmaps from stage four, as illustrated in Fig. 9. The multi-
head attention mechanism allows each head to focus on differ-
ent areas within the lesion region, enabling the model to capture
diverse contextual information surrounding the lesion, which
enhances its ability to discriminate between lesion types.
4.5. Hyperparameters tuning
To analyze the impact of different weights for the multi-scale
CAG loss as discussed in Section 3.6.1 on classification perfor-
mance, we varied the α over a range of {10−1, 10−2, 10−3, 10−4},
as depicted in Fig. 10. It is clear that the best performance is
achieved with a weight of 10−3. This suggests that the weights
Fig. 8.
Results of the proposed SAM-Swin with different numbers of
lesion-aware enhancement modules on the FAHSYSU dataset.
Fig. 9.
Visualizations of the multi-head lesion-aware attention maps at
Stage 4, illustrating how different heads focus on distinct parts of the lesion
region within the whole image.
like 10−1 and 10−2 are too large, causing the model to over-
focus on class-specific features, while 10−4 is too small to pro-
vide sufficient guidance. The weight 10−3 strikes an optimal
balance, allowing the model to leverage class-specific supervi-
sion effectively, leading to the highest overall performance.
5. Conclusion
In this study, we propose a novel SAM-driven Dual-Swin
Transformer framework integrated with adaptive lesion en-
hancement, tailored for the detection of laryngo-pharyngeal
tumors. It mainly consists of four key components: SAM2-
GLLM, WIB, LRB, and MS-LAEM. Furthermore, we imple-
ment a multi-scale CAG loss function, which provides class-
specific supervision to improve the model’s ability to differ-
entiate between various tumor categories. Comprehensive ex-
periments conducted on three distinct datasets, including both
internal and external sources, demonstrate the effectiveness of
our SAM-Swin model, achieving overall accuracies of 94.52%,
91.28%, and 93.06% in terms of FAHSYSU, SAHSYSU, and
NHSMU, respectively. These results significantly surpass exist-
Wei et al. / Medical Image Analysis (2024)
13
Fig. 10. Results of the SAM-Swin with different values of α for the class-
aware guidance loss on the FAHSYSU dataset.
ing state-of-the-art approaches in laryngo-pharyngeal tumor de-
tection. In addition, Grad-CAM visualizations, ablation studies,
and detailed module effectiveness analyses further validate the
reliability and superiority of our proposed method. Our com-
prehensive analysis underscores the potential of the SAM-Swin
framework as a powerful tool for LPC detection, paving the way
for more effective diagnostic applications in clinical practice.
Acknowledgments
We note that a regular conference version of this paper ap-
peared in the 2024 IEEE International Conference on Bioinfor-
matics and Biomedicine (IEEE BIBM 2024) [37]. This work is
partially supported by the National Natural Science Foundation
of China (62473267), the Basic and Applied Basic Research
Project of Guangdong Province (2022B1515130009), the Spe-
cial subject on Agriculture and Social Development, Key Re-
search and Development Plan in Guangzhou (2023B03J0172),
and the Natural Science Foundation of Top Talent of SZTU
(GDRC202318).
References
[1] Basu, S., Gupta, M., Rana, P., Gupta, P., Arora, C., 2023. Radformer:
Transformers with global–local attention for interpretable and accurate
gallbladder cancer detection. Medical Image Analysis 83, 102676.
[2] Cai, L., Li, Y., Lu, Y., Zhang, Y., Jiang, J., Dai, G., Zhang, B., Cao, J.,
Zhang, Z., Fan, X., 2024. Towards cross-domain single blood cell image
classification via large-scale lora-based segment anything model, in: 2024
IEEE International Symposium on Biomedical Imaging (ISBI), IEEE. pp.
1–5. Doi: 10.1109/ISBI56570.2024.10635629.
[3] Chen, C., Miao, J., Wu, D., Zhong, A., Yan, Z., Kim, S., Hu, J., Liu, Z.,
Sun, L., Li, X., et al., 2024. Ma-sam: Modality-agnostic sam adaptation
for 3d medical image segmentation. Medical Image Analysis 98, 103310.
[4] Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2017.
Deeplab: Semantic image segmentation with deep convolutional nets,
atrous convolution, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence 40, 834–848.
[5] Chen, T., Zhu, L., Deng, C., Cao, R., Wang, Y., Zhang, S., Li, Z., Sun,
L., Zang, Y., Mao, P., 2023. Sam-adapter: Adapting segment anything in
underperformed scenes, in: Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 3367–3375.
[6] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. Ima-
genet: A large-scale hierarchical image database, in: 2009 IEEE confer-
ence on computer vision and pattern recognition, Ieee. pp. 248–255.
[7] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
Uszkoreit, J., Houlsby, N., 2021. An image is worth 16x16 words: Trans-
formers for image recognition at scale, in: International Conference on
Learning Representations. URL: https://openreview.net/forum?
id=YicbFdNTTy.
[8] Fu, J., Chen, K., Dou, Q., Gao, Y., He, Y., Zhou, P., Lin, S., Wang, Y.,
Guo, Y., 2024. Ipnet: An interpretable network with progressive loss for
whole-stage colorectal disease diagnosis. IEEE Transactions on Medical
Imaging Doi: 10.1109/TMI.2024.3459910.
[9] Girshick, R., 2015. Fast r-cnn, in: 2015 IEEE International Conference on
Computer Vision (ICCV), pp. 1440–1448. Doi: 10.1109/ICCV.2015.169.
[10] Gong, S., Zhong, Y., Ma, W., Li, J., Wang, Z., Zhang, J., Heng, P.A., Dou,
Q., 2024. 3dsam-adapter: Holistic adaptation of sam from 2d to 3d for
promptable tumor segmentation. Medical Image Analysis 98, 103324.
[11] He, K., Gkioxari, G., Doll´ar, P., Girshick, R., 2017.
Mask r-cnn, in:
Proceedings of the IEEE international conference on computer vision, pp.
2961–2969.
[12] He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for
image recognition, in: Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 770–778.
[13] Hu, E.J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
L., Chen, W., 2022. LoRA: Low-rank adaptation of large language mod-
els, in: International Conference on Learning Representations.
URL:
https://openreview.net/forum?id=nZeVKeeFYf9.
[14] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely
connected convolutional networks, in: Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, pp. 4700–4708.
[15] Huang, Y., Yang, X., Liu, L., Zhou, H., Chang, A., Zhou, X., Chen, R.,
Yu, J., Chen, J., Chen, C., et al., 2024.
Segment anything model for
medical images? Medical Image Analysis 92, 103061.
[16] Hutcheson, K.A., Lewin, J.S., 2012. Functional outcomes after chemora-
diotherapy of laryngeal and pharyngeal cancers. Current oncology reports
14, 158–165.
[17] Jones, T., De, M., Foran, B., Harrington, K., Mortimore, S., 2016. Laryn-
geal cancer: United kingdom national multidisciplinary guidelines. The
Journal of Laryngology & Otology 130, S75–S82.
[18] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L.,
Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al., 2023. Segment
anything, in: Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 4015–4026.
[19] Li, L.J., Yu, Z., Zhu, J.Q., Wang, M.L., Li, Z.X., Yang, C., Ni, X.G.,
2021. Laryngoscopic characteristics related to the risk of cancerization of
vocal cord leukoplakia. Acta Oto-Laryngologica 141, 802–807.
[20] Ling, Y., Wang, Y., Dai, W., Yu, J., Liang, P., Kong, D., 2024. Mtanet:
Multi-task attention network for automatic medical image segmentation
and classification. IEEE Transactions on Medical Imaging 43, 674–685.
Doi: 10.1109/TMI.2023.3317088.
[21] Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang,
Z., Dong, L., et al., 2022. Swin transformer v2: Scaling up capacity and
resolution, in: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp. 12009–12019.
[22] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.,
2021. Swin transformer: Hierarchical vision transformer using shifted
windows, in: Proceedings of the IEEE/CVF international conference on
computer vision, pp. 10012–10022.
[23] Luo, X., Zhang, J., Li, Z., Yang, R., 2022. Diagnosis of ulcerative col-
itis from endoscopic images based on deep learning. Biomedical Signal
Processing and Control 73, 103443. Doi: 10.1016/j.bspc.2021.103443.
[24] Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B., 2024a. Segment any-
thing in medical images. Nature Communications 15, 654.
[25] Ma, J., Kim, S., Li, F., Baharoon, M., Asakereh, R., Lyu, H., Wang, B.,
2024b. Segment anything in medical images and videos: Benchmark and
deployment. arXiv preprint arXiv:2408.03322 .
[26] Mannelli, G., Cecconi, L., Gallo, O., 2016. Laryngeal preneoplastic le-
sions and cancer: challenging diagnosis. qualitative literature review and
meta-analysis. Critical reviews in oncology/hematology 106, 64–90.
[27] Marioni, G., Marchese-Ragona, R., Cartei, G., Marchese, F., Staffieri, A.,
2006. Current opinion in diagnosis and treatment of laryngeal carcinoma.
14
Wei et al. / Medical Image Analysis (2024)
Cancer treatment reviews 32, 504–515.
[28] Mazurowski, M.A., Dong, H., Gu, H., Yang, J., Konz, N., Zhang, Y.,
2023. Segment anything model for medical image analysis: an experi-
mental study. Medical Image Analysis 89, 102918.
[29] Nocini, R., Molteni, G., Mattiuzzi, C., Lippi, G., 2020. Updates on larynx
cancer epidemiology. Chinese Journal of Cancer Research 32, 18.
[30] Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., R¨adle,
R., Rolland, C., Gustafson, L., et al., 2024. Sam 2: Segment anything in
images and videos. arXiv preprint arXiv:2408.00714 .
[31] Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional net-
works for biomedical image segmentation, in: Medical image comput-
ing and computer-assisted intervention–MICCAI 2015: 18th international
conference, Munich, Germany, October 5-9, 2015, proceedings, part III
18, Springer. pp. 234–241.
[32] Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 .
[33] Tan, M., Le, Q., 2019. Efficientnet: Rethinking model scaling for convo-
lutional neural networks, in: International conference on machine learn-
ing, PMLR. pp. 6105–6114.
[34] Tang, Z.X., Gong, J.L., Wang, Y.H., Li, Z.H., He, Y., Liu, Y.X., Zhou,
X.H., 2018. Efficacy comparison between primary total laryngectomy
and nonsurgical organ-preservation strategies in treatment of advanced
stage laryngeal cancer: A meta-analysis. Medicine 97, e10625.
[35] Wang, K.N., Zhuang, S., Ran, Q.Y., Zhou, P., Hua, J., Zhou, G.Q., He, X.,
2023. Dlgnet: A dual-branch lesion-aware network with the supervised
gaussian mixture model for colon lesions classification in colonoscopy
images. Medical Image Analysis 87, 102832.
[36] Wang, R., Chen, S., Ji, C., Fan, J., Li, Y., 2022. Boundary-aware context
neural network for medical image segmentation. Medical Image Analysis
78, 102395.
[37] Wei, J., Li, Y., Qiu, M., Chen, H., Fan, X., Lei, W., 2024. Sam-fnet: Sam-
guided fusion network for laryngo-pharyngeal tumor detection.
arXiv
preprint arXiv:2408.05426 .
[38] Wu, Z., Ge, R., Wen, M., Liu, G., Chen, Y., Zhang, P., He, X., Hua, J.,
Luo, L., Li, S., 2021. Elnet: Automatic classification and segmentation
for esophageal lesions using convolutional neural network. Medical Im-
age Analysis 67, 101838.
[39] Yan, Z., Sun, W., Zhou, R., Yuan, Z., Zhang, K., Li, Y., Liu, T., Li, Q.,
Li, X., He, L., et al., 2024. Biomedical sam 2: Segment anything in
biomedical images and videos. arXiv preprint arXiv:2408.03286 .
[40] Yang, Z., Qiu, M., Fan, X., Dai, G., Ma, W., Peng, X., Fu, X., Li,
Y., 2024.
cvan: A novel sleep staging method via cross-view align-
ment network. IEEE Journal of Biomedical and Health Informatics Doi:
10.1109/JBHI.2024.3413081.
[41] Zhang, K., Liu, D., 2023. Customized segment anything model for medi-
cal image segmentation. arXiv preprint arXiv:2304.13785 .
[42] Zhu, J., Qi, Y., Wu, J., 2024. Medical sam 2: Segment medical images as
video via segment anything model 2. arXiv preprint arXiv:2408.00874 .
