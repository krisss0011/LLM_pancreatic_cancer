Renal Cell Carcinoma subtyping: learning from
multi-resolution localization
Mohamad Mohamada, Francesco Ponziob, Santa Di Cataldob, Damien
Ambrosettic, Xavier Descombesb
aUniversit´e Cˆote d’Azur, INRIA, CNRS, Sophia Antipolis, France
bPolitecnico di Torino, Torino, Italy
cDepartment of Pathology, CHU Nice, Universit´e Cˆote d’Azur, Nice, France
Abstract
Renal Cell Carcinoma is typically asymptomatic at the early stages for many
patients. This leads to a late diagnosis of the tumor, where the curability
likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma
high, with respect to its incidence rate. To increase the survival chance, a
fast and correct categorization of the tumor subtype is paramount. Nowa-
days, computerized methods, based on artificial intelligence, represent an
interesting opportunity to improve the productivity and the objectivity of
the microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much
of their exploitation is hampered by the paucity of annotated dataset, es-
sential for a proficient training of supervised machine learning technologies.
This study sets out to investigate a novel self supervised training strategy
for machine learning diagnostic tools, based on the multi-resolution nature of
the histological samples. We aim at reducing the need of annotated dataset,
without significantly reducing the accuracy of the tool. We demonstrate the
classification capability of our tool on a whole slide imaging dataset for Renal
Cancer subtyping, and we compare our solution with several state-of-the-art
classification counterparts.
Keywords:
self supervised learning, digital pathology, renal cell carcinoma
subtyping
1. Introduction
Renal Cell Carcinoma (RCC) is a highly malignant tumor and the most
widespread type of kidney cancer, accounting for 90% of the overall entities.
Preprint submitted to Computer Methods and Programs in BiomedicineNovember 15, 2024
arXiv:2411.09471v1  [cs.CV]  14 Nov 2024
This tumor is also the 7th most common histological type in the west, and
it is continuously increasing [1]. Its mortality rate is considered high, with
respect to its incidence rate, as this tumor is typically asymptomatic at the
early stages for many patients [1, 2]. This leads to a late diagnosis of the
tumor, where the curability likelihood is lower.
RCC can be categorized into multiple histological subtypes, mainly: Clear
Cell Renal Cell Carcinoma (ccRCC) forming 75% of RCCs, Papillary Renal
Cell Carcinoma (pRCC) accounting for 10%, and Chromophobe Renal Cell
Carcinoma (chRCC) accounting for 5%. Some of the other sutypes include
Collecting Duct Renal Cell Carcinoma (cdRCC), Tubulocystic Renal Cell
Carcinoma (tRCC), and unclassified [1]. Approximately 10% of renal tumors
belong to the benign entities neoplasms, being Oncocytoma (ONCO) the
most frequent subtype with an incidence of 3–7% among all RCCs [3, 2].
These subtypes show different cytological signature as well as histological
features [2], which ends up in significantly different prognosis.
The correct categorization of the tumor subtype is indeed of major im-
portance, as prognosis and treatment approaches depend on it and on the
disease stage. For instance, the overall 5-year survival rate significantly differs
among the different histological subtypes, being 55–60% for ccRCC, 80–90%
for pRCC and 90% for chRCC. This points out the need for the most accurate
subclassification [4, 5].
Existing literature emphasizes also the critical role of the differential di-
agnosis between chromophobe and oncocytoma, known to be arduous and
prone to errors due to overlapping morphological characteristics in a relevant
number of cases [2, 6, 3].
Currently, the gold standard to classify RCC subtypes consists in the
microscopic visual assessment of Hematoxylin and Eosin (H&E) samples,
performed by a pathologist though the microscope. These specimen consist
most often of physical slides and, in some centers provided with scanner, of
virtual slides: the so-called Whole histological Slide Images (WSIs).
The visual diagnosis of the large WSIs is known to be both effort-requiring
and time-consuming, resulting in poor alignment between pathologists in
some cases.
Ultimately, these aspects have a great impact on diagnosis,
prognosis and treatment of RCC neoplasms [7].
Computerized methods represents an interesting opportunity to improve
the productivity, as well as the objectivity, of the microscopy-based RCC
diagnosis [7]. In this regards, recent evidences suggest that Convolutional
Neural Networkss (CNNs), a famous class of supervised deep learning algo-
2
rithms, may be proficiently applied to the classification of RCC subtypes [7].
This is mainly due to the CNNs’ capability to discover unseen data struc-
tures and extract robust features representation [7, 8]. Nonetheless, much of
their strength depend on the exploitation of large labeled datasets: CNNs
are data-hungry supervised algorithms, which demand a large amount of an-
notated training sample to learn an effective data representation [9, 10]. This
aspect is a strong limitation, especially in the histological field [11], where
the samples annotation requires a skilled pathologist to visually scrutinise
each WSIs and to divide it into sub-regions, homogeneous in terms of tissue
architecture, lastly assigned to a corresponding label. This approach, known
in the literature as “Region Of Interest (ROI)-cropping procedure” [11], is
known to be tedious and time consuming for the pathologist, and also prone
to error as well, with the concrete risk of compromising the models training
phase due to an inaccurate labelling.
As a consequence, Self-Supervised Learning (SSL) has been recently at-
tracting considerable interest, being able to describe data structures via
robust featurization, without requiring any supervision in term of samples
annotations [9, 12]. Unfortunately, most self-supervised techniques exploit
natural-scene image properties, which are not suitable for histopathology
specimen, as evidenced by some recent works [9, 12].
This study sets out to assess RCC subtype classification, based on WSIs,
leveraging a novel self-supervised paradigm. Our solution, inspired by the
decision-making procedure of the pathologist, incorporates features learnt at
different magnification level. With our experiments, we show that through
histologic-specific SSL paradigm, it is possible to classify the four most com-
mon RCC subtypes with performance comparable to the fully supervised
solutions, but without requiring massive data annotation.
2. Background
2.1. RCC subtyping
Much of the studies on RCC subtyping come from the exploitation of the
TCGA database [4, 5, 13], without furnishing results at the WSI level [4, 5]
or at the patient-level [4, 5, 13]. Furthemore, much of the current literature
considers only two [14, 15] (or three [4, 5, 13, 16]) RCC main malignant
tumor subtypes: ccRCC and pRCC (and chRCC). This is mainly due to the
fact that, being dedicated to malignant tumors, TCGA data portal excludes
renal oncocytoma cases.
3
A considerable part of these studies seek to evaluate the performance
of canonical machine learning pipelines, based on morphological or textu-
ral hand-crafted features, to discriminate between two [14, 15] or three [16]
classes of RCC subtypes. A more recent work by Fenstermak and colleagues,
investigated a CNN-based strategy to classify a selection of 3486 patches re-
trieved from WSIs, and belonging to three classes of interest: ccRCC, pRCC
and chRCC. The authors achieved a patch-level accuracy up to 99%, but
they do not provide the WSI-level statistics [4].
Although these works are significant in view of the feasibility of an au-
tomatic RCC subtyping framework, they suffer from two main limitations:
i) they do not take into consideration the difficult differential diagnosis be-
tween chRCC and ONCO; ii) they rely on data annotation, requiring the
ROI-cropping procedure previously defined.
Answering the first limitation, in 2021, Zhu et al.[17] investigated the
RCC subtype classification in more than three classes, including the on-
cocytoma one.
In their study, the authors designed a classification pro-
cess trained on a proprietary dataset and then tested on TCGA and biopsy
database. Nonetheless, the proposed strategy is fully supervised, requiring a
massive WSIs annotation to proficiently train the employed ResNet18 classi-
fier. A more recent study by Ponzio et al., proposed a 4 classes RCC subtype
categorization through a tree-based classification pipeline [7]. The authors
proposed a supervised deep learning solution which leverages pathologist’s
expertise and decision-making algorithm to substantially improve the classi-
fication performance of state-of-the-art CNN models. Although interesting,
the proposed methodology was still based on massive labelled dataset, being
built on top of several backbone CNNs classifier.
Up to our knowledge, no published study investigates the RCC subtyping
task with SSL paradigm, although previous research has established that his-
tological classification can be proficiently faced with self-supervised method-
ologies, as detailed in the next subsection.
2.2. Self-supervised learning in histology
Self-supervision was introduced as a solution for annotation-hungry mod-
els in their application domains. SSL exploits labels obtained from the data
itself, through a semi-automatic process, to delineate a supervised pretext
task capable of learning an effective representation of the original data. Al-
though the pretext task conducts the learning by means of a supervised loss
function, the performance of the model on the pretext is peripheral, being
4
only a means of getting a general and powerful data representation, capable
of solving a number of practical downstream tasks. Most of the popular SSL
techniques, among which rotation prediction [18], jigsaw puzzles [19] or im-
age colorization [20], were created and tested primarily for natural images.
Therefore, using these raw techniques without any modification, may not pro-
duce the desired results in other domains [9]. For instance, in histopathology
at the cellular level, objects like cells and nuclei have symmetrical shapes,
which are invariant with respect to rotation. Additionally, the tissue region is
homogeneous, making it difficult to allocate patches or identify their relative
positions using only them as the model’s input. It is simply a problem with
no definitive solution.
For this reason, a number of recent studies have proposed SSL strategies
specifically suited for histopathology calssification. Koohbanani et al. [21]
introduced three novel strategies for WSI data, comparing them with stan-
dard SSL approaches on different cancer classification datasets. The authors
developed a magnification prediction task, where a CNN is fed with patches
taken at different magnification levels and learnt to predict their correspond-
ing level. The second task, called jigmag, is inspired by the jigsaw puzzle
[19], where the network should identify the magnification ordering scheme
of a sequence of four images. Images are taken out of four different levels,
stacked randomly in a square, and fed to the model which assigns them to
one of twenty-four possible outputs. Lastly, they modified image colorization
to make it more suitable for histopathology, by identifying nuclei through
segmentation of hematoxylin channel pixels. Then, the authors compared
their three pretext tasks to several canonical SSL strategis, such as rotation
and flipping prediction, autoencoder, and generative models, on three can-
cer classification datasets. They show that the novel histology-specific tasks
perform better than the standard ones by a significant margin, especially in
low annotation regimes.
Boyd et al. [22] deployed an improved version of a well-know unsupervised
learning task, known as visual field expansion, which aims at creating a wider
view of an existing image, and employed it as a SSL pretext task. By taking
a crop of an initial image, an autoencoder-inspired network can be used to
expand the crop and form the entire image. The authors’ model leverages the
adversarial autoencoder framework [23], and their objective contains three
losses: a reconstruction and two discriminator losses. The first discriminator
loss is applied at the latent space level to ensure the contextual representation
conforms to a predefined distribution. The second loss is at the generated
5
image level, where the discriminator tries to distinguish between real and
expanded images.
More recently, Srinidhi et al. [24] proposed a three-phase training scheme,
termed Resolution Sequence Prediction (RSP), that integrates SSL, semi-
supervised learning, and consistency learning. Their self-supervised task, is
a variation of the magnification puzzle, presented in the work by Koohbanani
et al. [21]. The main differences lie in two aspects: first, the inputs are
processed as a sequence of three independent images, transforming them into
latent vectors, which are then fused to produce a prediction, while jigmag
stacks all four images in a single input. Second, for RSP, they ensure every
patch is taken from the center of the lower resolution one. After pretraining
the network with RSP, the authors fine-tune it on the main task with the
available annotations. In the last phase, they combine consistency learning
[25] and teacher-student semi-supervised paradigm to calculate a consistency
loss mixed with a supervised cross-entropy loss to train the model.
One significant theme emerges from the studies discussed so far: the
importance of low and high magnifications and the various features available
at each level. Nonetheless, the role played by the interconnection between
the patches taken from the different levels, has not been closely examined.
A recent study by Ding et al. [26] tackled this point by designing a pretext
task where the model learns to understand if a magnified patch lies inside or
outside of a zoomed-out one. They found that this simple task outperforms
magnification predictions and other techniques, including transfer learning,
in lung adenocarcinoma subtyping.
Taking inspiration from this latter study, we propose a novel approach
based on the concept of inter-level connectivity, which we prove to be more
accurate with respect of the above-mentioned SSL solutions in the RCC
subtyping task.
3. Materials and methods
3.1. Patients cohorts and dataset
In our database, we collected tissue samples from 91 consecutive patients,
who encountered nephrectomy in the Nice Hospital Urology Department, di-
agnosed with ccRCC (n=56), papRCC (n=22), chrRCC (n=6) or ONCO
(n=7). As defined by the 2022 WHO criteria, the diagnosis was based on
pathology and cytogenetic analysis. H&E stained WSI (scanned using a Le-
ica AT2 Digital Slide Scanner, Leica Microsystems CMS GmbH, Wetzlar,
6
Subtypes
Train
Test
Total
ccRCC
33
23
56
pRCC
15
7
22
Onco
3
4
7
Chromo
3
3
6
Total
54
37
91
Table 1: Patients distribution among dataset folds and RCC subtypes
Germany) used for diagnosis were gathered to define a dataset consisting in
a total of 201 WSIs. The mean number of slides per subject is 2.2(±1.9).
From the whole WSIs dataset, with the supervision of two skilled patholo-
gists, ROIs depicting the five classes of interest (namely, healthy renal tis-
sue, ccRCC, papRCC, chrRCC, ONCO) where extracted. Each category is
equally represented in the ROIs dataset.
3.2. Formulation
Suppose a WSI has N magnification levels, where 0 is the base level,
featuring the lowest resolution, and N is the maximum, providing the highest-
resolution images of smaller areas of the tissue (see Figure 1).
We can denote as pt a generic patch, taken from the WSI at level t, with
size (Wt, Ht). At a higher magnification level t + 1, pt is represented by a
higher magnification patch having size (Wt+1 = 2 × Wt
,
Ht+1 = 2 × Ht),
being the zoom factor between two consecutive levels ×2. Thus, as shown in
Figure 1, pt can be represented as 4 non-overlapping patches in level t + 1,
each one having the same dimensions as pt. The formulation of our SSL task
specifically relies on such relation between patches in different WSI levels, as
detailed in the following.
Consider two resolution levels x and y ∈[0 . . . N], such that y < x, and
let py be a patch extracted from level y (see Figure 2(a)). We can term Cx|y
the set in x containing all the non-overlapping patches that jointly describe
py.
Lastly, let px ∈Cx|y be a patch belonging to level x, randomly extracted
from Cx|y. The objective in our pre-text task is the prediction of the location
of px inside py.
We formalize our task as a classification problem by randomly sampling
a patch from set Cx|y, which consists of 4n patches, being n = y −x the
7
N
0
𝑝𝑡
𝐻𝑡
𝑊𝑡
𝑝𝑡+1
𝐻𝑡+1 = 2𝐻𝑡
𝑊𝑡+1 = 2𝑊𝑡
Figure 1: The typical pyramidal structure of a WSI: a collection of images that represents
the same content at different magnification levels.
difference between the two magnification levels taken into account.
The
ground-truth label is generated using the index of px in Cx|y. We use the
cross-entropy loss function to measure the discrepancy between the predicted
class probabilities and the actual class labels. In the classification formula-
tion, the representation of labels can pose some flexibility. Specifically, the
range of the ground-truth is dependent on n, which is equal to y −x. To
ensure that such range is consistent during the training, as well as during the
inference phase, we need to freeze the difference between x and y. In other
words, we are free to extract patches from different levels, but the zooming
difference between the two levels must remain fixed.
To train our model on the pre-text task, we employed the pipeline shown in
Figure 2(b). We applied distinct augmentations to patches px and py. The
former is augmented both geometrically and on the color space, while the
latter undergoes only color transformations (further details will follow in the
next section). We found that the data augmentation phase helps to lower the
over-fitting and to add robustness, avoiding simple solutions, based the tissue
edge or on the white space location. Our backbone convolutional encoder,
pretrained on ImageNet as later detailed, processes the augmented images
using the same weights for both px and py. Lastly, their latent vector repre-
8
py
px
WSI
py
(a)
(b)
(c)
Cx|y
Cx|y
RCC classifier
WSIs
Input
Augment
Concatenate
& fuse
Classify
Encoding
px
?
py
px
Figure 2: comprehensive picture of the proposed SSL framework including three main
panels. (a), the data preparation for our pre-text task. Consider two magnification levels
x and y in a given WSI, such that y < x. Let py be a patch extracted from level y, Cx|y
the set containing all non-overlapping patches in x and px ∈Cx|y. The objective in our
pre-text task is to predict the location of px inside py. (b), the overview of the training
pipeline, with the augmentation layers, the shared encoding models for both px and py,
the fusion layer, and the final output classifier, made up of 16 possible classes, namely
possibile location of px in py. (c), the knowledge-transfer from the obtained SSL encoding
to the main downstream RCC subtyping task.
sentations are fused using a fully connected layer, following the approach of
Ding et al. [26]. Once proficiently trained on the pretext task, the backbone’s
weights are transferred on the classification level, where are fine-tuned on the
RCC subtyping downstream task, as represented in Figure 2(c).
3.3. Implementation details
In this section, we give the implementation details of the proposed archi-
tecture.
3.4. The pretext
We split the 91 patients between training and test set, with a split ratio
not fixed among the different classes to ensure proper representation of all
9
the RCC subtypes in the test set (refer to table 1). We start preparing our
pre-text task dataset by fixing the zooming difference n between levels x and
y equal to two. The patches px and py, with a size of 256×256, are extracted
randomly such that px belongs to one of the three highest magnification levels
with probabilities (0.4, 0.4, 0.2). The abundant patches available at higher
magnification is the main reason for our choice. To obtain one sample and its
label, py and its magnified version in level x, namely Cx|y, are individuated.
px is randomly sampled from Cx|y and its location is one-hot encoded to serve
as ground truth for the cross-entropy loss.
Some filtering steps are also implemented to remove images with large
white areas. The final dataset, consisting of 784,495 tiles with size 1000 ×
1000, is further split into training and validation folds (92/8% respectively).
As before mentioned, py is only augmented with random contrast, while px
is augmented with random flipping, rotation, contrast, and cropping. The
original cropped tiles are re-scaled to 112×112 before augmentation, to min-
imize computational time. Following the findings in [7], we deploy a VGG16
network as the encoder backbone, changing only the final max pooling layer
to an average pooling one. As previously mentioned, both the encoder of px
and py are trained jointly. The two latent vectors are then concatenated in a
1024 vector, fed to a fully connected layer of 256 neurons, and finally to the
last layer, made up of 16 outputs (namely, 16 classes or possible locations of
py in px). Training on our pretext task required a very low learning rate of
2 × 10−5, a batch of 32, and a weight decay of 10−5. We opted for an Adam
optimizer. In our experiment, training reached a saddle point early on (this
can be due to the small learning rate we had to start with), and defining a
scheduler was non-trivial. Specifically, we changed the learning rate twice
during training. After the first stall, we increased the learning rate to 10−4.
It must be mentioned that implementing cosine warm-up could enhance this
approach, but further study is needed to determine the optimal rate of the
learning rate increase. At the second stall, we increased the batch size to
64, obtaining a similar effect as decreasing the learning rate, but without
suffering its long-run issues.
3.5. RCC subtype classification
We start by dividing the ROIs into 1024x1024 patches at the maximal
resolution, filtering out the white background and resizing the rest to 112x112
pixels. We balance and split the resulting data into a training (85%) and a
validation subset (15%). For the test set, we follow the same pipeline, except
10
that we extract patches from the entirety of the WSIs, thus without ROIs.
To infer the final patient-wise label, we feed all the patches belonging to a
single patient to the network and aggregate the results by a majority voting
scheme. The training of the networks, pre-trained using the self-supervised
tasks, undergoes two stages. The first is an initialization for the classifier
weights where we freeze the backbones and train the network for 4 epochs,
the first 2 with a learning rate of 10−3, and the last 2 with a learning rate
of 10−4. In the second stage, we unfreeze the backbones and deploy a cosine
warmup, which allows the learning rate to reach the maximum value of 10−4
within 5% of the total training iterations. We train the models for 120 epochs
using a batch size of 2.
3.6. Counterpart models
In our study, we compare the proposed architecture with three counter-
part solutions, representing a picture of the different state-of-the-art training
paradigms for machine learning models adopted in the classification of his-
tological WSI:
i) the SSL methodology, proposed by Ding et al [26];
ii) a canonical CNN pre-trained on the ImageNet dataset and fine-tuned
on the RCC classification task
iii) a fully supervised solution, proved to be the top performing one in this
classification scenario in a recent study [7].
3.6.1. SSL methodology
As previously mentioned, the solution by Ding et al. [26] consists of a
model which learns a powerful data representation by understanding if a
magnified patch lies inside or outside of a zoomed-out one. To put into effect
such method, we started from our SSL task dataset and we generated the
data by taking a tuple of samples and switching their high-resolution patch
with a probability of 0.5. If the patches were switched, both samples became
negative samples, and if they were not switched, the samples were considered
positive. We used the same fusion scheme and hyper-parameters they used
in their experiment and trained until the performance plateau at around 50
epochs.
3.6.2. Imagenet-based
To define a solid transfer learning based counterpart, we refer to our previ-
ous work, where we individuated the VGG16 CNN pre-trained on ImageNet
11
as optimal transfer learning framework in this classification scenario [7]. The
model is trained with a batch size of 32 and a learning rate of 10−4 for a total
of 120 epochs.
3.6.3. Fully supervised solution: the ExpertDT
As fully supervised counterpart, we select ExpertDT, a classification model
which leverages a combination of supervised deep learning models (specifi-
cally CNNs) and pathologist’s expertise to substantially improve the accuracy
in the RCC subtyping task [7].
All the counterparts frameworks are trained using Adam optimizer and
a weight decay of 10−5. The best-performing models in terms of validation
accuracy is always selected and all of them use a linear classifier for the
downstream task.
4. Results
A competent SSL framework should ideally have two main characteristics.
First, it should be accurate in a way that is not too far from fully supervised
solutions. On the other hand, it should be robust and capable of scaling well
with the reduction of available dataset, used in the downstream supervised
learning task. Thus, we designed two peculiar experiments to test them. Fig-
ure 3 shows the outcome of the first set of experiments, reporting the confu-
sion matrices obtained on the RCC subtyping task of our solution (top-left),
Ding et al.’s counterpart [26] (top-right), ImageNet-based (bottom-left), and
the fully supervised solution ExpertDT [7] (bottom-right). Such confusion
matrices report the absolute number of patients correctly categorized (see
the main diagonal) and incorrectly assigned (other positions of the grid) for
each of the four classes of interest. We were the second-best solution in terms
of mean accuracy between classes, following only the supervised one (76.5%
ours versus 87% ExpertDT). Furthermore, we outperformed the counterpart
SSL methodology by around 12% (76.5% ours versus 64.3% Ding et al.), as
well as the ImageNet-based one by more than 6% of mean accuracy (76.5%
ours versus 69.8% ImageNet).
In absolute terms, our solution misclassified 11 patients, the SSL counter-
part 13, the Imagenet-based 13 as well, while ExpertDT only 10 (all rounded
up). By running this set of experiments five times, we were able to also as-
sess the robustness of the solutions tested among different execution of the
learning. This aspect is embodied in the standard deviation of the number
12
ccRCC
pRCC
chRCC
ONCO
ccRCC
pRCC
chRCC
ONCO
20.2
±0.8
2.8
±0.8
0.0
±0.0
0.0
±0.0
0.0
±0.0
7.0
±0.0
0.0
±0.0
0.0
±0.0
0.6
±0.5
0.2
±0.4
1.6
±0.5
0.6
±0.5
0.2
±0.4
0.8
±0.4
0.4
±0.5
2.6
±0.5
Ours
ccRCC
pRCC
chRCC
ONCO
ccRCC
pRCC
chRCC
ONCO
17.0
±1.0
6.0
±1.0
0.0
±0.0
0.0
±0.0
0.0
±0.0
7.0
±0.0
0.0
±0.0
0.0
±0.0
1.0
±0.0
0.0
±0.0
1.0
±0.0
1.0
±0.0
0.2
±0.4
0.8
±0.4
1.0
±0.0
2.0
±0.0
Ding et al. [26]
ccRCC
pRCC
chRCC
ONCO
ccRCC
pRCC
chRCC
ONCO
17.0
±1.7
5.8
±1.3
0.0
±0.0
0.2
±0.4
0.0
±0.0
7.0
±0.0
0.0
±0.0
0.0
±0.0
0.6
±0.5
0.0
±0.0
1.2
±0.8
1.2
±0.4
0.0
±0.0
0.8
±0.4
0.6
±0.5
2.6
±0.5
ImageNet-based
ccRCC
pRCC
chRCC
ONCO
ccRCC
pRCC
chRCC
ONCO
18.6
±1.1
1.4
±1.1
1.8
±0.4
1.2
±0.4
0.2
±0.4
6.8
±0.4
0.0
±0.0
0.0
±0.0
0.0
±0.0
0.2
±0.4
2.4
±0.9
0.4
±0.9
0.2
±0.4
0.0
±0.0
0.2
±0.4
3.6
±0.5
ExpertDT [7]
Figure 3:
Confusion matrices obtained on the RCC subtyping task of our solution (top-
left), Ding et al.’s counterpart [26] (top-right), ImageNet-based (bottom-left) and the fully
supervised solution ExpertDT [7] (bottom-right).
of patients assigned to each class, as reported in Figure 3. We can notice
that the proposed solution shows a reasonable low standard deviation cou-
pled with correctly classified patients (see the main diagonal of the top-left
confusion matrix of Figure 3).
With the second set of experiments, we assessed the resilience of the differ-
ent evaluated approaches to the reduction of the size of the learning dataset,
employed for the downstream task. Figure 4 shows the mean accuracy of
the RCC classification task of the tested approaches, with a progressively
reduced size of the learning dataset. We can observe that our solution (see
the blue line) is the most performing with only 33% of the available anno-
tated dataset employed for the training, and progressively improves its mean
13
33
66
100
Dataset used for ﬁne-tuning, %
0.70
0.75
0.80
0.85
0.90
Mean accuracy
Ours
Ding et al. [26]
ImageNet-based
ExpertDT [7]
Figure 4:
Mean accuracy with standard deviation among five runs (see the error bars)
of the RCC classification task of the tested approaches, with a progressively reduced
size of the learning dataset.
We can observe that our solution (see the blue line) is
the most performing with only 33% of the available dataset employed for the training,
and progressively improves its mean accuracy with the growing of the training set size.
Interestingly, we can not observe a similar behaviour for the other two SSL counterparts
(see the green and the orange lines), which both show a reduced accuracy going from 33
to 66 % of the training set size. Lastly, as expected, the fully supervised ExperDT [7]
(see the red line), despite being the most accurate with the whole dataset employed for
training, it substantially suffers the reduction of the number of the training samples,
rapidly decreasing with the reduction of the annotated cohort.
accuracy with the growing of the training set size. Interestingly, we can not
observe similar behaviours for the other two SSL counterparts (see the green
and the orange lines), which both show a reduced accuracy going from 33 to
66 % of the training set size. Furthermore, the proposed solution presents
the lowest standard deviation among the five different runs, as represented
by the error bars in Figure 4, which suggests a superior learning robustness
with respect to the other considered counterparts. Lastly, as expected, the
fully supervised ExperDT [7] (see the red line), despite being the most ac-
curate with the whole dataset employed for training, it substantially suffers
the reduction of the number of the training samples, rapidly decreasing in
mean accuracy with the reduction of the annotated cohort.
14
5. Discussion
In this study, we proposed a frontier SSL approach based on the concept
of inter-level connectivity of WSI, which mimics the reasoning and the pro-
cedure of a pathologist, diagnosing RCC lesions. The comparison between
our solution and different baselines reveled a number of interesting aspects.
First, despite being substantially less accurate with respect to a fully su-
pervised solution, our method outperforms the recent SSL procedure pro-
posed by Ding et al [26], as well as a common baseline made up of an
ImageNet-pretrained CNN, fine tuned on the final classification task.
Our solution seems also to be quite robust among different runs of the
learning procedure, showing relatively low variations of the classes assigned to
the different collected subjects. This can be evinced from both the standard
deviations reported in Figure 3, and the error bars of Figure 4.
Additionally, the proposed methodology shows a good robustness in pres-
ence of a progressively reduced size of the annotated training set. This aspect
is paramount, as it is the main reason of researching learning solutions able
to minimize the needing of annotations, which are costly, especially in the
pathology domain. Our method still performs with a mean accuracy above
80% with only 33% of employed training set. This suggests superior robust-
ness with respect to the two SSL counterparts, although, as expected, both of
them mitigate the performance degradation, caused by the reduction of the
employed training set, especially when compared to the supervised method-
ology. Indeed, with only 33% of the training set employed for the learning,
the supervised solution classifies with a mean accuracy below 75%, which is
more than 8% lower than our SSL methodology. This largely assess the good-
ness of the proposed pre-text task, which significantly reduces the amount of
required annotated samples, with respect to a canonical supervised learning.
As future directions, we intend to expand our dataset with other RCC
cancers, including rare subtypes and classes. Moreover, we intend to investi-
gate the performance of the proposed solutions in other histological domain,
to assess the portability of the system in other cancer subtyping tasks.
References
[1] J.
Hsieh,
M.
Purdue,
S.
Signoretti,
C.
Swanton,
L.
Albiges,
M. Schmidinger, D. Heng, J. Larkin, V. Ficarra, Renal cell carcinoma,
Nature Reviews Disease Primers 3 (2017) 17009. doi:10.1038/nrdp.
2017.9.
15
[2] L. D. Truong, S. S. Shen, Immunohistochemical diagnosis of renal neo-
plasms, Archives of pathology & laboratory medicine 135 (1) (2011)
92–109.
[3] S. K. Tickoo, M. B. Amin, Discriminant nuclear features of renal oncocy-
toma and chromophobe renal cell carcinoma: analysis of their potential
utility in the differential diagnosis, American journal of clinical pathol-
ogy 110 (6) (1998) 782–787.
[4] M. Fenstermaker, S. A. Tomlins, K. Singh, J. Wiens, T. M. Morgan,
Development and validation of a deep-learning model to assist with renal
cell carcinoma histopathologic interpretation, Urology 144 (2020) 152–
157.
[5] S. Tabibu, P. Vinod, C. Jawahar, Pan-renal cell carcinoma classification
and survival prediction from histopathology images using deep learning,
Scientific reports 9 (1) (2019) 1–9.
[6] A. B. Rosenkrantz, N. Hindman, E. F. Fitzgerald, B. E. Niver,
J. Melamed, J. S. Babb, Mri features of renal oncocytoma and chromo-
phobe renal cell carcinoma, American Journal of Roentgenology 195 (6)
(2010) W421–W427.
[7] F. Ponzio, X. Descombes, D. Ambrosetti, Improving cnns classification
with pathologist-based expertise: the renal cell carcinoma case study,
Scientific Reports (2023).
[8] F. Ponzio, G. Urgese, E. Ficarra, S. Di Cataldo, Dealing with lack of
training data for convolutional neural networks: The case of digital
pathology, Electronics 8 (3) (2019) 256.
[9] A. Mascolini, D. Cardamone, F. Ponzio, S. Di Cataldo, E. Ficarra, Ex-
ploiting generative self-supervised learning for the assessment of biolog-
ical images with lack of annotations, BMC bioinformatics 23 (1) (2022)
1–17.
[10] S.-C. Huang, A. Pareek, M. Jensen, M. P. Lungren, S. Yeung, A. S.
Chaudhari, Self-supervised learning for medical image classification: a
systematic review and implementation guidelines, NPJ Digital Medicine
6 (1) (2023) 74.
16
[11] F. Ponzio, E. Macii, E. Ficarra, S. Di Cataldo, W2wnet: a two-module
probabilistic convolutional neural network with embedded data cleans-
ing functionality, Expert Systems with Applications 214 (2023) 119121.
[12] O. Ciga, T. Xu, A. L. Martel, Self supervised contrastive learning for
digital histopathology, Machine Learning with Applications 7 (2022)
100198.
[13] Z. Gao, P. Puttapirat, J. Shi, C. Li, Renal cell carcinoma detection
and subtyping with minimal point-based annotation in whole-slide im-
ages, in: International Conference on Medical Image Computing and
Computer-Assisted Intervention, Springer, 2020, pp. 439–448.
[14] J. Cheng, Z. Han, R. Mehra, W. Shao, M. Cheng, Q. Feng, D. Ni,
K. Huang, L. Cheng, J. Zhang, Computational analysis of pathological
images enables a better diagnosis of tfe3 xp11. 2 translocation renal cell
carcinoma, Nature communications 11 (1) (2020) 1–9.
[15] R. Xiao, E. Debreuve, D. Ambrosetti, X. Descombes, Renal cell carci-
noma classification from vascular morphology, in: M. de Bruijne, P. C.
Cattin, S. Cotin, N. Padoy, S. Speidel, Y. Zheng, C. Essert (Eds.), Med-
ical Image Computing and Computer Assisted Intervention – MICCAI
2021, Springer International Publishing, Cham, 2021, pp. 611–621.
[16] S. Chen, N. Zhang, L. Jiang, F. Gao, J. Shao, T. Wang, E. Zhang, H. Yu,
X. Wang, J. Zheng, Clinical use of a machine learning histopathological
image signature in diagnosis and survival prediction of clear cell renal
cell carcinoma, International Journal of Cancer 148 (3) (2021) 780–790.
[17] M. Zhu, B. Ren, R. Richards, M. Suriawinata, N. Tomita, S. Hassan-
pour, Development and evaluation of a deep neural network for histologic
classification of renal cell carcinoma on biopsy and surgical resection
slides, Scientific reports 11 (1) (2021) 1–9.
[18] S. Gidaris, P. Singh, N. Komodakis, Unsupervised representation learn-
ing by predicting image rotations (2018). doi:10.48550/ARXIV.1803.
07728.
URL https://arxiv.org/abs/1803.07728
17
[19] M. Noroozi, P. Favaro, Unsupervised learning of visual representations
by solving jigsaw puzzles (2016). doi:10.48550/ARXIV.1603.09246.
URL https://arxiv.org/abs/1603.09246
[20] R. Zhang, P. Isola, A. A. Efros, Colorful image colorization (2016). doi:
10.48550/ARXIV.1603.08511.
URL https://arxiv.org/abs/1603.08511
[21] N. A. Koohbanani, B. Unnikrishnan, S. A. Khurram, P. Krishnaswamy,
N. Rajpoot, Self-path: Self-supervision for classification of pathology
images with limited annotations (2020). doi:10.48550/ARXIV.2008.
05571.
URL https://arxiv.org/abs/2008.05571
[22] J. Boyd, M. Liashuha, E. Deutsch, N. Paragios, S. Christodoulidis,
M. Vakalopoulou, Self-supervised representation learning using visual
field expansion on digital pathology (2021).
doi:10.48550/ARXIV.
2109.03299.
URL https://arxiv.org/abs/2109.03299
[23] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, B. Frey, Adversarial
autoencoders (2015). doi:10.48550/ARXIV.1511.05644.
URL https://arxiv.org/abs/1511.05644
[24] C. L. Srinidhi, S. W. Kim, F.-D. Chen, A. L. Martel, Self-supervised
driven consistency training for annotation efficient histopathology image
analysis, Medical Image Analysis 75 (2022) 102256. doi:10.1016/j.
media.2021.102256.
URL https://doi.org/10.1016%2Fj.media.2021.102256
[25] Q. Xie, Z. Dai, E. Hovy, M.-T. Luong, Q. V. Le, Unsupervised data
augmentation for consistency training (2019). doi:10.48550/ARXIV.
1904.12848.
URL https://arxiv.org/abs/1904.12848
[26] R. Ding, A. Yadav, E. Rodriguez, A. C. A. L. da Silva, W. Hsu, Improv-
ing the self-supervised pretext task for histopathologic subtype classifi-
cation, in: Medical Imaging with Deep Learning, 2022.
URL https://openreview.net/forum?id=7QWzEwByMXq
18
