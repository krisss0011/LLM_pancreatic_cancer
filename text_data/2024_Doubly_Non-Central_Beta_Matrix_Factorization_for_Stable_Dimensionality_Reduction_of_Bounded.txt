Stable Dimensionality Reduction for Bounded Support Data
Doubly Non-Central Beta Matrix Factorization for Stable
Dimensionality Reduction of Bounded Support Matrix Data
Anjali N. Albert
gnagulpally@umass.edu
Department of Mathematics & Statistics
University of Massachusetts
Amherst, MA 01003, USA
Patrick Flaherty
pflaherty@umass.edu
Department of Mathematics & Statistics
University of Massachusetts
Amherst, MA 01003, USA
Aaron Schein
schein@uchicago.edu
Department of Statistics & Data Science Institute
University of Chicago
Chicago, IL 60637, USA
Editor:
Abstract
We consider the problem of developing interpretable and computationally efficient matrix
decomposition methods for matrices whose entries have bounded support. Such matrices
are found in large-scale DNA methylation studies and many other settings. Our approach
decomposes the data matrix into a Tucker representation wherein the number of columns in
the constituent factor matrices is not constrained. We derive a computationally efficient
sampling algorithm to solve for the Tucker decomposition. We evaluate the performance
of our method using three criteria: predictability, computability, and stability. Empirical
results show that our method has similar performance as other state-of-the-art approaches
in terms of held-out prediction and computational complexity, but has significantly better
performance in terms of stability to changes in hyper-parameters. The improved stability
results in higher confidence in the results in applications where the constituent factors are
used to generate and test scientific hypotheses such as DNA methylation analysis of cancer
samples.
Keywords:
Matrix Decomposition, Bounded Support Data, Tucker Decomposition
1. Introduction
Bounded-value data is common in many data science applications. In computational biology,
DNA methylation data is (0, 1)-bounded because each entry is a measurement of the fraction of
CpG sites that are methylated at a genetic locus in a sample (Shafi et al., 2018). Collaborative
filtering and recommendation systems make use of data on pairs of entities (e.g. users and
movies) whose value is an integer from one to five (Bennett et al., 2007). In these fields, a
central scientific goal is to find a low-dimensional representation of the high-dimensional
data that yields meaningful and reproducible scientific insights.
1
arXiv:2410.18425v1  [cs.LG]  24 Oct 2024
Albert et. al.
Matrix decomposition methods are widely used to learn low-dimensional representa-
tions (Lee and Seung, 1999; Roweis and Saul, 2000; Van der Maaten and Hinton, 2008).
Tensor decomposition methods extend matrix decomposition methods to multi-dimensional
data and have been used in computer vision, graph analysis, signal processing and many
other applications (Kolda and Bader, 2009). It was originally developed as a method for
extending factor analysis to three-mode tensors (Tucker, 1966). It was later extended to
generalize singular value decomposition and principal component analysis to higher-order
tensors (Grasedyck, 2010). Recently, Tucker decomposition has been combined with ideas in
sketching (Malik and Becker, 2018) and randomized linear algebra to improve computational
efficiency for large-scale data sets (Minster et al., 2020).
Following the notation in Minster et al. (2020), the Tucker decomposition represents
a d-mode tensor X ∈RI1×···×Id as the tensor product between a lower-dimensional core
matrix G ∈Rr1×···×rd and factor matrices {Aj}d
j=1 where Aj ∈RIj×rj. The decomposition
is such that X = G×
d
j=1 Aj and the Tucker representation of X is written compactly as
[G; A1, . . . , Ad].
Other tensor representations exist: CANDECOMP/PARAFAC (CP) (Kolda and Bader,
2009), hierarchical Tucker (Grasedyck, 2010), and tensor train (Oseledets, 2011). The CP
representation decomposes the tensor into an outer product of rank-1 tensors. For 2-mode
tensors typically found in recommendation problems and topic models, Tucker decomposition
has some unique benefits compared to other alternatives. Matrix factorization methods that
represent a data table as product of two matrices constraint the number of columns in the
first factor matrix to be the same as the number of rows in the second factor matrices. For
example, in a topic model decomposition of a document-term matrix, the number of groups
of documents is constrained to be equal to the number of topics. This constraint is often
not realistic in practice and undesirable when interpreting the model. Tucker decomposition
has the benefit, due to the core matrix, that the reduced ranks of the factor matrices are
decoupled from one another making for more interpretable models.
Singular value decomposition of real-valued matrices to a Tucker representation can be
accomplished using higher order SVD (HOSVD) and sequentially truncated higher order SVD
(STHOSVD) (Minster et al., 2020). Recently, Nonnegative Tucker decomposition (NTD)
has been developed for non-negative-valued matrices and tensors (Kim and Choi, 2007).
NTD has proven to be empirically useful in identifying meaningful latent features in image
data (Zhou et al., 2015a). Using the fact that all entries are nonnegative leads to dramatic
improvements in computational efficiency (Wang and Zhang, 2012). However, there are not
yet any methods, to our knowledge, for finding a Tucker decomposition of tensors whose
entries are (0, 1)-bounded.
Matrix factorization and Tucker decomposition is particularly relevant for DNA methy-
lation data sets. DNA methylation enables the cell to exert epigenetic control over the
expression of genetic material to affect cellular processes such as development, inflammation,
and genomic imprinting (Bock, 2012). There is evidence that has associated DNA methyla-
tion patterns with environmental exposures, and aberrant DNA methylation patterns have
been used as biomarkers for cancer prognosis (Laird, 2003). DNA methylation data can be
represented as an I × J matrix where an entry, xij is the proportion (bounded by (0, 1)) of
CpG sites that are methylated in sample i at CpG island or “locus” j.
2
Stable Dimensionality Reduction for Bounded Support Data
A particular application of the analysis of bounded support DNA methylation data is in
continuous monitoring of cell-free DNA for metastatic cancer recurrence (Cisneros-Villanueva
et al., 2022). A primary tumor may be treated through surgical or other means into remission,
but the possibility of a distant metastatic recurrence remains. DNA methylation patterns
are highly correlated with cell-type and thus tissue of origin of the cell (Loyfer et al.,
2023). Therefore, DNA methylation patterns preserved in circulating cell-free DNA represent
an opportunity to monitor and possibly localize metastatic recurrence in a non-invasive
manner (Lau et al., 2023). Several recent papers have developed matrix factorization for
DNA methylation data. Yoo and Choi (2009) develop a matrix tri-factorization model
without priors and estimate the factor matrices with an EM algorithm. Čopar et al. (2017) is
concerned primarily with scalability and develop a block-wise update algorithm that can be
parallelized on a GPU. Finally, Park et al. (2019) use an exponential prior on the elements
of the middle factor in the tri-factorization, and a Gaussian prior for the right matrix. A
variational inference algorithm is used to provide estimates of the latent matrices. The need
for interpretable and predictive methods for analysis of this type of data motivates the need
for computationally efficient methods for finding Tucker representations of bounded support
data.
Contributions
This paper significantly extends the work of Schein et al. (2021) to develop
a family of hierarchical statistical models for matrix decomposition of bounded support data.
These models are based on the doubly non-central beta (DNCB) distribution as a flexible
likelihood term. We build this family of generative models to encompass both standard
matrix factorization (DNCB-MF) and Tucker decomposition (DNCB-TD). We derive a
fast Markov-chain Monte Carlo inference method for both matrix factorization and Tucker
decomposition using an augment-and-marginalize approach. We measure prediction accuracy,
computational efficiency, and stability to changes in hyper-parameters for DNCB-MF and
DNCB-TD. An analysis of real methylation data (both array-based and sequence-based) shows
that DNCB-TD identifies coherent clusters of samples and clusters of features (pathways)
that correlate with cancer development mechanisms.
An analysis of facial image data
shows that the model is flexible and can identify coherent clusters for a wide range of
bounded support data distributions. Finally, we provide a replicable implementation at
https://github.com/flahertylab/dncb-matrix-fac.
2. Doubly Non-Central Beta Factorization
To analyze matrices of bounded support data, we introduce a family of probabilistic matrix
decomposition models that are unified under the assumption that entries βij ∈(0, 1) of an
observed I × J matrix are doubly non-central beta (DNCB) random variables,
βij ∼DNCB

ϵ
(1)
0 , ϵ
(2)
0 , λ
(1)
ij , λ
(2)
ij

,
(1)
where the shape parameters, ϵ
(1)
0
and ϵ
(2)
0 , are shared across all (i, j) ∈[I] × [J], and the non-
centrality parameters, λ
(1)
ij and λ
(2)
ij , are unique to each (i, j) and decompose linearly into latent
factors—e.g., λ
(1)
ij = PK
k=1 θ
(1)
ik ϕkj. We introduce, later, two different ways of decomposing the
non-centrality parameters (CP and Tucker decomposition) that yield qualitatively different
interpretations of the latent factors.
3
Albert et. al.
The DNCB distribution (Johnson et al., 1995) is a generalization of the standard beta
distribution whose properties have been recently explicated by Ongaro and Orsi (2015)
and Orsi (2017). We provide a definition of the DNCB distribution here and visualize it
in Figure 1.
Definition 1 (Doubly non-central beta distribution) A doubly non-central beta ran-
dom variable β ∼DNCB (ϵ1, ϵ2, λ1, λ2) is continuous with bounded support β ∈(0, 1). Its
distribution is defined by positive shape parameters ϵ1 > 0 and ϵ2 > 0, non-negative non-
centrality parameters λ1 ≥0 and λ2 ≥0, and probability density function (PDF) equal to
DNCB (β; ϵ1, ϵ2, λ1, λ2) = Beta(β; ϵ1, ϵ2) e−λ• Ψ2 [ϵ•; ϵ1, ϵ2; λ1β, λ2(1 −β)]
where ϵ• ≜ϵ1 + ϵ2, λ• ≜λ1 + λ2, and Ψ2 [·; ·, ·; ·, ·] is Humbert’s confluent hypergeometric
function.
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
probability density
Doubly Non-Central Beta Distribution
          ( 1,
2,
1,
2)
(1, 1, 0, 1)
(1, 1, 10, 1)
(2, 3, 4, 5)
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
probability density
Doubly Non-Central Beta Distribution
          ( 1,
2,
1,
2)
(0.75, 0.75, 2, 2)
(0.75, 0.25, 2, 5)
(3.00, 0.75, 1, 2)
Figure 1: The doubly non-central beta (DNCB) distribution can assume a shape like the
standard beta (left). Alternatively, the DNCB distribution can take a multi-modal shape
if ϵ1 < 1 or ϵ2 < 1 (right). This expressiveness is particularly useful when modeling DNA
methylation datasets, which are typically highly dispersed and multi-modal.
The DNCB distribution has several properties that make it useful for modeling bounded
matrix data. First, as shown by by Ongaro and Orsi (2015), the density is well-behaved
(has finite and positive density) when approaching the bounds of support (0 or 1), unlike the
standard beta whose density goes to 0 or +∞. This fact suggests that probabilistic models
based on a DNCB likelihood may be more flexible and stable than ones based on the standard
beta, particularly if data are highly dispersed around the extremes. Second, the DNCB
distribution yields several augmentations that are useful for building hierarchical Bayesian
models and deriving efficient posterior inference algorithms, as we will show. In particular,
the DNCB distribution has a mixture representation in terms of Poisson-distributed auxiliary
variables. The DNCB random variable βij given in Equation (1) can be generated from a
standard beta distribution, conditional on draws of two Poisson random variables:
βij | y
(1)
ij , y
(2)
ij ∼Beta

ϵ
(1)
0 + y
(1)
ij , ϵ
(2)
0 + y
(2)
ij

,
y
(t)
ij ∼Pois

λ
(t)
ij

t ∈{1, 2}.
(2)
4
Stable Dimensionality Reduction for Bounded Support Data
There is a large literature on latent factor and hierarchical Bayesian models based on the
Poisson likelihood (Cemgil, 2009; Gopalan et al., 2015b; Zhou and Carin, 2012). The Poisson
enjoys closed-form conjugate priors, unlike the standard beta, alongside other well-understood
properties and relationships that makes it easy to build flexible and tractable models. The
mixture representation in Equation (2) allows us to build tractable latent factor models
for bounded support data by linking observed data βij to latent count variables, y(1)
ij
and
y(2)
ij , and then hierarchically modeling those counts using techniques and motifs from the
literature on Poisson-based models.
Representing the DNCB distribution as a Poisson-Beta mixture allows for yet another
useful augmentation based on the relationship between the Beta and Gamma distributions.
If γ1 ∼Gam (a1, c) and γ2 ∼Gam (a2, c) are independent Gamma random variables with
possibly different shape parameters, a1 and a2, but shared rate parameter c, then the
proportion β = γ1/(γ1+γ2) is marginally Beta-distributed as β ∼Beta(a1, a2).
A fully
augmented version of the marginal likelihood in Equation (1) can therefore be written as
βij | γ
(1)
ij , γ
(2)
ij =
γ
(1)
ij
γ
(1)
ij + γ
(2)
ij
γ
(t)
ij | y
(t)
ij ∼Gam

ϵ
(t)
0 + y
(t)
ij , cj

t ∈{1, 2}
y
(t)
ij ∼Pois

λ
(t)
ij

t ∈{1, 2}.
(3)
where cj is an additional (optional) parameter that effectively represents variance hetero-
geneity across columns of the observed matrix.
2.1 Decomposition Strategies
We present two factorization approaches for the non-centrality parameters of the DNCB
distribution, λ1 and λ2. These two approaches are guided by the Poisson factorization
literature (Gopalan et al., 2015a).
The first, matrix factorization, is based on the CP
decomposition for tensor factorizations, and the second, matrix tri-factorization, is based on
the Tucker decomposition.
Matrix Factorization
DNCB matrix factorization (DNCB-MF) assumes λ
(1)
ij and λ
(2)
ij are
linear functions of low-rank latent factors,
λ
(1)
ij =
K
X
k=1
θ
(1)
ik ϕkj
and
λ
(2)
ij =
K
X
k=1
θ
(2)
ik ϕkj.
(4)
In the context of DNA methylation data, the vector of latent factors ϕj := ⟨ϕ1j . . . ϕKj⟩
describes how relevant gene j is to each of the K latent components. The vectors θ(1)
i
:=
⟨θ
(1)
i1 . . . θ
(1)
iK⟩, θ
(2)
i
:= ⟨θ
(2)
i1 . . . θ
(2)
iK⟩represent how methylated or unmethylated, respectively,
genes in latent component k are in sample i. The ratio ρik :=
θ(1)
ik
θ(1)
ik +θ(2)
ik
∈(0, 1) indicates that
pathway k is hypermethylated in sample i when ρik ≫0.5 and hypomethylated in sample i
when ρik ≪0.5. The vector ρi ∈(0, 1)K can also be interpreted as an embedding of sample
i.
5
Albert et. al.
Matrix Tri-Factorization
DNCB matrix tri-factorization (DNCB-TD) assumes λ
(1)
ij and
λ
(2)
ij factorize into three factor matrices
λ(1)
ij =
C
X
c=1
θic
K
X
k=1
π
(1)
ck ϕkj
and
λ(2)
ij =
C
X
c=1
θic
K
X
k=1
π
(2)
ck ϕkj.
(5)
The vector of sample factors θi := ⟨θi1 . . . θiC⟩describes how much sample i belongs to each
of C overlapping sample clusters. Similarly, the vector of gene factors ϕj := ⟨ϕ1j . . . ϕKj⟩
describes how much gene j belongs to each of K overlapping pathways or feature clusters.
The vectors θi and ϕj can be viewed as embeddings of sample i and gene j, respectively.
The core matrices π
(1)
ck , π
(2)
ck connect the latent factors θi, ϕj by describing the extent to which
samples in cluster c have genes that are hypermethylated in pathway k.
DNCB-TD decouples the ranks of the factor matrices, allowing the number of latent
factors in θic to differ from that of ϕkj. This offers a more flexible representation that is
useful in applications where I, J are different orders of magnitude, as in methylation data.
Both DNCB-MF and DNCB-TD are forms of 2-mode Tucker decomposition (Hoff, 2005;
Nickel et al., 2012; Li et al., 2009; Tucker, 1964). DNCB-MF is a special case of Tucker
decomposition, where the cardinalities of the latent factors θ
(t)
ik , ϕkj are equal.
Priors
To fully specify our Bayesian hierarchical model, the latent factors are endowed
with Gamma priors giving
θ
(1)
ik , θ
(2)
ik ∼Gam (η1, η2)
ϕkj ∼Gam (ν1, ν2)
(6)
for DNCB-MF, and
θic ∼Gam (η1, η2)
ϕkj ∼Gam (ν1, ν2)
π
(1)
ck , π
(2)
ck ∼Gam (ζ1, ζ2)
(7)
for DNCB-TD.
Generative Model
The following algorithm can be used to draw samples from the
generative DNCB-MF model:
1. Sample from ϕkj
i.i.d.
∼Gam (ν1, ν2) for k = 1, . . . , K, and j = 1, . . . , J.
2. Sample from θ
(1)
ik , θ
(2)
ik ∼Gam (η1, η2) for i = 1, . . . , I, and k = 1, . . . , K.
3. Compute λ
(t)
ij = PK
k=1 θ
(t)
ik ϕkj for t = 1, 2, i = 1, . . . , I, and j = 1, . . . , J.
4. Sample from y
(t)
ij ∼Pois

λ
(t)
ij

for t = 1, 2, i = 1, . . . , I, and j = 1, . . . , J.
5. Sample from γ
(t)
ij
y
(t)
ij
∼Gam

ϵ
(t)
0 + y
(t)
ij , cj

for t = 1, 2, i = 1, . . . , I, and j =
1, . . . , J.
6. Compute βij
γ
(1)
ij , γ
(2)
ij =
γ(1)
ij
γ(1)
ij +γ(2)
ij
for t = 1, 2, i = 1, . . . , I, and j = 1, . . . , J.
The following algorithm can be used to draw samples from the generative DNCB-TD model:
6
Stable Dimensionality Reduction for Bounded Support Data
1. Sample from π
(1)
ck , π
(2)
ck ∼Gam (ζ1, ζ2) for c = 1, . . . , C, and K = 1, . . . , K.
2. Sample from ϕkj ∼Gam (ν1, ν2) for k = 1, . . . , K, and j = 1, . . . , J.
3. Sample from θic ∼Gam (η1, η2) for i = 1, . . . , I, and c = 1, . . . , C.
4. Compute λ
(t)
ij = PC
c=1 θic
PK
k=1 π
(t)
ckϕkj for t = 1, 2, i = 1, . . . , I, and j = 1, . . . , J.
5. Sample y
(t)
ij ∼Pois

λ
(t)
ij

, for t = 1, 2, i = 1, . . . , I, and j = 1, . . . , J.
6. Sample γ
(t)
ij
y
(t)
ij ∼Gam

ϵ
(t)
0 + y
(t)
ij , cj

, for t = 1, 2, i = 1, . . . , I, and j = 1, . . . , J.
7. Compute βij
γ
(1)
ij , γ
(2)
ij =
γ(1)
ij
γ(1)
ij +γ(2)
ij
for i = 1, . . . , I, and j = 1, . . . , J.
3. MCMC Inference
In this section, we provide the conditional posteriors for all the latent variables which
collectively define a Gibbs sampler that is straightforward to implement and asymptotically
guaranteed to generate samples from the posterior. Despite the lack of conjugacy to the beta
likelihood (2), we show that, under the augmented likelihood (3), the conditional posteriors
for the counts y(1)
ij
and y(2)
ij
are available in closed form via the Bessel distribution (Yuan
and Kalbfleisch, 2000). Graphical model representations of the augmented DNCB-MF and
DNCB-TD decompositions are shown in Figure 2.
3.1 Sampling γ(1)
ij
and γ(2)
ij
The latent variables, γ
(1)
ij and γ
(2)
ij , may be conditioned on and treated as data, if available.
However, when only βij is observed, we can sample the latent variables. We do so by
appealing to the property given in Definition 2.
Definition 2 (Proportion-sum independence of gammas (Lukacs, 1955)) If γ1 ∼
Gam (α1, c) and γ2 ∼Gam (α2, c) are independent gamma random variables with shared
rate parameter c, then their sum γ· := γ1 + γ2 and proportion β := γ1/γ· are independent
random variables:
γ· ∼Gam (α1 + α2, c) ,
β ∼Beta(α1, α2)
This holds if, and only if, γ1 and γ2 are gamma distributed.
Since βij is already defined as a proportion of the latent variables, we can redefine the
latent variables as functions of their sum γ(·)
ij := γ
(1)
ij + γ
(2)
ij ,
γ
(1)
ij := βijγ(·)
ij ,
γ
(2)
ij := (1 −βij)γ(·)
ij .
(8)
This implies that γ(·)
ij is a gamma random variable,
γ(·)
ij ∼Gam

ϵ
(1)
0 + ϵ
(2)
0 + y
(1)
ij + y
(2)
ij , cj

,
(9)
which, by Definition 2, is independent of the data βij. We can therefore sample it from the
prior, as in Equation (9) and then set γ
(1)
ij and γ
(2)
ij to the values given in Equation (8).
7
Albert et. al.
βij
Φkj
yij
(1)
yij
(2)
γij
(•)
γij
(1)
γij
(2)
Θik
(1)
Θik
(2)
k ϵ [K]
j ϵ [J]
i ϵ [I]
(a) Graphical model representation of the
fully augmented form of DNCB-MF given in
eqs. (3), (4) and (6).
βij
Φkj
yij
(1)
yij
(2)
γij
(•)
γij
(1)
γij
(2)
πck
(2)
πck
(1)
Θic
j ϵ [J]
i ϵ [I]
k ϵ [K]
c ϵ [C]
(b) Graphical model representation of the
fully augmented form of DNCB-TD given in
eqs. (3), (5) and (7).
Figure 2: A graphical comparison of the DNCB-MF and DNCB-TD generative processes.
The plate notation represents exchangeability across the specified indices. Shaded nodes are
observed variables; unshaded nodes are latent variables. Solid edges denote random variables;
dotted edges denote deterministic variables.
3.2 Sampling y(1)
ij , y(2)
ij
As shown by Yuan and Kalbfleisch (2000), if γ ∼Gam(b + y, c) is a gamma random variable
with rate c and shape b + y, and y ∼Pois(ζ) is a Poisson random variable with rate ζ, then
the conditional posterior over y follows the Bessel distribution, so
P(y|γ, c, b, ζ) = Bes(y; b−1, 2
p
cγζ).
We can therefore sample y(1)
ij
from its conditional posterior,

y
(1)
ij | −

∼Bes

ϵ
(1)
0 −1, 2
q
cjγ
(1)
ij λ
(1)
ij

.
(10)
The equation for y(2)
ij
is the same but with ϵ
(2)
0
and λ
(2)
ij , rather than ϵ
(1)
0
and λ
(1)
ij .
The Bessel distribution.
Since the Bessel distribution is so uncommon, we take some
space here to define some of its key properties. A Bessel random variable y ∼Bes(v, a) is
discrete y ∈{0, 1, 2, . . . }. The distribution has two parameters v > −1 and a > 0 and its
unnormalized probability mass function is
Bes(y; v, a) ∝
1
y!Γ(y + v)
a
2
2y+v
.
(11)
8
Stable Dimensionality Reduction for Bounded Support Data
The distribution is named for its normalizing constant
I(v, a) =
∞
X
n=0
1
n!Γ(n + v)
a
2
2n+v
(12)
which is the modified Bessel function of the first kind. A Bessel random variable’s mean and
variance are
E[Y |v, a] = µ = aR(v, a)
2
(13)
Var(Y |v, a) = µ
 1 + µ(R(v + 1, a) −R(v, a))),
(14)
where R(v, a) := I(v+1,a)
I(v,a)
is called the Bessel quotient. Since the Bessel quotient is strictly
decreasing in v (Devroye, 2002), the quantity R(v + 1, a) −R(v, a) in the expression for the
variance is always negative. As a result, the Bessel distribution is underdispersed since its
variance divided by its mean is upper-bounded by 1:
Var(Y |v, a)
µ
= 1 + µ
 R(v + 1, a) −R(v, a)

≤1
(15)
There does not currently exist any open-source implementations of Bessel sampling. One
contribution of this paper is an open-source Cython library that provides fast Bessel sampling
methods. Our library implements the four rejection samplers of Devroye (2002). Some of
these methods rely on direct computation of the Bessel function, which is available in the
GNU Standard Library. Others avoid calling the Bessel function—which can be numerically
unstable—by relying entirely on computing the Bessel quotient, which can be computed
without any special functions using the dynamic program of Amos (1974). We also implement
the Gaussian approximation method of Yuan and Kalbfleisch (2000) and the table sampling
method based on direct evaluation of the PMF, used by Zhou et al. (2015b).
3.3 Sampling y(1)
ijck, y(2)
ijck
Due to the additive property of Poisson random variables (Kingman, 1972), the count y(t)
ij
can be defined as the sum of subcounts, y(t)
ij := PC
c=1
PK
k=1 y(t)
ijck, each of which are Poisson:
y(t)
ijck
ind.
∼Pois(θicπ
(t)
ckϕkj)
∀c, k and t ∈{1, 2}
(16)
These latent subcounts are required as sufficient statistics to compute the conditional
posteriors over θi, ϕj, π
(t)
ck, and ζ. Conditioned on their sum, the posterior over {y
(1)
icjk}c,k is
multinomial (Steel, 1953),

y
(1)
icjk
	
c,k | −

∼Mult

y
(1)
ij ,
nθicπ
(1)
ck ϕkj
ρij
o
c,k

.
(17)
As described by Schein et al. (2016), the multinomial probabilities can be computed efficiently
by exploiting the compositional structure of the Tucker product, ρij = θiΠϕT
j .
9
Albert et. al.
3.4 Sampling θi, ϕj, π(t)
ck
By gamma-Poisson conjugacy, we can sample
 θ
(t)
ik | −

∼Gam

η1 +
J
X
j=1
y
(t)
ijk, η2 +
J
X
j=1
ϕkj

, and
(ϕkj | −) ∼Gam
 
ν1 +
I
X
i=1
2
X
t=1
y
(t)
ijk, ν2 +
I
X
i=1
2
X
t=1
θ
(t)
ik
!
(18)
for DNCB-MF and
(θic | −) ∼Gam

η1 +
J
X
j=1
K
X
k=1
2
X
t=1
y
(t)
icjk, η2 +
J
X
j=1
K
X
k=1
2
X
t=1
π
(t)
ckϕkj

,
(ϕkj | −) ∼Gam
 
ν1 +
I
X
i=1
C
X
c=1
2
X
t=1
y
(t)
icjk, ν2 +
I
X
i=1
C
X
c=1
2
X
t=1
θicπ
(t)
ck
!
, and
 π
(t)
ck | −

∼Gam

ζ1 +
I
X
i=1
J
X
j=1
y
(t)
icjk, ζ2 +
I
X
i=1
θic
J
X
j=1
ϕkj


(19)
for DNCB-TD.
4. Predictability, Computability, and Stability
In many application areas, data analysis methods must be reliable, reproducible, and
transparent. The principles of predictability, computability, and stability (PCS) help to
ensure that these goals are achieved (Yu and Kumbier, 2020). In this section, we apply the
PCS framework to analyze the predictibility, computability, and stability of DNCB-MF and
DNCB-TD. To measure predictability, we imputed missing data values; to measure stability,
we measure the co-clustering of samples and features while varying the hyper-parameters of
the model; and to address computability, we report computational complexity. As DNCB-TD
is designed to model matrix data where I ≪J, we focus on cases where C < K in these tasks.
We are looking for methods that have good accuracy on held-out data, are stable to variations
in model hyper-parameters, and are computable in a time scale that is appropriate for the
problem, and we find that DNCB-TD achieves the veridical goals of the PCS framework.
Microarray Methylation Data.
We compiled a dataset of 400 cancer samples from
the Cancer Genome Atlas (TCGA) (Network et al., 2013). The 400 samples consisted of
four clusters of 100 samples from four different cancer types: breast, ovarian, colon and
lung cancer. Of the 27,578 loci that are available, we considered only the 5,000 with the
highest variance across as samples, as has been done previously (Ma et al., 2014). The
resultant matrix of β values was 400 × 5,000. We make the dataset we used available:
https://github.com/flahertylab/dncb-fac/tree/main/data/methylation.
Bisulfite Methylation Data.
We downloaded the dataset studied by Sheffield et al. (2017).
This dataset consists of 156 Ewing sarcoma cancer samples and 32 healthy samples (N =188),
10
Stable Dimensionality Reduction for Bounded Support Data
whose methylation was profiled using bisulfite sequencing (bi-seq). Bi-seq data consists of
binary “reads” of methylation at many loci per gene. Following standard framework, we
processed this data into “beta values” by first counting all the methylated-mapped reads
dij and non-methylated reads uij for all loci within a given gene j and then calculating
βij =
s0+dij
2s0+dij+uij with the smoothing term set to s0 = 0.1 (Du et al., 2010). As with the
microarray data, we selected the 5,000 genes with the highest variance to obtain a 188×5, 000
matrix.
Olivetti Faces Data.
We loaded the Olivetti faces dataset (Samaria and Harter, 1994)
from scikit-learn’s dataset library. The dataset consists of ten distinct 64 × 64 images of each
of 40 different subjects. The ten images of each subject vary in facial expression, angle, and
lighting. Each image is in greyscale and quantized to floating point values on the interval
[0, 1]. We subset to the first 20 subjects, selecting the three most similar photos of each
subject measured by Euclidean distance, and vectorized each of the resulting 60 images to
create a 60 × 4096 matrix with entries between 0 and 1. We make this dataset available:
https://github.com/flahertylab/dncb-fac/tree/main/data/faces.
BG-NMF Method.
Our model is most closely related to beta-gamma non-negative matrix
factorization (BG-NMF), which was developed by Ma et al. (2015) specifically for DNA
methylation datasets. BG-NMF is the first (and, to our knowledge, only) matrix factorization
model to assume a beta likelihood. Specifically, it assumes that each element βij ∈(0, 1) in
a sample-by-gene matrix is drawn,
βij ∼Beta

α
(1)
ij , α
(2)
ij

,
(20)
where the two shape parameters α
(1)
ij and α
(2)
ij are defined to be the same linear functions
of low-rank latent factors as those given in Equation (4). BG-NMF also places the same
gamma priors over these factors as those given in Equation (6).
DNCB-MF and BG-NMF both factorize a sample-by-gene matrix into three non-negative
latent factor matrices; however, DNCB-MF factorizes the non-centrality parameters of the
DNCB distribution, while BG-NMF factorizes the shape parameters of the beta distribution.
Deriving an efficient and modular posterior inference algorithm for BG-NMF is hampered
by the lack of a closed-form conjugate prior for the beta distribution. Ma et al. (2015)
propose a variational inference algorithm that maximizes nested lower bounds on the model
evidence. Their derivation is sophisticated, but highly tailored to the specific structure of
the model, which makes the model difficult to modify or extend. Moreover, the quality of
this algorithm’s approximation to the posterior distribution is not well understood. For
biomedical settings, in which precise quantification of uncertainty is often necessary, the lack
of an efficient MCMC algorithm therefore limits BG-NMF’s applicability.
4.1 Predictability
Prior Predictive Check.
The first condition of predictability in a Bayesian workflow is
that the model be capable of producing data that reflects the statistics of the actual observed
data (Cemgil, 2009). In order to test whether this criterion was met, we performed a prior
predictive check. A set of simulated data was generated from the model prior to fitting and
the distribution was compared to the distribution of the observed data.
11
Albert et. al.
0.0
0.2
0.4
0.6
0.8
1.0
Data
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Density
Bisulfite-sequenced methylation data
0.0
0.2
0.4
0.6
0.8
1.0
Data
0.0
0.5
1.0
1.5
2.0
2.5
Density
Array methylation data
0.0
0.2
0.4
0.6
0.8
1.0
Data
0.0
0.5
1.0
1.5
2.0
Density
Olivetti faces
simulated
observed
Figure 3: Prior predictive checks for DNCB-TD on three datasets.
Samples were drawn from the generative distribution for DNCB-TD given in Equation (7),
Equation (5), and Equation (3). The similarity between the observed and simulated data
was measured using mean squared error (MSE), 1
N
PN
n=1(βn −ˆβn)2, for the N total observed
beta value entries βn and predictions ˆβn. This process was repeated 1,000 times. Figure 3
and table 1 contain results of the prior predictive analysis. The density plots indicate that
DNCB-TD can produce data sets that are comparable to the methylation and Olivetti face
datasets.
Dataset
MSE
Bisulfite sequenced methylation
0.2000 ± 0.0002
Array methylation
0.1755 ± 0.0001
Olivetti faces
0.0538 ± 0.0001
Table 1: Similarity between the observed and simulated data from DNCB-TD measured
by mean squared error (MSE). MSE values are averaged over 1000 trials plus or minus one
standard deviation.
Heldout Prediction.
We randomly generated three masks that censored 10% of the data
matrix. Using the mask as input, we fit three models designed for bounded data (DNCB-TD,
DNCB-MF, and BG-NMF) using three different random initializations and imputed the
held-out values.
To assess out-of-sample predictive performance, we used the pointwise predictive density
(PPD) (Gelman et al., 2014). For the three models, the PPD is given by
PPDpost =
Y
i,j∈M
h
1
S
S
X
s=1
P
 βij | Θ(1)
s , Θ(2)
s , Φs
i
,
(21)
where Θ(1)
s , Θ(2)
s , Φs are samples from the posterior distribution, either saved during MCMC
for DNCB-MF and DNCB-TD or drawn from the fitted variational distribution for BG-NMF.
For both models, we used S =100. The predictive density P(·|·) is the beta distribution for
BG-NMF and the DNCB distribution for DNCB-MF and DNCB-TD.
Figure 4 shows PPD
1
|M| , where |M| is the number of held-out values, varying the factor
matrix cardinality, K. The performance metric is equivalent to the geometric mean of
12
Stable Dimensionality Reduction for Bounded Support Data
5
10
15
20
25
30
K
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3.0
Predictive density
Bisulfite sequenced methylation data
5
10
15
20
25
30
K
1.6
1.8
2.0
2.2
2.4
Array methylation data
5
10
15
20
25
30
K
2.5
3.0
3.5
4.0
Olivetti faces
DNCB-MF
DNCB-TD, C = 4
DNCB-TD, C = 6
DNCB-TD, C = 8
DNCB-TD, C = 10
BGNMF
Figure 4: Heldout prediction results on three datasets; higher is better. Random test-train
splits were generated by creating three binary masks, each holding out a random 10% of the
data. Using three random initializations for each model, we fit the models on the training
data and imputed the held-out values, varying K across 6 values. We plot the rescaled
pointwise predictive density (PPD) obtained by each model; the error bars denote 95%
confidence intervals. All three models perform comparably well on held-out prediction.
the predictive densities across the held-out values and is therefore comparable across all
experiments. All three models perform comparably well on held-out prediction.
While imputation tasks may be useful for revealing the ways in which different models’
inductive biases differ, they are not the inferential focus of tasks in the problem domain
of DNA methylation. The real motivating task for statistical models of DNA methylation
is unsupervised discovery of cancer subtypes and the pathways that are relevant to those
subtypes (Laird, 2010). Towards this end, it is critical that the inferred subtypes and
pathways are stable for small variations of model parameters.
4.2 Stability
Stability assesses how experimental outcomes are affected by human judgment calls in the
modeling process. A stable model’s results should not significantly change with reasonable
perturbations to the data or model architecture (Yu and Kumbier, 2020). A typical human
judgment call in matrix factorization is the cardinality of the factor matrices. For DNCB-TD,
cardinality is controlled by two parameters C, K, that respectively define the cardinality of
the ΘIC and ΦKJ factor matrices; for DNCB-MF and BG-NMF, cardinality is controlled by
K. It is important in cancer subtype detection that cluster and pathway assignments are
stable to reasonable changes in cardinality.
For each combination of C, K ∈{2, . . . , 50}, DNCB-TD was fit on bisulfite-sequencing
methylation data consisting of 156 Ewing sarcoma samples and 32 healthy samples. To
measure the stability of cluster and pathway assignments to changes in factor matrix
cardinality, we constructed sample and feature co-occurrence matrices for each combination
of C and K, and computed the KL divergence between the true and model-induced co-
13
Albert et. al.
occurrences while varying one of C, K. We followed the same process for BG-NMF and
DNCB-MF, varying the cardinality parameter K across {2, . . . , 50}. Figure 5 shows that
DNCB-TD is the only model for which both cluster and pathway stability remains relatively
constant across a range of C and K.
Sample Clusters
Feature Clusters
DNCB-TD
DNCB-MF
BG-NMF
Figure 5: Stability results for BG-NMF, DNCB-MF, and DNCB-TD on bisulfite sequencing
methylation data. DNCB-TD is the only model for which the stability of both cluster and
pathway assignments remains relatively constant as factor matrix cardinality increases.
4.3 Computability
The complexity of the algorithm is dominated by the Bessel and multinomial steps of
equations 10 and 17. The Bessel step scales linearly with the size of the data matrix O(2IJ).
The complexity of the multinomial step is O(CKN>0) where N>0 ≤2IJ is the number of
non-zero y
(1)
ij , y
(2)
ij counts. For very sparse counts, this step may be faster than the Bessel
step. In the worst case, the multinomial step is O(2IJCK). How closely the model can fit
the data is a direct function of the magnitude of the counts, because the concentration of
the likelihood Beta(βij; ϵ
(1)
0 + y
(1)
ij , ϵ
(2)
0 + y
(2)
ij ) is proportional to y
(1)
ij + y
(2)
ij . The likelihood
is maximized by taking the counts to infinity while keeping their proportion fixed to βij.
The more sparse the counts, the more efficient the process. The sparsity of the counts is
affected by the magnitude of the Gamma priors, wherein very small values of the shape
and rate parameters induce smaller counts. In practice, we initialize the Gibbs sampler by
running several iterations of the BG-NMF method, which allows the model to rapidly acquire
a coarse-grained representation of the data.
14
Stable Dimensionality Reduction for Bounded Support Data
5. Real Data Analysis
In this section, we explore how the latent structures in the DNCB-TD model reveal methy-
lation profiles associated with cancer subtypes. We also explore DNCB-TD inferences for
the Olivetti faces image data set. The distribution of observed values within the bounded
interval for methylation and image data is very different, yet the flexibility of the DNCB-TD
model enables it to capture salient features.
5.1 DNA Methylation Data
We fit DNCB-TD to the Microarray Methylation Data described previously with C = 4 and
K = 6. We ran 2,000 iterations of MCMC inference. Due to label switching, one cannot
average over samples, so the inferred latent variables values are from the last sample.
DNCB-TD infers C overlapping clusters of samples. The extent to which sample i is
well-described by each of the C clusters is given by the vector θi. Figure 6 shows that
DNCB-TD concentrates breast, ovarian, and colon samples in distinct clusters, indicating
that the model infers structure that differentiates the cancer types.
1
2
3
4
Clusters
1
21
41
61
81
101
121
141
161
181
201
221
241
261
281
301
321
341
361
381
Samples
Breast cancer
Ovarian cancer
Colon cancer
Lung cancer
Array methylation data
0.0
0.2
0.4
0.6
0.8
Figure 6: The inferred matrix ˆΘ for methylation array data. Rows are samples and columns
are clusters—each entry represents the strength of association between sample i and cluster
c.
Similarly, our model infers K overlapping pathways of genes, and the extent to which
gene j is active in each of the K pathways is given by the vector ϕj (Figure 7). The core
matrix of cluster-pathway factors Π(1) can be interpreted as a map between the space of
15
Albert et. al.
clusters and pathways describing the extent to which samples in cluster c methylate genes in
pathway k.
1
2
3
4
5
6
Pathways
Loci
Array methylation data
0.0
0.2
0.4
0.6
0.8
Figure 7: The inferred matrix ˆΦ for methylation array data. Rows are genes and columns
are pathways—each entry represents the strength of association between pathway k and gene
j. Row indices are sorted according to pathway association.
Figure 8 illustrates a sparse, interpretable mapping of cancer types to genetic pathways.
The learned pathways are consistent with cancer development mechanisms. Pathway 1
is strongly associated with sample cluster 3, which contains predominantly colon cancer
samples; hypermethylation of genes KCNQ5 and ZNF625 in pathway 1 is a known biomarker
of colorectal cancer (Cao et al., 2021; Lin et al., 2014). Pathway 3 is associated with samples
in sample cluster 4 which is composed primarily of breast cancer samples. A constituent of
pathway 3 is the gene ASCL2 which has been shown to be associated with poor prognosis in
breast cancer patients (Xu et al., 2017). Pathway 4 is associated with samples in sample
clusters 1 and 4. Sample cluster 1 is composed of ovarian cancer samples among others.
MUC13 is a component of pathway 4 and has been reported as a candidate biomarker for
ovarian cancer detection (Ren et al., 2023).
A parallel analysis of the DNA methylation bisulfite sequencing data is in Appendix 6.
The pathways/feature clusters show a clear block-diagonal structure similar to the array data.
Two sample clusters are characterized by absence of several pathways similar to Figure 8.
5.2 Olivetti Faces Data
We fit DNCB-TD to the Olivetti Faces Dataset with C = 20 and K = 16. We ran 5, 000
iterations of MCMC inference. The K feature clusters are shown in Figure 9. Clearly, the
DNCB-TD representation is identifying salient groups of pixels that co-vary across samples.
For example, feature cluster 8 seems to infer a correspondence between glasses and facial
hair. These features are brighter pixel intensities indicating that the model is identifying a
feature with a lack of glasses and facial hair.
16
Stable Dimensionality Reduction for Bounded Support Data
1
2
3
4
5
6
Pathways
1
2
3
4
Clusters
Array methylation data
0.0
0.2
0.4
0.6
0.8
1.0
Figure 8: The inferred core matrix ˆΠ(1) for methylation array data. Each entry represents
the strength of association between samples in cluster c and genes in pathway k.
Feature cluster 1
Feature cluster 2
Feature cluster 3
Feature cluster 4
Feature cluster 5
Feature cluster 6
Feature cluster 7
Feature cluster 8
Feature cluster 9
Feature cluster 10
Feature cluster 11
Feature cluster 12
Feature cluster 13
Feature cluster 14
Feature cluster 15
Feature cluster 16
Olivetti Faces
Figure 9: Inferred matrix ˆΦ. Each feature cluster ˆϕk is converted to a 64 × 64 image.
Figure 10 shows that DNCB-TD concentrates individuals with similar features (eg.
glasses) into distinct clusters that map interpretably to “eigenfaces” derived from the inferred
matrix ˆΦ. Sample cluster 11 makes use of feature cluster 8. These samples and features
correspond to individuals who have neither facial hair nor glasses. Sample cluster 14 makes
use of feature cluster 6. These samples and features correspond to individuals who have light
pixels on the upper cheeks. Sample cluster 16 makes use of feature cluster 1. These samples
and features correspond to individuals who have glare on the glasses.
The assignment of samples to sample clusters in DNCB-TD is given by the inferred
matrix ˆΘ shown in Figure 11. It is clear from the blocking structure that the DNCB-TD
model is clustering samples from the same person into clusters. Furthermore, samples from
individuals that are similar fall into the same clusters.
17
Albert et. al.
Figure 10: The inferred core matrix ˆΠ(1) Olivetti faces data set. Each entry represents the
strength of association between samples in cluster c and features in pathway k.
6. Conclusion
Our goal in this work is to develop a predictive, stable, and computationally efficient method
for matrix factorization of bounded support data. One of the challenges of modeling general
bounded support data is that there is a wide variety of empirical distributions of observed
data. By using the doubly non-central beta distribution as the likelihood distribution, we
have achieved a level of flexibility that is required for general bounded support data.
To model the observed data using latent factor representations, we proposed two methods:
one based on the CP decomposition and one based on the Tucker decomposition. The Tucker
decomposition is more flexible than the CP decomposition because it allows the number of
latent feature factors to be independently determined from the number of sample factors.
CP decomposition and BG-NMF require these two dimensions to be the same.
The increased flexibility of the doubly non-central beta distribution and the Tucker
decomposition come at an apparent cost - the distributions in the corresponding statistical
model are not Bayesian conjugates and sampling may be computationally challenging. We
show that by using the augment-and-marginalize trick, we are able to find closed form Gibbs
sampling updates and mitigate the computational costs of the modeling choices.
We show that the model identifies informative latent factors in both DNA methylation
data and image data. In clinical and experimental applications, it is undesirable for a model
to be sensitive to hyper-parameters and DNCB-TD is empirically stable to hyper-parameter
18
Stable Dimensionality Reduction for Bounded Support Data
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Clusters
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Samples
Olivetti faces
0
1
2
3
4
Figure 11: The inferred matrix ˆΘ for the Olivetti faces data set. Each entry represents the
strength of association between sample i and sample cluster c.
changes. DNCB-TD has competitive held-out predictive performance as well, even though
the primary objective of the model is to learn informative low-dimensional representations of
the data.
Methylation patterns have the potential to improve our capability to use circulating
cell-free DNA to monitor for cancer recurrence and localize metastatic events for further
investigation. The models proposed in this work could be used to identify informative
biomarkers for those applications. Indeed, matrix-valued data with bounded support are
ubiquitous and there are many other applications of models for such data beyond clinical
applications.
Acknowledgments
P.F. and A.N.A. acknowledge funding from NSF 1934846 and NIH 1R01GM135931-01.
19
Albert et. al.
Appendix A.
Real Data Analysis: Bisulfite sequencing methylation data
We fit DNCB-TD with C = 3, K = 8 to a matrix of 188 bisulfite-sequenced samples, of which
156 were Ewing sarcoma cancer samples and 32 were healthy. We ran 2,000 iterations of
MCMC inference. The resulting inferred matrices ˆΘ, ˆΦ, ˆΠ are visualized in Figures 16, ??,and
17. DNCB-TD is able to distinguish the Ewing sarcoma cell lines and tumors, and groups
together the healthy and Ewing mesenchymal stem cells (MSCs).
1
2
3
Clusters
0
9
18
27
36
45
54
63
72
81
90
99
108
117
126
135
144
153
162
171
180
Samples
Samples 0-16: Ewing cell lines
Samples 16-156: Ewing tumors
Samples 158-178: Healthy MSCs
Samples 179-189: Ewing MSCs
Bisulfite sequencing methylation data
0.0
0.2
0.4
0.6
0.8
1.0
Figure 12: The inferred matrix ˆΘ for bisulfite methylation data. Rows are samples and
columns are clusters—each entry represents the strength of association between sample i
and cluster c.
20
Stable Dimensionality Reduction for Bounded Support Data
1
2
3
4
5
6
7
8
Pathways
Loci
0.0
0.2
0.4
0.6
0.8
Figure 13: The inferred matrix ˆΦ for bisufite methylation data. Rows are genes and columns
are pathways—each entry represents the strength of association between pathway k and gene
j. Row indices are sorted according to pathway association.
21
Albert et. al.
1
2
3
4
5
6
7
8
Pathways
1
2
3
Clusters
0.0
0.2
0.4
0.6
0.8
1.0
Figure 14: The inferred core matrix ˆΠ(1) for bisulfite methylation data. Each entry represents
the strength of association between samples in cluster c and genes in pathway k.
22
Stable Dimensionality Reduction for Bounded Support Data
References
Donald E Amos. Computation of modified Bessel functions and their ratios. Mathematics of
Computation, 28(125):239–251, 1974.
James Bennett, Stan Lanning, et al. The netflix prize. In Proceedings of KDD cup and
workshop, volume 2007, page 35. Citeseer, 2007.
Christoph Bock. Analysing and interpreting DNA methylation data. Nature Reviews Genetics,
13(10):705–719, 2012.
Herbert Buchholz. The confluent hypergeometric function: with special emphasis on its
applications, volume 15. Springer Science & Business Media, 2013.
Yaping Cao, Guodong Zhao, Mufa Yuan, Xiaoyu Liu, Yong Ma, Yang Cao, Bei Miao, Shuyan
Zhao, Danning Li, Shangmin Xiong, et al. Kcnq5 and c9orf50 methylation in stool DNA
for early detection of colorectal cancer. Frontiers in Oncology, 10, 2021.
A. T. Cemgil. Bayesian inference for nonnegative matrix factorisation models. Computational
Intelligence and Neuroscience, 2009.
M. Cisneros-Villanueva, L. Hidalgo-Perez, M. Rios-Romero, et al. Cell-free DNA analysis in
current cancer clinical trials: a review. British Journal of Cancer, 126:391–400, 2022.
Andrej Čopar, Blaž Zupan, et al. Scalable non-negative matrix tri-factorization. BioData
mining, 10(1):1–16, 2017.
Luc Devroye. Simulating Bessel random variables. Statistics & probability letters, 57(3):
249–257, 2002.
Pan Du, Xiao Zhang, Chiang-Ching Huang, Nadareh Jafari, Warren A. Kibbe, Lifang Hou,
and Simon M. Lin. Comparison of Beta-value and M-value methods for quantifying
methylation levels by microarray analysis. BMC Bioinformatics, 11, 2010.
Walter Gautschi. The incomplete gamma functions since tricomi. In In Tricomi’s Ideas
and Contemporary Applied Mathematics, Atti dei Convegni Lincei, n. 147, Accademia
Nazionale dei Lincei. Citeseer, 1998.
Andrew Gelman, Jessica Hwang, and Aki Vehtari. Understanding predictive information
criteria for bayesian models. Statistics and Computing, 24, 2014.
Amparo Gil, D Ruiz-Antolín, Javier Segura, and NM Temme. Computation of the incomplete
gamma function for negative values of the argument. arXiv preprint arXiv:1608.04152,
2016.
P. Gopalan, J. Hofman, and D. Blei. Scalable recommendation with Poisson factorization. In
Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, 2015a.
P. Gopalan, J.M. Hofman, and D.M. Blei. Scalable recommendation with hierarchical poisson
factorization. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial
Intelligence, pages 326–335, 2015b.
23
Albert et. al.
Lars Grasedyck. Hierarchical singular value decomposition of tensors. SIAM journal on
matrix analysis and applications, 31(4):2029–2054, 2010.
Peter D Hoff. Bilinear mixed-effects models for dyadic data. Journal of the american
Statistical association, 100(469):286–295, 2005.
Norman L Johnson, Samuel Kotz, and Narayanaswamy Balakrishnan. Continuous Univariate
Distributions, Volume 2, volume 2. John wiley & sons, 1995.
Yong-Deok Kim and Seungjin Choi. Nonnegative Tucker decomposition. In 2007 IEEE
conference on computer vision and pattern recognition, pages 1–8. IEEE, 2007.
J. F. C. Kingman. Poisson Processes. Oxford University Press, 1972.
Tamara G. Kolda and Brett W. Bader. Tensor Decompositions and Applications. SIAM
Review, 51(3):455–500, August 2009. ISSN 0036-1445, 1095-7200. doi: 10.1137/07070111X.
Peter W Laird. The power and the promise of DNA methylation markers. Nature Reviews
Cancer, 3(4):253–266, 2003.
Peter W Laird. Principles and challenges of genome-wide DNA methylation analysis. Nature
Reviews Genetics, 11(3):191–203, 2010.
Billy Lau, Alison Almeda, Marie Schauer, et al. Single-molecule methylation profiles of
cell-free DNA in cancer with nanopore sequencing. Genome Medicine, 15, 2023.
Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401(6755):788–791, 1999.
Tao Li, Yi Zhang, and Vikas Sindhwani. A non-negative matrix tri-factorization approach
to sentiment classification with lexical prior knowledge.
In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages
244–252. Association for Computational Linguistics, 2009.
Pei-Ching Lin, Jen-Kou Lin, Chien-Hsing Lin, Hung-Hsin Lin, Shung-Haur Yang, Jeng-Kai
Jiang, Wei-Shone Chen, Chih-Chi Chou, Shih-Feng Tsai, Shih-Ching Chang, et al. Clinical
relevance of plasma DNA methylation in colorectal cancer patients identified by using a
genome-wide high-resolution array. Annals of Surgical Oncology, 22:1419–1427, 2014.
Netanel Loyfer, Judith Magenheim, Ayelet Peretz, et al. A DNA methylation atlas of normal
human cell types. Nature, 613:355–364, 2023.
Eugene Lukacs. A characterization of the gamma distribution. The Annals of Mathematical
Statistics, 26(2):319–324, 1955.
Zhanyu Ma, Andrew E Teschendorff, Hong Yu, Jalil Taghia, and Jun Guo. Comparisons
of non-gaussian statistical models in DNA methylation analysis. International journal of
molecular sciences, 15(6):10835–10854, 2014.
24
Stable Dimensionality Reduction for Bounded Support Data
Zhanyu Ma, Andrew E Teschendorff, Arne Leijon, Yuanyuan Qiao, Honggang Zhang, and
Jun Guo. Variational bayesian matrix factorization for bounded support data. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 37(4):876–889, 2015.
Osman Asif Malik and Stephen Becker. Low-rank tucker decomposition of large tensors using
tensorsketch. Advances in neural information processing systems, 31, 2018.
Rachel Minster, Arvind K Saibaba, and Misha E Kilmer. Randomized algorithms for low-
rank tensor decompositions in the tucker format. SIAM Journal on Mathematics of Data
Science, 2(1):189–215, 2020.
The Cancer Genome Atlas Research Network, John N. Weinstein, Eric A. Collison, Gordon B.
Mills, Kenna R. Mills Shaw, Brad Ozenberger, et al. The cancer genome atlas pan-cancer
analysis project. Nature Genetics, 45:1113–1120, 2013.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. Factorizing yago: scalable machine
learning for linked data. In Proceedings of the 21st international conference on World Wide
Web, pages 271–280. ACM, 2012.
Andrea Ongaro and Carlo Orsi. Some results on non-central beta distributions. Statistica,
75, 2015.
Carlo Orsi. New insights into non-central beta distributions. arXiv preprint arXiv:1706.08557,
2017.
Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33
(5):2295–2317, 2011.
Sunho Park, Nabhonil Kar, Jae-Ho Cheong, and Tae Hyun Hwang. Bayesian semi-nonnegative
matrix tri-factorization to identify pathways associated with cancer phenotypes.
In
PACIFIC SYMPOSIUM ON BIOCOMPUTING 2020, pages 427–438. World Scientific,
2019.
Annie H. Ren, Panagiota S. Filippou, Antoninus Soosaipillai, Lampros Dimitrakopoulos,
Dimitrios Korbakis, Felix Leung, Vathany Kulasingam, Marcus Q. Bernardini, and Eleft-
herios P. Diamandis. Mucin 13 (MUC13) as a candidate biomarker for ovarian cancer
detection: potential to complement ca125 in detecting non-serous subtypes. Clinical
Chemistry and Laboratory Medicine, 61:464–472, 2023.
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear
embedding. science, 290(5500):2323–2326, 2000.
FS Samaria and AC Harter. Parameterisation of a stochastic model for human face identifi-
cation, 1994. URL https://cam-orl.co.uk/facedatabase.html.
Aaron Schein, Mingyuan Zhou, David M Blei, and Hanna Wallach. Bayesian Poisson Tucker
decomposition for learning the structure of international relations. In Proceedings of the
33rd International Conference on Machine Learning, New York, NY, USA, 2016.
25
Albert et. al.
Aaron Schein, Anjali Nagulpally, Hanna Wallach, and Patrick Flaherty. Doubly non-central
beta matrix factorization for dna methylation data. In Proceedings of Machine Learning
Research, pages 1895–1904. UAI, 2021.
Adib Shafi, Cristina Mitrea, Tin Nguyen, and Sorin Draghici. A survey of the approaches for
identifying differential methylation using bisulfite sequencing data. Briefings in bioinfor-
matics, 19(5):737–753, 2018.
Nathan C Sheffield, Gaelle Pierron, Johanna Klughammer, Paul Datlinger, Andreas Schöneg-
ger, Michael Schuster, Johanna Hadler, Didier Surdez, Delphine Guillemot, Eve Lapouble,
et al. DNA methylation heterogeneity defines a disease spectrum in Ewing sarcoma. Nature
medicine, 23(3):386–395, 2017.
RGD Steel. Relation between Poisson and multinomial distributions. Biometrics Unit
Technical Reports, pages 1–2, 1953.
L. R. Tucker. The extension of factor analysis to three-dimensional matrices. In N. Frederiksen
and H. Gulliksen, editors, Contributions to Mathematical Psychology. Holt, Rinehart and
Winston, 1964.
Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika,
31(3):279–311, 1966.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of
machine learning research, 9(11), 2008.
Yu-Xiong Wang and Yu-Jin Zhang. Nonnegative matrix factorization: A comprehensive
review. IEEE Transactions on knowledge and data engineering, 25(6):1336–1353, 2012.
WolframAlpha. Wolfram Alpha. Solution to infinite sum. Accessed February 23, 2017.,
2017. URL http://www.wolframalpha.com/input/?i=sum+n%3D0+to+infinity+z%5En%
2Fn!*(b%2Bp*n)%2F(2b%2Bn). [Online; accessed 23-February-2017].
Hui Xu, Xi-Long Zhao, Xue Liu, Xu-Gang Hu, et al. Elevated ASCL2 expression in breast
cancer is associated with the poor prognosis of patients. American Journal of Cancer
Research, 7:955–961, 2017.
Jiho Yoo and Seungjin Choi. Probabilistic matrix tri-factorization. In 2009 IEEE International
Conference on Acoustics, Speech and Signal Processing, pages 1553–1556. IEEE, 2009.
Bin Yu and Karl Kumbier. Veridical data science. PNAS, 117(8):3920–3929, 2020.
Lin Yuan and John D Kalbfleisch. On the Bessel distribution and related problems. Annals
of the Institute of Statistical Mathematics, 52(3):438–447, 2000.
Guoxu Zhou, Andrzej Cichocki, Qibin Zhao, and Shengli Xie. Efficient nonnegative tucker
decompositions: Algorithms and uniqueness. IEEE Transactions on Image Processing, 24
(12):4990–5003, 2015a.
Mingyuan Zhou and Lawrence Carin. Negative binomial process count and mixture modeling.
2012.
26
Stable Dimensionality Reduction for Bounded Support Data
Mingyuan Zhou, Yulai Cong, and Bo Chen.
Gamma belief networks.
arXiv preprint
arXiv:1512.03081, 2015b.
Appendix A. Proofs
Conditioned on the local latent variables y(1)
ij , y(2)
ij , the complete likelihood is
βij ∼Beta

b0 + y(1)
ij , b0 + y(2)
ij

. The expectation of βij under the complete likelihood is:
E
h
βij | b0, y(1)
ij , y(2)
ij
i
=
b0 + y(1)
ij
2b0 + y(1)
ij + y(2)
ij
(22)
From here on, we’ll refer to this as E
h
βij | y(1)
ij , y(2)
ij
i
, leaving implicit the conditioning on b0.
We will refer to the expectation of βij with the local latent variables integrated out
simply as E[βij].
This leaves implicit the conditioning on b0, ζ, and ρij—i.e., E[βij] ≡
E [βij | b0, ζ, ρij]. In general, we will leave implicit the conditioning on all model parameters
other than y(1)
ij , y(2)
ij
(about which we will be explicit). By Law of Total Expectation, we
have:
E [βij] = Ey(1)
ij ,y(2)
ij
h
E
h
βij | y(1)
ij , y(2)
ij
i i
(23)
= Ey(1)
ij ,y(2)
ij
"
b0 + y(1)
ij
2b0 + y(1)
ij + y(2)
ij
#
(24)
We can then re-represent this in terms of y(·)
ij ≜y(1)
ij + y(2)
ij :
= Ey(1)
ij ,y(·)
ij
"
b0 + y(1)
ij
2b0 + y(·)
ij
#
(25)
Then again by Law of Total Expectation and linearity of expectation:
= Ey(·)
ij
"
Ey(1)
ij
"
b0 + y(1)
ij
2b0 + y(·)
ij
| y(·)
ij
##
(26)
= Ey(·)
ij


b0 + Ey(1)
ij
h
y(1)
ij |y(·)
ij
i
2b0 + y(·)
ij


(27)
Since y(1)
ij ∼Pois(ζ ρij) we know that y(1)
ij |y(·)
ij ∼Binom(y(·)
ij , ρij). Thus, the expectation is:
Ey(1)
ij
h
y(1)
ij |y(·)
ij
i
= ρij y(·)
ij , where recall that ρij = θi Π ϕT
j is computed from global variables
only. Plugging this in we get:
= Ey(·)
ij
"
b0 + ρij y(·)
ij
2b0 + y(·)
ij
#
(28)
27
Albert et. al.
Lemma 3 E

βij

has analytic closed form, equal to:
E

βij

= 0.5 M(1, 2b0+1, −ζ) + ρij ζ
2b0+1M(1, 2b0+2,−ζ)
where M(a, b, c) is Kummer’s confluent hypergeometric function (Buchholz, 2013).
Proof: As established above, we have:
E[βij] = Ey(·)
ij
"
b0 + ρij y(·)
ij
2b0 + y(·)
ij
#
(29)
Plugging in the Poisson probability mass function into the expectation, we get:
=
∞
X
n=0
P

y(·)
ij =n |−
 b0 + ρij n
2b0 + n

(30)
=
∞
X
n=0
Pois (n; ζ)
b0 + ρij n
2b0 + n

(31)
=
∞
X
n=0
ζn e−ζ
n!
b0 + ρij n
2b0 + n

(32)
= e−ζ
∞
X
n=0
ζn
n!
b0 + ρij n
2b0 + n

(33)
The infinite sum converges and has an analytic solution (WolframAlpha, 2017):
= e−ζ(−ζ)−2b0
 
b0

Γ(2b0) −Γ(2b0, −γ)

−ρij

Γ(2b0 + 1) −Γ(2b0 + 1, −γ)
!
(34)
where Γ(s, x)=
R ∞
x dt ts−1e−t is the upper incomplete gamma function. The upper incomplete
gamma function is defined for negative values of its second argument; however almost all
implementations suport only positive arguments. As shown by Gil et al. (2016), the upper
incomplete gamma function with a negative second argument can be defined as:
Γ(s, x) = Γ(s)

1 −xs γ∗(s, x)

(35)
where γ∗(s, x)= x−s
Γ(s)γ(s, x) is Tricomi’s incomplete gamma function (Gautschi, 1998), itself
defined in terms of the lower incomplete gamma function γ(s, x) =
R x
0 dt ts−1e−t.
The
Tricomi’s incomplete gamma function can then be defined as:
γ∗(s, x) = e−xM(1, s + 1, x)
Γ(s + 1)
(36)
where M(a, b, c) is Kummer’s confluent hypergeometric function (Buchholz, 2013). We can
therefore express the upper incomplete gamma function in terms of Kummer’s confluent
hypergeometric function, for which there are stable implementations that support negative
arguments:
Γ(s, x) = Γ(s)

1 −xs e−xM(1, s + 1, x)
Γ(s + 1)

(37)
28
Stable Dimensionality Reduction for Bounded Support Data
Using this identity we will re-express equation 34 in terms of confluent hypergeometric
functions. First, we rewrite the term b0

Γ(2b0) −Γ(2b0, −γ)

:
b0

Γ(2b0) −Γ(2b0, −γ)

= b0

Γ(2b0) −Γ(2b0)

1 −(−ζ)2b0 eζM(1, 2b0 + 1, −ζ)
Γ(2b0 + 1)

(38)
= b0 Γ(2b0)

1 −

1 −(−ζ)2b0 eζM(1, 2b0 + 1, −ζ)
Γ(2b0 + 1)

(39)
= b0 Γ(2b0)

(−ζ)2b0 eζM(1, 2b0 + 1, −ζ)
Γ(2b0 + 1)

(40)
= b0 Γ(2b0)
Γ(2b0 + 1) (−ζ)2b0 eζM(1, 2b0 + 1, −ζ)
(41)
= 0.5 (−ζ)2b0 eζM(1, 2b0 + 1, −ζ)
(42)
Next we rewrite the following term (using nearly identical steps):
ρij

Γ(2b0 + 1) −Γ(2b0 + 1, −γ)

= ρij Γ(2b0 + 1)
Γ(2b0 + 2)
(−ζ)2b0+1 eζM(1, 2b0 + 2, −ζ)
(43)
=
ρij
2b0 + 1 (−ζ)2b0+1 eζM(1, 2b0 + 2, −ζ)
(44)
Plugging these two expressions back into equation 34 and then canceling some terms, we get:
E[β] = Ey(·)
ij
"
b0 + ρij y(·)
ij
2b0 + y(·)
ij
#
= 0.5 M(1, 2b0 + 1, −ζ) + ρij
ζ
2b0 + 1 M(1, 2b0 + 2, −ζ)
(45)
which is in terms of two confluent hypergeometric functions that can be computed stably
with a negative third argument in standard code libraries.
■
Lemma 4 For any b0 > 0 and ζ > 0, the following holds:
M(1, 2b0+1, −ζ) +
ζ
2b0+1M(1, 2b0+2,−ζ) = 1.
Thus, E[β] is a linear function of ρij that can be written as:
E[β] = 0.5 qb,ζ −ρij(1 −qb,ζ)
where we define qb,ζ ≜M(1, 2b0+1, −ζ).
Proof: The confluent hypergeometric function can be expressed as an infinite sum:
M(a, b, c) =
∞
X
n=0
a(n)zn
b(n)n!
(46)
29
Albert et. al.
where x(n) denotes a rising factorial, equivalent to x(n) = Γ(x+n)
Γ(x) . When a = 1 (as in our
case), the rising factorial a(n) in the numerator equals n! and cancels with the n! term in the
denominator. If we also rewrite b(n) explicitly as a ratio of gamma functions, we get:
M(1, b, c) =
∞
X
n=0
znΓ(b)
Γ(b + n)
(47)
Proving the lemma reduces to showing that M(1, b, c)−c
b M(1, b+1, c) = 1, where b = 2b0 +1
and c = −ζ. To show this, we represent the hypergeometric functions as infinite sums:
M(1, b, c) −c
b M(1, b + 1, c) =
∞
X
n=0
cnΓ(b)
Γ(b + n) −c
b
∞
X
n=0
cnΓ(b + 1)
Γ(b + 1 + n)
(48)
=
∞
X
n=0
cnΓ(b)
Γ(b + n) −
∞
X
n=0
cn+1Γ(b)
Γ(b + 1 + n)
(49)
=
∞
X
n=0
cnΓ(b)
Γ(b + n) −
∞
X
n=1
cnΓ(b)
Γ(b + n)
(50)
= 1 +
∞
X
n=1
cnΓ(b)
Γ(b + n) −
∞
X
n=1
cnΓ(b)
Γ(b + n)
(51)
= 1
(52)
■
Theorem 5 The model expectation of βij, with the local latent variables y(1)
ij , y(2)
ij marginalized
out, approaches ρij for small b0 and large ζ: lim
b0→0 lim
ζ→∞E

βij

→ρij.
Proof: With the result of Lemma 2, we can rewrite the expression for the expectation
obtained in Lemma 1 simply as:
E[β] = 0.5 qb,ζ + ρij (1−qb,ζ)
where qb,ζ = M(1, 2b0 + 1, −ζ). From this it’s easy to see that the expectation becomes ρij
as qb,ζ goes to 0:
lim
qb,ζ→0E[β] →ρij. Thus the proof reduces to showing:
lim
b0→0 lim
ζ→∞M(1, 2b0 + 1, −ζ) →0
To show this, we first apply Kummer’s transformation M(a, b, c) = ecM(b −a, b, −c):
lim
b0→0 lim
ζ→∞M(1, 2b0 + 1, −ζ) = lim
b0→0 lim
ζ→∞e−ζ M(2b0, 2b0 + 1, ζ)
(53)
We may then appeal to a limiting form of the hypergeometric function, M(a, b, c) for when
c →∞: M(a, b, c) ∼ecca−b/Γ(a). Applying this transformation, we get:
= lim
b0→0 lim
ζ→∞e−ζ eζ ζ2b0−(2b0+1)
Γ(2b0)
(54)
= lim
b0→0 lim
ζ→∞ζ−1
1
Γ(2b0) = 0
(55)
■
30
Stable Dimensionality Reduction for Bounded Support Data
Appendix B. DNA Methylation Sequencing Data Analysis
Figure 15: Prior Predictive Check. Density plot of the observed [red] vs. simulated [blue]
distribution of beta values. The MSE is approximately 0.241 and supports the idea that the
model can be considered to be generating the data.
1
2
3
Clusters
0
9
18
27
36
45
54
63
72
81
90
99
108
117
126
135
144
153
162
171
180
Samples
Samples 0-16: Ewing cell lines
Samples 16-156: Ewing tumors
Samples 158-178: Healthy MSCs
Samples 179-189: Ewing MSCs
Bisulfite sequencing methylation data
0.0
0.2
0.4
0.6
0.8
1.0
Figure 16: The inferred matrix ΘI×C. Rows are samples and columns are clusters—each
entry represents the strength of association between sample i and cluster c.
31
Albert et. al.
1
2
3
4
5
6
7
8
Pathways
1
2
3
Clusters
0.0
0.2
0.4
0.6
0.8
1.0
Figure 17: The inferred core matrix Π(1)
C×K. Each entry represents the strength of association
between samples in cluster c and genes in pathway k.
32
Stable Dimensionality Reduction for Bounded Support Data
1
2
3
4
5
6
7
8
Pathways
Loci
0.0
0.2
0.4
0.6
0.8
Figure 18: The inferred matrix ΦK×J. Rows are genes and columns are pathways—each
entry represents the strength of association between pathway k and gene j. Row indices are
sorted according to pathway association.
33
