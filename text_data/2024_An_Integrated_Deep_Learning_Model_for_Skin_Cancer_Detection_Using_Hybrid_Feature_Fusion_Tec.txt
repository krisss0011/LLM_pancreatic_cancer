An Integrated Deep Learning Model for Skin
Cancer Detection Using Hybrid Feature Fusion
Technique
Maksuda Akter Akter1, Rabea Khatun1,2, Md. Alamin Talukder3,
Md. Manowarul Islam1, Dr. Md. Ashraf
Uddin1*
1*Department of Computer Science and Engineering, Jagannath
University, 9-10 Chittaranjan Ave, Dhaka, 1100, Bangladesh.
2Department of Computer Science and Engineering, Green University of
Bangladesh, Purbachal American City, Kanchan, Rupganj,
Narayanganj, 1461, Dhaka, Bangladesh.
3Department of Computer Science and Engineering, International
University of Business Agriculture and Technology, Dhaka, Bangladesh.
*Corresponding author(s). E-mail(s): ashraf@cse.jnu.ac.bd;
Contributing authors: maksudaoni6@gmail.com;
rabeakhatun650@gmail.com; alamin.cse@iubat.edu;
manowar@cse.jnu.ac.bd;
Abstract
Skin cancer is a serious and potentially fatal disease caused by DNA damage.
Early detection significantly increases survival rates, making accurate diagno-
sis crucial. In this groundbreaking study, we present a hybrid framework based
on Deep Learning (DL) that achieves precise classification of benign and malig-
nant skin lesions. Our approach begins with dataset preprocessing to enhance
classification accuracy, followed by training two separate pre-trained DL mod-
els, InceptionV3 and DenseNet121. By fusing the results of each model using the
weighted sum rule, our system achieves exceptional accuracy rates. Specifically,
we achieve a 92.27% detection accuracy rate, 92.33% sensitivity, 92.22% speci-
ficity, 90.81% precision, and 91.57% F1-score, outperforming existing models and
demonstrating the robustness and trustworthiness of our hybrid approach. Our
study represents a significant advance in skin cancer diagnosis and provides a
promising foundation for further research in the field. With the potential to save
1
arXiv:2410.14489v2  [eess.IV]  29 Oct 2024
countless lives through earlier detection, our hybrid deep-learning approach is a
game-changer in the fight against skin cancer.
Keywords: Skin cancer, InceptionV3, Densenet121, Feature Fusion, Hybrid Deep
Learning, Transfer Learning.
1 Introduction
Cancer happens to be one of the most lethal diseases that may occur in humans. It
is the world’s second leading cause of death [1]. There are many different kinds of
cancer in the human body, and skin cancer is one of the most frequent, lethal, and
rapidly spreading malignancies that can lead to death. Skin cancer develops from
abnormal growth of cells that make up skin, which shields the body from heat, light
and infection [2]. It is caused by factors such as genetic factors, alcohol consumption,
smoking, infections, allergies, viruses, environmental change, physical exercise, expo-
sure to ultraviolet light, and so on. Fungal growth and bacteria are the most likely
reasons. In addition, Skin cancer is also caused by unusual swellings of the human
body [3].
There are various kinds of skin cancer, and among them, melanoma is the most
widespread and deadliest type of skin cancer. Melanoma evolves from maliciously
transformed skin melanocytes. Melanoma cells are known to spread to other areas of
the body, including the liver, lungs, spleen, or brain [2, 4]. Melanocytes are responsible
for the production of dark pigments on the hair, skin, eyes, and different areas of the
body. Melanoma lesions tend to be black or brown in color. But in a few instances, it
is also found in different colors such as azure, royal purple, rosy pink, or even colorless
[5].
According to the World Health Organization (WHO), cancer is one of the leading
causes of death worldwide and is responsible for about 10 million deaths annually [6]. It
is responsible for about 1 in every 6 deaths worldwide. Between 2008 and 2030, global
cancer-related deaths are expected to go up by 45% [7]. According to the WHO, one
out of every three people in the globe has skin cancer. Melanoma and non-melanoma
are the most typical cancers about 300,000 new cases were found in 2018. Melanoma
cost the lives of an estimated 2490 females and 4740 males in 2019[5].
Deep learning (DL) is the method by which a computer model directly learns to
perform categorization tasks from images, text, or sound [8–11]. Multi-layered neu-
ral network architectures and labeled data are used to train models. DL lessens the
requirement for some of the data preparation that machine learning generally demands
[12, 13]. Unstructured data like text and images can be processed by it. Gradient
descent and backpropagation are used to update and tune the DL algorithm for accu-
racy, enabling it to make predictions with greater precision. DL capabilities have had a
significant positive impact on the healthcare sector. It has several uses in various areas
of medical imaging because of its high level of precision. By using these techniques,
the diagnosing procedure is expedited and human error is decreased.
2
The most cutting-edge methods are without a doubt DL algorithms, which have
lately achieved remarkable classification and segmentation results not only for medi-
cal images but also for others applications [14, 15]. So, the primary reason to create
the proposed hybrid DL system is to use it as a diagnostic instrument to assist indi-
viduals who make decisions in health and medical facilities in quickly and accurately
identifying melanoma in dermatological images. This reduces the burden on patholo-
gists and hospitals at a time when the number of individuals infected with skin cancer
is increasing dramatically. The following are the primary contributions of this study:
• We proposed a hybrid DL model that can accurately classify skin cancer even in its
early stages utilizing the weighted sum rule at the score-level.
• The proposed hybrid framework has been developed by fusing the results of two dif-
ferent DL methods (e.g., InceptionV3 and Densenet121) that automatically extract
features from the dataset and correctly detect skin cancer.
• With a large dataset, our suggested method outperforms other existing DL models
in terms of accuracy, creating a reliable and practical system that can be applied in
a clinical environment to provide quick diagnoses and effective treatments.
2 Literature Review
Dermatological photos are used to identify skin cancer. This section provides a quick
overview of some recent studies on the application of DL techniques to the study of
medical data.
In order to detect skin cancer, Yuvika Gautam et al. [16] presented FusionEXNet,
a novel fused deep-learning model. In order to take use of their capacities for feature
extraction from dermoscopic images, the model combines two architectures: Xception-
Net and EfficientNetV2S. Approximately 10,000 high-resolution photos from seven
different kinds of skin lesions are included in the HAM10000 dataset, which was
used for training and evaluation. SmoothGrad and Faster Score-CAM, two Explain-
able Artificial Intelligence (XAI) approaches, were integrated into the model to
improve interpretability and provide insights into the model’s decision-making pro-
cess. FusionEXNet demonstrated superior performance in accuracy, achieving 90.83%,
compared to XceptionNet (88.82%) and EfficientNetV2S (88.01%).
In order to overcome the problems of light reflections, colour fluctuations, and
lesion diversity, Md Shahin Ali et al. [3] suggested a deep convolutional neural network
(DCNN) model for identifying benign and malignant skin lesions from dermoscopic
pictures. Using filters, preprocessing eliminates noise and artefacts from the input
images, normalises them, and applies data augmentation to grow the dataset size and
enhance classification accuracy. The model’s performance is contrasted with that of
various other transfer learning models, such as MobileNet, AlexNet, ResNet, VGG-16,
and DenseNet. The suggested DCNN model outperformed the other models in terms
of robustness and reliability using the HAM10000 dataset, with testing accuracy of
91.93% and training accuracy of 93.16%.
Rehan Ashraf et al.
[2] suggested an intelligent Region of Interest (ROI)-based
melanoma detection system to overcome the difficulties caused by nevi’s difficult-to-see
3
visual appearance and deep learning models’ restricted data availability. The method-
ology ensures that only pertinent melanoma features are incorporated for system
training by using an updated k-means algorithm to extract ROIs from dermoscopic pic-
tures. This ROI-based method concentrates on the areas that include melanoma cells,
which improves feature discrimination. Subsequently, a Convolutional Neural Network
(CNN) transfer learning model is trained on the ROI pictures from the DermIS and
DermQuest datasets using data augmentation approaches. The suggested system out-
performs current techniques that depend on comprehensive picture categorisation in
terms of classification accuracy.
In this study [17], dermoscopy images from the HAM10000 dataset were used
to create a proprietary Convolutional Neural Network (CNN) model for skin cancer
classification. The seven categories bearing and malignant that the CNN model was
built to identify from the photos are essential for early detection and lowering the
death rate from skin cancer. An Enhanced Super Resolution Generative Adversarial
Network (ESRGAN) was used to preprocess the dataset in order to improve image
quality by increasing the resolution of smaller sized images.
In this study [18], a novel hybrid dynamic Bayesian Deep Learning model is intro-
duced, utilizing Three-Way Decision (TWD) theory to adaptively apply different UQ
methods and neural networks in distinct classification phases. The model’s effective-
ness is validated on two skin cancer datasets, achieving: accuracy of 88.95% and
F1-score of 89.00% for first dataset and accuracy of 90.96% and F1-score of 91.00%
for second dataset.
This study [19] uses dermoscopic images to offer a two-stream deep neural network
information fusion framework for multiclass skin cancer classification. A pretrained
DenseNet201 model is fed with the improved image quality through a fusion based
contrast enhancement technique in the first stream of the methodology, which extracts
features. An approach called skewness controlled mothflame optimisation is used to
optimise these properties. A refined MobileNetV2 model’s features are taken out and
further downsampled in the second stream through the use of a feature selection
framework. A new parallel multimax coefficient correlation technique is employed to
fuse discriminant characteristics from both networks. To classify lesion images, a mul-
ticlass extreme learning machine classifier is used. Three unbalanced skin datasets
HAM10000, ISBI2018, and ISIC2019, respectively are used to evaluate the concept.
This article [7] introduces two novel hybrid CNN models that incorporate a support
vector machine (SVM) classifier at the output layer. The approach involves: Extracting
features from both CNN models. Concatenating these features and inputting them
into the SVM for final classification.The proposed models outperformed existing state-
of-the-art CNN models achieving accuracies of 88.02% and 87.43%.
This study [20] compared the diagnostic ability of dermatologists with a Faster
Region-based Convolutional Neural Network (FRCNN) trained to categorise clinical
photos of pigmented skin lesions. The dataset included 3551 patients’ 5846 photos,
with lesions classified into six classes: benign hematoma/hemangioma, nevus, sebor-
rhoeic keratosis, senile lentigo, and malignant melanoma. A test set including 666
photos was generated, and the remaining 4732 images were annotated with bounding
4
boxes to form the training set. In a six-class classification, the FRCNN model outper-
formed doctors with board certification (79.5%) and dermatology trainees (75.1%),
with an accuracy of 86.2%. FRCNN achieved 91.5% accuracy in binary classification
(benign vs. malignant), with 83.3% sensitivity and 94.5% specificity, in that order. In
comparison to dermatologists, it also showed a lower false positive rate of 5.5%. The
results show that the FRCNN model performed better in picture classification tasks
than 20 dermatologists, indicating that the public may benefit from using it in the
future to improve skin cancer prognosis.
The goal of this work [21] is to create deep learning models for the classification
of dermal cell pictures in order to detect skin cancer. Deep learning techniques are
used at the heart of the system to improve prediction accuracy through the use of a
model-driven architecture hosted in the cloud. Building these models and using them
to categorise dermal cells constitute the process; standard datasets are used for testing.
With this method, practitioners may quickly develop and use deep learning models
for early skin cancer identification.
This work [4] addressed the difficulties caused by the fine-grained heterogeneity in
diagnostic categories by developing a preprocessing pipeline for multiclass skin cancer
classification. Image hair removal, dataset enhancement, and image resizing to accom-
modate various models were all part of the pipeline. To fine-tune the EfficientNet
B0-B7 models for classification, transfer learning was used on pre-trained ImageNet
weights using the HAM10000 dataset. With an F1 Score of 87% and Top-1 Accuracy
of 87.91%, EfficientNet B4 had the best results when the models’ performance was
assessed using Precision, Recall, Accuracy, F1 Score, and Confusion Matrices. More
complicated models do not always perform better; models with intermediate com-
plexity, such EfficientNet B4 and B5, showed notable superiority over more complex
models.
This research [22] utilizes images from various publicly available ISIC datasets to
create a balanced dataset consisting of 10,500 images for training and testing. An
ensemble of four convolutional neural network (CNN) architectures-ResNet50, Effi-
cientNet B6, InceptionV3, and Xception was employed for melanoma classification.The
experimental results demonstrate that the proposed ensemble model effectively clas-
sifies melanoma skin cancer with high accuracy, outperforming many state-of-the-art
methods.
3 Methodology
The main principle of our idea is depicted in the block diagram in figure 1. This section
presents the framework, architecture, classification and performance evaluation of a
hybrid DL model for detecting skin cancer.
3.1 Dataset
Our dataset basically consists of two distinct categories of skin carcinoma images. DL
requires a significant amount of data in order to produce a good result. The collection
of skin cancer pictures, on the other hand, is critical. To address these issues, we used
a freely available dataset of 3297 dermoscopic images from Kaggle [23]. It includes
5
Fig. 1 The block diagram of the Hybrid model for Skin cancer detection.
1800 images of benign and 1497 images of malignant categorized moles. The data was
obtained via ISIC Archive. Each picture is 224x244 pixels in size. The professional
pathologist confirmed the database’s ground truth.
To prevent overfitting, the dataset is split between 80% training data and 20%
testing data that results in 2637 training images and 660 test images out of the total
number of images. 10% of the training set was arbitrarily selected during the learning
process and used as a validation set to criterion the generalizability of the Model
and save the weights set that produces the lowest level of an error on the validation
collection.
3.2 Image Preprocessing
The main goal of the image preprocessing process is to make the data suitable for
deep learning models and improve the initial medical images by getting rid of noise,
air bubbles, and artifacts caused by gel that was sprayed before the picture was taken.
To attain a high classifying rate, we removed artifacts and noise from the pictures.
The elimination of this noise and artefacts was crucial to guaranteeing high-quality
6
input images since they could mask important features required for classification. The
image was smoothed using noise reduction algorithms, which also removed any random
deviations that could confound the model. We employed feature scaling and turned
the categorical label data into number data[15]. In order to ensure that the input
data had consistent numerical ranges, we used feature scaling to normalise the pixel
intensity values, which can vary significantly in photos. This step prevents gradient
instability problems and helps models converge more quickly during training.
3.2.1 Class Labeling
We used label encoding techniques to transform the categorical data into numerical
format because the labels in medical picture datasets are typically categorical (e.g.,
different forms of skin cancer). In order to enable the deep learning model to process
and learn from the labels during training, each distinct category was given a numerical
value. The labeling data collection is presented as follows: the image’s label 0 indicates
benign patients, while 1 symbolizes malignant patients[24].
3.2.2
Feature scaling
The feature scaling method is well recognized in the field of machine learning and
pattern recognition for being used to normalize data. To avoid outliers, this approach
sets all data items to the same scale and thus improves prediction quality. As the
features in cancer datasets have a high variance, One of the pre-processing strategies
for normalization is feature scaling. We divided the grayscale value of an image by
255 to standardize our image pixels between 0 and 1. As a result, the numbers will be
relatively little, and the computation will be simpler and faster[25, 26].
3.3 Transfer Learning
Transfer Learning has emerged as a popular method in Deep Learning (DL) that
utilizes a pre-trained model for a similar task to train a new model. This approach
is particularly useful in medical imaging, where training on large datasets such as
ImageNet with all neural network parameters can be computationally intensive [6].
The ImageNet dataset, for instance, contains 14 million images with 20,000 categories
for visual recognition tasks [27].
In this study, we propose a model for skin lesion classification, which was trained
and evaluated using advanced Convolutional Neural Networks (CNNs) such as Incep-
tion V3, Densenet121, and a Hybrid model [3, 21]. To optimize our model, we replaced
the last softmax layer with a sigmoid activation, reducing the classification output to
two classes for identifying benign and malignant skin lesions.
The Inception V3 CNN model achieved remarkable accuracy of over 78.1% on the
ImageNet dataset and has been widely used for image classification tasks [25]. Its
architecture includes inception modules that use filters of different sizes on the same
input, and auxiliary classifiers for regularization weight loss ratio. The convolution
layer implements factorization to minimize dimensionality and avoid overfitting.
Another CNN architecture utilized in our model is DenseNet121, which resolves
the vanishing gradient problem by directly connecting every layer to every other layer.
7
Instead of summing feature maps from previous layers, DenseNet121 concatenates
them, resulting in a compact model with fewer parameters and enabling feature reuse
[28].
Overall, our proposed model incorporating Inception V3, Densenet121, and a
Hybrid model shows promising results for skin lesion classification and has the
potential to improve diagnostic accuracy in dermatology.
3.4 Proposed Model
After the preparation stages, the following section involves feeding data into the sug-
gested model. This section introduces the Inceptionv3, Densenet121 and Hybrid model
for skin cancer detection and classication.
3.4.1 Inception V3
InceptionV3 is a CNN-based DL model used for image classification. It is a highly
renowned image detection model that has been developed over time by a number of
researchers. On the ImageNet dataset, the model achieved more than 78.1% accuracy.
Google’s InceptionV3 is the third version in the DL Convolutional Architecture series.
The model was developed based on 1000 classes from the ImageNet dataset, which was
trained on approximately 1 million images. It is made up of inception modules that
apply filters of various sizes to the same amount of input. In the Inception network,
the auxiliary classifiers add to the regularization weight loss ratio. Factorization was
implemented in the convolution layer in order to minimize dimensionality and therefore
overfitting [25].
Fig. 2 InceptionV3 network structure of proposed model.
In this study, the weights of the pre-trained InceptionV3 model were used as
an initial step, followed by two fullyconnected layers. Then, It was fine-tuned using
Adam optimizer over the current training set. The main idea is that the pre-trained
InceptionV3 model has extensive knowledge of detecting various types of feature rep-
resentations (e.g., edges, curves, corners, etc.), and that fine-tuning its parameters will
allow the InceptionV3 model to quickly learn the specific feature representations of
the current task ,as shown in figure 2.
8
3.4.2 Densenet121
DenseNets resolve the vanishing gradient problem arising from traditional CNN. The
term ”Densely Connected Convolutional Network” refers to an architecture in which
every layer is directly connected to every other layer. Instead of summing the fea-
ture maps from the prior layers, they are concatenated and utilized as inputs in each
layer. Consequently, DenseNets need less parameters than typical CNNs, allowing for
the removal of duplicate feature maps and feature reuse. DenseNets are divided into
DenseBlocks, where the number of filters between them changes but the size of the
feature map within a block stays the same. DenseNets produce more compact models
and achieve state-of-the-art performance because they require fewer parameters and
enable feature reuse. In this study, weights of the pre-trained Densenet121 model the
were used as an initial step, followed by one convolutional layer and one global max-
pooling layer. Then, three fully connected layers were used and It was fine-tuned using
Adam optimizer over the current training set. The main idea is that the pre-trained
Densenet121 model has extensive knowledge of detecting various types of feature rep-
resentations (e.g., edges, curves, corners, etc.), and that fine-tuning its parameters will
allow the Densenet121 model to quickly learn the specific feature representations of
the current task, as shown in figure 3.
Fig. 3 Densenet121 network structure of proposed model.
3.5 Feature Fusion Technique
Feature fusion methods are computer programs that use scores from different models to
make a decision. A single scalar score is often generated as a result of this consolidation
process. It typically utilizes relatively easy fusion operators and doesn’t require a lot
of computing. It has been found that using outputs with similar performances when
combining different score-level fusion algorithms results best. A reasonably simple
score-level fusion technique that works directly with raw score data is the sum rule.
Sum rule fusion can be either a simple sum rule or a weighted sum rule. The weighted
sum rule is shown as follows:
fs = w1x1 + w2x2 + ... + wnxn
(1)
Where wi is the weight value assigned to the score value of the ith model. According to
the sensitivity analysis, the sum rule is the most durable method for error estimation.
9
It has been found that the Sum rule performs better than most other state-of-the-art
score-level fusion algorithms[29].
3.6 Feature Fusion Hybrid Model
In this study, we proposed the use of Densenet121 and InceptionV3 models for the
accurate classification of skin cancer. The previously trained InceptionV3 model was
utilized by employing its weighted values as an initial phase, which was then followed
by two fully connected layers. Subsequently, the model was fine-tuned using the Adam
optimizer on the present training set. The rationale behind this approach is based on
the premise that the pre-trained InceptionV3 architecture has a deep understanding
of detecting various feature representations, and by tuning its parameters, the model
can quickly discover the particular feature representations of the given task.
Similarly, for the Densenet121 model, we utilized the pre-trained weights as an
initial step, which was then followed by one convolutional layer and one global max-
pooling layer. To further enhance the model’s classification accuracy, we employed
three fully connected layers, which were fine-tuned on the present training set using
the Adam optimizer. The key insight behind this approach is that the pre-trained
Densenet121 model has in-depth expertise in identifying various kinds of feature repre-
sentations, and by changing its parameters, the model can swiftly learn the particular
feature representations of its present task.
In this paper, we introduced a feature fusion hybrid model for improved per-
formance and novel technique for skin cancer classification. Our proposed system
combines the output from two powerful models, InceptionV3 and Densenet121, to
make the final decision. For each input image, two anticipated probability ratings are
generated, and the input image is classified into either the benign or melanoma class
based on the highest probability score.
To combine the results from two models, we used the weighted sum rule, taking
into account the parallel architecture in the suggested system. This approach provides
dermatologists with a high level of confidence to make the final judgment and accu-
rately discriminate between cancer-infected and healthy patients. During the model
implementation, we assigned a slightly higher weight value to the Densenet121 model
than the InceptionV3 model, as it outperformed the latter in our experiments.
Our proposed hybrid framework significantly improves the accuracy of skin can-
cer classification when compared to using either the InceptionV3 or Densenet121
model alone. With our feature fusion approach, we aim to provide a powerful tool for
dermatologists to improve their diagnoses and ultimately improve patient outcomes.
Overall, our study demonstrates that leveraging pre-trained deep learning models
such as Densenet121 and InceptionV3 can significantly improve the accuracy of skin
cancer classification. By fine-tuning these models on the present training set, we were
able to achieve state-of-the-art performance, highlighting the potential of deep learning
in the field of medical image analysis. Table 1 highlights the parameters that are
amended in our proposed hybrid model.
10
Table 1 Hyperparameters used in proposed models
Hyperparameter
Description
MaxPooling2D layer
Shrink input pictures for faster learning.
Batch size
32 (photos processed per iteration)
Learning rate
0.0001 (rate of learning process start)
Optimizer
Adam (stochastic gradient descent replacement)
Loss function
Sparse categorical cross-entropy (binary classification)
Epochs
50 (number of passes through training dataset)
Classes
(2) Melanoma and benign
4 Result
4.1 Environment Setup
The model was implemented in Kaggle on a system with Intel Core i7-1165G7 CPU
processor, 8 GB of RAM, and NVIDIA GeForce MX330 GPU card.
4.2 Evaluation Metrics
The effectiveness of the suggested model is assessed by taking into account the eval-
uation metrics based on Accuracy Score, Precision, Recall, F1-measure, Specificity,
confusion matrix and ROC curve.
• Confusion Matrix: As illustrated in fig 4, the confusion matrix is a technique for
assessing performance in the form of a table that incorporates information about
both actual and expected classes. The dimension of the confusion matrix would be
nxn if the proposed problem to be examined has an n row, with the rows representing
the actual row and the columns representing the predicted row. For two or more
classes, the matrix depicts actual and anticipated values.
Fig. 4 The Confusion Matrix
The explanation of the terms of TP, FP, TN, FN are-
11
– The total number of correct outcomes or forecasts where the actual class was
positive is known as true positive (TP).
– The total number of incorrect results or forecasts while the actual class was
positive is known as false positive (FP).
– The total number of correct outcomes or predictions where the actual class was
negative is known as true negative (TN).
– The total number of incorrect outcomes or forecasts when the actual class was
negative is known as false negative (FN).
• Accuracy Score: Accuracy measures how properly the classifier anticipates the
classes [30]. The average number of samples is correctly classified by the classifier.
It is mathematically defined as:
Accuracy = Correctly Predicted Data
Total Testing Data
× 100% =
TP + TN
TP + TN + FP + FN × 100%
(2)
• Precision:
Precision is the proportion of all the model’s positive classifications
that are actually positive. It is mathematically defined as:
Precision = Correctly Classfied Actual Positive
Everything Classified as Positive =
TP
TP + FP
(3)
• Recall: Recall is also known as true Sensitivity, which is the percentage of all real
positives that were appropriately identified as positives. It is mathematically defined
as:
Recall = Correctly Classified Actual Positive
All Actual Positives
=
TP
FN + TP
(4)
• F1 measure :
The harmonic mean of precision and recall is F1. For the best
classification results, it should be close to one, while for the worst classifiers, it
should be close to zero. It is mathematically defined as:
F1 = 2 × Precision × Recall
Precision + Recall
(5)
• Specificity:
Specificity is the metric that evaluates a model’s ability to predict
true negatives of each available category. These metrics apply to any categorical
model. The equations for calculating this metrics are as follows-
Recall =
TN
FP + TN
(6)
• Receiver Operating Characteristic curve:
The Receiver Operating Charac-
teristic curve, or the ROC curve, is a valuable tool for forecasting the likelihood of
a binary result. It’s a graph that shows the false positive rate (x-axis) against the
true positive rate (y-axis) for a variety of options. When the actual outcome is pos-
itive, the true positive describes how well the model predicts the positive class. The
curve’s shape offers a wealth of information, including what we would care about
most for an issue, the expected false positive rate, and the expected false-negative
rate. To be precise about this:
12
– Lower false positives and higher true negatives are shown by lower values on the
x-axis of the plot.
– Greater true positives and smaller false negatives are shown by higher values on
the y-axis of the graphic.
4.3 Result analysis
The proposed hybrid model combines the InceptionV3 and DenseNet121 models.
Table 2 reports the quantitative performance evaluation of three deep learning mod-
els, namely InceptionV3, Densenet121, and Hybrid, in terms of precision, sensitivity,
specificity, F1-score, and accuracy. The results indicate that the Hybrid model out-
performs the other two models, achieving an accuracy of 92.27%, which is 1.67% and
1.67% higher than InceptionV3 (90%) and Densenet121 (90.6%), respectively. The
precision and sensitivity of all models are relatively high, with values above 89%.
Specifically, the precision of InceptionV3, Densenet121, and Hybrid is 89.6%, 89.7%,
and 90.8%, respectively, while their sensitivity is 89.6%, 90%, and 92.33%, respec-
tively. The Hybrid model also achieves the highest F1-score of 91.57% among the three
models, with InceptionV3 and Densenet121 obtaining F1-scores of 89.6% and 90.3%,
respectively. The specificity of all models is around 91.3%, with Hybrid achieving a
slightly higher value of 92.22%. Overall, the Hybrid model, fuse the results from three
deep learning models using the weighted sum rule, demonstrates superior performance
compared to the other two models in terms of accuracy, F1-score, and sensitivity.
Table 2 Performance analysis of our models
Model Name
Precision
Sensitivity
Specificity
F1 score
Accuracy
InceptionV3
89.6
89.6
91.3
89.6
90
Densenet121
89.7
90
91.3
90.3
90.6
Hybrid
90.80
92.33
92.22
91.57
92.27
Figures 5(a), 5(b), 5(c) and 5(d) display the performance curves (accuracy and
loss) of Densenet121 and InceptionV3 deep learning models during training. These
curves depict changes in accuracy and loss with increasing epochs. Performance curves
are crucial in assessing model effectiveness in classification tasks, aiding in identifying
overfitting or underfitting, and gauging the model’s performance improvement over
time.
The confusion matrix for our proposed skin cancer classification model is presented
in Fig 6. The rows correspond to the actual classes, while the columns represent the
predicted classes. The cells of the matrix show the number of instances that were
classified in each class. In our proposed model, 332 instances of benign skin cancer
were correctly classified as benign (true negatives), while 28 instances of benign skin
cancer were wrongly classified as malignant (false positives). Similarly, 27 instances
of malignant skin cancer were correctly classified as malignant (true positives), while
23 instances of malignant skin cancer were wrongly classified as benign (false nega-
tives). The results show that the proposed system shows great potential to reduce the
13
(a) Accuracy graph for Densenet121
(b) Loss graph for Densenet121
(c) Accuracy graph for InceptionV3
(d) Loss graph for InceptionV3
Fig. 5 Accuracy and Loss graph of Densenet121 and InceptionV3
Fig. 6 Confusion Matrix of testing data.
workload of dermatologists and assist in correctly identifying malignant skin lesions
in clinical practice.
Fig.7 displays the Receiver Operating Characteristic (ROC) curve, which was con-
structed by calculating the true positive ratio and false positive ratio for various
accuracy thresholds. It is evident from the graph that the Hybrid model proposed in
this study has achieved an impressive area under the ROC curve of 92.3%.
14
Fig. 7 Receiver Operating Characteristic curve of our proposed Hybrid model.
5 Discussion
Table 3 presents a comparison of five state-of-the-art models proposed for skin cancer
detection by different authors. M. S. Ali et al. [3] proposed a deep convolutional neural
network (DCNN) model, achieving an accuracy of 91.93% on the HAM10000 dataset.
K. Ali et al. [4] proposed an EfficientNetB4-based model, achieving an accuracy of
87.91% on the same dataset. D. Keerthan et al. [7] proposed an SVM-based model,
achieving an accuracy of 88.2% on the ISBI2016 dataset. M. Abdar et al. [18] proposed
a two-way deep belief network with deep learning (TWDBDL) model, achieving an
accuracy of 88.95% on the Kaggle dataset.
In comparison, the model proposed in this study, which is a hybrid approach,
achieved the highest accuracy of 92.27% on the ISIC dataset, outperforming all other
models. This model holds promise for clinical application in skin cancer detection.
Moreover, the performance of the recommended is sufficient for the diagnosis of
melanoma under various environmental scenarios.
Table 3 Comparison of Proposed Model.
Authors
Year
Methodology
Dataset
Accuracy
M. S. Ali et al. [3]
2021
DCNN
HAM10000
91.93%
K. Ali et al. [4]
2022
EfficientNetB4
HAM10000
87.91%
D. Keerthan et al. [7]
2023
SVM
ISBI2016
88.2%
M. Abdar et al. [18]
2021
TWDBDL
Kaggle
88.95%
H.-Y. Huang et al. [31]
2023
YOLOv5
ISIC
79.2%
Yuvika Gautam et al. [16]
2024
FusionEXNet
HAM10000
90.83%
Hatice Catal Reis et al. [32]
2024
MABSCNET & ViT
ISIC
78.63% & 76.50%
Proposed Model
2024
Hybrid
ISIC
92.27%
15
6 Conclusions
This research looked into the feasibility of developing a hybrid system for skin cancer
detection that uses deep learning techniques to provide accurate and real-time diag-
nostics. Data preparation enhanced the image quality and reduced noise levels. Both
the InceptionV3 and DenceNet121 were trained on the preprocessed images to improve
the accuracy performance. By using the sigmoid for the output activation layer, the
suggested technique can differentiate between malignant and benign skin lesions. Like-
wise, we tested the proposed approach on a dataset from Kaggle and found that it
performed better for training and testing accuracy. The proposed method has managed
to accomplish an acceptable level of efficiency with an accuracy of 92.27%, precision
of 90.81%, sensitivity of 92.33%, specificity of 92.22%, and F1-score of 91.57%. This
study may lessen the amount of pressure on decision-makers (such as clinicians and
therapists) as a result of the growing number of cancer patients in comparison to the
shortage of medical resources. In the future to achieve the most accurate prediction
and classification accuracy, we will employ more advanced pre-processing methods to
successfully build a model from a sizable dataset that has more identified skin lesions.
Declarations
• Funding : This research was supported by the National Science and Technology
for research fellowship of 2022-23.
• Conflict of interest/Competing interests: The authors declare no conflict of
interest.
• Ethics approval and consent to participate: Not Applicable
• Statement of human and animal rights: On behalf of all authors, the cor-
responding author affirms that human and animal rights were upheld in the
study.
• Consent statement: On behalf of all authors, the corresponding author states
that informed consent was obtained from all participants involved in the study.
• Data
availability:
The
selected
datasets
are
sourced
from
free
and
open-access
source:
https://www.kaggle.com/datasets/fanconic/
skin-cancer-malignant-vs-benign?select=train
• Materials availability: Not Applicable
• Code availability: Not Applicable
• Author contribution: Conceptualization: Maksuda Akter, Rabea Khatun, Md.
Manowarul Islam; Supervision: Md. Manowarul Islam; Methodology: Maksuda
Akter, Rabea Khatun, Md. Manowarul Islam,; Data curation: Maksuda Akter,
Rabea Khatun; Resources: Maksuda Akter, Rabea Khatun, Md. Manowarul Islam;
Software: Maksuda Akter, Rabea Khatun, Md. Manowarul Islam; Formal analysis,
Maksuda Akter, Rabea Khatun, Md. Manowarul Islam, Md. Alamin Talukder; Writ-
ing – original draft: Maksuda Akter, Rabea Khatun; Validation, Md. Manowarul
Islam, Dr. Md. Ashraf Uddin.
16
References
[1] Khatun, R., Akter, M., Islam, M.M., Uddin, M.A., Talukder, M.A., Kamruz-
zaman, J., Azad, A., Paul, B.K., Almoyad, M.A.A., Aryal, S., et al.: Cancer
classification utilizing voting classifier with ensemble feature selection method
and transcriptomic data. Genes 14(9), 1802 (2023)
[2] Ashraf, R., Afzal, S., Rehman, A.U., Gul, S., Baber, J., Bakhtyar, M., Mehmood,
I., Song, O.-Y., Maqsood, M.: Region-of-interest based transfer learning assisted
framework for skin cancer detection. IEEE Access 8, 147858–147871 (2020)
[3] Ali, M.S., Miah, M.S., Haque, J., Rahman, M.M., Islam, M.K.: An enhanced
technique of skin cancer classification using deep convolutional neural network
with transfer learning models. Machine Learning with Applications 5, 100036
(2021)
[4] Ali, K., Shaikh, Z.A., Khan, A.A., Laghari, A.A.: Multiclass skin cancer classifica-
tion using efficientnets–a first step towards preventing skin cancer. Neuroscience
Informatics 2(4), 100034 (2022)
[5] Zhang, N., Cai, Y.-X., Wang, Y.-Y., Tian, Y.-T., Wang, X.-L., Badami, B.:
Skin cancer diagnosis based on optimized convolutional neural network. Artificial
intelligence in medicine 102, 101756 (2020)
[6] Talukder, M.A., Islam, M.M., Uddin, M.A., Akhter, A., Hasan, K.F., Moni,
M.A.: Machine learning-based lung and colon cancer detection using deep feature
extraction and ensemble learning. Expert Systems with Applications 205, 117695
(2022)
[7] Keerthana, D., Venugopal, V., Nath, M.K., Mishra, M.: Hybrid convolutional
neural networks with svm classifier for classification of skin cancer. Biomedical
Engineering Advances 5, 100069 (2023)
[8] Talukder, M.A., Layek, M.A., Kazi, M., Uddin, M.A., Aryal, S.: Empowering
covid-19 detection: Optimizing performance through fine-tuned efficientnet deep
learning architecture. Computers in Biology and Medicine 168, 107789 (2024)
[9] Rasa, S.M., Islam, M.M., Talukder, M.A., Uddin, M.A., Khalid, M., Kazi, M.,
Kazi, M.Z.: Brain tumor classification using fine-tuned transfer learning models on
magnetic resonance imaging (mri) images. Digital health 10, 20552076241286140
(2024)
[10] Talukder, M.A., Islam, M.M., Uddin, M.A., Kazi, M., Khalid, M., Akhter, A.,
Ali Moni, M.: Toward reliable diabetes prediction: Innovations in data engineering
and machine learning applications. Digital Health 10, 20552076241271867 (2024)
17
[11] Islam, M.M., Talukder, M.A., Uddin, M.A., Akhter, A., Khalid, M.: Brain-
net: precision brain tumor classification with optimized efficientnet architecture.
International Journal of Intelligent Systems 2024(1), 3583612 (2024)
[12] Uddin, M.A., Talukder, M.A., Uzzaman, M.S., Debnath, C., Chanda, M., Paul,
S., Islam, M.M., Khraisat, A., Alazab, A., Aryal, S.: Deep learning-based human
activity recognition using cnn, convlstm, and lrcn. International Journal of
Cognitive Computing in Engineering 5, 259–268 (2024)
[13] Rana, M.M., Islam, M.M., Talukder, M.A., Uddin, M.A., Aryal, S., Alotaibi, N.,
Alyami, S.A., Hasan, K.F., Moni, M.A.: A robust and clinically applicable deep
learning model for early detection of alzheimer’s. IET Image Processing 17(14),
3959–3975 (2023)
[14] Kora, P., Ooi, C.P., Faust, O., Raghavendra, U., Gudigar, A., Chan, W.Y.,
Meenakshi, K., Swaraja, K., Plawiak, P., Acharya, U.R.: Transfer learning tech-
niques for medical image analysis: A review. Biocybernetics and Biomedical
Engineering 42(1), 79–107 (2022)
[15] Talukder, M.A., Hasan, K.F., Islam, M.M., Uddin, M.A., Akhter, A., Yousuf,
M.A., Alharbi, F., Moni, M.A.: A dependable hybrid machine learning model for
network intrusion detection. Journal of Information Security and Applications
72, 103405 (2023)
[16] Gautam, Y., Gupta, P., Kumar, D., Kalra, B., Kumar, A., Hemanth, J.D.:
Fusionexnet: an interpretable fused deep learning model for skin cancer detection.
International Journal of Computers and Applications, 1–11 (2024)
[17] Mukadam, S.B., Patil, H.Y.: Skin cancer classification framework using enhanced
super resolution generative adversarial network and custom convolutional neural
network. Applied Sciences 13(2), 1210 (2023)
[18] Abdar, M., Samami, M., Mahmoodabad, S.D., Doan, T., Mazoure, B., Hashemife-
sharaki, R., Liu, L., Khosravi, A., Acharya, U.R., Makarenkov, V., et al.:
Uncertainty quantification in skin cancer classification using three-way decision-
based bayesian deep learning. Computers in biology and medicine 135, 104418
(2021)
[19] Attique Khan, M., Sharif, M., Akram, T., Kadry, S., Hsu, C.-H.: A two-stream
deep neural network-based intelligent system for complex skin cancer types
classification. International Journal of Intelligent Systems 37(12), 10621–10649
(2022)
[20] Jinnai, S., Yamazaki, N., Hirano, Y., Sugawara, Y., Ohe, Y., Hamamoto, R.: The
development of a skin cancer classification system for pigmented skin lesions using
deep learning. Biomolecules 10(8), 1123 (2020)
18
[21] Kadampur, M.A., Al Riyaee, S.: Skin cancer detection: Applying a deep learning
based model driven architecture in the cloud for classifying dermal cell images.
Informatics in Medicine Unlocked 18, 100282 (2020)
[22] Javid, M.H., Jadoon, W., Ali, H., Ali, M.D.: Design and analysis of an improved
deep ensemble learning model for melanoma skin cancer classification. In: 2023 4th
International Conference on Advancements in Computational Sciences (ICACS),
pp. 1–6 (2023). IEEE
[23] Fanconi, C.: Skin Cancer: Malignant vs. Benign. /datasets/fanconic/skin-cancer-
malignant-vs-benign (2021)
[24] Ahmad, S., Ullah, T., Ahmad, I., Al-Sharabi, A., Ullah, K., Khan, R.A., Rasheed,
S., Ullah, I., Uddin, M., Ali, M., et al.: A novel hybrid deep learning model for
metastatic cancer detection. Computational Intelligence and Neuroscience 2022
(2022)
[25] Basavegowda, H.S., Dagnew, G.: Deep learning approach for microarray cancer
data classification. CAAI Transactions on Intelligence Technology 5(1), 22–33
(2020)
[26] Sun, J., Cao, X., Liang, H., Huang, W., Chen, Z., Li, Z.: New interpretations of
normalization methods in deep learning. In: Proceedings of the AAAI Conference
on Artificial Intelligence, vol. 34, pp. 5875–5882 (2020)
[27] Hossain, M.U., Rahman, M.A., Islam, M.M., Akhter, A., Uddin, M.A., Paul, B.K.:
Automatic driver distraction detection using deep convolutional neural networks.
Intelligent Systems with Applications 14, 200075 (2022)
[28] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected
convolutional networks. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4700–4708 (2017)
[29] Ezichi, S.I., Ezika, I.J., Nkpume, C., Iloanusi, O.N.: Biometric security: A review
of the sum rule and the likelihood ratio fusion algorithms for multibiometric
systems
[30] Fakoor, R., Ladhak, F., Nazi, A., Huber, M.: Using deep learning to enhance can-
cer diagnosis and classification. In: Proceedings of the International Conference
on Machine Learning, vol. 28, pp. 3937–3949 (2013). ACM New York, NY, USA
[31] Huang, H.-Y., Hsiao, Y.-P., Mukundan, A., Tsao, Y.-M., Chang, W.-Y., Wang,
H.-C.: Classification of skin cancer using novel hyperspectral imaging engineering
via yolov5. Journal of Clinical Medicine 12(3), 1134 (2023)
[32] Reis, H.C., Turk, V.: Fusion of transformer attention and cnn features for skin
cancer detection. Applied Soft Computing 164, 112013 (2024)
19
