Fully Automated CTC Detection, Segmentation
and Classification for Multi-Channel IF Imaging
Evan Schwab, Bharat Annaldas, Nisha Ramesh, Anna Lundberg, Vishal
Shelke, Xinran Xu, Cole Gilbertson, Jiyun Byun, Ernest T. Lam
Epic Sciences, San Diego, CA, USA
Abstract. Liquid biopsies (eg., blood draws) offer a less invasive and
non-localized alternative to tissue biopsies for monitoring the progression
of metastatic breast cancer (mBCa). Immunofluoresence (IF) microscopy
is a tool to image and analyze millions of blood cells in a patient sample.
By detecting and genetically sequencing circulating tumor cells (CTCs)
in the blood, personalized treatment plans are achievable for various can-
cer subtypes. However, CTCs are rare (about 1 in 2M), making manual
CTC detection very difficult. In addition, clinicians rely on quantita-
tive cellular biomarkers to manually classify CTCs. This requires prior
tasks of cell detection, segmentation and feature extraction. To assist
clinicians, we have developed a fully automated machine learning-based
production-level pipeline to efficiently detect, segment and classify CTCs
in multi-channel IF images. We achieve over 99% sensitivity and 97%
specificity on 9,533 cells from 15 mBCa patients. Our pipeline has been
successfully deployed on real mBCa patients, reducing a patient average
of 14M detected cells to only 335 CTC candidates for manual review.
Keywords: Metastatic Breast Cancer · IF Imaging · CTCs · Detection
1
Introduction
Liquid biopsies (eg., blood draws) offer a less invasive and non-localized alter-
native to tissue biopsies for a more continuous monitoring of metastatic breast
cancer (mBCa). Immunofluoresence (IF) microscopy is a tool to image and ana-
lyze millions of cells in a sample of blood. Circulating tumor cells (CTCs) in the
blood are indicative of metastasis and the combination of protein biomarker and
single-cell genetic analyses of CTCs provides clinicians with a comprehensive
cancer profile for mBCa patients from a single blood draw [16,18].
In mBCa patients, CTCs average 1 in every 2M cells [18], motivating the
need for machine learning (ML) to reduce the burden of manual classification by
automatically detecting and classifying CTCs. Deep learning has been developed
for classifying CTCs in multiple IF modalities [1,2,8,12,15,20,21]. However, these
methods fall short of providing a fully automated pipeline to detect and classify
CTCs to present for manual review. Furthermore, deep learning loses the utility
and interpretability of biomarker features, which clinicians already rely on for
manual classification.
arXiv:2410.02988v1  [cs.CV]  3 Oct 2024
2
E. Schwab et al.
Fig. 1: Example user interface of ML classified CTC candidates output by our
pipeline and presented to clinicians (bottom row thumbnails). One thumbnail is
selected for review and cell and nuclear MFI values reported per channel. The
yellow nuclear mask is overlayed on each channel and the color composite.
In this work, we have developed a fully automated ML-based production-level
pipeline to efficiently detect, segment, and classify mBCa CTCs in multi-channel
IF images to present to clinicians for final confirmation and genetic sequenc-
ing. Our automated pipeline, which we call BReast cancer Imaging Algorithm
(BRIA), combines image processing, deep learning and interpretable feature-
based ML. BRIA has been fully deployed at Epic Sciences within a comprehen-
sive clinical workflow that integrates patient sample collection, slide preparation,
fluorescence scanning, cloud-based analysis using the proposed pipeline, database
management, quality control, and reporting through a proprietary clinical viewer
that interacts with data sources through an API.
In Sec. 2 we provide a background on IF imaging and the clinical workflow.
Then, in Sec. 3 we present each component of the BRIA pipeline and share our
ground truth labeling and experimental results in Sec. 4.
2
Background
Widefield fluorescence microscopy is an IF imaging technique used to capture
2D images of blood samples collected on whole slides [10,19]. Fluorescent dyes
are applied in an assay system to capture specific biomarkers of interest and are
digitized in multiple imaging channels. At Epic Sciences, we have developed a
clinical diagnostic test for mBCa, called DefineMBC, which uses IF imaging as
one of the core components for protein biomarker expression analysis. The De-
fineMBC assay includes DAPI, CK, and CD45/31 channels which are designed to
visualize cell components and provide inclusionary and exclusionary biomarkers
for CTCs.1 The DAPI stain (4’,6-diamidino-2-phenylindole) is used to highlight
1 BRIA is agnostic to additional BCa biomarker channels like HER2 or ER.
Automated CTC Detection, Segmentation and Classification
3
the nucleus of the cell. The CK stain (Cytokeratin) is an indicator of CTCs and
CD45/312 is an indicator of non-CTCs like white blood cells. Therefore, high
CK and low CD45/31 values are indicative of CTCs (see Fig. 2 for examples).
These channel values are summarized using statistical features such as the
mean florescence intensity (MFI), ie. the average pixel intensity within an object
like the cell or nucleus, for each channel (e.g., Nuclear CK MFI). To compute
these features, each individual cell on a slide of roughly 3M cells must be detected
and cropped as individual thumbnail images. Then the cell and nucleus need to
be segmented to compute MFIs for each channel. In the absence of ML, the
standard practice to classify CTCs is to first apply biomarker thresholds (eg.
Nuclear CK MFI >269 and Nuclear CD45/31 MFI ≤3000, etc.) to filter 99% of
non-CTCs and artefacts in our data. However, validating biomarker thresholds
is a challenging task and clinicians are still left with 10’s of thousands of CTC
candidates per patient to sort through.
Fig. 2: Example CTCs, non-CTCs, and artefacts in 3-channel IF images with
a color composite. CTCs have prominent CK signal and low CD45/31. Some
Non-CTCs are visually similar which makes manual classification difficult.
Within this workflow, clinicians manually classify CTCs based on MFI val-
ues and the visual inspection of features like cell size, shape, texture and other
biomarkers. Because clinicians already rely heuristically on a list of qualitative
2 Cluster of differentiation (CD) 31 is a protein associated with white blood cells and
endothelial cells. CD 45 is a protein marker of leukocyte lineage.
4
E. Schwab et al.
cellular features, we are motivated to quantify these biomarkers and utilize in-
terpretable feature-based ML instead of deep learning to classify CTCs.
The BRIA pipeline assists clinicians by automating these steps of cell detec-
tion, nuclear and cell segmentation, feature extraction and CTC classification
to present CTC candidates to clinicians for final review. Figure 1 shows a user
interface of CTC candidates presented to clinicians for manual review. For this
slide example, the pipeline reduced 3M total cells to 48 CTC candidates instead
of what would be 10s of thousands of candidates to sort through.
3
Methods
We outline the main steps of the BRIA pipeline leading up to CTC classification
in Sec 3.1 cell detection, 3.2 nucleus segmentation, 3.3 cell segmentation, and
3.4 feature extraction. These steps are also required for manual classification in
the absence of an ML CTC classifier. See Figure 3 for a visual overview.
3.1
Cell Detection
The first step of BRIA is to efficiently detect each individual cell center and crop
thumbnail images for each cell. Whole slides are too large to image fully and so
a grid of 588 (14×42) fields of view (FOVs) of size 2040×2040 pixels are imaged
and processed in parallel and stitched together. We compared three classical cell
detection methods including watershed, radial symmetry [11] and the Laplacian
of Gaussian (LoG) [14]. These are applied to the DAPI channel which highlights
the nucleus of each cell (CTCs and non-CTCs alike).
For validation, the centroid coordinates of 24,110 cells from 5 FOVs were
manually identified. We measured performance based on the cell count output
by the detection algorithm as well as the average distance between the ground
truth and estimated centroids. With an exhaustive parameter search, the best
performing algorithm was the LoG with cell count F1 score of 0.997 and a 1.12
µm average distance between estimated and ground truth centroids. This is an
acceptable error compared to the average radius of a cell nucleus of about 5 µm
and whole cell size of up to 15 µm. Small 24×24 pixel thumbnail images are
cropped around the centroids based on the average size of nuclei. In total, our
cell detection algorithm takes 10 min to detect ∼3M cells from a single slide.
3.2
Nuclear Segmentation
Once a cell is detected, segmentation of the nucleus is important for extracting
features like nuclear MFI for each channel. Since the DAPI channel is already
designed to highlight nuclei, nuclear segmentation can be accomplished with
classical image processing. Given each thumbnail image, we compute the normal
vector field of each pixel to the center and the gradient vector field. These two
fields are multiplied using a Gaussian weighted dot product with a radius pa-
rameter output by the detection algorithm. This results in a transformed image
Automated CTC Detection, Segmentation and Classification
5
that isolates the cell of interest. We compared the watershed method and Otsu’s
method [7] applied the transformed image to segment the nucleus.
To evaluate performance, the nuclei of the same set of 24,110 cells that were
used for the cell detection ground truth were also manually segmented by 40 ex-
pert annotators. On a sample of 25 nuclei, we used the Simultaneous Truth and
Performance Level Estimation (STAPLE) [17] to measure the concordance be-
tween annotators and found 93% sensitivity for segmenting the nucleus correctly.
Watershed achieved F1=0.83 while Otsu’s method achieved F1=0.934 which is
a good performance in comparison to human annotators. With parallelization
the nuclear segmentation step takes an average of 10 min per slide.
Fig. 3: BRIA Pipeline Overview. After IF image acquisition and slide processing,
the main algorithmic steps include Cell Detection, Nuclear and Cell Segmenta-
tion, Feature Extraction, and CTC Classification.
3.3
Cell Segmentation
For whole cell segmentation we utilize all three IF channels to account for signal
variation across each channel for both CTCs and non-CTCs alike. To address the
scarcity of CTCs in our data, simulated montages of CTCs are created to increase
their occurrence by placing manually segmented CTCs at randomly generated
coordinates on a simulated background image based on real IF images.
We utilize a 3-channel U-Net [9] to segment cells within overlapping 512×512
image patches within each FOV. Overlap was enforced to avoid cutting cells at
the edge of a patch. To accelerate this process we use our cell detection output
to discard patches that do not contain any cells. The outputs of the U-Net are
pixel-level probability maps for three classes: cell, boundary, and background.
Including the boundary as a third class proved useful in separating cell clusters.
The probability maps for each overlapping patch are merged into a single FOV
by taking the maximum among the cell, boundary, and inverted background
6
E. Schwab et al.
probability maps. Then instance segmentation is performed using watershed to
identify the masks of each individual cell. These masks are mapped back to each
thumbnail using the cell centroids.
Training was performed on 22 patient slides and 9 CTC montages with a total
of 10, 443 manually segmented cells. The STAPLE algorithm was again used to
evaluate human level segmentation performance. We gave 10 annotators 1,206
cells (including 165 CTCs) to manually segment and their concordance level
was an F1=0.936. In comparison our U-Net achieved a comparable F1=0.933
averaged over 5-fold cross validation of our training set. The best performing
parameters were learning rate=0.0001, dropout=0.3, weighted classes, network
depth=4, epochs=300, and Adam optimizer. Our algorithm took an average
of 126s per FOV and with parallelization, segmentation of an entire slide (588
FOVs, 3M cells) was accomplished in 39 min.
3.4
Feature Extraction
Feature-based ML is important for clinicians to better interpret classification
results. Since clinicians already rely on quantitative values like nuclear MFI
and visually inspect cell morphology and texture, we are motivated to extract
additional morphology, intensity, and texture features from the nucleus, whole
cell, and entire thumbnail image. In total we extract 122 features which are
summarized with equations in Supp. Table 1.
We extract 8 morphology features, 4 each from the nuclear and cell masks:
size, roundness, elongation and the first Hu moment [3], which captures more
subtle shape variability. (BCa CTCs are often larger than non-CTCs.) We next
compute a total of 44 intensity features from nuclear and cell masks across the
three channels: MFI, lower, median, and upper quartiles, interquartile range,
as well as Pearson’s correlations and Ranked-Weighted Co-Localizations [13]
between channels. We also extract CK specific features like CK+ ratio, defined
as the number of CK pixels that are greater than a cutoff value, divided by the
area of the mask. CTCs will exhibit a higher CK+ ratio. We also compute the
mean and standard deviation of pixels in the entire CK channel thumbnail which
may help eliminate CK+ artefacts like flares.
Finally, we extract 70 texture features. First, a 32D Gabor feature vector
is constructed using the mean and standard deviation of filtered images for 16
parameter combinations: θ =0◦, 45◦, 90◦, 135◦, λ=0.1, 0.4, and σ =1, 3, selected
based on cell size. Gabor features identify frequency changes in an image at var-
ious orientations and sizes. This helps to identify small dye-aggregates, flares as
well as CTCs in an image. Next, a 32D Laws [4] feature vector is constructed by
ordered multiplications of 1D filters L5 (Level), E5 (Edge), S5 (Spot), and R5
(Ripple) to detect spatial patterns. Finally, Local Binary Pattern (LBP) [6] en-
codes edges, corners, raised areas, and lines. LBP results in a transformed image
per channel and we calculate the correlation and normalized mutual information
between each channel pair, resulting in 6 final texture features.
Automated CTC Detection, Segmentation and Classification
7
4
Experiments
Once cells are detected, segmented and features are extracted using the steps of
the pipeline described above, we are ready to collect and label ground truth data
to train and evaluate a CTC classifier. The CTC classifier is used to replace the
standard rule-based biomarker thresholding and substantially reduce the number
of CTC candidates presented to clinicians for final review.
4.1
Ground Truth Data
Ground truth data is collected from 15 mBCa patients using the same cell de-
tection and segmentation steps of the pipeline. From 15 patients, a total of
241,644,731 cells were identified by the cell detection algorithm. Then 99.8% of
non-CTCs and artefacts were filtered using the rule-based thresholds described
in Section 2 leaving 500,255 CTC candidates presented for manual classification.
(Without a CTC classifier, this is the number of candidates clinicians would
have to manually review in their workflow.) Ground truth CTCs were labeled
by four annotators and subjected to a round of adjudication for consensus. They
each labeled a comparable number of non-CTCs and non-cellular artefacts. The
Dataset
Patients Labeled Samples CTCs Non-CTCs Artefacts
Training
7
4,680
1,667
1,632
1,381
Verification
5
1,931
340
882
709
Validation
3
2,922
1,221
969
732
Total
15
9,533
3,228
3,483
2,822
Table 1: Summary of class sample counts for each dataset split.
result was 9,533 labeled samples with 3,228 CTCs, 3,483 non-CTCs, and 2,822
artefacts. The ground truth data was split at the patient level into training,
verification, and validation sets, stratified by number of CTCs (See Table 1).
4.2
CTC Classifier Training
For CTC classification, we combined the non-CTC and artefacts into a sin-
gle negative class for binary classification. We evaluated four SVM models, in-
cluding a linear SVM and three non-linear kernel SVMs: radial basis function
(RBF), sigmoid, and polynomial. To optimize the classifier performance, the
hyper-parameter tuning of C, γ, and polynomial degree were performed using
grid search over 5-fold cross validation. All image features were normalized using
min-max normalization fit on the training dataset and applied to the verification
and validation sets. For our application, it is imperative for patient outcomes
to miss as few CTCs as possible (false negatives). Alternatively, allowing false
positive CTC candidates only increases the burden of the manual reviewers.
Therefore, optimal performance hinges on maximizing sensitivity while control-
ling for specificity.
8
E. Schwab et al.
Fig. 4: Top ten features weights for True Positive (CTC) and True Negative (non-
CTC/artefact) classes using SHapley Additive exPlanations (SHAP) [5] averaged
over 100 random samples per class in the verification set. (Note: Nuc_IQR_ck
is the inter-quartile range (IQR) of pixels in the nucleus (nuc) in CK; STD_ck
is the standard deviation of CK; Cell_Coloc_ck_dapi is the co-localization of
pixels between CK and DAPI within the cell. See Supp. Table 1 for feature
definitions.)
Dataset
True Pos. True Neg. False Pos. False Neg. Sens. Spec. Acc.
Training
1,667
3,007
6
0
100% 99.8% 99.9%
Verification
340
1,571
20
0
100% 98.7% 99.0%
Validation
1,210
1,648
53
11
99.1% 96.9% 97.8%
Table 2: Summary of True Positive, True Negative, False Positive, False Negative
counts and Sensitivity, Specificity, and Accuracy for each dataset split.
4.3
Results
The SVM with RBF Kernel and hyper-parameters of C = 10 and γ = 1 scored
the highest average accuracy across the 5 folds. The chosen model was retrained
on the complete training set and a 0.3 probability threshold was selected to
maintain 100% sensitivity on the training and verification sets with greater than
98% specificity. Of the 26 total false positives in the training and verification
sets, 8 were actually Non-CTCs and 18 were determined to be artefacts. Since
artefacts are typically easier to inspect by humans this majority of false positives
further alleviates the manual review of CTC candidates. We then applied our
model to the hold-out validation set and achieved over 99% sensitivity and 97%
specificity. See Table 2 for these results.
The ranked feature weights in Fig. 4 indicate that CK intensity features are
most important for the CTC class while DAPI and CD45/31 features are more
useful for classifying non-CTCs/artefacts. This result coincides with clinical im-
portance of CK in classifying CTCs and can be used for additional interpretation
by clinicians.
Automated CTC Detection, Segmentation and Classification
9
Finally, we applied BRIA on the full patient data of the combined train-
ing and verification sets. With a total of 12 patients, 171M cells were detected
(avg. 14M per patient). By biomarker thresholding, 401,608 CTC candidates
would have been presented for manual review. In contrast, our ML CTC classi-
fier achieved a 100× reduction for a total of 4,019 CTC candidates (avg. 335 per
patient) of which 2,007 are known to be true positive CTCs. Because we achieved
100% sensitivity in the combined training and verification sets, we know that
no CTCs were missed on the full patient data. This showcases the true clinical
value of our ML-based pipeline to substantially reduce manual workloads.
The BRIA pipeline is configured to leverage AWS Batch jobs that run in
parallel, enabling the simultaneous analysis of multiple slides. Each slide takes
an average of 90 minutes to complete the analysis. The pipeline requires 32 GB
of memory and up to 12 GB of temporary storage for decompressed TIFF image
files, while the analysis output files, including slide QC, identified CTC MFI
values in JSON format, and cell thumbnails as PNG images, require less than
50 MB of S3 storage.
5
Conclusion
In conclusion, we have demonstrated the clinical utility of our fully automated
BRIA pipeline for detection, segmentation, feature extraction, and classification
of mBCa CTCs in multi-channel IF imaging. This work is important to deliver
patient-specific profiles for mBCa by assisting clinicians in detecting the rare oc-
currences of CTCs in liquid biopsies. Validated on a hold-out set with over 99%
sensitivity our CTC classifier detects nearly all CTCs while reducing substan-
tial burden by presenting 100x fewer candidates for manual confirmation. With
BRIA in production, future efforts will focus on continuous learning to moni-
tor and maintain performance of CTC classification over time and expansion to
additional cancer types for additional generalization and validation.
Acknowledgments. We thank our ground truth labeling teams, our medical direc-
tor and the Assay Development, MTT and Clinical groups at Epic Sciences for their
support in this work.
Disclosure of Interests. The authors are current or past employees of Epic Sciences
and may own company stock options.
References
1. Guo, Z., Lin, X., Hui, Y., Wang, J., Zhang, Q., Kong, F.: Circulating tumor cell
identification based on deep learning. Frontiers in Oncology 12, 843879 (2022)
2. Hashimoto, K., Kamiya, T., Li, G., Yoneda, K., Tanaka, F.: Automatic identifi-
cation of tumor cells for circulating tumor cells by convolutional neural networks.
International Journal of Innovative Computing, Information and Control 19(1)
(2023)
10
E. Schwab et al.
3. Hu, M.K.: Visual Pattern Recognition by Moment Invariants. In: IRE Trans. Info.
Theory. vol. IT-8, pp. 179–187 (January 1962)
4. Laws, K.: Texture Image Segmentation (January 1980)
5. Lundberg, S.M., Lee, S.I.: A unified approach to interpreting model predictions.
In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,
S., Garnett, R. (eds.) Advances in Neural Information Processing Systems 30, pp.
4765–4774. Curran Associates, Inc. (2017)
6. Ojala, T., Pietikäinen, M., Harwood, D.: A comparative study of texture measures
with classification based on featured distributions. Pattern recognition 29(1), 51–
59 (1996)
7. Otsu, N.: A threshold selection method from gray level histograms. IEEE Trans.
Systems, Man and Cybernetics 9, 62–66 (Mar 1979)
8. Park, J., Ha, S., Kim, J., Song, J.W., Hyun, K.A., Kamiya, T., Jung, H.I.: Clas-
sification of circulating tumor cell clusters by morphological characteristics using
convolutional neural network-support vector machine. Sensors and Actuators B:
Chemical 401, 134896 (2024)
9. Ronneberger, O., P.Fischer, Brox, T.: U-net: Convolutional networks for biomed-
ical image segmentation. In: Medical Image Computing and Computer-Assisted
Intervention (MICCAI). LNCS, vol. 9351, pp. 234–241. Springer (2015)
10. Scher, H., Armstrong, A., Schonhoft, J., Gill, A., Zhao, J., Barnett, E., Carbone, E.,
Lu, J., Antonarakis, E., Luo, J., et al.: Development and validation of circulating
tumour cell enumeration (Epic Sciences) as a prognostic biomarker in men with
metastatic castration-resistant prostate cancer. European Journal of Cancer 150,
83–94 (2021)
11. Schmitt, O., Hasse, M.: Radial symmetries based decomposition of cell clusters in
binary and gray level images. Pattern Recognition 41(6), 1905–1923 (Jun 2008)
12. Shen, C., Rawal, S., Brown, R., Zhou, H., Agarwal, A., Watson, M.A., Cote, R.J.,
Yang, C.: Automatic detection of circulating tumor cells and cancer associated
fibroblasts using deep learning. Scientific reports 13(1), 5708 (2023)
13. Singan, V.R.: Dual channel rank-based intensity weighting for quantitative co-
localization of microscopy images. BMC Bioinformatics 12, 1471–2105 (Oct 2011)
14. Stegmaier, J., Otte, J.C., Kobitski, A., Bartschat, A., Garcia, A., Nienhaus, G.U.,
Strähle, U., Mikut, R.: Fast segmentation of stained nuclei in terabyte-scale, time
resolved 3d microscopy image stacks. PLoS ONE 9(2), e90036 (02 2014)
15. Tsuji, K., Lu, H., Tan, J.K., Kim, H., Yoneda, K., Tanaka, F.: Detection of circu-
lating tumor cells in fluorescence microscopy images based on ann classifier. Mobile
Networks and Applications 25, 1042–1051 (2020)
16. Vidlarova, M., Rehulkova, A., Stejskal, P., Prokopova, A., Slavik, H., Hajduch,
M., Srovnal, J.: Recent advances in methods for circulating tumor cell detection.
International Journal of Molecular Sciences 24(4), 3902 (2023)
17. Warfield, S.K., Zou, K.H., Wells, W.M.: Simultaneous truth and performance level
estimation (staple): an algorithm for the validation of image segmentation. IEEE
transactions on medical imaging 23(7), 903–921 (2004)
18. Werner, S.L., Graf, R.P., Landers, M., Valenta, D.T., Schroeder, M., Greene, S.B.,
Bales, N., Dittamore, R., Marrinucci, D.: Analytical validation and capabilities
of the Epic CTC platform: enrichment-free circulating tumour cell detection and
characterization. Journal of Circulating Biomarkers 4, 3 (2015)
19. Wilson,
M.:
Introduction
to
widefield
microscopy
(Jun
2017),
https://www.leica-microsystems.com/science-lab/
introduction-to-widefield-microscopy/
Automated CTC Detection, Segmentation and Classification
11
20. Zeune, L.L., Boink, Y.E., van Dalum, G., Nanou, A., de Wit, S., Andree, K.C.,
Swennenhuis, J.F., van Gils, S.A., Terstappen, L.W., Brune, C.: Deep learning of
circulating tumour cells. Nature Machine Intelligence 2(2), 124–133 (2020)
21. Zhang, A., Zou, Z., Liu, Y., Chen, Y., Yang, Y., Chen, Y., Law, B.N.F.: Automated
detection of circulating tumor cells using faster region convolution neural network.
Journal of Medical Imaging and Health Informatics 9(1), 167–174 (2019)
Supplementary
Object
Feature
Dim
Equation
Description
Morphology Features (8)
Nuc/Cell
Size
2
A = |O|
Area of object O.
Nuc/Cell
Roundness
2
R = 4πA/C2
p
Closeness to a circle.
Cp, convex perimeter.
Nuc/Cell
Elongation
2
E = 4A/πl2
m
Area to major axis lm
Nuc/Cell
1st Hu
2
M1 = µ2,0 + µ0,2
Shape descriptor using
Moment
moments, µ0,2, µ2,0
Intensity Features (44)
Nuc/Cell
MFI
6
MFIch = 1
A
P
p∈O Ich(p) Mean Fluorescence Intensity
of object O for channel Ich
Nuc/Cell
Quartile
24
LQIch, MQIch, UQIch,
Lower, median, upper, inter-
Intensities
IQRch =UQIch−LQIch
quartile range per channel
Nuc/Cell
CK+ ratio
2
CK+ =|Ick+(p)p∈O|/A
Fraction of pixels greater
than CK cutoff in object
Nuc/Cell
Channel
6
Pearson’s Correlation
Similarity of pixels between
Correlation
two channels within object
Nuc/Cell
Channel
4
Ranked-Weighted
Co-occurrence and corr.
Co-localization
Co-localization
of two channels in object
CK
Statistics
2
µck, σck
Mean & Std. Dev. of CK
Texture Features (70)
CK
2D Gabor
32
g(λ, θ, σ, ψ, γ)
Frequency patterns in
Filters
various orientations
CK
2D Laws
32
Pairs of Level, Edge,
Spatial patterns in
Filters
Spot, and Ripple filters
various orientations
DAPI,
LBP
6
LBP(p)=P
i s(ni −p)2i
Local geometric pattern
CK,
Correlation
s(x)=1 if x>1, else 0
encodings and similarity
CD45/31 & Mutal Info
pixel ni around p
between channel LBPs
Table 1: Summary of 122 Morphology, Intensity and Texture features extracted
from 3-channel IF images (DAPI, CK, CD45/31) within objects of nucleus
(Nuc.), cell or full image thumbnail.
