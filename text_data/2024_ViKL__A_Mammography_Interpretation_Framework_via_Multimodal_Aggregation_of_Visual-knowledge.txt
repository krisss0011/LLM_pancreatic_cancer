JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
1
ViKL: A Mammography Interpretation
Framework via Multimodal Aggregation of
Visual-knowledge-linguistic Features
Xin Wei, Yaling Tao, Changde Du, Gangming Zhao,
Yizhou Yu, Fellow, IEEE, and Jinpeng Li, Member, IEEE
Abstractâ€”Mammography is the primary imaging tool for breast cancer diagnosis. Despite significant strides in applying deep learning
to interpret mammography images, efforts that focus predominantly on visual features often struggle with generalization across
datasets. We hypothesize that integrating additional modalities in the radiology practice, notably the linguistic features of reports and
manifestation features embodying radiological insights, offers a more powerful, interpretable and generalizable representation. In this
paper, we announce MVKL, the first multimodal mammography dataset encompassing multi-view images, detailed manifestations and
reports. Based on this dataset, we focus on the challanging task of unsupervised pretraining and propose ViKL, a innovative framework
that synergizes Visual, Knowledge, and Linguistic features. This framework relies solely on pairing information without the necessity for
pathology labels, which are often challanging to acquire. ViKL employs a triple contrastive learning approach to merge linguistic and
knowledge-based insights with visual data, enabling both inter-modality and intra-modality feature enhancement. Our research yields
significant findings: 1) Integrating reports and manifestations with unsupervised visual pretraining, ViKL substantially enhances the
pathological classification and fosters multimodal interactions. 2) Manifestations can introduce a novel hard negative sample selection
mechanism. 3) The multimodal features demonstrate transferability across different datasets. 4) The multimodal pretraining approach
curbs miscalibrations and crafts a high-quality representation space. The MVKL dataset and ViKL code are publicly available at
https://github.com/wxwxwwxxx/ViKL to support a broad spectrum of future research.
Index Termsâ€”Mammography, Computer-aided Diagnosis, Self-supervised Learning, Multimodal Learning.
âœ¦
1
INTRODUCTION
B
REST cancer is the foremost cause of morbidity and mor-
tality among women. Timely detection, intervention,
and treatment are critical to improving the prognosis, as
emphasized in existing literature [1]. Mammography stands
as the principal tool for early breast cancer screening. Yet,
the early indicators of breast cancer are subtle and subject to
varying interpretations by different radiologists, presenting
challenges in screening accuracy. In the past five years,
the advent of deep learning in medical image analysis has
sparked a wave of innovation. Models such as convolutional
neural networks (CNN) [2, 3] and transformers [4, 5] have
been increasingly applied for the pathological classification
of mammography images. However, these models primarily
rely on image data, creating a disconnect from the nuanced
clinical practices of radiologists. This gap highlights the
need for more integrated and clinically-aligned approaches
in breast cancer screening methodologies.
â€¢
Wei, X is with the Ningbo Institute of Life and Health Industry, UCAS,
Ningbo 315000, China.
â€¢
Tao, Y, and Li, J are with School of Automation Science and Engineering,
South China University of Technology, Guangzhou 510641, Guangdong,
China.
â€¢
Du, C is with the Research Center for Brain-Inspired Intelligence, State
Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of
Automation, Chinese Academy of Sciences, Beijing 100190, China.
â€¢
Zhao, G and Yu, Y are with the Department of Computer Science,
University of Hong Kong, Pokfulam, Hong Kong.
This work was supported by the National Natural Science Foundation of
China under Grant 62106248. Coprresponding authors: Jinpeng Li (jin-
peng.li@ieee.org) and Yizhou Yu (yizhouy@acm.org).
Fig. 1 illustrates the workflow of radiologists in assess-
ing suspected breast lesions. This process begins with the
radiologist drawing on both the consensus in radiology
and their experience. They observe the lesion from various
perspectives, focusing on its manifestations such as shape,
contour, and density. The findings are then documented in
mammography reports. This workflow encompasses three
key elements: 1) Mammography Images providing visual in-
formation. However, the disease-related features in these
images are often complex and subtle, making accurate in-
terpretation challenging. 2) Manifestations representing the
expert knowledge in analyzing breast lesions, which are
rich in dense, semi-semantic information. 3) Mammography
Reports standing for the linguistic summaries of the radi-
ologistâ€™s observations with containing highly sparse and
abstract semantic information. The reports encapsulate the
radiologistâ€™s final interpretation and conclusions based on
the visual and manifestation data.
To align computer-aided diagnostic systems to the diag-
nostic protocol of experts, some studies have investigated
the role of reports or manifestations in medical image
analysis. The LIDC-IDRI dataset has seven manifestations,
e.g., pulmonary consolidation and atelectasis to facilitate
the benign/malignant classification of pulmonary nodules
[6]. However, few manifestations were considered, and
manifestations specific to pulmonary nodules are difficult
to apply to the mammography interpretation. The ChestX-
ray14 dataset has 14 disease labels mined from the X-ray
reports via NLP techniques, which shows the possibility of
arXiv:2409.15744v1  [eess.IV]  24 Sep 2024
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
2
Manifestation?
Round
Ovoid
Lobulated
Irregular
Microlobulated
Obscured
Branching
Crescentic
Thread-like
Gritty
Large Rod-like
Yes
Shape
Edge
Calcification 
Shape
Granular
Popcorn-like
Eggshell-like
Comet Tail Sign
No
â€¦
â€¦
Mammography 
Image
A mass-like shadow in the posterior and upper outer 
quadrant of the left nipple with calcified shadow in the 
upper outer quadrant is considered to have a high 
possibility of malignancy. BI-RADS: 4C
Mammography Report
Mammography Knowledge 
Spiculated
Well-circumscribed
Annular
Fig. 1: Knowledge-guided diagnostic protocol of radiologists for breast cancer. They identify suspected lesions on mammography images and refer
to the consensus of radiology and their experience to analyze the lesion from the perspective of manifestations, e.g., shape, margin and density.
Based on these impressions, radiologists write concise and conclusive mammography reports. This process inspires the visual pretraining paradigm
of ViKL, which aggregates visual, knowledge and linguistic features to build intelligent machines for mammography analysis.
extracting manifestations in reports [7]. In contrast, mam-
mography reports are very succinct, and the diagnostic
manifestations are not fully reflected in the report. For exam-
ple, a manifestationâ€™s absence in the report does not imply
its absence in the image. Therefore, the keywords extracted
from the report are far from representing the expertise of
radiologists. To achieve a solid multimodal learning, an
exhaustive, generic, and systematic set of manifestations for
mammography should be developed and annonated.
The past three years have witnessed a surge in multi-
modal pretraining algorithms, which have aimed to aug-
ment visual features with corresponding linguistic data
for various tasks [8, 9, 10, 11, 12, 13, 14]. In medical
imaging analysis, most studies have focused solely on
radiology-related modalities (such as imaging and reports)
for multimodal self-supervised pretraining. For example,
REFERS [14] leveraged supervision signals from X-ray re-
ports and employed self-supervised contrastive learning to
develop joint representations. Demonstrating exceptional
performance under limited supervision, REFERS even out-
performs approaches reliant on structured labels manually
added to X-rays. However, REFERS does not incorporate
manifestations, which are rich in dense semantic infor-
mation. We posit that the role of manifestations warrants
independent and systematic investigation. In this paper, we
aim to systematically formulate, annotate, and explore next-
generation multimodal pretraining paradigms tailored to
mammography interpretation based on the manifestations.
This work contributes in four aspects.
1) We introduce ViKL, a new self-supervised multimodal
pretraining paradigm integrating visual, knowledge, and
linguistic features. Using triple contrastive learning, ViKL
aggregates inter-modality features among paired data and
enhances intra-modality feature alignment. To resolve the
issue of semantically similar reports or manifestations re-
sulting in inappropriate negative samples, we have ex-
ploited engineering solutions including label smoothing and
manifestation deduplication.
2) We propose a novel hard negative sample selec-
tion method based on manifestations, termed ManiNeg. It
estimates the hardness of negative samples through the
Hamming distance between manifestations, effectively ad-
dressing the issues of limited sampling space and mismatch
between representation and semantics, which exist in tradi-
tional representation-based hard negative sample selection
methods.
3) We contribute MVKL, the first trimodal mammog-
raphy dataset, which includes images, manifestations, and
reports. This dataset uniquely features a systematically for-
mulated and annotated mammography manifestations.
4) Extensive experiments on MVKL and two pub-
lic
datasets
show
that
the
integration
of
knowledge
and linguistic information with visual pretraining signif-
icantly improves pathological classification and demon-
strates strong cross-dataset generalization. In-depth analysis
reveals ViKLâ€™s capability in detecting subtle lesions and its
generation of high-quality representation spaces.
The rest of this paper is organized as follows. Section
2 reviews relevant works to ViKL. Section 3 introduces
the MVKL dataset. Section 4 presents the ViKL framework
and optimization method. Section 5 and Section 6 include
experimental results with in-depth analysis. Section 7 sum-
marizes take-away information, with limitations and future
directions for follow-up studies.
2
RELATED WORKS
Self-supervised Learning (SSL). SSL operates without tar-
get task annotations, positioning it within the domain of
unsupervised learning. It relies on pretext tasks, which
can vary in nature, to provide supervisory signals. The
task may involve predicting the rotation angle of a rotated
image [15], reconstructing the original image from partially
masked regions using an auto-encoder (AE) [16, 17, 18],
or restoring the original order of randomly shuffled image
patches [19, 20]. After training, the modelâ€™s feature extrac-
tion part is retained while the pretext task-specific head
is discarded. Subsequently, a simpler head, like a linear
classifier, is trained on this feature extractor for the target
task. A distinct category within SSL is instance discrimination,
which treats each instance as a unique class, leading to the
emergence of contrastive learning exemplified by SimCLR
[21] and MOCO [22]. In this framework, an image is aug-
mented through rotation, flipping, colorization, etc., and in
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
3
Image
ð¸ð¼
ð¸ð¼
ð¸ð¼
Fuse Projector
Image
Text
Manifestation
ð¸ð¼
ð¸ð‘‡
ð¸ð‘€
Fuse Projector
Image
Text
ð¸ð¼
ð¸ð‘‡
ð¸ð‘€
Fuse Projector
Attract
Repel
Image
ð¸ð¼
Breast Lump Classification
Intra-modality Alignment
Inter-modality Alignment
Inter-modality Uniformity
A
B
C
D
Manifestation
Different Views
Task Head
Fuse Projector
Report Retrieval
Manifestation Est.
Nearest
Average
Fig. 2: The training (A-C) and test phases of ViKL. A. Mammography views are projected into a 128-dimensional hypersphere embedding space,
normalized with L2 norm. Features from different views are attracted to each other, facilitating intra-modality alignment. B. Data from the three
modalities are projected into the same embedding space using respective encoders. Matched modalities of the same instance are attracted to
each other, ensuring inter-modality alignment. C. Features from distinct instances are repelled to enhance uniformity across the hypersphere,
preserving information effectively. D. The image encoder is capable of improved pathological classification of breast lumps through a simple task
head. ViKL model is also versatile, suitable for multimodal tasks like image-report retrieval and manifestation estimation.
the embedding space, distances between augmentations of
the same image (positive pairs) are reduced, while those
between different images (negative pairs) are increased.
MOCO uniquely approaches this as a dictionary look-up
task, akin to identifying positive matches of a query in a vast
dictionary. The effectiveness of SSL hinges on the alignment
between the pretext and target tasks, a key determinant of
performance.
Vision-language Pretraining (VLP). VLP involves two
stages: pretraining and testing. A prime example of VLP
is CLIP [11]. During pretraining, CLIP uses image and
text encoders to map these modalities into an embedding
space, training the encoders to recognize paired images
and text. During test, simple classifiers are added to the
image encoder to achieve target tasks. CLIP has inspired
numerous follow-ups, most utilizing BERT [23] as the lan-
guage encoder, while adopting various vision encoders.
For instance, region-based encoders like Faster R-CNN [24]
extract regions for VLP, offering high accuracy but increased
running time. Conversely, patch-based encoders, such as
ViLT [25], use patch projection, trading a slight decrease
in accuracy for reduced running time. Expanding on CLIP,
VSE++ [26] enhances visual-semantic embeddings for cross-
modal retrieval by considering the hard negatives. ALBEF
[12] aligns image and text representations with a contrastive
loss before fusing them via cross-modal attention, facilitat-
ing more grounded learning without requiring bounding
box annotations or high-resolution images. VLMo [27] of-
fers a unified vision-language model, leveraging a modular
transformer network to jointly learn a dual encoder and a
fusion encoder. BLIP [28] adapts to both understanding and
generation tasks in vision-language by bootstrapping web
data captions, filtering out noise to enhance learning. CoCa
[29] combines contrastive and captioning loss in an image-
text encoder-decoder foundation model, encompassing both
contrastive and generative methods. BEiT [30]pretrains vi-
sion transformers for masked image modeling tasks, re-
constructing original visual tokens from corrupted image
patches. PaLI [31] integrates visual and textual inputs for
text generation, efficiently reusing a large unimodal back-
bone for both language and vision modeling, thereby trans-
ferring existing capabilities and reducing training costs.
Pretraining in Medical Image Analysis. The scarcity of
annotated data makes pretraining an especially critical ap-
proach, generally pursued through three strategies. 1) Some
studies opt to pretrain models on general vision datasets
like ImageNet. However, the significant disparity between
natural and medical images often renders this approach less
than ideal. 2) Another method involves seeking free super-
visory signals from unannotated data. For instance, Models
Genesis utilizes an AE-based SSL approach to reconstruct
CT images from their augmented versions [17, 18], where
the encoder is then adapted for downstream tasks. Simi-
larly, our preliminary work with MVCNet leverages mul-
tiview contrastive learning to aggregate two-dimensional
(2D) views of the same three-dimensional (3D) CT lesion for
initial model training [32]. These approaches, however, are
limited to single image modalities. ReFs [33] integrated self-
supervised learning with a supervised reference task, opti-
mizing feature representations for medical image segmenta-
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
4
tion through gradient matching and dual-task alignment. 3)
A third strategy seeks supervision from image reports. For
example, REFERS [14] employs radiography transformers to
encode X-ray images and minimizes contrastive alignment
loss to pair images with their corresponding free-text reports
[14]. This approach, capitalizing on the complex logic and
abstract reasoning in reports, shows promise in supplanting
traditional pretraining methods. The existing studies rarely
take the manifestation modality into consideration, but we
argue that while images are rich repositories of fundamental
yet latent disease features and reports encapsulate unstruc-
tured, abstract, and semantic information, manifestations
stand out as structurable, semi-abstract, and semi-semantic
features, serving as a vital bridge between images and
reports. Incorporating manifestations into the pretraining
process has the potential to yield more relevant and higher-
quality representations.
Manifestations in Medical Image Analysis. Recognizing
the crucial role of domain knowledge, several studies have
explored automatic extraction of semi-manifestations. For
instance, assessing shape and texture heterogeneity in ma-
lignant pulmonary nodules based on segmentation results
[34], and utilizing manually annotated manifestations for
refined modeling. The LIDC-IDRI dataset includes seven
attributes for benign/malignant classification of pulmonary
nodules. Building on these attributes, various studies have
advanced pulmonary nodule diagnosis, such as employing
Bayesian frameworks for attribute-based reasoning [35] or
integrating these attributes into neural network inputs [36].
In our preliminary work, we developed the first tuberculo-
sis dataset featuring seven manifestations and employed a
multi-scale feature interaction model for tuberculosis clas-
sification and detection [37]. Another recent study fused
clinical tabular data (like smoking status) and morphometric
features (such as ventricle volume) with cardiac MR images
to predict myocardial infarction and coronary artery disease
risks, leveraging data from the UK Biobank [38]. Given
the distinct radiological manifestations across diseases, it is
essential to tailor and meticulously label the manifestation
list for each specific disease.
Additionally, radiomics shares structural characteristics
similar to manifestations, making it applicable to multi-
modal contrastive learning [39, 40]. However, manifesta-
tions distinctly differ from radiomics features in their na-
ture and application. While radiomics involves the high-
throughput extraction of quantitative image features, man-
ifestations offer a more concise and disease-specific ap-
proach. They encapsulate higher-level features compared to
radiomics, focusing on specific characteristics closely asso-
ciated with particular diseases. This distinction highlights
manifestations as a more targeted and nuanced method in
medical imaging, in contrast to the broader, quantitative
scope of radiomics.
Mammography Image Analysis. While pathology slides are
the gold standard for diagnosing breast cancer, with several
deep learning studies focused on their analysis [41, 42],
mammography remains an essential tool in breast cancer
screening. A variety of research has developed multi-scale,
multi-view models for detecting breast masses in mammog-
raphy [43, 44]. For instance, AGN employed a bipartite
graph convolutional network alongside an inception graph
convolutional network, enhancing correspondence reason-
ing by integrating information from ipsilateral and bilateral
views [44]. Also, in light of the limited availability of la-
beled data, weakly-supervised learning approaches such as
multi-instance learning have shown promise in improving
sensitivity, utilizing labels of varying granularity [45].
3
DATASET
We have compiled the first trimodal dataset for mammog-
raphy: The Mammography Visual-Knowledge-Linguistic
dataset (MVKL). The dataset consists of 2671 mammog-
raphy examinations containing breast lumps, along with
their imaging reports, manifestations, pathological labels,
and segmentation masks for breast lumps. In this section,
we will provide a detailed introduction to this dataset.
Collection Procedure. The MVKL dataset comprises
data from clinical cases at Ningbo NO.2 Hospital. We
first selected mammography examinations with pathology
virified benign-malignant evaluations, and utilized their
mammography images and corresponding reports as the
initialization of the dataset. Based on this, we designed the
annotation process for manifestations.
While drafting reports, physicians typically mention
only those manifestations they deem significant. Yet, other
less emphasized manifestations can also be crucial for rep-
resentation learning. Therefore, we developed a compre-
hensive manifestation table to capture a broad spectrum of
relevant breast lump traits. The annotation process involved
two phases for accuracy. Initially, five attending and chief
physicians annotated the breast lumps and their manifesta-
tions, referencing the reports and pathology results. In the
second phase, a different physician reviewed these annota-
tions, resolving any discrepancies through discussion. This
process also included lump segmentation, not utilized in
this study but potentially valuable for future research.
Ultimately, this rigorous methodology yielded a collec-
tion of 2,764 annotated lumps from 2,671 examinations,
complete with corresponding imaging reports and manifes-
tations. Reflecting real clinical case distribution, the dataset
aligns closely with actual clinical settings.
Mammography Imaging. In radiological examinations,
multiple views are captured to thoroughly represent lesions.
For mammography, these typically include four views:
left mediolateral oblique (LMLO), left craniocaudal (LCC),
right mediolateral oblique (RMLO), and right craniocaudal
(RCC). To ensure dataset completeness and versatility for
various tasks, we retained all views in their original form,
irrespective of the presence breast lumps. However, views
without breast lumps are not involved in this study.
Manifestations. We crafted a generic manifestation table
based on radiological expertise to describe breast lumps,
detailed in Table 1. This table encompasses 8 primary traits
of mass and calcification, plus 9 additional traits to char-
acterize breast lumps. Post-annotation, these options are
transformed into a binary vector of 35 dimensions to aid
neural network interpretation.
Labels. The MVKL dataset offers pathologically verified
benign-malignant evaluations for all lumps, providing de-
pendable diagnostic labels. The label distribution is sum-
marized in Table 2. Besides, accompanying reports include
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
5
TABLE 1: The header of the manifestation table. Options are separated with commas, and slash separators indicate these traits are treated as the
same option. The options in the miscellaneous row are independent, while the options in other rows are mutually exclusive.
Manifestations
Options
mass shape
irregular, lobulated, ovoid, round
mass edge
microlobulated, obscured, spiculated, well-circumscribed
mass density
low, median, high
mass size
â‰¤2cm, 2-5cm, >5cm
calcification shape
branching, crescentic/annular/gritty/thread-like, granular/popcorn-like/large rod-like/eggshell-like
calcification size
coarse, tiny, uneven
calcification density
low, high, uneven
calcification distribution
scattered, clustered, linear/segmental
miscellaneous
architectural distortion, focal asymmetrical density, duct sign, comet tail sign, halo sign, focal skin thicken-
ing/retraction, nipple retraction, abnormal blood vessel shadow, abnormal lymph node shadow
BI-RADS categories assigned by radiologists. The Breast
Imaging Reporting and Data System (BI-RADS) categorizes
breast images into seven risk levels: 0) inconclusive, 1)
negative, 2) benign finding(s), 3) probably benign, 4) suspi-
cious abnormality, 5) highly suggestive of malignancy, and
6) proven malignancy. These categories reflect radiologistsâ€™
assessments based on imaging alone, akin to the reports.
Privacy Statement. Upon collection, patient-identifiable
data was promptly removed. This includes specific details
and identifiers in the DICOM headers such as patient
and physician names, birth dates, institution names and
addresses, as well as various identification numbers like
Accession Number, Patient ID, Study ID, Study Instance
UID, Series Instance UID, and SOP Instance UID.
4
METHOD
In this section, we will first illustrate the architecture of
ViKL, and elaborate on the pretraining objectives of it.
Finally, we will summary the pretraining objectives and
describe the implementation details.
4.1
Model Architecture
ViKL is composed of four key components: an image en-
coder, a text encoder, a manifestation encoder, and a multi-
modal fuse projector. Specifically, we utilize ResNet50 [46]
as the image encoder, extracting its penultimate layer output
as the image feature. For text encoding, we employ a BERT
[23] encoder. Given that manifestations can be represented
as vectors, they are encoded using two linear layers. Ad-
ditionally, an alignment linear layer is appended to each
unimodal encoder, standardizing the output dimensions to
128 in our experiments. For both the manifestation and
text branches, an extra dropout layer follows the alignment
layer, acting as a feature augmenter akin to data augmenta-
tion, with a dropout probability of 0.5.
The multimodal fuse projector is pivotal in integrating
features from each modality into a unified representation
space. Inspired by SimCLR [21], this projector is structured
with two linear layers, both input and output features
dimensioned at 128. Furthermore, to enhance interactivity
in the representation space, the output features undergo
L2 normalization, positioning them on a hypersphere. The
schematic diagram of the network is shown in Fig. 2,
while a more detailed network structure can be found in
Appendix B.
4.2
Pretraining Objectives
We pretrain ViKL with dual objectives: image multi-view con-
trastive learning (IMC) and image-text-manifestation contrastive
learning (ITM). To establish an effective feature space, spe-
cific strategies like label smoothing and hard negative sample
selection are incorporated into the loss functions, tailored to
the characteristics of each modality. The subsequent sections
will delve into these aspects in greater detail.
General Objective. Before proceeding, itâ€™s crucial to
understand the foundational concept of the contrastive loss
function used in our approach. At its core, contrastive learn-
ing facilitates unsupervised feature extraction by drawing
positive samples closer and pushing negative samples fur-
ther apart. In unimodal contrastive learning, positive pairs
typically consist of different views of the same instance,
while negative pairs are formed from distinct instances.
Once we achieve alignment in multimodal representations,
this principle can effectively be expanded to include pairs
across various modalities.
Drawing insights from existing studies [47, 21, 22, 11],
we can distill the essence of contrastive learning into a
fundamental loss function. For two modalities m1 and m2,
and a mini batch of N pairs, the contrastive loss for a pair
of positive sample pair
 zi
m1, zi
m2

is defined as
Lp
 zi
m1, zi
m2
 = âˆ’log
exp
 sim
 zi
m1, zi
m2
 /Ï„

P
kâˆˆN
mâˆˆ{m1,m2}
kÌ¸=i
exp
 sim
 zim1, zkm
 /Ï„
,
(1)
where Ï„ represents a temperature parameter, crucial in
modulating the loss functionâ€™s dynamic range. In this study,
we utilize cosine similarity (denoted as sim(Â·)) as the metric
for measuring feature similarity. Importantly, m1 and m2
are not restricted to being different modalities; when both
refer to the same modality, the equation applies to unimodal
contrastive learning. In such cases, m1 and m2 correspond
to different views of the same instance. Itâ€™s noteworthy that
m1 and m2 are interchangeable. Thus, for a given pairâ€™s
loss function, the cumulative loss for a mini batch can be
obtained through summation:
Lcl (zm1, zm2) =
X
iâˆˆN
n1âˆˆ{m1,m2}
n2âˆˆ{m1,m2}
n1Ì¸=n2
Lp
 zi
n1, zi
n2
 .
(2)
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
6
The following sections will delve deeper into the applica-
tion of these contrastive learning principles across the three
modalities in our study, employing these loss functions.
Image Multi-view Contrastive Learning is designed
to develop a representation space that effectively extracts
meaningful image representations. For mammography im-
ages, we leverage the contrastive loss to amplify the sim-
ilarity within ipsilateral breasts and increase separability
between patients from different datasets.
We illustrate the process with two views (represented
by vcc and vmlo, e.g., RCC and RMLO in the right breast).
Following previous works [21, 47], we extract the feature
vector y for each view using the image encoder fI. i.e.,
ycc = fI(vcc) and ymlo = fI(vmlo). Then, we map the
feature vectors to the representation z by the multimodal
fuse projector g, i.e., zcc
= g(ycc) = g(fI(vcc)) and
zmlo = g(ymlo) = g(fI(vmlo)). By substituting the repre-
sentation into Eq. (2), we can obtain the basic form of IMC
loss function, where I represents the image modality.
Lbasic
IMC (zI) = Lcl (zcc, zmlo) .
(3)
Image-Text-Manifestation
Contrastive
Learning ex-
tends the scope of image contrastive learning to encompass
additional modalities. By aligning the representations of
image, text, and manifestations, it enables the seamless
application of the unimodal image multi-view loss function
to multiple modalities:
Lbasic
IT M (zI, zT , zM) =
1
3 (Lcl (zI, zT ) + Lcl (zI, zM) + Lcl (zT , zM)) ,
(4)
where I, T, M represent the image, text, and manifestation
modalities, respectively. For images, a specific view (e.g. cc
or mlo) is randomly selected. The processes to derive zT
and zM mirror that of the image modality and are therefore
not reiterated here. A weighting factor of 1/3 is introduced
to balance the contributions of unimodal and multimodal
objectives. A notable aspect of multimodal contrastive learn-
ing is the typical avoidance of selecting negative sample
pairs within the same modality [11, 38], implying that m
in Eq. (1) would be limited to m2. In this framework,
such pairs are invariably negative samples. However, we
retain this term, as in a data-limited context, it can aid in
achieving a more uniform distribution of representations,
thereby preserving information more effectively [48].
While we have established the basic form of these loss
functions, it is crucial to acknowledge that different modal-
ities exhibit varying densities of information, resulting in
distinct correspondence levels between them. To address
these nuances and ensure model convergence, additional
mechanisms are integrated into the objective functions.
These will be elaborated upon in the forthcoming sections.
Label Smoothing. In contrastive learning, our goal is
to establish a correlation between images and their cor-
responding reports, but without excessively emphasizing
this correlation to the point of pushing away semantically
similar reports. This scenario relates to the issue of over-
confidence in neural networks, as noted in [49]. Models
trained on hard targets tend to assign excessive confi-
dence to their predictions. Similarly, in contrastive learn-
ing, which optimizes inter-sample relationships via cross-
entropy, thereâ€™s a risk of over-emphasizing correlations and
exhibiting over-confidence. To address this, we integrate
label smoothing into the loss function for text, a method
proven effective in mitigating over-confidence in the litera-
ture [50, 51].
Label smoothing works by softening classification tar-
gets, thereby reducing over-confidence. It modifies the basic
contrastive loss function (Eq. (1)), which is derived from the
cross-entropy loss function. The original ground truth label
distribution in this loss function is defined as follows:
P (i, k) =
(
1, i = k
0, i Ì¸= k
i, k âˆˆN,
(5)
With label smoothing, we introduce a hyperparameter
Î± âˆˆ[0, 1]. This parameter adjusts the ground truth label
distribution, effectively softening it to:
P ls (i, k) = (1 âˆ’Î±)P (i, k) + Î±/N.
(6)
By implementing label smoothing, we aim to strike a bal-
ance in the correlation between images and texts, enhancing
the modelâ€™s ability to learn nuanced distinctions without
generate overly confident predictions.
Hard Negative Sample Selection. The selection of nega-
tive samples is critical for the success of contrastive learning.
For an anchor sample za and its set of negative samples
zâˆ’âˆˆ{zi
m|i Ì¸= aâˆ€m}, the standard contrastive loss function
selects negatives uniformly, which often leads to optimiza-
tion dominated by more pronounced features. However, in
mammography, breast lumps typically occupy a small area
and may lack distinct boundaries, rendering their features
subtle. To effectively extract this information, contrastive
learning must focus on these subtle differences. Thus, itâ€™s
essential to choose negative samples that are similar yet
distinct from the anchor at the lesion level.
However, selecting lesion-level negative samples is not
straightforward. Current work on hard negative samples,
such as the cross-entropy contained in Eq. (2), or Robinson
et al. [52], typically introduces two assumptions. The first
is that the representation of samples will strictly align with
their semantics, which allow the representation to be uti-
lized as the basis for hard negative sample selection. The
second is that there will always be ideal hard negative
samples in the mini batch. However, for mammography,
due to the small and concealed features of lesions, the rep-
resentations during training often cannot strictly align with
semantics, which can lead to the selection of suboptimal
hard negative samples, thereby harming model optimizing.
Worse still, the scales of medical datasets often limit batch
sizes, which means that ideal hard negative samples may
not always be present in the mini batch.
Encouragingly, the situation changed when manifesta-
tion is introduced. Manifestation has several crucial advan-
tages. First, manifestation summarizes the core character-
istics of breast lumps, meaning it is a reliable proxy for the
semantic of mammogram and can be used for hard negative
sample selection. Secondly, distance between binary mani-
festations can be easily measured using Hamming distance,
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
7
ensuring computational efficiency in the selection of hard
negative samples. Lastly, unlike representations that may
change with model optimizes, manifestation is an intrinsic
attributes of breast lumps and remain constant. This allows
for the selection of hard negative samples from the entire
dataset rather than just from mini batches.
Based
on
these
observations,
we
propose
a
manifestation-centered strategy for sampling hard negative
samples, termed as ManiNeg. ManiNeg modifies the
default batch sampler, implementing the following batch
sampling process:
1) Randomly select a sample from the training set to
serve as the anchor.
2) Centered on the anchor, with the Hamming distance
of the manifestation as the metric, a distribution for negative
sampling is constructed. Specifically, we use a truncated
Gaussian distribution as the distribution for negative sam-
pling, as detailed in Eq. (7).
f(x; Âµ, Ïƒ2, a, b) =
(
Ï•(x;Âµ,Ïƒ2)
Î¦(b;Âµ,Ïƒ2)âˆ’Î¦(a;Âµ,Ïƒ2),
if a â‰¤x â‰¤b
0,
others
(7)
In the formula, x represents the Hamming distance between
the sample and the anchorâ€™s manifestations. Î¦ and Ï• are the
cumulative distribution function and the probability density
function of the Gaussian distribution N(Âµ, Ïƒ2), respectively.
a and b represent the lower and upper bounds of the trun-
cated Gaussian distribution. Specifically, we set the lower
bound a to 1 to prevent the anchor itself from being sampled
as a negative sample. The upper bound b is determined by
analyzing the distribution of Hamming distances within the
dataset and is set to 18. Âµ and Ïƒ define the distribution of
the hardness of the negative samples, which are treated as
hyperparameters.
3) Under this distribution, we will sample batch size âˆ’1
Hamming distances. For each specific Hamming distance,
we uniformly sample one from all samples that have that
exact Hamming distance to the anchor, and include it in the
mini batch.
4) Finally, we will perform deduplication based on man-
ifestations within the batch. Specifically, if multiple samples
in the mini batch have the same manifestation, only one
sample will be randomly retained. During the training pro-
cess, every pair of samples in the mini batch will be treated
as a negative sample pair. Deduplication helps prevent the
model from mistakenly treating potential positive sample
pairs as negative ones.
Additionally, we will incorporate a hardness annealing
mechanism during the training process. Hardness anneal-
ing gradually increases the hardness of the hard negative
samples as training progresses, allowing the model better
extract the representations of lesions from easy to challeng-
ing. Hardness annealing is implemented by modifying Âµ
in Eq. (7) during the training process. For specific settings,
please refer to the Appendix A.
4.3
Final Objective and Implementations
Integrating the mechanisms discussed, the final loss func-
tion for ViKL is articulated as follows:
LIMC (zI) = Lcl (zcc, zmlo) ,
LIT M (zI, zT , zM) =
1
3

Lls
cl (zI, zT ) + Lcl (zI, zM) + Lls
cl (zT , zM)

,
Lall (zI, zT , zM) = LIMC (zI) + LIT M (zI, zT , zM) .
(8)
In these equations, ls represents the use of label smoothing.
The cumulative function, Lall, forms the core of ViKLâ€™s
pretraining objective. The model underwent pretraining for
300 epochs on a single A100 GPU, totaling approximately
3 hours. For the text branch, we used bert-base-chinese [23]
as the initial weight. In the image branch, we explored
both with and without ImageNet pretrained weights to
better understand ViKLâ€™s characteristics. Unlike text and
manifestations, where no data augmentation is applied at
the input stage (though dropout is introduced at the encoder
output), image data undergo augmentation. This augmenta-
tion, along with the dataset split used in pretraining, aligns
with the practices in the downstream tasks phase, which we
will discuss in the next section.
5
EXPERIMENTS AND RESULTS
5.1
Evaluation Protocols
In our experimental section, we focus on evaluating how
ViKL augments the image branch via multimodal con-
trastive learning, for this is essential clinically. Inspired by
CLIP [11] and SimCLR [21], we assess the modelâ€™s ability
to extract features from mammography images using the
following protocols:
Linear Evaluation (LE) involves maintaining the pre-
trained weights of the image branch and adding a trainable
linear layer for the classification task. This method evaluates
the quality of the extracted features by observing their per-
formance in classification with minimal additional training.
Linear Probe (LP) also keeps the pretrained weights of
the image branch fixed. However, classification is performed
using a logistic regression classifier, employing the L-BFGS
algorithm. This approach allows us to gauge the effec-
tiveness of the extracted features in a standard statistical
classification method.
Fine-tuning (FT) uses the pretrained image branch of
ViKL as the starting point and further fine-tunes the entire
network, including an added linear layer for task-specific
purposes. This approach assesses ViKLâ€™s adaptability and
performance in real-world scenarios.
Both LE and LP limit the number of trainable parame-
ters, thus serving as robust methods to assess the intrinsic
quality of ViKLâ€™s features while minimizing the influence of
hyperparameters. On the other hand, FT provides insights
into the modelâ€™s practical applicability and performance
enhancement in real-world tasks.
5.2
Experimental Setups
We now delve into the specific setups of our experiments,
providing a clear understanding of how ViKL is evaluated.
Downstream Task. Across all three experimental proto-
cols, we use complete mammography images for classifi-
cation, identifying the presence of malignant lumps. Given
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
8
the often indistinct boundaries of breast lumps, this task
presents inherent challenges, making it an apt scenario to
assess the impact of multimodal pretraining.
Label Acquisition. The labels for these tasks are de-
rived from pathologically validated benign-malignant eval-
uations, ensuring an objective analysis.
Metric. To ensure consistent and clear comparisons be-
tween models, we exclusively use the area under the re-
ceiver operating characteristic (AUC). This metric provides
a straightforward measure of performance without the com-
plexities introduced by thresholding.
Data Augmentation. The same data augmentation strat-
egy is applied in both pretraining and downstream task
phases. This includes RandomResizedCrop with a scaling
range of (0.5, 1), RandomHorizontalFlip, and two layers of
RandAugment [53]. Notably, as the mammography images
are single-channel, color enhancements are not incorporated
in RandAugment.
Dataset Split. The in-house MVKL dataset is divided
into training, validation, and test sets with a 7:1:2 ratio.
Care is taken to ensure that different images from the same
patient are not included in the same set. Public datasets may
follow different splitting schemes, which will be discussed
in a subsequent section.
Hyperparameters. Specific hyperparameters for each
protocol are listed in Appendix A.
5.3
Pulic Datasets for External Evaluation
In addition to the in-house MVKL dataset, we extend our
experiments to two publicly available datasets, CBIS-DDSM
[54] and INbreast [55], to assess the transferability of ViKL.
CBIS-DDSM. This dataset comprises mammograms
specifically annotated for breast cancer diagnosis, including
cases marked for calcifications and masses. We adhere to
the datasetâ€™s provided split scheme, designating 10% of the
training set as the validation set. For CBIS-DDSM, each case
comes with a pathology label, which we use directly for
classification.
INbreast. This dataset features a range of mammogram
types: normal, those with masses, calcifications, architec-
tural distortions, asymmetries, and multiple findings. We
divide this dataset into training, validation, and test sets
in a 7:1:2 ratio. INbreast cases are manually categorized
into six groups according to BI-RADS standards. For binary
labeling, we assign BI-RADS âˆˆ{1, 2, 3} as non-malignant
and BI-RADS âˆˆ{4, 5, 6} as malignant. Notably, BI-RADS 0
is not present in the INbreast dataset.
To ensure comparability, we apply the same experi-
mental protocols to these public datasets as we did with
MVKL. The initial weights are pretrained on MVKL to eval-
uate ViKLâ€™s cross-dataset effectiveness. Given the variations
in collection years and equipment, these public datasets
present mammograms with notable differences in clarity
and appearance compared to MVKL, posing a substantial
challenge for cross-dataset validation. A detailed summary
of these public datasets is provided in Table 2.
5.4
A Glance on the MVKL Dataset
Prior to our main experiments, we conducted a quantita-
tive analysis to assess the individual performance of each
TABLE 2: Dataset category distribution. Ben and Mal are short for
Benign and Malignant.
Partition
Training
Validation
Test
Label
Ben.
Mal.
Ben.
Mal.
Ben.
Mal.
MVKL
621
1325
102
174
158
384
CBIS-DDSM
1494
1083
189
98
428
276
INbreast*
212
75
32
9
66
16
* BI-RADS 1 category is presented in the INbreast dataset, which
typically indicates that no lumps were found in the mammogram.
For readability, we merged it into class Benign.
modality in ViKL when applied to the downstream task.
This involved using the untrained encoders of each modal-
ity, followed by a linear layer for binary classification. We
employed the Adam optimizer [56] with optimally chosen
learning rates to ensure convergence, without additional
regularization. The results of these preliminary experiments
are detailed in Table 3.
TABLE 3: A Glance on the MVKL Dataset (%).
AUC
Accuracy
Sensitivity
Specificity
F1
Image
68.34
67.59
75.33
48.73
76.72
Text
86.30
70.90
62.60
91.14
75.31
Manifestation
84.84
79.74
89.61
55.69
86.25
Our primary focus is on the AUC results to minimize the
impact of classification thresholding. Interestingly, we ob-
served that despite both text and manifestations originating
from images, the classification performance using images
alone was the weakest. This suggests that neural networks
might struggle to extract sufficient useful information from
images using simple binary labels. Text and manifestations,
in this context, act as refined versions of the original images,
potentially guiding the network towards more effective
information extraction and representation refinement. This
phenomenon echoes findings in natural image domains, as
exemplified by CLIP [11], where models pretrained with
multimodal contrastive learning showed enhanced robust-
ness compared to those trained solely on ImageNet.
5.5
Challenge in Cross-Dataset Generalization
Supervised learning, due to its simplicity and effectiveness,
is a prevalent pretraining approach in deep learning. A
typical example is using ImageNet pretrained weights as
a starting point for diverse downstream tasks. However,
in medical imaging analysis, the limited data volume and
label diversity often hinder the generalization capability
of models pretrained in a supervised manner. ViKL, with
its unsupervised multimodal contrastive learning approach,
adeptly overcomes these challenges.
To empirically assess model generalization, we con-
ducted a cross-dataset evaluation using the three datasets.
Models were pretrained on each dataset using supervised
learning, then tested across all three datasets on a down-
stream task. We employed the LP protocol for a robust
evaluation, with each model beginning pretraining from
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
9
ImageNet pretrained weights and focusing on benign-
malignant classification in both pretraining and down-
stream phases. The experimental outcomes are detailed in
Table 4, alongside ViKLâ€™s results for direct comparison.
The results reveal that while supervised pretrained mod-
els show commendable performance on downstream tasks
within their original training datasets, their efficacy notably
diminishes when applied to different datasets. This un-
derscores the limited generalization capacity of supervised
pretraining methods. In contrast, ViKL demonstrates strong,
consistent performance across various datasets, indicating
its superior ability in generating robust and generalizable
representations.
TABLE 4: Cross-dataset evaluation results with the LP protocol. Results
are in AUC (Mean Â± Std). Column headers represent the datasets for
downstream evaluation, while row headers indicate the datasets for
supervised pretraining. The last row includes results for unsupervised
ViKL. Italics highlight results with the same pretraining and down-
stream datasets, while the best performances in cross-dataset scenarios
are marked in bold.
Dataset
MVKL
CBIS-DDSM
INbreast
MVKL
66.51Â±0.82
61.23Â±1.40
64.06Â±3.22
CBIS-DDSM
58.68Â±1.96
68.98Â±0.79
64.04Â±2.69
INbreast
56.74Â±1.63
59.87Â±1.15
70.05Â±1.79
ViKL
61.24Â±1.84
65.58Â±0.66
64.11Â±0.63
5.6
Downstream Tasks Validation
In the subsequent sections, we evaluate ViKLâ€™s performance
on downstream tasks using the previously outlined experi-
mental protocols and setups. To reiterate, we applied LP, LE,
and FT protocols across the in-house MVKL, CBIS-DDSM,
and INbreast datasets. The findings from these experiments
are presented in Tables 5 and 6.
5.6.1
Comparison with Supervised Models
In our analysis, we primarily compare three key results
from Table 5: ViKL, ViKL-R, and IM. ViKL represents our
proposed model, where the image branch is preinitialized
with ImageNet weights prior to multimodal contrastive
learning. In contrast, ViKL-R denotes a variant with a ran-
domly initialized image branch for representation learning.
IM serves as a baseline, representing a ResNet50 model
pretrained only with supervised learning on ImageNet. This
comparison allows us to assess the benefits of multimodal
contrastive learning over traditional supervised learning.
The results reveal that ViKL generally outperforms IM
across most protocols, highlighting the added value of mul-
timodal learning. Interestingly, ViKL-R, devoid of ImageNet
pretraining, attains comparable results to IM, even with a
relatively smaller dataset. This indicates that the incorpo-
ration of text and manifestation data through contrastive
learning effectively enhances the image branchâ€™s sensitivity
to lesions. This improvement is not only evident in the
MVKL dataset used for pretraining but also extends to
CBIS-DDSM and INbreast datasets, suggesting the modelâ€™s
adaptability across diverse datasets.
5.6.2
Ablation Study
In the ablation study, we explore two crucial aspects: 1) the
contribution of each modality, and 2) the effectiveness of
the loss function mechanisms (hard negative sample selec-
tion and label smoothing) during pretraining in enhancing
downstream tasks. The findings are presented in Table 6,
demonstrating that these components perform as intended
on all the three datasets.
In our modality-related experiments, two findings are
particularly notable. The first issue relates to the use of only
images for contrastive learning, which achieved a baseline
performance comparable to the IM model (Table 5). This
indicates that the representation learning did not effectively
acquire lesion-related knowledge during pretraining. We
hypothesize that this is because, although contrastive learn-
ing can effectively derive image representations, these rep-
resentations may not necessarily correlate well with lesion
characteristics, especially in scenarios with limited data or
small lesions. This phenomenon highlights a crucial consid-
eration in medical image analysis: the potential limitations
of applying contrastive learning, successful in natural image
domains, might not directly translate to the unique chal-
lenges of medical imaging, especially when relying solely
on images.
The second key finding in our modality-related experi-
ments involves comparing the combination of images with
manifestations against images with text. Introducing text,
particularly imaging reports, into contrastive learning is a
well-established approach, proven effective in both natural
image analysis and medical imaging. However, incorporat-
ing manifestations presents a different scenario. While they
are fundamental in writing reports and making diagnoses,
manifestations are not always explicitly included in reports.
Gathering comprehensive manifestations entails some ad-
ditional annotation effort. Given the emphasis on reducing
annotation costs in medical image analysis, justifying such
an expense is crucial. Our experiment provides this justi-
fication, showing the value of including manifestations in
enhancing the learning process.
Finally, we also demonstrate the effectiveness of Ma-
niNeg through ablation study. Table 6 shows the results
without a specific hard negative sample selection (i.e.,
weighting hard negatives with cross-entropy) and with
the hard negative sample selection method proposed by
Robinson et al. [52]. Compared to the others, ManiNeg
demonstrated the best performance, proving its enhanced
capability to extract features from minute lesions.
5.6.3
Comparison with Off-the-shelf SSL Models
In the comparative study section, we evaluate ViKL against
several notable works: CLIP [11], ALBEF [12], BLIP [28],
Vlmo [27], BEiT3 [58] for natural image-text pairs, and
MedCLIP [57], MedKLIP [59], PubMedCLIP [60], Biomed-
CLIP [61] for medical image-text pairs. Follow the afore-
mentioned protocols, we use the image branches from these
models and apply the three experimental protocols (LE, LP,
FT) across the three datasets, with results detailed in Table 5.
The results indicate that comparative methods exhibit
less optimal classification performance compared to ViKL.
This disparity can partly be attributed to the differing
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
10
TABLE 5: Comparative AUC percentages (%) across the MVKL, CBIS-DDSM, and INbreast datasets, expressed as Mean Â± Std. The IM model
denotes the one pretrained with supervision on ImageNet. ViKL-R refers to the variant of ViKL where the image branch is initialized randomly.
The -M suffix indicates that the model has been fine-tuned on the MVKL dataset, where manifestations are textualized and combined with imaging
reports to form the text modality. The abbreviations LP, LE, and FT correspond to the experimental protocols: linear probe, linear evaluation, and
fine-tuning, respectively. The top-performing results in each category are highlighted in bold for easy reference.
MVKL
CBIS-DDSM
INbreast
Methods
LP
LE
FT
LP
LE
FT
LP
LE
FT
Mean
IM
59.64Â±1.76
71.42Â±1.04
72.74Â±1.51
59.97Â±1.24
67.04Â±1.11
75.52Â±1.15
55.68Â±3.32
52.08Â±8.06
73.90Â±4.85
65.33
CLIP [11]
51.60Â±0.55
66.57Â±0.66
75.70Â±1.20
57.57Â±0.52
64.75Â±0.17
73.02Â±1.23
52.58Â±3.61
57.77Â±7.52
74.62Â±5.48
63.79
MedCLIP [57]
52.90Â±0.92
65.27Â±0.26
70.03Â±1.12
55.90Â±1.41
61.85Â±0.33
72.41Â±0.75
49.10Â±2.08
59.19Â±4.80
76.52Â±4.57
62.57
ALBEF [12]
60.93Â±2.37
68.90Â±0.42
77.59Â±0.33
62.08Â±1.04
67.28Â±0.43
72.09Â±1.00
62.03Â±2.18
62.90Â±6.41
77.43Â±3.03
67.91
BLIP [28]
57.80Â±0.68
70.48Â±0.44
74.65Â±0.27
62.55Â±0.71
66.48Â±0.35
72.67Â±1.56
57.65Â±6.09
71.88Â±2.45
77.26Â±1.91
67.94
Vlmo [27]
50.17Â±0.21
53.56Â±2.76
68.43Â±1.03
54.94Â±1.37
60.01Â±0.53
68.82Â±0.44
54.76Â±4.76
59.33Â±4.32
61.57Â±1.79
59.07
BEiT3 [58]
57.46Â±1.77
70.94Â±0.58
75.33Â±1.40
59.05Â±0.90
62.88Â±0.42
69.65Â±1.53
60.18Â±1.92
69.98Â±0.73
78.58Â±3.86
67.11
MedKLIP [59]
52.55Â±2.09
67.14Â±0.64
70.83Â±1.95
53.95Â±0.77
61.35Â±0.37
70.53Â±1.77
52.46Â±2.85
57.33Â±3.48
69.10Â±4.15
61.69
PubMedCLIP [60]
60.74Â±2.60
71.26Â±0.53
70.41Â±4.06
61.89Â±0.99
64.63Â±0.32
63.72Â±1.22
64.96Â±5.47
66.52Â±5.45
69.37Â±4.40
65.94
BiomedCLIP [61]
60.54Â±0.73
73.62Â±0.66
75.95Â±1.30
62.03Â±1.00
68.54Â±0.67
68.86Â±0.78
61.98Â±0.20
74.34Â±3.75
74.62Â±5.12
68.94
CLIP-M
52.09Â±0.42
65.74Â±0.95
73.48Â±2.45
57.30Â±0.61
64.54Â±0.58
73.03Â±1.90
52.63Â±2.50
59.55Â±5.27
78.79Â±3.00
64.12
MedCLIP-M
53.20Â±1.10
65.12Â±0.57
71.21Â±0.78
54.70Â±0.90
62.21Â±0.52
72.70Â±0.88
52.32Â±3.52
58.71Â±5.02
77.65Â±3.31
63.09
ALBEF-M
61.38Â±1.10
72.19Â±0.39
77.88Â±0.30
60.38Â±1.80
65.21Â±0.26
71.43Â±1.19
61.84Â±2.40
67.28Â±2.45
76.49Â±2.15
68.23
BLIP-M
59.12Â±0.44
70.55Â±0.26
75.32Â±0.67
63.21Â±1.01
65.76Â±0.40
72.15Â±0.82
58.31Â±2.52
72.15Â±3.72
76.52Â±3.71
68.12
MedKLIP-M
57.12Â±1.04
69.43Â±0.74
74.49Â±0.95
56.85Â±0.97
62.55Â±0.77
73.10Â±1.12
54.31Â±3.12
66.53Â±4.28
72.57Â±3.04
65.22
PubMedCLIP-M
60.36Â±1.15
72.17Â±0.74
75.13Â±0.68
62.16Â±0.75
65.75Â±0.64
70.21Â±1.01
62.15Â±4.47
67.15Â±4.43
71.53Â±3.34
67.40
BiomedCLIP-M
61.66Â±1.10
74.15Â±0.76
76.13Â±0.57
63.64Â±0.68
67.98Â±0.49
71.84Â±1.02
59.62Â±3.46
75.15Â±3.75
78.69Â±4.12
69.87
ViKL-R
61.16Â±2.04
72.50Â±0.27
71.92Â±0.49
61.46Â±1.29
66.08Â±0.55
70.00Â±1.30
53.22Â±1.44
61.28Â±7.71
68.49Â±3.93
65.12
ViKL
63.81Â±1.51
75.14Â±0.53
79.06Â±0.53
64.59Â±1.02
70.94Â±0.35
72.52Â±0.81
64.55Â±2.49
75.86Â±0.81
79.47Â±2.15
71.77
TABLE 6: Ablation results in AUC percentages (%) on the MVKL, CBIS-DDSM, and INbreast datasets, presented as MeanÂ±Std. The abbreviations
I, M, and T correspond to the image, manifestation, and text modalities, respectively. LS refers to label smoothing, and HN denotes hard negative
sample selection. In the HN section, â–³indicates the method proposed by Robinson et al. [52], while âœ“indicates ManiNeg. Other abbreviations
used in this table are consistent with those in Table 5.
MVKL
CBIS-DDSM
INbreast
I
M
T
LS
HN
LP
LE
FT
LP
LE
FT
LP
LE
FT
Mean
âœ“
âœ“
55.82Â±0.83
70.38Â±0.96
75.45Â±0.39
62.54Â±0.40
66.50Â±0.13
71.00Â±0.40
50.31Â±2.61
59.90Â±9.03
76.30Â±1.44
65.35
âœ“
âœ“
âœ“
61.69Â±1.41
71.24Â±0.43
76.38Â±0.26
60.80Â±0.71
65.21Â±0.32
69.59Â±0.98
65.08Â±2.15
63.66Â±4.00
78.72Â±1.44
68.04
âœ“
âœ“
âœ“
âœ“
59.71Â±1.00
73.60Â±0.33
74.80Â±0.53
62.13Â±0.92
65.68Â±0.32
71.80Â±1.10
50.55Â±2.18
62.93Â±2.80
76.11Â±1.10
66.37
âœ“
âœ“
âœ“
âœ“
64.93Â±1.03
75.48Â±0.22
77.91Â±0.53
61.83Â±0.39
67.37Â±0.56
70.51Â±1.10
62.73Â±0.66
69.22Â±0.96
74.57Â±1.60
69.39
âœ“
âœ“
âœ“
âœ“
62.52Â±0.45
74.89Â±0.43
77.37Â±0.57
62.09Â±1.03
63.08Â±0.78
69.17Â±1.10
59.10Â±2.21
67.93Â±3.91
80.22Â±1.21
68.48
âœ“
âœ“
âœ“
âœ“
â–³
61.24Â±1.84
75.56Â±0.44
77.46Â±0.53
65.58Â±0.66
68.24Â±0.49
72.48Â±1.12
64.11Â±0.63
68.68Â±1.78
79.16Â±3.60
70.28
âœ“
âœ“
âœ“
âœ“
âœ“
63.81Â±1.51
75.14Â±0.53
79.06Â±0.53
64.59Â±1.02
70.94Â±0.35
72.52Â±0.81
64.55Â±2.49
75.86Â±0.81
79.47Â±2.15
71.77
nature of the data used during pretraining. Comparative
methods often focus on extracting representations of the
salient objects in the images, while ViKL leverages text
and manifestations that are finely tuned to describe smaller,
localized lumps. This focus enables ViKL to better represent
and extract features from small, localized regions, a capa-
bility crucial for analyzing challenging lesions in medical
imaging.
To demonstrate this, we further pretrained some com-
parative methods using the MVKL dataset. For a fair com-
parison, during pretraining, we textualized the manifesta-
tion modality (e.g., mass shape: irregular, mass edge: microlobu-
lated, etc.), and combined it with imaging reports to form the
text modality. The results of downstream task evaluations
for such models are marked with a -M suffix in Table 5.
It can be observed that after pretraining with MVKL, the
models showed improvements in their capability of extract-
ing mammographic representations. ViKL, which utilizes a
dedicated manifestation encoder in conjunction with label
smoothing and ManiNeg mechanism, demonstrated the best
feature extraction capability.
5.6.4
Comparison with Clinical Radiologists
To further analyze the significance of ViKL for clinical work-
flow, we invited experienced clinical radiologists to perform
benign and malignant diagnoses on the test set of MVKL
and compared their performance with that of ViKL. The
specific experimental procedures and results are as follows:
We invited two radiologists specialized in mammograms
analysis to participate in this experiment. Similar to the
input for ViKL, both radiologists were asked to read single-
view mammogram images and determine whether they
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
11
contained malignant breast lumps. To provide a more di-
versified comparison, the two radiologists were asked to
read the mammograms in different scenarios, henceforth
referred to as Radiologist A and Radiologist B. Radiologist
A was tasked with judging the nature of the breast lumps as
accurately as possible, while Radiologist B was instructed
to record results based on first impressions and to make
diagnosis about the nature of the breast lumps as quickly as
possible. We randomly split the test set into two equal parts
and assigned them to the two doctors for diagnosis, with
their results recorded in the Table 7. We also included the
results of ViKL at different thresholds for a comprehensive
comparison.
TABLE 7: Results of clinical comparison experiments (%).
Time
Threshold Sensitivity Specificity Accuracy
Radiologist A 10.091s
-
58.20
82.05
65.17
Radiologist B
5.935s
-
39.68
97.44
56.55
ViKL
0.005s
0.87
40.24
96.62
56.13
ViKL
0.005s
0.74
58.50
86.12
67.76
ViKL
0.005s
0.50
89.70
45.33
76.46
0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
True Positive Rate
ViKL ROC (area = 0.79)
Radiologist A
Radiologist B
Fig. 3: The ROC curve for ViKL, along with the metrics from two
radiologists. When diagnosing with single-view mammograms, their
classification performance is comparable, but the probabilistic outputs
from ViKL provide it with greater flexibility.
For a intuitive comparison, we plotted the ROC curve
for ViKL and marked the metrics of the two radiologists on
it in the Fig. 3. It is evident that ViKL exhibited classification
performance on par with that of the radiologists. Based on
this observation, We can analyze the potential implications
of ViKL for clinical workflows.
Firstly, unsurprisingly, ViKL possesses a significantly
faster reading speed than humans, which offers a distinct
advantage in terms of time cost for analyzing large datasets,
such as retrospective analyses of historical data. Secondly,
diagnostic habits of radiologists are shaped over years of
learning and practice, implying a unchanging preference for
sensitivity and specificity. For example, in our experiments,
radiologists tend to diagnose images as benign in the ab-
sence of sufficient evidence, leading to higher specificity
and lower sensitivity. Since ViKL outputs probabilities of
benignity or malignancy, it offers greater flexibility. Users
can adjust the threshold according to their needs, balanc-
ing between sensitivity and specificity, thus complementing
human diagnostic capability with the advantages of ViKL.
Finally, the focus of ViKL is on multimodal representa-
tion learning. To intuitively compare the performance of
representation learning and reduce the impact of complex
network structures, we adopted a straightforward down-
stream task for evaluating the model: using the complete
single-view mammogram to determine whether it contains
malignant lumps. This also means that there is significant
potential for further improvement in the modelâ€™s perfor-
mance. For instance, ViKL could leverage transfer learning
techniques to transfer weights to models that involve mul-
tiple mammographic views or lump localization, thereby
further optimizing the modelâ€™s usability and performance.
It is noteworthy that diagnosing breast lumps with a
single-view mammogram is a challenging task. This chal-
lenge arises from factors such as overlapping breast tissues,
breast density, and lump size. Meanwhile, the lack of the
paired mammogram view further exacerbates these chal-
lenges. This is also why currently the diagnostic metrics for
both radiologists and ViKL are relatively low. Building on
ViKLâ€™s demonstrated capabilities in representation learning,
as outlined above, enhancing the modelâ€™s diagnostic perfor-
mance will be a key focus of our future work.
5.7
Multimodal Evaluation
Interactions among modalities represent a cornerstone in
multimodal research. The alignment of representations from
disparate modalities within a shared space unlocks the
potential for complex tasks like retrieval and generation
across these modalities. However, the relatively small scale
of the MVKL dataset imposes a limitation, resulting in a
representation space that is sparse. This sparsity potentially
hampers the robustness and practicality of intermodal inter-
actions. Nonetheless, even within this limited representation
space, we can still illustrate the existence of meaningful cor-
relations between modalities. This assertion will be further
validated through two retrieval-based experiments that we
present in the following sections.
Manifestation Estimation. This experiment uses images
as inputs to estimate the manifestations of lumps within
them, achieved by retrieving neighboring manifestations in
the representation space based on the image representations.
We first compute the representations of all manifestations in
the training set using ViKL. For a given test image represen-
tation, we select associated training set manifestations based
on their cosine distances from the image representation,
mirroring the distance metric used in pretraining.
The structured and discrete attributes of manifestations
contribute to their more sparse distribution in the represen-
tation space, compared to other modalities. To counteract
this and achieve robust retrieval outcomes, we implement
a distance threshold strategy. This involves selecting nearby
manifestations relative to the input image and determin-
ing the classification result based on the proportion of
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
12
positive votes in each dimension. Adjusting the classifica-
tion threshold below this value allows for fine-tuning the
balance between sensitivity and specificity. Comprehensive
classification metrics, calculated across the 35 manifestation
dimensions, are detailed in Table 8.
TABLE 8: Average metrics over 35 dimensions of manifestation (%).
For each input image, manifestations whose representationsâ€™ distances
are less than Tdistance will be averaged, and the dimensions exceed
Tclassification in this averaged vector are regarded as positive.
Tdistance
Tclassification
Accuracy
Sensitivity
Specificity
0.4
0.15
72.72
73.30
72.62
0.4
0.12
66.33
80.55
63.81
0.4
0.25
82.80
50.21
88.57
Image-Report Retrieval. In this experiment, we compute
the results by identifying the single nearest neighbor report
in the representation space, leveraging the relatively dense
distribution of reports. This process involves pre-computing
representations for all reports in the training set and then
locating the nearest neighbors within this set using image
representations from the test set. However, evaluating the
accuracy of the retrieved results is challenging due to the
specialized medical knowledge required. To address this,
we employ a straightforward and objective method: analyz-
ing the consistency of BI-RADS categories in the retrieved
reports with those in the ground-truth reports of the corre-
sponding images. BI-RADS evaluations by radiologists are
a universal component of mammography reports and thus
offer a reliable basis for assessing the effectiveness of the
retrieval task.
0
1
2
3
4
5
Ground-Truth BI-RADS
0
1
2
3
4
5
Retrieved BI-RADS
0
0
1
2
1
0
0
3
6
8
1
1
0
6
11
28
6
0
0
6
30
122
21
0
0
2
5
37
408
28
0
0
0
3
33
5
Fig. 4: Confusion matrix of image-report retrieval experiment.
The confusion matrix is depicted in Fig. 4. Notably, the
retrieval results predominantly cluster near the diagonal,
indicating that ViKL effectively matches images with reports
through multimodal contrastive learning.
5.8
ViKL Reduces the Data Requirement in Fine-tuning
A key goal of pretraining is to alleviate the data constraints
often encountered in downstream tasks, where data scarcity
and high annotation costs are prevalent. To evaluate this, we
deliberately reduced the training set of the MVKL dataset to
20%, 40%, 60%, and 80% of its full size. We then conducted
fine-tuning experiments on these subsets to assess how
ViKL and the supervised IM model adapt to varying data
availability in downstream tasks. The outcomes of these
experiments are illustrated in Fig. 5. As anticipated, ViKL
demonstrates a markedly lower sensitivity to reductions in
training data volume compared to the supervised IM model.
This finding underscores ViKLâ€™s efficiency in leveraging
limited data for training.
Moreover, even during pretraining, ViKL exhibits an
ability to reduce data requirements. As discussed earlier,
ViKL-R, pretrained on just a few thousand images from the
MVKL dataset, achieves a performance level comparable
to that of the IM model, which is pretrained on the ex-
tensive ImageNet dataset. This indicates that multimodal
contrastive learning, by providing nuanced guidance for
model optimization, enables more efficient use of limited
training data.
0.2
0.4
0.6
0.8
1.0
Training Set Ratio
68
70
72
74
76
78
AUC%
 
 
ViKL
IM
Fig. 5: ViKL shows superior adaptability compared to the supervised
IM model when fine-tuning on the MKVL dataset with reduced train-
ing data, illustrating less performance reduction as the availability of
downstream data decreases.
5.9
ViKL Provides Evidence for Clinical Decision
In our earlier sections, we discussed how ViKL enhances
the image branchâ€™s capability for detailed feature extraction
by using text and manifestations that describe localized
lumps. To directly observe this enhanced feature extraction
ability, we utilize the class activation map (CAM) [62], a
common tool for visualizing neural network activations.
Specifically, we employ Grad-CAM [63] to generate acti-
vation maps from the third layer of the ResNet50 model,
which has been fine-tuned using ViKL on the MKVL dataset.
Additionally, for accurate visualization and comparison, we
include markings of lumps as annotated by professional
radiologists. The results of this visualization are showcased
in Fig. 6. These visualizations demonstrate ViKLâ€™s effective
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
13
localization of small lesions, highlighting its refined ability
to capture minute features in medical imaging.
5.10
ViKL Enhances Confidence Calibration
Guo et al. [49] delves into the critical issue of confidence
calibration in neural networks, emphasizing that an ideal
classifierâ€™s predicted confidence should accurately reflect
the actual likelihood of a correct prediction. They noted
that modern neural networks, despite improved classifi-
cation metrics, often suffer from miscalibrated confidence
estimates. This issue is particularly pertinent in healthcare
applications, where users interpret a classifierâ€™s confidence
as an indicator of lesion severity and ambiguity.
To address neural network miscalibration, MÂ¨uller et al.
[50] confirmed the effectiveness and ease of integration of
label smoothing. We incorporated label smoothing during
ViKLâ€™s pretraining phase, primarily to address the challenge
of multiple images corresponding to the same report. In this
section, we shift our focus to explore how label smoothing
contributes to improving model calibration.
Following the method outlined by Guo et al. [49], we
assess calibration by categorizing results into bins based
on predicted confidence, and then calculating classification
accuracy within each bin. This approach allows us to create
reliability diagrams, where a line graph plots confidence
against accuracy. Additionally, we calculate the expected
calibration error (ECE) by weighting the absolute difference
between confidence and accuracy in each bin by the binâ€™s
sample size. Ideally, in these diagrams, the plotted curve
should closely follow the diagonal line.
We compared the calibration performance of three mod-
els fine-tuned on the MKVL dataset: ViKL with label
smoothing (ViKL w/ LS), ViKL without label smoothing
(ViKL w/o LS), and the ImageNet-supervised model (IM).
Notably, label smoothing was only applied during pretrain-
ing, with no additional implementation during fine-tuning.
Our analysis employed 10 bins across the 0.0-1.0 confidence
range, excluding bins with fewer than 10 samples for nu-
merical stability. The resulting reliability diagrams and ECEs
are illustrated in Fig. 7. The analysis reveals that multimodal
contrastive learning inherently improves model calibration.
This may stem from the limitations of IM in building a
semantically meaningful representation space using one-hot
labels in supervised training, which undermines the relia-
bility of confidence scores. Conversely, integrating text and
manifestations appears to alleviate this limitation. Further
enhancing this, label smoothing refines the representation
space, granting ViKL w/ LS superior calibration capabilities.
5.11
ViKL Crafts A High-quality Representation Space
In the preceding sections, we explored how ViKL enhances
its representation space, particularly from the perspective
of calibration. This section delves deeper, focusing squarely
on the representation space itself to understand this en-
hancement. Wang and Isola [48] provided a framework to
evaluate the quality of a representation space based on
two key properties: Alignment and Uniformity. Alignment
implies that semantically similar samples should be proxi-
mal in the representation space, while uniformity suggests
that representations should be evenly dispersed throughout
the space to maximize information preservation. Balancing
these two, often conflicting criteria, is crucial in the realm of
representation learning.
Adopting the methodology of Wang and Isola [48], we
aim for an intuitive understanding of these properties. We
configured ViKLâ€™s representation dimension to 2 (i.e. z âˆˆS2)
and trained it using both multimodal contrastive and su-
pervised learning methods. For the latter, we used labels
from the downstream task. After training with the training
set, we used test set images to visualize the representation
space. To assess alignment, we generated histograms show-
ing distances among positive sample pairs. For uniformity,
we employed kernel density estimation (KDE) to depict
the spread of representations: Gaussian KDE for planar
distribution and von Mises KDE for angular distribution.
These visualizations are presented in Fig. 8.
Multimodal contrastive learning, being the direct train-
ing objective, showcases superior alignment. Additionally,
its representations are more uniformly distributed across
the space, indicating a richer preservation of information
from the input images and thus enhanced adaptability and
robustness in downstream tasks. Beyond a general visual-
ization of the entire test set, we also conducted separate vi-
sualizations for samples classified as benign and malignant.
Intriguingly, despite benign-malignant classification not be-
ing a direct objective of multimodal contrastive learning, the
peak points of their angular distributions are distinct. This
observation suggests that the network has indeed learned
features pertinent to downstream tasks via text and mani-
festations, even if these features are subtle.
6
DISCUSSIONS
Experiment results clearly show that the combination of
images and manifestations outperforms that of images and
text. This suggests that the key improvements in ViKL are
largely due to the inclusion of manifestations. Text-based
feature extraction, requiring complex models like BERT and
a diverse data set, is challenging in the medical field. This
often results in text features incorporating noise that can
hinder representation learning. Additionally, the issue of
a single report corresponding to multiple images, albeit
mitigated by label smoothing, still presents a challenge for
contrastive learning.
Manifestations effectively address these concerns. Each
dimension of a manifestation is closely linked to specific
lesion characteristics, reducing potential noise. The issue
of multiple images matching the same manifestation can
be resolved through deduplication. Remarkably, encoding
for manifestations doesnâ€™t necessitate complex models like
BERT; a simple two-layer linear model is adequate. This
aligns with recent research findings, such as those by Hager
et al. [38], which confirm the effectiveness of combining
tabular data with image data for enhanced performance
in downstream tasks. Moreover, manifestations are well-
integrated into medical practice, as radiologists routinely
interpret these features for diagnosis. This suggests that
annotating manifestations could be seamlessly incorporated
into routine assessments, possibly through a simple check-
box tool. However, depending solely on manifestations has
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
14
Fig. 6: Exemplary Grad-CAM visualizations from ViKL. The first row displays the original mammogram images, while the second row presents the
corresponding Grad-CAMs generated from layer 3 in ViKL. In these second-row images, breast lumps, as annotated by professional radiologists,
are outlined for clear comparison. These visualizations demonstrate ViKLâ€™s capability, achieved through unsupervised multimodal contrastive
learning, to precisely localize the breast lump regions within the entire mammogram.
0.0
0.2
0.4
0.6
0.8
1.0
Confidence
0.0
0.2
0.4
0.6
0.8
1.0
Accuracy
ViKL w/ LS, ECE=0.026
ViKL w/o LS, ECE=0.072
IM, ECE=0.2
Fig. 7: Reliability diagrams highlighting confidence calibration. Benefit-
ing from the integration of multimodal information and the implemen-
tation of label smoothing mechanisms, ViKL demonstrates excellent
properties of confidence calibration.
its drawbacks, particularly in limiting the flexibility of infor-
mation extraction.
In our comparative analysis across three datasets, ViKL
consistently outperformed models pretrained on ImageNet.
This is particularly noteworthy considering the scale of data
used: ViKL was trained on about two thousand samples,
while ImageNet pretraining involved million-level labeled
images; ViKL operates under an unsupervised learning
paradigm, relying solely on pairing information without
the need for pathology labels, whereas ImageNet uses la-
beled images for supervised training. These results not only
demonstrate the efficacy of ViKLâ€™s training approach but
also highlight the importance of domain-specific pretraining
for medical imaging tasks. The substantial domain gap
between general imagery and medical images suggests that
pretraining on similar or related modalities is more benefi-
cial.
While ViKL demonstrates outstanding performance, it is
important to recognize its limitations. 1) ViKL necessitates
disease-specific manifestation sets and relies on the anno-
tation efforts of radiologists. 2) In radiology, manifestations
are defined for specific organs or diseases, which restricts
ViKL from being entirely task-agnostic. In an era that foun-
dation models are increasingly prevalent, possessing task-
agnostic capabilities is highly advantageous. It facilitates
seamless adaptation across diverse datasets and paves the
way for the development of general models capable of zero-
shot learning across tasks. However, this challenge is not
unmanageable. Considering the high information density of
tabular data and the finite variety of lesions and organs, the
prospect of creating a comprehensive, universal manifesta-
tion table for wider applications remains a viable possibility.
Building on the current findings, we foresee several
exciting future research directions for ViKL:
1) Manifestations present a fertile ground for future ex-
ploration. The prospect of creating a universal, task-agnostic
manifestation table and reducing data requirements for
manifestations compared to other modalities opens up nu-
merous research opportunities.
2) The effort involved in developing and annotating
manifestations is noteworthy. ViKL was trained with just
over two thousand complete trimodal data pairs, yet it
outperforms models pretrained on ImageNet. Thereâ€™s room
for improvement as data volumes increase. Clinically, many
image-report pairs exist, and we can infer manifestations
from these to expand our dataset. This expansion could
further train and enhance ViKL, leveraging clinical data
abundance for greater performance.
3) While our primary focus has been on unimodal image
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
15
(a)
0.0
0.5
1.0
1.5
2.0
Cosine Distance
0
50
100
150
200
250
300
Counts
Alignment
Positive Pair Feature Distances
1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0
Uniformity
Feature Distribution
1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0
Benign
Feature Distribution
1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0
Malignant
Feature Distribution
/2
0
/2
Angle
0.00
0.25
0.50
Density
Angular Distribution
/2
0
/2
Angle
0.00
0.25
0.50
Density
Angular Distribution
/2
0
/2
Angle
0.00
0.25
0.50
Density
Angular Distribution
(b)
0.0
0.5
1.0
1.5
2.0
Cosine Distance
0
50
100
150
200
250
300
350
Counts
Alignment
Positive Pair Feature Distances
1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0
Uniformity
Feature Distribution
1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0
Benign
Feature Distribution
1.0
0.5
0.0
0.5
1.0
1.0
0.5
0.0
0.5
1.0
Malignant
Feature Distribution
/2
0
/2
Angle
0.00
0.25
0.50
Density
Angular Distribution
/2
0
/2
Angle
0.00
0.25
0.50
Density
Angular Distribution
/2
0
/2
Angle
0.00
0.25
0.50
Density
Angular Distribution
Fig. 8: Alignment and uniformity analyses of the representation spaces. Panel (a) illustrates the model trained via supervised learning, and panel (b)
depicts the model trained with multimodal contrastive learning. Alignment is demonstrated through histograms representing the distances within
positive sample pairs. Uniformity is conveyed via kernel density estimation (KDE) of the representation distribution. The planar distribution
is visualized using Gaussian KDE, while the angular distribution employs von Mises KDE, with the peak points indicated by vertical lines.
Additionally, on the right side of each panel, uniformity analyses for benign and malignant samples are separately presented, allowing for a
nuanced comparison of the modelâ€™s performance across different sample classifications.
performance for diagnosis, the potential applications of
multimodal pretrained models are vast and extend well
beyond this scope. We can delve into more intricate mul-
timodal tasks, such as inter-modality generation and re-
trieval. Particularly intriguing is the possibility of gener-
ating more comprehensive, reliable, and information-dense
reports by transitioning from images to manifestations, and
ultimately to detailed reports.
4) Although ViKL has showcased implicit capabilities
in lump localization with full mammography images as
input, there is significant scope to extend these capabilities
to explicit localization through specialized task heads. The
current focus has been on benign-malignant screening tasks
in breast lumps. However, with the MVKL dataset provid-
ing detailed lump location labels and segmentation masks,
there is ample room for further research in localization
and segmentation. Given that manifestations offer targeted
feature descriptors for lump areas, diagnoses based on lump
regions could achieve higher precision, albeit contingent on
preceding detection or segmentation models.
5) The manifestation-driven reasoning, particularly in-
tegrating probabilistic graphical models, like Bayesian net-
works, with deep learning for intelligent inference models
that more closely align with the reasoning and decision-
making processes of imaging experts.
ViKL paves the way for future studies that integrate
data-driven and knowledge-driven approaches in medical
AI, offering extensive opportunities for advancing the field.
7
CONCLUSION
We have introduced ViKL, an innovative multimodal con-
trastive learning framework that harmonizes three criti-
cal elements of radiology practice: vision, language, and
knowledge. Each modality contributes its unique strengths,
particularly in terms of information density and flexibility,
collectively enriching the diagnostic accuracy and enhanc-
ing the generalization potential of the framework in var-
ious downstream tasks. A noteworthy aspect of ViKL is
its training approach, which necessitates only paired radi-
ological modalities without the need for pathology labels.
This feature is particularly advantageous, as acquiring large
datasets with pathology labels is often a challenging and
resource-intensive endeavor.
Benefiting from its capability to construct a high-quality
representation space, ViKL effectively calibrates the modelâ€™s
accuracy and confidence, significantly reducing the risk
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
16
of over-confidence. Additionally, ViKL achieves a feature
space characterized by excellent alignment and uniformity.
This well-structured representation space is fundamental
to ViKLâ€™s success in downstream tasks and its ability to
generalize across different datasets. The robustness and
adaptability of ViKL, driven by its advanced representation
learning, demonstrate its potential as a versatile tool in
medical image analysis, capable of providing reliable and
accurate insights across a variety of scenarios.
Looking forward, we see tremendous potential for ViKL
to spur further research in this domain. We encourage the
research community to utilize the MKVL dataset to explore
and expand the boundaries of multimodal learning. We
envision this work contributing significantly to the advance-
ment of medical AI, with the ultimate goal of improving
healthcare outcomes. ViKL thus stands as a testament to the
power of integrating diverse data modalities and a beacon
for future innovations in medical imaging and analysis.
REFERENCES
[1]
A. G. Waks and E. P. Winer, â€œBreast cancer treatment: a review,â€
Jama, vol. 321, no. 3, pp. 288â€“300, 2019.
[2]
F. F. Ting, Y. J. Tan, and K. S. Sim, â€œConvolutional neural network
improvement for breast cancer classification,â€ Expert Systems with
Applications, vol. 120, pp. 103â€“115, 2019.
[3]
M. H. Yap, G. Pons, J. Marti, S. Ganau, M. Sentis, R. Zwiggelaar,
A. K. Davison, and R. Marti, â€œAutomated breast ultrasound le-
sions detection using convolutional neural networks,â€ IEEE jour-
nal of biomedical and health informatics, vol. 22, no. 4, pp. 1218â€“1226,
2017.
[4]
Z. He, M. Lin, Z. Xu, Z. Yao, H. Chen, A. Alhudhaif, and F. Alenezi,
â€œDeconv-transformer (dect): A histopathological image classifica-
tion model for breast cancer based on color deconvolution and
transformer architecture,â€ Information Sciences, vol. 608, pp. 1093â€“
1112, 2022.
[5]
Y. Mo, C. Han, Y. Liu, M. Liu, Z. Shi, J. Lin, B. Zhao, C. Huang,
B. Qiu, Y. Cui et al., â€œHover-trans: Anatomy-aware hover-
transformer for roi-free breast cancer diagnosis in ultrasound
images,â€ IEEE Transactions on Medical Imaging, 2023.
[6]
S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R.
Meyer, A. P. Reeves, B. Zhao, D. R. Aberle, C. I. Henschke, E. A.
Hoffman et al., â€œThe lung image database consortium (lidc) and
image database resource initiative (idri): a completed reference
database of lung nodules on ct scans,â€ Medical physics, vol. 38,
no. 2, pp. 915â€“931, 2011.
[7]
X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers,
â€œChestx-ray8: Hospital-scale chest x-ray database and benchmarks
on weakly-supervised classification and localization of common
thorax diseases,â€ in Proceedings of the IEEE conference on computer
vision and pattern recognition, 2017, pp. 2097â€“2106.
[8]
A. Mogadala, M. Kalimuthu, and D. Klakow, â€œTrends in integra-
tion of vision and language research: A survey of tasks, datasets,
and methods,â€ Journal of Artificial Intelligence Research, vol. 71, pp.
1183â€“1317, 2021.
[9]
Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng,
and J. Liu, â€œUniter: Universal image-text representation learning,â€
in Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow,
UK, August 23â€“28, 2020, Proceedings, Part XXX.
Springer, 2020,
pp. 104â€“120.
[10] Y. Cui, Z. Yu, C. Wang, Z. Zhao, J. Zhang, M. Wang, and J. Yu,
â€œRosita: Enhancing vision-and-language semantic alignments via
cross-and intra-modal knowledge integration,â€ in Proceedings of the
29th ACM International Conference on Multimedia, 2021, pp. 797â€“806.
[11] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-
wal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., â€œLearning
transferable visual models from natural language supervision,â€
in International conference on machine learning.
PMLR, 2021, pp.
8748â€“8763.
[12] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi,
â€œAlign before fuse: Vision and language representation learning
with momentum distillation,â€ Advances in neural information pro-
cessing systems, vol. 34, pp. 9694â€“9705, 2021.
[13] J. Yang, J. Duan, S. Tran, Y. Xu, S. Chanda, L. Chen, B. Zeng,
T. Chilimbi, and J. Huang, â€œVision-language pre-training with
triple contrastive learning,â€ in Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, 2022, pp. 15 671â€“
15 680.
[14] H.-Y. Zhou, X. Chen, Y. Zhang, R. Luo, L. Wang, and Y. Yu, â€œGen-
eralized radiograph representation learning via cross-supervision
between images and free-text radiology reports,â€ Nature Machine
Intelligence, vol. 4, no. 1, pp. 32â€“40, 2022.
[15] N. Komodakis and S. Gidaris, â€œUnsupervised representation
learning by predicting image rotations,â€ in International conference
on learning representations (ICLR), 2018.
[16] L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara, and
D. Rueckert, â€œSelf-supervised learning for medical image analysis
using image context restoration,â€ Medical image analysis, vol. 58, p.
101539, 2019.
[17] Z.
Zhou,
V.
Sodha,
M.
M.
Rahman
Siddiquee,
R.
Feng,
N. Tajbakhsh, M. B. Gotway, and J. Liang, â€œModels genesis:
Generic autodidactic models for 3d medical image analysis,â€
in Medical Image Computing and Computer Assisted Interventionâ€“
MICCAI 2019: 22nd International Conference, Shenzhen, China, Oc-
tober 13â€“17, 2019, Proceedings, Part IV 22.
Springer, 2019, pp. 384â€“
393.
[18] Z. Zhou, V. Sodha, J. Pang, M. B. Gotway, and J. Liang, â€œModels
genesis,â€ Medical image analysis, vol. 67, p. 101840, 2021.
[19] M. Noroozi and P. Favaro, â€œUnsupervised learning of visual
representations by solving jigsaw puzzles,â€ in Computer Visionâ€“
ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
October 11-14, 2016, Proceedings, Part VI. Springer, 2016, pp. 69â€“84.
[20] J. Zhu, Y. Li, Y. Hu, K. Ma, S. K. Zhou, and Y. Zheng, â€œRubikâ€™s
cube+: A self-supervised feature learning framework for 3d medi-
cal image analysis,â€ Medical image analysis, vol. 64, p. 101746, 2020.
[21] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, â€œA simple
framework for contrastive learning of visual representations,â€ in
International conference on machine learning. PMLR, 2020, pp. 1597â€“
1607.
[22] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, â€œMomentum contrast
for unsupervised visual representation learning,â€ in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition,
2020, pp. 9729â€“9738.
[23] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBert: Pre-
training of deep bidirectional transformers for language under-
standing,â€ arXiv preprint arXiv:1810.04805, 2018.
[24] S. Ren, K. He, R. Girshick, and J. Sun, â€œFaster r-cnn: Towards real-
time object detection with region proposal networks,â€ Advances in
neural information processing systems, vol. 28, 2015.
[25] W. Kim, B. Son, and I. Kim, â€œVilt: Vision-and-language transformer
without convolution or region supervision,â€ in International Con-
ference on Machine Learning.
PMLR, 2021, pp. 5583â€“5594.
[26] F. Faghri, D. J. Fleet, J. R. Kiros, and S. Fidler, â€œVse++: Improving
visual-semantic embeddings with hard negatives,â€ in Proceedings
of the British Machine Vision Conference (BMVC), 2018. [Online].
Available: https://github.com/fartashf/vsepp
[27] H. Bao, W. Wang, L. Dong, Q. Liu, O. K. Mohammed, K. Ag-
garwal, S. Som, and F. Wei, â€œVlmo: Unified vision-language
pre-training with mixture-of-modality-experts,â€ arXiv preprint
arXiv:2111.02358, 2021.
[28] J. Li, D. Li, C. Xiong, and S. Hoi, â€œBlip: Bootstrapping language-
image pre-training for unified vision-language understanding
and generation,â€ in International Conference on Machine Learning.
PMLR, 2022, pp. 12 888â€“12 900.
[29] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and
Y. Wu, â€œCoca: Contrastive captioners are image-text foundation
models,â€ arXiv preprint arXiv:2205.01917, 2022.
[30] H. Bao, L. Dong, S. Piao, and F. Wei, â€œBeit: Bert pre-training of
image transformers,â€ arXiv preprint arXiv:2106.08254, 2021.
[31] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski,
D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer et al.,
â€œPali: A jointly-scaled multilingual language-image model,â€ arXiv
preprint arXiv:2209.06794, 2022.
[32] P. Zhai, H. Cong, E. Zhu, G. Zhao, Y. Yu, and J. Li, â€œMvcnet:
Multiview contrastive network for unsupervised representation
learning for 3-d ct lesions,â€ IEEE Transactions on Neural Networks
and Learning Systems, 2022.
[33] Y. Xie, J. Zhang, L. Liu, H. Wang, Y. Ye, V. Johan, and Y. Xia,
â€œRefs: A hybrid pre-training paradigm for 3d medical image
segmentation,â€ Medical Image Analysis, p. 103023, 2023.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
17
[34] Y. Xie, Y. Xia, J. Zhang, Y. Song, D. Feng, M. Fulham, and
W. Cai, â€œKnowledge-based collaborative deep learning for benign-
malignant lung nodule classification on chest ct,â€ IEEE transactions
on medical imaging, vol. 38, no. 4, pp. 991â€“1004, 2018.
[35] G. Zhao, Q. Feng, C. Chen, Z. Zhou, and Y. Yu, â€œDiagnose like
a radiologist: Hybrid neuro-probabilistic reasoning for attribute-
based medical image diagnosis,â€ IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 44, no. 11, pp. 7400â€“7416,
2021.
[36] S. Hussein, K. Cao, Q. Song, and U. Bagci, â€œRisk stratification of
lung nodules using 3d cnn-based multi-task learning,â€ in Infor-
mation Processing in Medical Imaging: 25th International Conference,
IPMI 2017, Boone, NC, USA, June 25-30, 2017, Proceedings 25.
Springer, 2017, pp. 249â€“260.
[37] C. Pan, G. Zhao, J. Fang, B. Qi, J. Liu, C. Fang, D. Zhang, J. Li,
and Y. Yu, â€œComputer-aided tuberculosis diagnosis with attribute
reasoning assistance,â€ in Medical Image Computing and Computer
Assisted Interventionâ€“MICCAI 2022: 25th International Conference,
Singapore, September 18â€“22, 2022, Proceedings, Part I. Springer, 2022,
pp. 623â€“633.
[38] P. Hager, M. J. Menten, and D. Rueckert, â€œBest of both worlds:
Multimodal contrastive learning with tabular and imaging data,â€
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2023, pp. 23 924â€“23 935.
[39] Z. Zhao and G. Yang, â€œUnsupervised contrastive learning of ra-
diomics and deep features for label-efficient tumor classification,â€
in Medical Image Computing and Computer Assisted Interventionâ€“
MICCAI 2021: 24th International Conference, Strasbourg, France,
September 27â€“October 1, 2021, Proceedings, Part II 24.
Springer,
2021, pp. 252â€“261.
[40] Y. Han, C. Chen, A. Tewfik, B. Glicksberg, Y. Ding, Y. Peng,
and Z. Wang, â€œCross-modal contrastive learning for abnormality
classification and localization in chest x-rays with radiomics using
a feedback loop,â€ arXiv preprint arXiv:2104.04968, 2021.
[41] B. Gecer, S. Aksoy, E. Mercan, L. G. Shapiro, D. L. Weaver, and
J. G. Elmore, â€œDetection and classification of cancer in whole
slide breast histopathology images using deep convolutional net-
works,â€ Pattern recognition, vol. 84, pp. 345â€“356, 2018.
[42] B. He, L. BergenstrËšahle, L. Stenbeck, A. Abid, A. Andersson,
ËšA. Borg, J. Maaskola, J. Lundeberg, and J. Zou, â€œIntegrating
spatial gene expression and breast tumour morphology via deep
learning,â€ Nature biomedical engineering, vol. 4, no. 8, pp. 827â€“834,
2020.
[43] Z. Cao, Z. Yang, X. Zhuo, R.-S. Lin, S. Wu, L. Huang, M. Han,
Y. Zhang, and J. Ma, â€œDeeplima: Deep learning based lesion
identification in mammograms,â€ in Proceedings of the IEEE/CVF
International Conference on Computer Vision Workshops, 2019, pp. 0â€“
0.
[44] Y. Liu, F. Zhang, C. Chen, S. Wang, Y. Wang, and Y. Yu, â€œAct like a
radiologist: towards reliable multi-view correspondence reasoning
for mammogram mass detection,â€ IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 44, no. 10, pp. 5947â€“5961,
2021.
[45] M. Kallenberg, K. Petersen, M. Nielsen, A. Y. Ng, P. Diao, C. Igel,
C. M. Vachon, K. Holland, R. R. Winkel, N. Karssemeijer et al.,
â€œUnsupervised deep learning applied to breast density segmenta-
tion and mammographic risk scoring,â€ IEEE transactions on medical
imaging, vol. 35, no. 5, pp. 1322â€“1331, 2016.
[46] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for
image recognition,â€ in Proceedings of the IEEE conference on computer
vision and pattern recognition, 2016, pp. 770â€“778.
[47] Y. Tian, D. Krishnan, and P. Isola, â€œContrastive multiview coding,â€
in Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow,
UK, August 23â€“28, 2020, Proceedings, Part XI 16.
Springer, 2020,
pp. 776â€“794.
[48] T. Wang and P. Isola, â€œUnderstanding contrastive representation
learning through alignment and uniformity on the hypersphere,â€
in International Conference on Machine Learning.
PMLR, 2020, pp.
9929â€“9939.
[49] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, â€œOn calibration
of modern neural networks,â€ in International conference on machine
learning.
PMLR, 2017, pp. 1321â€“1330.
[50] R. MÂ¨uller, S. Kornblith, and G. E. Hinton, â€œWhen does label
smoothing help?â€ Advances in neural information processing systems,
vol. 32, 2019.
[51] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,
â€œRethinking the inception architecture for computer vision,â€ in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2016, pp. 2818â€“2826.
[52] J. Robinson, C.-Y. Chuang, S. Sra, and S. Jegelka, â€œContrastive
learning with hard negative samples,â€ International Conference on
Learning Representations, 2021.
[53] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, â€œRandaugment:
Practical automated data augmentation with a reduced search
space,â€ in Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition workshops, 2020, pp. 702â€“703.
[54] R. S. Lee, F. Gimenez, A. Hoogi, K. K. Miyake, M. Gorovoy,
and D. L. Rubin, â€œA curated mammography data set for use in
computer-aided detection and diagnosis research,â€ Scientific data,
vol. 4, no. 1, pp. 1â€“9, 2017.
[55] I. C. Moreira, I. Amaral, I. Domingues, A. Cardoso, M. J. Cardoso,
and J. S. Cardoso, â€œInbreast: toward a full-field digital mammo-
graphic database,â€ Academic radiology, vol. 19, no. 2, pp. 236â€“248,
2012.
[56] D. Kingma and J. Ba, â€œAdam: A method for stochastic optimiza-
tion,â€ in International Conference on Learning Representations (ICLR),
San Diega, CA, USA, 2015.
[57] Z. Wang, Z. Wu, D. Agarwal, and J. Sun, â€œMedclip: Contrastive
learning from unpaired medical images and text,â€ in Proceedings
of the 2022 Conference on Empirical Methods in Natural Language
Processing, 2022, pp. 3876â€“3887.
[58] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal,
O. K. Mohammed, S. Singhal, S. Som et al., â€œImage as a foreign
language: Beit pretraining for vision and vision-language tasks,â€
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2023, pp. 19 175â€“19 186.
[59] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, â€œMedklip: Med-
ical knowledge enhanced language-image pre-training for x-ray
diagnosis,â€ in Proceedings of the IEEE/CVF International Conference
on Computer Vision, 2023, pp. 21 372â€“21 383.
[60] S. Eslami, C. Meinel, and G. De Melo, â€œPubmedclip: How much
does clip benefit visual question answering in the medical do-
main?â€ in Findings of the Association for Computational Linguistics:
EACL 2023, 2023, pp. 1181â€“1193.
[61] S. Zhang, Y. Xu, N. Usuyama, J. Bagga, R. Tinn, S. Preston,
R. Rao, M. Wei, N. Valluri, C. Wong, M. Lungren, T. Naumann,
and
H.
Poon,
â€œLarge-scale
domain-specific
pretraining
for
biomedical vision-language processing,â€ 2023. [Online]. Available:
https://arxiv.org/abs/2303.00915
[62] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba,
â€œLearning deep features for discriminative localization,â€ in Pro-
ceedings of the IEEE conference on computer vision and pattern recogni-
tion, 2016, pp. 2921â€“2929.
[63] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, â€œGrad-cam: Visual explanations from deep networks via
gradient-based localization,â€ in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 618â€“626.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
18
APPENDIX A
HYPERPARAMETERS
Hyperparameters.
Hyperparameters
Value
Pretraining
Peak learning rate
1e-4
Minimal learning rate
1e-7
Learning rate schedule
Cosine annealing
Training steps
9000
Warmup steps
300
Batch size
64
Temperature Ï„1 for img-mani and img-intra
0.03
Temperature Ï„2 for img-text and mani-text
0.3
Weight decay
1e-4
Input resolution
2562
ManiNeg, maximum Âµ
11
ManiNeg, minimum Âµ
0
ManiNeg, Ïƒ
3
ManiNeg, annealing schedule
Linear
ManiNeg, annealing, step at minimum Âµâ€ 
50th
Downstream, linear evaluation
Peak learning rate
1e-3
Minimal learning rate
1e-6
Learning rate schedule
Cosine annealing
Maximum epochs
1000
Early stopping patience
100
Batch size
48
Weight decay
1e-6
Input resolution
2562
Downstream, fine-tuning
Peak learning rate
5e-5
Minimal learning rate
5e-7
Learning rate schedule
Cosine annealing
Layer-wise learning rate decayâ€¡
0.1
Maximum epochs
1000
Early stopping patience
100
Batch size
48
Weight decay
5e-5
Input resolution
2562
Downstream, linear probe
L2 regularization strength Î»
3.16
Maximum iteration
1000
â€  During the training process, Âµ starts from a maximum value of
11 and stops at a minimum value of 0 at the 50th training step,
remaining unchanged thereafter. The decrease in Âµ is linear.
â€¡ The learning rate on the pretrained parameters is 0.1 times that
of the learning rate on the appended randomly initialized linear
layer.
APPENDIX B
DETAILED NETWORK STRUCTURE
ð¸ð¼:ResNet50
ð¸ð‘‡:BERT
ð¸ð‘€:MLPs
Pretraining
512
768
512
Image
aligner
Text 
aligner
Manifestation
aligner
Multimodal
fuse 
projector
128
128
128
128
128
128
Contrastive Loss
Backbone Model
MLP
Feature Vector (Dimension marked above)
L2 Norm
Detailed network structure of ViKL. For downstream tasks, the EI
branch will be utilized, and a linear layer will be added as required
by the specific evaluation protocol for benign-malignant classification.
