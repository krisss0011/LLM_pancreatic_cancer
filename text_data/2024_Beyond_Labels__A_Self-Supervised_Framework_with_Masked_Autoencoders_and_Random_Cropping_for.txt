Beyond Labels: A Self-Supervised Framework with Masked
Autoencoders and Random Cropping for Breast Cancer
Subtype Classification
Annalisa Chiocchetti2,3,4, Marco Dossena1,4,*,‡, Christopher Irwin1,4,‡ and Luigi Portinale1,4
1DISIT, Computer Science Institute, University of Piemonte Orientale, 15121 Alessandria, Italy
2Center for Translational Research on Autoimmune and Allergic Diseases (CAAD), Department of Health Sciences, University of Piemonte
Orientale, 28100 Novara, Italy
3Interdisciplinary Research Center of Autoimmune Diseases (IRCAD), Department of Health Sciences, University of Piemonte Orientale, Novara,
28100, Italy
4Interdepartmental Research Center on Artificial Intelligence (AI@UPO), University of Piemonte Orientale, 15121 Alessandria, Italy
Abstract
This work contributes to breast cancer sub-type classification using histopathological images. We utilize masked autoencoders
(MAEs) to learn a self-supervised embedding tailored for computer vision tasks in this domain. This embedding captures
informative representations of histopathological data, facilitating feature learning without extensive labeled datasets. During
pre-training, we investigate employing a random crop technique to generate a large dataset from WSIs automatically.
Additionally, we assess the performance of linear probes for multi-class classification tasks of cancer sub-types using the
representations learnt by the MAE. Our approach aims to achieve strong performance on downstream tasks by leveraging
the complementary strengths of ViTs and autoencoders. We evaluate our model’s performance on the BRACS dataset and
compare it with existing benchmarks.
Keywords
histopathology, Cancer sub-typing, Cancer detection, Transformers, Autoencoders, Self-supervised learning
1. Introduction
Histopathological image analysis plays a critical role
in disease diagnosis, particularly in cancer.
Whole-
slide images (WSIs) offer high-resolution views of en-
tire tissue sections, enabling comprehensive evaluation
by pathologists. However, manual analysis of WSIs is
time-consuming and prone to inter-observer variability.
Deep learning models have emerged as powerful tools
to automate histopathological image analysis, offering
the potential for faster, more consistent, and potentially
more accurate diagnoses. Given the high-resolution na-
ture of WSIs, it is interesting to adopt a localized analysis
approach. This involved extracting image patches and ap-
plying tasks like classification at the level of bag of tissue
regions, employing a Multiple Instance Learning (MIL)
framework [1]. Furthermore, the dimensions of the ex-
tracted patches facilitate the application of deep learning
*Corresponding author.
†Authors are listed in alphabetical order.
‡Phd student enrolled in the National PhD in Artificial Intelligence
for Health and Life Sciences, XXXVIII cycle, Università Campus
Bio-Medico, Roma.
$ annalisa.chiocchetti@med.uniupo.it (A. Chiocchetti);
marco.dossena@uniupo.it (M. Dossena);
christopher.irwin@uniupo.it (C. Irwin); luigi.portinale@uniupo.it
(L. Portinale)
© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License
Attribution 4.0 International (CC BY 4.0).
models for computer vision tasks, including Convolu-
tional Neural Networks (CNN) and Vision Transformer
(ViT) architectures, for various objectives such as tissue
classification and cell segmentation.
CNNs have become the most used approach in this
domain due to their ability to capture spatial relation-
ships within images [2, 3]. Architectures such as VGG,
ResNet, and Inception excel at learning hierarchical fea-
tures directly from raw image data, making them ideal
for tasks like tissue classification, tumor segmentation,
and cell detection. However, CNNs struggle to capture
long-range dependencies within complex tissues.
This is where Graph Neural Networks (GNNs) offer a
promising alternative. GNNs represent tissue structures
as graphs [4, 5], where nodes represent cells and edges de-
pict their relationships. This allows GNNs to effectively
model complex interactions between cells, making them
particularly useful for analyzing cell-to-cell communica-
tion or studying the spatial distribution of different cell
types.
Alternatively, for tasks aiming to classify multiple bag
of regions under a single label, the graph representation
can be constructed at the patch level. This approach treats
each region as a node within the graph [6]. ViTs repre-
sent another interesting approach. Unlike CNNs, ViTs
process image patches directly, leveraging transformer
techniques to learn global dependencies across the en-
tire image [7, 8]. This approach proves advantageous
arXiv:2410.12006v1  [cs.CV]  15 Oct 2024
for tasks requiring analysis of intricate tissue patterns
or overcoming limitations imposed by pre-defined filter
sizes in CNNs.
Existing methods try to achieve an optimal latent rep-
resentation that facilitates accurate classification of tissue
regions or whole slide images. We propose a reconstruc-
tion framework called Histopathological Masked AutoEn-
coder (HMAE) followed by a simple classifier. The con-
struction of the latent space of this model is achieved
through the self-supervised objective of reconstructing
the original image. By masking a significant portion of
image patches during input, the encoder is forced to iden-
tify increasingly intricate patterns in the remaining data
to reconstruct the complete image. Finally, a simple MLP
with an output linear layer is applied after the training
of the transformer, in order to perform classification, so
that the model is never exposed to the image class labels
during the learning process.
The datasets we have considered in the present work
are the following.
BRACS. The BReAst Carcinoma Subtyping (BRACS)
dataset [9] is a collection of digital images used to
study breast lesions. It includes 547 WSIs, which are
high-resolution scans of entire tissue samples. Addi-
tionally, 4539 smaller, more specific areas called regions
of interest (ROIs) are extracted from these whole-slide
images.
Each WSI and its corresponding ROIs are
carefully examined and labeled by three pathologists.
These categories encompass three main lesion types:
benign (healthy tissue), malignant (cancerous tissue),
and atypical.
Further details are provided by seven
subcategories within these main types, allowing for a
more precise understanding of the specific lesion.
BACH.
The
BreAst
Cancer
Histology
(BACH)
dataset [10] is a significant resource for researchers
developing computer algorithms to automatically
diagnose breast cancer. It consists of a collection of
digitized images from breast biopsies (WSIs). The ROIs
extracted from each WSI are labeled according to four
classifications: normal tissue, benign tumors, in situ
carcinoma (precancerous cells), and invasive carcinoma
(cancerous cells). There are 100 ROIs for each class.
2. Methodologies
This section investigates the synergy between ViTs and
masking techniques for image reconstruction using a
Masked Autoencoding framework (MAE). We will begin
by investigating the theoretical foundations of ViT and
masked image encoding, exploring their architectural
details and functionalities. Next, we will describe the
process of acquiring and pre-processing a dataset suitable
for MAE training. This dataset will be extracted from
whole slide images (WSI) obtained from BRACS patients.
Finally, we will elucidate the methodology employed to
leverage the feature representations (embeddings) learnt
the ViT model for the task of cancer classification.
2.1. Vision Transformers
Vision Transformers (ViTs) [11] represent a paradigm
shift in computer vision, achieving state-of-the-art results
on image classification tasks while departing from the
traditional dominance of Convolutional Neural Networks
(CNNs). Unlike CNNs that rely on hand-crafted filters
for feature extraction, ViTs leverage the Transformer
architecture, originally successful in Natural Language
Processing (NLP). The key idea is to split an image into
fixed-size patches. These patches are then embedded
into a vector representation and fed into a Transformer
encoder. The Transformer utilizes self-attention [12]
mechanisms to learn long-range dependencies between
different parts of the image, allowing the model to capture
global context crucial for classification.
2.2. Masked Encoder
As first, we divide the image into equally sized non-
overlapping patches.
Then, instead of using all the
patches, we pick a certain number and hide the rest
(masking) [13]. These patches are chosen randomly and
without replacement. We also pick patches randomly
across the whole image to avoid favoring the center (cen-
ter bias).
Using random sampling with a lot of masking makes
it harder for the model to guess what’s missing by just
looking at the nearby patches. Finally, having very few
patches allows us to design a more efficient system for
processing the image, which we’ll explain next. Finally,
unmasked patches (25% of the original image) will be
used as input for a ViT encoder.
2.3. Masked Decoder
After the encoding phase, we have two sets of informa-
tion: encoded data for the visible patches and special
“mask tokens” [14]. Such mask tokens represent missing
patches that the model needs to predict. Positional en-
coding is then added to both the encoded patches and
mask tokens.
This information is passed through another series of
Transformer blocks, acting as a decoder. The decoder
is only used during training to learn how to fill in the
missing patches. It does this by predicting the actual
pixel values for each masked area. The decoder’s output
is a vector of pixel values representing a patch, and the
final step is to put all these patches back together to
form a complete reconstructed image. To measure the
goodness of the reconstruction process we compare the
reconstructed image with the original one pixel by pixel.
We use the mean squared error (MSE) to calculate this
difference.
2.4. Dataset Generation
Our ultimate goal is to distinguish tumor tissue from
healthy tissue and further classify tumor subtypes. This
requires the model to learn informative representations of
the tissues. To achieve this, we randomly extract a large
number of unlabeled image regions from each WSI. This
process aims to capture a diverse pool of tissue regions
encompassing both tumor and non-tumor areas.
The steps for extracting an image from WSI are listed
below:
Region Selection. A square-shaped image patch
is chosen from the whole slide image (WSI). The side
length of the region is determined by sampling from a
normal distribution. The mean and standard deviation of
this distribution is calculated based on the size statistics
of previously annotated ROIs in the BRACS dataset.
Region Quality Control. The average variation is
computed for the pixel intensities within the extracted
patch. This is a measure of dispersion relative to the
mean. If the average variation is greater than a prede-
fined threshold, the region is considered informative and
included in the dataset. This step aims to exclude regions
from uninformative zones, such as borders, which often
exhibit low variability with predominantly white or black
pixels.
2.5. Annotated ROIs classification
This task investigates the ability of a pre-trained model to
classify annotated regions of interest (ROIs) within whole
slide images (WSIs) for tissue type classification. The ap-
proach is divided in two-steps: first, ROIs are fed into the
pre-trained model, which utilizes MAE to generate in-
formative feature vectors for the patches. Subsequently,
a mean aggregation of these patch-level embeddings is
performed in order to obtain a single vector for every
ROI. These embeddings capture the essential character-
istics learnt by the model from the data. Secondly, in
order to carry out the classification task, we resort in a
MLP with one hidden layer. The MLP utilizes the ROI
embeddings as input features, where each class corre-
sponds to a specific tissue type present within the ROIs.
This approach serves as an evaluation of the representa-
tiveness and discriminative power of the unsupervised
embeddings generated by the pre-trained model. By suc-
cessfully classifying different tissue types solely based on
the embeddings, we can demonstrate that the learnt fea-
tures effectively capture relevant biological information
within the ROIs.
3. Experiments
During the experimental phase we applied our method
to breast cancer subtype classification using histopatho-
logical hematoxylin and eosin (H&E) tissue images. Fol-
lowing the BRACS group’s categorization (see Figure 3),
we designed two multi-class classification experiments.
The first experiment differentiates cancerous from non-
cancerous tissues, additionally incorporating an interme-
diate class for atypical cases.
The second experiment extends the classification by
attempting to categorize the tissue into seven distinct
subtypes. Furthermore, we explore the utilization of a
MAE network. Trained to reconstruct the image without
prior labels, the model generates embeddings that will
be analyzed.
We will evaluate the clustering behavior of these
embeddings in latent space and utilize attention maps
to qualitatively assess the image regions on which the
model focuses. The results will be discussed in section 4.
Generalization
experiment.
We
investigated
the generalization capabilities of MAE embeddings
for breast cancer histopathology image classification.
The BACH dataset [10], containing 400 breast cancer
histopathology images classified into four categories,
was used for this evaluation.
The pre-trained MAE
model’s encoder was used to extract feature represen-
tations (embeddings) from the BACH dataset images.
Subsequently, a simple linear classifier was trained on
the four classification labels specific to the BACH dataset.
Experimental setup. The datasets were split into train-
ing, validation, and test sets using an 70/10/20 split. To
account for potential variability, each classification ex-
periment was run 100 times, and the average value for
each chosen metric was reported. The MAE training
was performed on a single Nvidia A40 GPU and took 32
hours.
3.1. Cancer classification
This experiment evaluated our model’s ability to clas-
sify tissue samples as cancerous or non-cancerous. Ad-
ditionally, the model assigned ambiguous cases to an
’atypical tissue’ class. For a standardized comparison, all
models employed a ViT-S/16 encoder architecture. This
facilitated a comprehensive evaluation against various
baseline models, leveraging the groundwork established
in [15]. These models use either an attention mecha-
Figure 1: HMAE architetture. (top) Tissue regions are randomly sampled from the original image. Subsequently, a random
mask is applied, occluding 75% of the image data. (down) The following autoencoder architecture receives the masked image
as input and aims to reconstruct the hidden regions.
Figure 2: Classes in the bracs dataset
nism or a ViT to correlate different region of a tissue
image. They then classify the tissue by considering the
relationships between these regions (the first two in the
table perform the maximum and the average between the
image patches respectively). The model’s performance,
measured by F1-score and AUC, is presented in Table 1.
3.2. Sub-type Cancer classification
This experiment seeks to develop a model capable of
classifying diverse cancer subtypes within the regions
of interest. This task presents a heightened challenge
compared to prior endeavors due to the often subtle mor-
phological distinctions between certain cancer types. Ad-
ditionally, the inherent heterogeneity of the training data,
encompassing a significant proportion of non-tumor tis-
sue secondary to the random extraction process, intro-
duces an inherent class imbalance within the latent rep-
resentation space. We have evaluated performance of
various models based on the weighted F1 metric for a
Table 1
Results of the other models compared to ours.
Model
F1-score
AUC
Max-pooling
0.596±0.029
0.823±0.033
Mean-pooling
0.522±0.038
0.739±0.007
Clam-SB [16]
0.631±0.034
0.863±0.005
TransMIL [17]
0.631±0.030
0.841±0.006
DSMIL [18]
0.577±0.028
0.816±0.028
DTFD-MIL [19]
0.612±0.080
0.870±0.022
IBMIL [20]
0.645±0.041
0.871±0.014
MHIM-MIL [21]
0.625±0.060
0.865±0.017
ABMIL [22]
0.680±0.051
0.866±0.029
ACMIL [15]
0.722±0.030
0.888±0.010
HMAE (ours)
0.704±0.009
0.866±0.003
7-class classification task (Table 2).
Previous exposure to a larger proportion of non-
cancerous tissue during the training phase appears to
have influenced the model’s prediction distribution. This
is reflected in the superior F1-score achieved for the "nor-
mal" class compared to both the benchmark model and
other classes within this investigation (see Table 3). How-
ever, performance on the remaining classes, particularly
the "ic" class with the highest F1-score, remains compara-
ble to previously tested models. These observations sug-
gest that despite a potential bias towards non-cancerous
tissue introduced by the training data, the model retains
the ability to discriminate effectively between different
cancer sub-types.
Table 2
Results of the other models compared to ours in sub-type
cancer classification
Model
F1-weighted
CLAM-MB/B [16]
0.548±0.010
CGC-Net [4]
0.436±0.005
Patch-GNN [5]
0.521±0.006
TG-GNN [23]
0.559±0.001
CG-GNN [23]
0.566±0.013
HACT-Net [23]
0.615±0.009
TransPath [24]
0.567±0.02
TransMIL [17]
0.575±0.007
ScoreNet [25]
0.644±0.009
HMAE (ours)
0.578±0.015
Table 3
Single class classification F1-score (based on top-3 models in
Table 2).
Label
ScoreNet
HACT-Net
Ours
Normal
0.646±0.022
0.616±0.021
0.683±0.022
Benign
0.540±0.022
0.475±0.029
0.485±0.020
UDH
0.484±0.022
0.436±0.019
0.445±0.070
ADH
0.474±0.024
0.404±0.025
0.301±0.015
FEA
0.779±0.007
0.742±0.014
0.702±0.018
DCIS
0.629±0.020
0.664±0.026
0.633±0.019
Invasive
0.910±0.014
0.884±0.002
0.893±0.015
3.3. Generalization capabilities
To evaluate the representational capacity of our model,
we have investigated its ability to generalize to data com-
ing from completly unseen WSIs. This has been achieved
by assessing its performance on a classification task using
a dataset (BACH) not included in the training phase. The
BACH dataset contains ROIs from tumor and non-tumor
tissues, further categorized into four distinct classes. For
consistent evaluation and to isolate the classification per-
formance, the embeddings are fed into a linear classifier.
Results are shown on Table 4.
Table 4
Results using a linear classifier on the BACH dataset.
Model
BRACS →BACH
HACT-Net
0.402±.028
TransPath
0.618±0.048
TransMIL
0.465±0.100
CLAM-SB/B
0.575±0.036
ScoreNet
0.734±0.035
HMAE (ours)
0.673±0.032
3.4. Qualitative Evaluation
This section dives deeper into the quality of the learnt
representations in the MAE model. We have employed
t-SNE dimensionality reduction to visualize the embed-
dings in a 2D latent space, as shown in Figure 3. Each
embedding is plotted and colored according to its cor-
responding class label. Interestingly, even though the
model was trained unsupervised, some degree of class
separation is already evident.
Next, we have analyzed the attention maps, a crucial
component of the Transformer architecture. These rep-
resentations reveal which parts of the input image the
model focuses on the most. By visualizing them, we can
understand which image regions are most relevant for
the model’s predictions. Figure 4 showcases four original
images alongside the attention maps generated by the
final layer of the MAE. Notably, even during unsuper-
vised training, the model appears to differentiate between
connective and glandular tissue. This distinction likely
arises because glandular tissue is structurally more com-
plex, requiring the model to retain more information for
reconstruction.
4. Discussion and future works
In this work we presented an autoencoder architecture
using a Vision Transformer (ViT) as the embedding mod-
ule. This model effectively generates informative repre-
sentations of input images by masking random regions
and reconstructing the masked areas in a self-supervised
learning setting. Applied to histopathological breast can-
cer images, the model successfully captures relevant fea-
tures from both tumor and non-tumor regions. These
learnt representations were then utilized as input to a
classification model, achieving accurate cancer subtype
identification.
Our model exhibits performance on par with current
state-of-the-art techniques, consistently achieving sec-
ond or third place in the conducted benchmarks. Notably,
the latent space of the MAE is optimized solely for im-
age reconstruction, and not specifically for classification
tasks unlike the benchmark models. This observation
strengthens the positive outcomes and underscores the
efficacy of employing random input masking. Further-
more, the model’s generalization capabilities suggest the
potential for being applicable across diverse breast cancer
datasets.
Due to the high spatial resolution of whole slide im-
ages, data augmentation can be easily reachable. By ex-
tracting a larger number of random regions from each
WSI, the dataset size can be significantly expanded while
minimizing redundancy within the generated images.
This step aims to achieve a more balanced representa-
tion of cancerous versus non-cancerous tissue within the
training data without the need for large labeled datasets.
In future work, we will explore the impact of expand-
ing the training dataset to enhance the model’s ability
Figure 3: t-SNE of the RoI embeddings of the BRACS dataset. The sample are colored based on the classes.
Figure 4: Self attention heatmaps.
to differentiate cancerous and non-cancerous regions.
Additionally, to assess the model’s generalizability, we
propose training it on a collection of diverse datasets. Fi-
nally, we also want to explore more in depth the dataset
augmentation given by the random cropping of the WSIs
by experimenting with more sophisticated methods that
could potentially improve the model performance even
more.
References
[1] O. Maron, T. Lozano-Pérez,
A framework for
multiple-instance learning,
Advances in neural
information processing systems 10 (1997).
[2] C. L. Srinidhi, O. Ciga, A. L. Martel, Deep neural
network models for computational histopathology:
A survey, Medical Image Analysis 67 (2021) 101813.
doi:10.1016/j.media.2020.101813.
[3] L. Hou, D. Samaras, T. M. Kurc, Y. Gao, J. E. Davis,
J. H. Saltz, Patch-based convolutional neural net-
work for whole slide tissue image classification,
2016. arXiv:1504.07947.
[4] Y. Zhou, S. Graham, N. A. Koohbanani, M. Shaban,
P.-A. Heng, N. Rajpoot, Cgc-net: Cell graph convo-
lutional network for grading of colorectal cancer
histology images, 2019. arXiv:1909.01068.
[5] B. Aygüneş, S. Aksoy, G. Cinbiş, K. Kosemehme-
toglu, S. Önder, A. Üner,
Graph convolutional
networks for region of interest classification in
breast histopathology, 2020, p. 19. doi:10.1117/
12.2550636.
[6] P. P. et al., Hierarchical graph representations in
digital pathology, 2021. arXiv:2102.11057.
[7] A. G. Gul, e. a. Cetin, Histopathological image classi-
fication based on self-supervised vision transformer
and weak labels, 2022. URL: http://dx.doi.org/10.
1117/12.2624609. doi:10.1117/12.2624609.
[8] X. Wang, e. a. Yang, Transpath: Transformer-based
self-supervised learning for histopathological im-
age classification, 2021, p. 186–195. doi:10.1007/
978-3-030-87237-3_18.
[9] N. B. et al., Bracs: A dataset for breast carci-
noma subtyping in h&e histology images, 2021.
arXiv:2111.04740.
[10] Bach: Grand challenge on breast cancer histology
images, Medical Image Analysis 56 (????) 122–
139. doi:https://doi.org/10.1016/j.media.
2019.05.010.
[11] A. Dosovitskiy, L. e. a. Beyer, An image is worth
16x16 words: Transformers for image recognition
at scale, arXiv preprint arXiv:2010.11929 (2020).
[12] A. V. et al.,
Attention is all you need
(2017).
URL:
http://arxiv.org/abs/1706.03762.
arXiv:1706.03762.
[13] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, R. B. Girshick,
Masked autoencoders are scalable vision learners,
CoRR abs/2111.06377 (2021). URL: https://arxiv.org/
abs/2111.06377. arXiv:2111.06377.
[14] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova,
Bert:
Pre-training of deep bidirectional trans-
formers
for
language
understanding,
2019.
arXiv:1810.04805.
[15] Y. Zhang, H. Li, Y. Sun, S. Zheng, C. Zhu,
L. Yang, Attention-challenging multiple instance
learning for whole slide image classification, 2023.
arXiv:2311.07125.
[16] M. Y. Lu, D. F. K. Williamson, T. Y. Chen, R. J. Chen,
M. Barbieri, F. Mahmood, Data efficient and weakly
supervised computational pathology on whole slide
images, 2020. arXiv:2004.09666.
[17] Z. Shao, H. Bian, Y. Chen, Y. Wang, J. Zhang, X. Ji,
Y. Zhang, Transmil: Transformer based correlated
multiple instance learning for whole slide image
classification, 2021. arXiv:2106.00908.
[18] B. Li, Y. Li, K. W. Eliceiri, Dual-stream multiple in-
stance learning network for whole slide image clas-
sification with self-supervised contrastive learning,
2021. arXiv:2011.08939.
[19] H. Zhang, Y. Meng, Y. Zhao, Y. Qiao, X. Yang,
S. E. Coupland, Y. Zheng, Dtfd-mil: Double-tier
feature distillation multiple instance learning for
histopathology whole slide image classification,
2022. arXiv:2203.12081.
[20] T. Lin, Z. Yu, H. Hu, Y. Xu, C. W. Chen, Interven-
tional bag multi-instance learning on whole-slide
pathological images, 2023. arXiv:2303.06873.
[21] W. Tang, S. Huang, X. Zhang, F. Zhou, Y. Zhang,
B. Liu, Multiple instance learning framework with
masked hard instance mining for whole slide image
classification, 2023. arXiv:2307.15254.
[22] M. Ilse, J. M. Tomczak, M. Welling, Attention-
based deep multiple instance learning, 2018.
arXiv:1802.04712.
[23] P. P. et al., Hact-net: A hierarchical cell-to-tissue
graph neural network for histopathological image
classification, 2020. arXiv:2007.00584.
[24] X. Wang, S. Yang, J. Zhang, M. Wang, J. Zhang,
J. Huang, W. Yang, X. Han, Transpath: Transformer-
based self-supervised learning for histopatholog-
ical image classification,
2021. URL: https://api.
semanticscholar.org/CorpusID:238207468.
[25] T. Stegmüller, B. Bozorgtabar, A. Spahr, J.-P. Thiran,
Scorenet: Learning non-uniform attention and aug-
mentation for transformer-based histopathological
image classification, 2022. arXiv:2202.07570.
