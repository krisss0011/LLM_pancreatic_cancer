Mixture of Multicenter Experts in Multimodal Generative
AI for Advanced Radiotherapy Target Delineation
Yujin Oh1∗, Sangjoon Park2,3∗, Xiang Li1∗, Wang Yi4, Jonathan Paly4, Jason Efstathiou4, Annie
Chan4, Jun Won Kim5, Hwa Kyung Byun6, Ik Jae Lee2, Jaeho Cho2, Chan Woo Wee2, Peng Shu7,
Peilong Wang8, Nathan Yu8, Jason Holmes8, Jong Chul Ye9, Quanzheng Li1, Wei Liu8†, Woong
Sub Koom2†, Jin Sung Kim2,10†, and Kyungsang Kim1†
1Center for Advanced Medical Computing and Analysis, Department of Radiology, Massachusetts
General Hospital and Harvard Medical School, Boston, MA, USA
2Department of Radiation Oncology, Yonsei University College of Medicine, Seoul, South Korea
3Institute for Innovation in Digital Healthcare, Yonsei University, Seoul, South Korea
4Department of Radiation Oncology, Massachusetts General Hospital, Boston, MA, USA
5Department of Radiation Oncology, Gangnam Severance Hospital, Seoul, South Korea
6Department of Radiation Oncology, Yongin Severance Hospital, Yongin, Gyeonggi-do, Korea
7School of Computing, University of Georgia, GA, USA
8Department of Radiation Oncology, Mayo Clinic, AZ, USA
9Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science and Technology, Dae-
jeon, South Korea
∗These authors contributed equally: Yujin Oh, Sangjoon Park, and Xiang Li
†Correspondence to Wei Liu (liu.wei@mayo.edu), Woong Sub Koom (mdgold@yuhs.ac), Jin Sung
Kim (jinsung@yuhs.ac) or Kyungsang Kim (kkim24@mgh.harvard.edu)
1
arXiv:2410.00046v2  [eess.IV]  26 Oct 2024
Abstract
Clinical experts employ diverse philosophies and strategies in patient care, influenced by re-
gional patient populations. However, existing medical artificial intelligence (AI) models are
often trained on data distributions that disproportionately reflect highly prevalent patterns,
reinforcing biases and overlooking the diverse expertise of clinicians. To overcome this lim-
itation, we introduce the Mixture of Multicenter Experts (MoME) approach. This method
strategically integrates specialized expertise from diverse clinical strategies, enhancing the
AI model’s ability to generalize and adapt across multiple medical centers. The MoME-based
multimodal target volume delineation model, trained with few-shot samples including images
and clinical notes from each medical center, outperformed baseline methods in prostate can-
cer radiotherapy target delineation. The advantages of MoME were most pronounced when
data characteristics varied across centers or when data availability was limited, demonstrat-
ing its potential for broader clinical applications. Therefore, the MoME framework enables
the deployment of AI-based target volume delineation models in resource-constrained med-
ical facilities by adapting to specific preferences of each medical center only using a few
sample data, without the need for data sharing between institutions. Expanding the number
of multicenter experts within the MoME framework will significantly enhance the generaliz-
ability, while also improving the usability and adaptability of clinical AI applications in the
field of precision radiation oncology.
2
Introduction
The integration of artificial intelligence (AI) into clinical practice is increasingly recognized for
its potential to enhance patient care, particularly in fields where precision is critical, such as radia-
tion oncology 1,2. AI has shown promise in automating and improving critical aspects of radiation
therapy, such as target volume contouring and treatment planning, including determining the scope
and dose of treatment from a patient’s planning computed tomography (CT) scan 3–5. However,
a significant challenge remains: ensuring the generalizability of AI models across diverse insti-
tutional healthcare settings. As shown in Fig. 1(a), variations across centers, such as differences
in regional populations, imaging modalities, and clinical protocols, contribute to the difficulty of
applying pre-trained AI models developed in one context to distinct data distributions in others.
Recent advancements have begun to tackle this challenge by incorporating multimodal data
considerations into AI models. In radiation therapy, target volume delineation requires more than
just visual cues; factors such as patient’s surgical history, pathology, and disease-specific biomarker
levels are also essential. Multimodal AI models, which combine clinical context with imaging
data, have demonstrated superior generalization capabilities across various datasets compared to
their unimodal models. This is attributed to the crucial role of clinical text, typically presented in a
structured format, in improving the generalizability of AI models across various types of datasets.
Promising outcomes of multimodal models have been demonstrated across various cancer types
4,5. Moreover, the advancement of large language models (LLMs) in medicine 6–8 has accelerated
the development of multimodal AI, improving generalizability across different imaging modalities
and institutional settings.
Despite these advancements, traditional AI models trained on data from a limited number of
institutions continue to suffer from biases that reflect the characteristics of those specific settings.
This bias hinder the adaptability of AI models to diverse clinical settings, resulting in suboptimal
performance. Addressing this issue is especially critical, particularly in radiation therapy where
substantial variability in target volume delineation practices exists, even with consensus guidelines
9–11. Prostate cancer radiotherapy is a prime example, as treatment strategies can vary considerably
across institutions, driven by differences in regional patient populations and institutional protocols
12,13, as illustrated in Fig. 1(b). This variability complicates the implementation of AI-driven tools
for target volume contouring, compared to the relatively broader acceptance of AI for contouring
organs-at-risk (OAR) 14,15.
3
Addressing bias in medical AI models requires effective multicenter training strategies that
can integrate diverse data sources while maintaining confidentiality, given the restricted scope of
clinical data sharing. Federated learning offers a promising solution by decentralizing data storage
and enabling collaboration across institutions 16,17. However, despite its potential, the widespread
adoption of federated learning in practical applications remains limited due to several challenges.
Its performance often falls short compared to centralized data training methods, and issues such as
straggler problems can introduce instability in the training process. Moreover, federated learning is
vulnerable to security threats, including data poisoning and inference attacks, which compromise
model integrity. These limitations have constrained its widespread adoption. Recent advancements
in the Mixture of Experts (MoE) 18 training mechanism have transformed the adaptation of AI mod-
els to diverse data distributions within continual learning frameworks. This approach significantly
enhances the robustness and adaptability when confronted with previously unseen data patterns
19–21. Building upon these advances, we propose a Mixture of Multicenter Experts (MoME) as a
novel multimodal AI training approach to address biased inference and enable AI models to better
reflect the needs of individual institutions. The MoME can not only mitigate data bias but also im-
prove the generalizability and adaptability of medical AI, expanding its applicability across diverse
clinical settings.
The proposed MoME framework integrates a shared path with center-specific router paths,
enabling the model to adapt to diverse data distributions and clinical settings with minimal data
input. As illustrated in Fig. 1(b), this design allows the model to efficiently adapt to each medical
center data distribution with only 10 to 20 samples per center using the router to finetune the best
pre-trained model in the MoME framework. This is a significant improvement over traditional
training methods, which often require hundreds or even thousands of labeled datasets to mitigate
model bias to dominant institution’s data distribution. This adaptability ensures that the model can
account for the unique treatment approaches and delineation strategies of each institution, resulting
in more personalized and precise treatment planning. Moreover, the MoME framework supports
flexible and dynamic inference scenarios across diverse clinical settings. During deployment, the
model can quickly adjust to local practices and patient populations by using a few sample test
datasets from a new center to select the most suitable center-specific router path. Crucially, the
scalability of the MoME framework is enhanced by its distributed model weights1, facilitating
integration across multiple institutions globally and allowing for the selection of the most relevant
inference scenarios tailored to their practices. Moreover, the straightforward MoME framework can
be utilized as a plug-in module for diverse medical AI frameworks, facilitating efficient knowledge
1https://github.com/tvseg/MoME-RO
4
sharing between medical centers while allowing for continuous improvement as new data from
diverse sources is efficiently incorporated.
In this study, we applied MoME to address the limitations of current AI models in prostate
cancer target volume delineation, that exhibits significant variability across institutions. Further-
more, we extend our approach to MoME fine-tuning by incorporating in-house fine-tuning for
hospitals with restricted data sharing to address clinical deployment challenges when adapting AI
models to new data distributions. Our results demonstrate that a MoME-based model not only sig-
nificantly outperforms traditional AI models in target volume contouring, but also aligns its overall
distribution more closely with that of each institution. This improvement is especially pronounced
in cases with diverse data characteristics, highlighting the potential of the MoME approach to ad-
vance the adoption of AI in radiation oncology while addressing the challenges posed by subtle
differences in institutional treatment protocols and patient distributions across each site.
Results
Center-specific radiotherapy target delineation strategy We collected prostate cancer datasets
from five centers and analyzed their distinct characteristics and target delineation tendencies, as il-
lustrated in Fig. 1(a). Centers A (Yonsei Cancer Center; YCC) and B (Yongin Severance; YS), both
in South Korea, exhibit similar treatment protocols and image acquisition settings. Although Cen-
ter B has a different National Comprehensive Cancer Network (NCCN) 22 risk group distribution
compared to Center A, their treatment strategies remain largely similar, resulting in relatively ho-
mogeneous datasets. Center C (Gangnam Severance; GS), also based in South Korea, differs more
from Centers A and B in terms of image acquisition settings, NCCN risk group distribution, and
treatment strategies, although the racial distribution of patients is consistent. In contrast, Centers
D (Massachusetts General Hospital; MGH) and E (MAYO Clinic) in the US show greater variabil-
ity, with substantial differences in national healthcare systems, NCCN risk group distribution, and
treatment strategies. This increasing divergence across centers introduces significant challenges for
model generalization, particularly when both imaging environments and clinical strategies differ
substantially. These findings emphasize the need for adaptive approaches, such as the Mixture of
Multicenter Experts (MoME), which can robustly handle cross-center variability. A detailed sum-
mary of the data distribution is provided in Supplementary Table 1 and further discussed in the
Methods section.
5
Center A
Center B
Center C
EMR
Pelvic CT
Target Volume
Shared path 
Center-specific router path
Multicenter MoME Training
Multimodal AI
Single-center Training
Dataset
×N
×N
Multimodal MoME
Unipath
Center A
Center B
Center C
E
E
E
E
R
R
R
Center-specific 
strategy
Center-specific 
strategy
Gradient Flow
E
Experts
R
R
Dectivated Router
Activated Router
Closed
Center D/E
Dataset
Closed Center D/E
×N
Center-specific 
strategy
b
c
Data export restricted
Closed-center MoME Fine-tuning
In-house setting
E
E
E
R
Trained 
Model
Transfer
R
R
a
Closed Center E
Closed Center D
Center B
Center C
Center A
CT Scanner
NCCN
Risk Group
Distribution
Nation
Hospital
High/
Very high
Intermediate
Low
South Korea
United States
Mayo Clinic
MGH
Yongin Severance
Gangnam Severance
Yonsei Cancer Center
Figure 1: Schematic of multimodal-multicenter AI training using our proposed Mixture of Multicenter Ex-
perts (MoME) approach. (a) Characteristics of each center, which can influence their radiotherapy target
delineation tendencies, emphasize the need for an adaptive AI training approach. (b) Radiotherapy target
delineation strategies for prostate cancer patients vary slightly across various centers, which limits the gen-
eralizability of AI models. (c) Compared to traditional unipath single-center training, our MoME training
leverages both shared and center-specific routing paths. These paths activate relevant expert modules cus-
tomized to the unique characteristics of each center given a few-shot dataset. Moreover, where data export
is restricted, a closed center MoME fine-tuning is employed, allowing for in-house model adaptation.
Analysis of model performance across centers We first trained baseline methods under the tradi-
tional single-center AI training setting (Table 1(a)). Models trained exclusively on data from Center
6
A exhibited overfitting to the training distribution, leading to suboptimal performance on Centers
B and C datasets, which were not seen by the model during training under the single-center setting,
yielding Dice scores of 0.670 and 0.520 for Centers B and C, respectively. However, incorporating
both imaging and textual data, as described in our previous study 4, the multimodal approach im-
proved performance on external datasets compared with vision-only models, with Dice scores of
0.732 and 0.597 for Centers B and C, respectively.
Then, we performed experiments under the newly introduced multicenter AI training setting
(Table 1(b)), which utilizes few-shot datasets from Centers B and C in addition to the training
data from Center A. Despite this, vision-only models showed only marginal improvement over the
single-center training setting, with Dice scores of 0.680 and 0.561 for Centers B and C, respec-
tively. In contrast, when trained with the vanilla multimodal AI, planning target volume (PTV)
delineation performance significantly improved, particularly for Center C, showing 0.667 in Dice
metric, where acquisition settings differed from those at Centers A and B. Moreover, when trained
with our proposed MoME multimodal AI, PTV delineation performance improved further, demon-
strating stable performance above 0.7 by achieving Dice scores of 0.747, 0.750, and 0.716 for
Centers A, B, and C, respectively. For comparison, we re-trained a publicly available 3D text-
driven multimodal segmentation framework, ConTEXTualNet 23. ConTEXTualNet demonstrated
comparable performance to vanilla multimodal AI training but still showed suboptimal perfor-
mance for centers with few-shot datasets, with Dice scores of 0.721 and 0.663 for Centers B and
C, respectively. The performance gap and statistical significance between vision-only and vanilla
multimodal training methods compared to our proposed MoME training method for each center
dataset are visualized in Fig. 2(a).
We performed a qualitative comparison of different approaches under the multicenter AI
training setting to assess their clinical performance. As shown in Fig. 2(b), for an intermediate-
risk, node-negative case, the institutional policy at Center C was to typically avoid prophylactic
nodal irradiation. However, both the vision-only and the multimodal models mistakenly included
nodal regions in the target. In contrast, the proposed multimodal MoME model accurately focused
on the prostate while also providing margin settings that closely aligned with the institution’s
guidelines. Further, as illustrated in Fig. 2(c), in another intermediate-risk, node-negative case,
while all models correctly excluded elective nodal areas, significant differences were observed
in the PTV margins. The institutional policy at Center C tends to define the PTV with a more
generous margin around the gross tumor, a practice not reflected in the tighter margins predicted by
the vision-only and the multimodal models. In contrast, the multimodal MoME model accurately
7
Table 1: PTV delineation performance for prostate cancer patients with MoME.
Dataset
Metric
(a) Single-center AI Training
(b) Multicenter AI Training
Vision-only AI24
Multimodal AI
Vision-only AI24
Multimodal AI
Vanilla4
Vanilla4
MoME (Ours)
ConTEXTualNet23
Center A
(n=169 patients)
Dice ↑
0.727
(0.699-0.753)
0.736
(0.708-0.764)
0.717
(0.689-0.743)
0.755
(0.731-0.777)
0.747
(0.721-0.772)
0.744
(0.718-0.767)
IoU ↑
0.598
(0.569-0.626)
0.610
(0.580-0.639)
0.586
(0.557-0.614)
0.629
(0.602-0.653)
0.623
(0.594-0.648)
0.617
(0.591-0.643)
HD-95 ↓
3.338
(2.860-3.829)
3.690
(3.002-4.373)
3.319
(2.906-3.774)
3.032
(2.638-3.441)
3.407
(2.898-3.956)
3.409
(2.916-3.910)
Center B
(n=117 patients)
Dice ↑
0.670
(0.639-0.700)
0.732
(0.708-0.756)
0.680
(0.650-0.709)
0.738
(0.714-0.762)
0.750
(0.725-0.773)
0.721
(0.697-0.744)
IoU ↑
0.525
(0.492-0.557)
0.593
(0.567-0.620)
0.536
(0.503-0.567)
0.600
(0.574-0.628)
0.617
(0.588-0.643)
0.577
(0.551-0.602)
HD-95 ↓
3.191
(2.981-3.395)
3.155
(2.815-3.599)
3.196
(2.984-3.400)
3.097
(2.839-3.356)
3.040
(2.773-3.294)
3.042
(2.775-3.313)
Center C
(n=129 patients)
Dice ↑
0.520
(0.482-0.557)
0.597
(0.559-0.635)
0.561
(0.527-0.595)
0.667
(0.633-0.698)
0.716
(0.688-0.744)
0.663
(0.627-0.698)
IoU ↑
0.380
(0.343-0.415)
0.458
(0.419-0.495)
0.415
(0.381-0.449)
0.530
(0.495-0.564)
0.582
(0.553-0.613)
0.526
(0.489-0.560)
HD-95 ↓
5.384
(4.696-6.071)
6.402
(5.339-7.454)
5.417
(4.790-6.100)
4.101
(3.457-4.782)
4.132
(3.319-5.045)
5.817
(4.888-6.803)
predicted a broader PTV margin consistent with Center C’s practice, proving its ability to integrate
institutional policies and more precisely delineate the target volume.
To systematically compare and quantify the impact of institutional practices on target delin-
eation beyond individual examples, we introduced the GTV-to-PTV ratio (GPR), defined as the ra-
tio of gross tumor volume (GTV) to PTV. This metric was proposed because it effectively captures
two key factors where inter-institutional differences are likely to arise in prostate cancer radiother-
apy: whether prophylactic nodal irradiation (PNI) is performed and the extent of margin applied to
the gross tumor volume. When nodal irradiation is performed, the pelvic lymph nodes are included
in the PTV, leading to a lower GPR (Fig. 2(d)). Furthermore, the margin applied around the gross
tumor also influences the GPR, making it a comprehensive measure that reflects both these key
aspects of radiotherapy practice. To compare each institution’s approach, we exclusively analyzed
N0 patients, who have been pathologically diagnosed with no metastasis in the lymph nodes, and
excluded N1 patients, whose treatment planning might overlap across institutions.
We first compared the GPR distributions of the ground truth labels, vision-only AI, vanilla
multimodal AI, and MoME models on test data from centers A, B, and C (Figs. 2(e), (f), and (g),
respectively). The results from the ground truth GPR distribution for each dataset highlighted two
distinct peaks around GPR values of 20%, corresponding to cases with higher and lower GPR
values, respectively. This provides a clear visualization of the differences in clinical practices be-
tween the centers. Specifically, for Centers A and B, a greater concentration of data was found in
8
the lower GPR range, indicating that PNI is routinely performed in a significant proportion of pa-
tients, even in intermediate-risk cases. In contrast, the ground truth GPR distribution for Center C
displayed a higher distribution of GPR values, indicating a lower frequency of PNI, particularly for
intermediate-risk patients. This illustrates a critical divergence in clinical practice patterns, where
Centers A and B frequently employ PNI across very-high, high, and intermediate-risk groups.
Meanwhile, Center C tends to exclude PNI, particularly for intermediate-risk patients, reflecting a
more conservative approach in this cohort.
In the predicted GPR distributions across all datasets, the vision-only model consistently ex-
hibited a high prevalence of low GPR values, indicating over-inclusion of regional nodes and a fail-
ure to capture the clinical context accurately. In contrast, both the vanilla multimodal and MoME
models demonstrated GPR distributions closely aligned with the ground truth for Centers A and B,
showing no significant differences in the internal validation setting, as well as in external validation
settings with similar environments and practice patterns. However, in Center C, where acquisition
settings and clinical practices differ, the MoME model showed a closer alignment with the original
GPR distribution compared to the vanilla multimodal AI model, particularly where these varia-
tions were more pronounced. Overall, the MoME model consistently generated GPR values most
closely matching the ground truth, demonstrating its ability to adapt to institution-specific prac-
tices and outperform both the vision-only and vanilla multimodal AI models in customizing target
delineation.
When stratified by risk group, the vision-only model consistently exhibited a high concen-
tration of low GPR values across all risk levels, further confirming its limitations in capturing
crucial clinical subtleties. While the vanilla multimodal AI model showed some improvement, it
still failed to accurately reflect the ground truth distribution. In contrast, the MoME model pro-
duced the most accurate GPR distribution, closely aligning with the original ground truth values
across all risk groups (Supplementary Fig. 2). This highlights the MoME model’s superior capacity
not only to incorporate clinical context but also to adapt to institution-specific practices, resulting
in more precise and context-aware target delineation.
Center-specific inference reflects radiotherapy planning strategy During inference, a key ad-
vantage of our MoME module is its ability to select a center-specific router tailored to each cen-
ter’s dataset characteristics. This capability allows us to analyze model predictions by choosing a
corresponding or different router path. To evaluate the overall tendencies of each center-specific
router, we first tested the entire test dataset from each center as input, activating the corresponding
9
center-specific routers. Specifically, we visualized the overall trends in how each router captures
the center’s unique target delineation strategy using violin plots of GPR values, enabling us to
assess how target distribution shifts with the application of these center-specific routers.
For Center A, the GTV-to-PTV ratio (GPR) distribution aligned most closely with the cen-
ter’s original practice when using the Center A expert, characterized by frequent prophylactic nodal
irradiation (PNI) and more generous margins, resulting in lower GPR values. The Center B expert
also produced a GPR distribution that closely followed Center A’s practice. In contrast, using the
Center C expert shifted the GPR distribution toward higher values, indicating a lower proportion
of nodal irradiation and the application of tighter margins, consistent with Center C’s practice
pattern. This trend was particularly evident in the intermediate/low-risk group, where clinical dis-
cretion plays a greater role compared to the high/very high-risk group, where PNI was more clearly
indicated (Supplementary Fig. 3(a), (b)). Furthermore, even among the Center A and B experts, the
Center A expert produced a GPR distribution that more closely matched the original ground truth
across all risk groups.
Despite having the smallest proportion of high/very high-risk patients, Center B’s GPR re-
mained relatively low, reflecting treatment practices similar to those of Center A. When using the
Center B expert, the GPR distribution closely aligned with Center B’s original practice. In contrast,
using the Center C expert led to an increase in GPR, indicating less frequent PNI and tighter PTV
margins. Risk group analysis further demonstrated that the Center B expert produced the most
consistent GPR values across all risk groups, closely matching the original GPR distribution (Sup-
plementary Fig. 3(c), (d)). Although the Center B expert provided the closest match, the Center
A expert also produced a GPR distribution more aligned with the ground truth than the Center C
expert, suggesting that treatment protocols at Centers A and B are more similar to each other than
to those of Center C.
Center C exhibited the most distinct pattern among the three centers. While the proportion of
high/very high-risk patients was lower than at Center A, it exceeded that of Center B. In general,
Center C demonstrated higher GPR values, reflecting its clinical practice of performing PNI less
frequently and applying tighter PTV margins. When using the Center C expert, the GPR distri-
bution aligned more closely with Center C’s original clinical patterns compared to other expert
routers, consistently yielding higher GPR values. This suggests a reduced frequency of PNI and
the application of smaller PTV margins, a trend observed across all risk groups, including both
high/very high-risk and low/intermediate-risk groups. Notably, even in the high/very high-risk
10
group, where PNI is typically performed, the GPR remained higher than the ground truth. This
indicates that the Center C expert router may have overemphasized the institution’s practice of
less frequent PNI, potentially reflecting an excessive alignment with Center C’s specific clinical
tendencies (Supplementary Fig. 3(e), (f)).
To further analyze the working mechanism of the proposed MoME module, we visualized
the intermediate processes within the center-specific router and its effects on expert module activa-
tion. During inference, the center-specific router automatically assigns the most appropriate expert
layers for each input within our MoME module. To evaluate how this unique routing process con-
tributes to the model’s adaptation to each center, we counted the number of times each expert
was selected for every patch within the input and normalized these counts by the total number of
patches. Then, we analyzed the frequency of each expert activation for every MoME modules in
each network layer, designating the bottom-most layer as Layer #0 and the top-most layer as Layer
#3.
Firstly, to visualize trends for each center dataset, we averaged the results across all patients
for each center. As shown in Fig. 3(b), the frequency of activated top-k expert modules for each
dataset by using the corresponding center-specific router was visualized, e.g., the Center A-specific
router for the Center A dataset. The activation patterns revealed sparse and distinct expert usage
across layers, with different experts being activated for each center-specific router path. These
variations demonstrates how MoME’s expert activation adapts to the specific characteristics of
each center, potentially enhancing performance across diverse data distributions. Next, we investi-
gated the frequency of activated top-k expert modules for an identical input CT scan. As shown in
Fig. 3(c), the activation patterns for each center-specific router exhibited frequency trends similar
to those observed in Fig. 3(b), indicating the MoME module’s ability to adapt its center-specific
activation, even when processing an individual patient’s CT scan from Center C. In qualitative
analysis for this case, as shown in Fig. 3(d), different center-specific routers resulted in notable
differences in the predicted outputs. Applying the Center A- and B-specific routers to the same
input CT scan produced a result with a slightly enlarged PTV margin and included PNI compared
to the ground truth PTV. In contrast, using the corresponding Center C-specific router yielded a
prediction where the Prostate PTV closely aligned with the ground truth, featuring an appropriate
PTV margin and no PNI. These results indicate that MoME’s center-specific router adaptively se-
lects expert layers, optimizing the model predictions to each center’s characteristics and enhancing
its generalizability across diverse data distributions.
11
Data efficient few-shot fine-tuning on closed center dataset To evaluate the proposed MoME
training method in closed center settings, where data export is restricted, we conducted few-shot
fine-tuning in-house. We adapted the pre-trained model to the newly introduced center dataset,
leveraging the computing resources available within the center. To compare the training efficiency
of each vision-only, vanilla multimodal, and MoME model, we monitored the performance of each
method in PTV contouring as the size of the few-shot fine-tuning dataset progressively increased.
Table 2 summarizes the performance of these methods across diverse closed center datasets.
For Center D, in the 0-shot inference setting, both vision-only and multimodal models showed
suboptimal Dice scores of 0.384 and 0.410, respectively. In contrast, the MoME approach, which
utilized the Center B-specific router due to its policy similarities with Center D, achieved a higher
Dice score of 0.465 with Center B-specific inference. In the subsequent few-shot fine-tuning pro-
cess starting from the pre-trained checkpoint, the performance gap between methods became more
pronounced as the number of fine-tuning shots increased. With 1-shot, 2-shot, and 3-shot fine-
tuning, the MoME model demonstrated superior performance, yielding Dice scores of 0.503,
0.558, and 0.582, respectively. Notably, the MoME model consistently outperformed both the
vision-only and multimodal models across all fine-tuning settings. Fig. 4(a) further compares the
performance of different methods for each fine-tuning shot.
For Center E, in the 0-shot inference setting as shown in Table 2, both vision-only and mul-
timodal models exhibited limited effectiveness, with Dice scores of 0.327 and 0.476, respectively.
In contrast, the MoME approach, when utilizing Center C-specific routers, achieved a higher Dice
score of 0.558. This is notable because Center C shares the most similar data acquisition condi-
tions with Center E, as detailed in Supplementary Table 1. Then, we further fine-tuned the network
using Center E dataset. In the few-shot fine-tuning settings, the multimodal MoME consistently
outperformed baseline models. Specifically, the MoME approach achieved Dice scores exceeding
0.6 across all datasets, with Dice scores of 0.630, 0.631, and 0.648 for the 1-shot, 2-shot, and 3-
shot settings, respectively. The performance gains compared to vision-only and vanilla multimodal
training mechanisms are also illustrated in Fig. 4(b). For both closed center fine-tuning experi-
ments, our proposed MoME fine-tuning method demonstrated generalizability and adaptability for
distinct radiotherapy delineation strategies.
We further analyzed the GPR distribution within the closed center setting (Fig. 4(c-d)). The
ground truth GPR distribution, characterized by two peaks around 20%, served as the reference
point. During zero-shot inference, the vision-only model consistently skewed towards lower GPR
12
Table 2: PTV delineation performance on closed center dataset with different size of few-shot fine-tuning
dataset.
Method
Metric
Closed Center D (n=45 patients)
0-shot
1-shot
2-shots
3-shots
Vision-only AI
Dice ↑
0.384
(0.320-0.448)
0.401
(0.334-0.468)
0.420
(0.360-0.476)
0.473
(0.433-0.513)
IoU ↑
0.263
(0.211-0.316)
0.279
(0.223-0.334)
0.288
(0.238-0.336)
0.321
(0.287-0.355)
HD-95 ↓
6.507
(5.577-7.511)
6.651
(5.712-7.646)
6.706
(5.760-7.762)
6.119
(5.197-7.170)
Multimodal AI
Dice ↑
0.410
(0.368-0.452)
0.411
(0.359-0.464)
0.482
(0.430-0.533)
0.505
(0.451-0.556)
IoU ↑
0.273
(0.238-0.307)
0.276
(0.232-0.322)
0.335
(0.292-0.378)
0.358
(0.311-0.402)
HD-95 ↓
7.202
(6.117-8.315)
9.003
(7.498-10.521)
7.303
(5.944-8.724)
6.402
(5.162-7.685)
Multimodal MoME
Center-specific Inference
Center A
Center B
Center C
Dice ↑
0.444
(0.395-0.495)
0.465
(0.419-0.516)
0.423
(0.377-0.469)
0.503
(0.455-0.551)
0.558
(0.506-0.607)
0.582
(0.529-0.633)
IoU ↑
0.302
(0.262-0.346)
0.318
(0.280-0.363)
0.281
(0.244-0.320)
0.353
(0.309-0.396)
0.407
(0.361-0.452)
0.433
(0.381-0.482)
HD-95 ↓
7.978
(6.451-9.768)
9.152
(7.429-10.999)
10.056
(8.119-11.991)
8.272
(6.620-9.995)
6.853
(5.410-8.585)
5.327
(4.131-6.707)
Method
Metric
Closed Center E (n=73 patients)
0-shot
1-shot
2-shots
3-shots
Vision-only AI
Dice ↑
0.327
(0.295-0.359)
0.500
(0.469-0.533)
0.510
(0.478-0.540)
0.476
(0.438-0.513)
IoU ↑
0.205
(0.180-0.232)
0.345
(0.315-0.374)
0.352
(0.324-0.379)
0.327
(0.293-0.360)
HD-95 ↓
8.790
(7.994-9.567)
5.898
(5.211-6.630)
5.644
(4.982-6.350)
8.248
(7.328-9.177)
Multimodal AI
Dice ↑
0.476
(0.434-0.517)
0.609
(0.571-0.648)
0.604
(0.568-0.641)
0.636
(0.603-0.669)
IoU ↑
0.331
(0.294-0.369)
0.456
(0.418-0.493)
0.449
(0.414-0.485)
0.482
(0.449-0.516)
HD-95 ↓
7.825
(6.487-9.252)
5.355
(4.090-6.778)
4.299
(3.367-5.405)
5.660
(4.734-6.730)
Multimodal MoME
Center-specific Inference
Center A
Center B
Center C
Dice ↑
0.514
(0.471-0.555)
0.511
(0.471-0.550)
0.558
(0.526-0.590)
0.630
(0.588-0.668)
0.631
(0.591-0.667)
0.648
(0.610-0.683)
IoU ↑
0.367
(0.327-0.404)
0.361
(0.325-0.397)
0.400
(0.370-0.430)
0.482
(0.441-0.520)
0.480
(0.440-0.516)
0.497
(0.460-0.532)
HD-95 ↓
6.954
(5.622-8.355)
8.417
(6.880-10.103)
6.716
(5.140-8.495)
3.671
(2.830-4.540)
3.345
(2.657-4.114)
3.970
(3.301-4.684)
Note. Bold metric indicates best performance among different few-shot dataset settings, whereas, underline for among entire trials.
values, failing to adequately capture the clinical context for both Center D and Center E. Simi-
larly, the vanilla multimodal model produced a GPR distribution that deviated significantly from
the ground truth in both centers. Although the MoME method outperformed the vision-only and
vanilla multimodal models, it still showed considerable discrepancies compared to the ground truth
GPR distribution. However, when fine-tuning was performed using limited closed center data in
13
a resource-restricted environment, employing 1-shot, 2-shot, and 3-shot learning, a clear trend of
improvement emerged. As the number of fine-tuning samples from the closed centers increased,
the GPR distribution became progressively more accurate. This trend was observed across all risk
groups in both Center D and Center E, but was particularly pronounced in the high/very high-risk
group of Center D. In this group, the GPR distribution closely aligned with the ground truth after
3-shot fine-tuning, demonstrating significant improvement (Supplementary Fig. 4).
Ablation studies in MoME training strategy We conducted further ablation studies by replacing
each module within the proposed MoME training strategy to assess the contribution of each com-
ponent. First, to evaluate the multicenter training method, we designed different strategies to handle
diverse data distribution: Text Prompt and Vanilla MoE methods. The Text Prompt method incor-
porated the center title, such as Center C, appended to the input clinical data within the baseline
multimodal AI training framework. The Vanilla MoE method used a unified router for all center
data, without a center-specific router. The results in Table 3(a) compare these different training
methods across three datasets. For both Center A and Center B, the Dice score and IoU were
relatively consistent across the three methods, indicating no significant differences among them
when applied to the primary training dataset or datasets with similar settings and distributions. In
contrast, for Center C, our proposed MoME approach showed a significant improvement in Dice
metric by achieving 0.716 compared to the Text Prompt and Vanilla MoE methods with Dice scores
of 0.623 and 0.619, respectively. Moreover, MoME training achieved the lowest HD-95 value of
4.132 compared to 5.760 and 5.630 for Text Prompt and Vanilla MoE, respectively, indicating its
superior performance with reduced significant contouring errors. These results suggest that incor-
porating the center-specific router within our proposed MoME structure enhances adaptability dur-
ing multi-center training, especially when substantial differences in data distribution exist among
centers.
We also evaluated the impact of varying the number of experts within the proposed MoME
framework (ranging from Top-1 to Top-3), as shown in Table 3(b). Our network consists of a
total of 8 expert modules, with the Top-2 method being the baseline. Reducing the number of
selected experts to Top-1 led to sparser center-specific training, while increasing it to Top-3 allowed
greater overlap of experts across centers. The results showed that using Top-1 experts achieved the
best performance for Centers A and B across all metrics, but its performance on Center C was
significantly lower with a Dice score of 0.674 compared to the Top-2 strategy with a Dice score
of 0.716. Meanwhile, Top-3 experts also demonstrated comparable performance for Centers A and
B, but the performance dropped for Center C to below 0.7 in Dice score. These results suggest
14
that optimal performance in the allocation of experts requires balancing the number of experts in
relation to the number of centers to avoid both excessive overlap and excessive sparsity in expert
assignments, ensuring the MoME module is effectively adapted to each center-specific strategy.
Table 3: Ablation studies on the network training strategy.
Dataset
Metric
(a) Multicenter Training Method
(b) Top-k Experts for MoME
Text Prompt
Vanilla MoE
MoME (Top-2 Experts)
Top-1
Top-3
Center A
(n=169 patients)
Dice ↑
0.747
(0.720-0.771)
0.744
(0.718-0.768)
0.747
(0.721-0.772)
0.753
(0.727-0.778)
0.752
(0.727-0.776)
IoU ↑
0.622
(0.592-0.648)
0.618
(0.589-0.643)
0.623
(0.594-0.648)
0.630
(0.602-0.657)
0.627
(0.599-0.653)
HD-95 ↓
3.343
(2.823-3.896)
3.360
(2.870-3.870)
3.407
(2.898-3.956)
2.901
(2.539-3.302)
3.108
(2.695-3.537)
Center B
(n=117 patients)
Dice ↑
0.745
(0.722-0.768)
0.726
(0.699-0.752)
0.750
(0.725-0.773)
0.759
(0.730-0.784)
0.731
(0.703-0.757)
IoU ↑
0.608
(0.582-0.634)
0.588
(0.558-0.618)
0.617
(0.588-0.643)
0.630
(0.598-0.657)
0.595
(0.564-0.624)
HD-95 ↓
3.181
(2.755-3.647)
3.207
(2.932-3.489)
3.040
(2.773-3.294)
3.084
(2.766-3.426)
3.542
(3.186-3.915)
Center C
(n=129 patients)
Dice ↑
0.623
(0.587-0.656)
0.619
(0.583-0.655)
0.716
(0.688-0.744)
0.674
(0.627-0.718)
0.660
(0.609-0.709)
IoU ↑
0.479
(0.442-0.513)
0.477
(0.441-0.514)
0.582
(0.553-0.613)
0.536
(0.486-0.583)
0.528
(0.475-0.580)
HD-95 ↓
5.760
(4.770-6.854)
5.630
(4.834-6.449)
4.132
(3.319-5.045)
5.238
(4.263-6.210)
9.362
(7.510-11.279)
Note. Bold metric indicates best performance among (a), whereas, underline indicates best performance among (b).
Discussion and Conclusion
Since 2016, following the first FDA approval of a deep learning-based computer-aided diagnos-
tic tool for clinical use 25, research on AI applications in healthcare has grown rapidly, leading
to the integration of various AI models into clinical settings 26. Despite these advances, AI in
clinical practice still faces persistent challenges, particularly in biases that arise from limited or
skewed training data distributions 27,28. Specifically, many healthcare datasets underrepresent mi-
nority populations and lack critical demographic data, such as ethnicity and race 29. For instance, a
systematic review of cancer image datasets in dermatology revealed that limited population repre-
sentation hinders these models’ applicability to diverse real-world clinical settings 28. Additionally,
in radiology, where data labeling by experts is standard, label inconsistency often arises due to sub-
jective biases of individual clinicians, impacting the reliability of AI models 30. Data distribution
shifts further complicate model performance in real-world applications over time, as they can lead
to reduced accuracy. Our Mixture of Multicenter Expert (MoME) framework is designed to tackle
these challenges by creating tailored center-specific paths that utilize small, diverse samples to ad-
15
dress inter-institutional variability. This approach aims to enhance model robustness across varied
patient demographics and institutional approaches, potentially aligning AI tools more closely with
real-world clinical diversity and evolving treatment philosophies.
Radiation oncology is a field where these challenges are particularly pronounced. Despite the
existence of international standards such as the National Comprehensive Cancer Network (NCCN)
guidelines 22, expert panel consensus, and textbooks based on extensive clinical evidence, inter-
clinician variability persists in how treatment protocols are applied. Differences in radiotherapy
dose regimens, target structures, and institutional practices are notable. These variations are influ-
enced by factors such as institutional resources, training methodologies, and even cultural factors,
which complicate the establishment of a fully standardized approach across regions and clinical
settings. To address the challenges of variability in patient characteristics and treatment strategies
across different institutions and countries, we proposed the multicenter training strategy that incor-
porates diverse clinical approaches using MoME modules within a multimodal alignment learning
framework. Our center-specific adaptation aims to manage the complexity and heterogeneity of pa-
tient populations and treatment protocols. This method allows the model to learn essential knowl-
edge from large-scale datasets for patient treatment planning while simultaneously specializing in
each center’s unique patient patterns with just one-shot training across five classes, requiring only
10 additional training samples per center, including a one-shot validation dataset.
The superiority of our multimodal MoME training is demonstrated through both quantitative
analyses using traditional metrics like the Dice coefficient and qualitative GPR analysis, which
converts center-specific practice patterns into comparable distributions. This analysis showed that
the MoME framework effectively aligns with individual center practices. Fig. 3(a) reinforces these
findings, showing that center-specific routers enable the model to closely adapt to each center’s
treatment patterns, with similar centers exhibiting comparable trends. Our method proves highly
adaptable in clinical settings with restricted data sharing but necessary adaptation to new data dis-
tributions. As shown in Table 4, MoME enables versatile zero-shot inference scenarios, allowing
selection of the most relevant center-specific router using only a few sample test datasets. This
addresses long-standing challenges in deploying AI models clinically, demonstrating that the most
relevant center-specific inference significantly outperforms the least relevant one and surpasses tra-
ditional vision-only and vanilla multimodal inference. Furthermore, one- to three-shot fine-tuning
using the selected center-specific router network with MoME outperforms traditional AI training
mechanisms for optimizing pre-trained models in deployed clinical settings (Fig. 4). This approach
is particularly valuable for real-world applications with limited sample datasets. Experimental re-
16
sults with varying dataset sizes suggest that our MoME fine-tuning approach efficiently adapts to
each institution’s treatment strategies with minimal multicenter data, especially for complex tasks
like radiotherapy target volume delineation. This demonstrates the potential for AI-based models to
be effectively deployed in clinical scenarios, offering a promising solution for adapting to diverse
patient populations and treatment protocols efficiently.
Despite advantages over traditional AI methods, our multicenter training mechanism has
limitations. The primary drawback is reliance on limited samples for fine-tuning, potentially re-
stricting model robustness in complex clinical environments. Future research should incorporate
larger, more diverse datasets to enhance generalizability. Our method showed relative effective-
ness in closed-center fine-tuning, but overall Dice metric performance remained below 0.7, with
only marginal improvement compared to multimodal AI. The closed-center setting diverges from
MoME’s core rationale of cross-center dataset integration. However, assessment of center A-C
synergistic training revealed notable performance gains with joint dataset use. This suggests utiliz-
ing publicly accessible datasets could significantly improve closed-center fine-tuning, warranting
further exploration. Additionally, evaluation metrics remain constrained. The GPR metric, while
useful for visualizing practice pattern differences, is an indirect measure capturing only certain as-
pects. For instance, in prostate cancer target delineation, the inclusion of seminal vesicles may not
be fully reflected in GPR analysis. To address this, future work will incorporate multiple metrics
into the MoME model for comprehensive fine-tuning performance evaluation.
Lastly, a crucial next step is addressing inter-clinician variability, expanding beyond insti-
tutional differences. This approach aims to account for and mitigate variations in individual clini-
cians’ strategies and practices, potentially enhancing the model’s adaptability and clinical usability
across diverse settings. Implementation will involve an interactive correction system, refining AI-
driven outputs through minimal parameter adjustments based on center-specific learning paths.
This approach aligns the model with diverse clinical perspectives and supports continual learning
reflecting evolving treatment philosophies. Incorporating clinician-specific adaptability could lead
to more personalized patient care and improve the model’s generalizability in diverse real-world
applications.
In conclusion, our study marks a significant step toward enabling collaboration on multicen-
ter datasets despite challenges associated with large-scale data collection and practical constraints
across institutions. The proposd MoME offers an effective method for addressing variability in
radiotherapy target delineation practices. Our approach significantly outperformed recent AI train-
17
ing methods, demonstrating strong generalization to diverse clinical settings and adaptability to
distribution shifts. This adaptability further positions the MoME framework as a promising can-
didate for multicenter collaborations, especially in addressing complex and often debated clinical
decision-making tasks by fostering collaborative synergy and aligning with unique institutional
strategies.
Methods
Ethic committee approval The hospital data deliberately collected for this study were ethically
approved by the Institutional Review Boards (IRB) of the Department of Radiation Oncology
at Yonsei Cancer Center, Department of Radiation Oncology at Yongin Severance Hospital, and
Department of Radiation Oncology at Gangnam Severance Hospital (IRB numbers 4-2023-0179,
9-2023-0161, and 3-2023-0396, respectively), Department of Radiation Oncology at Mayo Clinic
(IRB number 13-005709), and Massachusetts General Hospital (MGH) (IRB number 2022P001512).
The requirement for informed consent was waived due to the retrospective nature of the study.
Target volume delineation in prostate cancer radiotherapy In radiation oncology, treatment
target volumes are classified as Gross Tumor Volume (GTV), Clinical Target Volume (CTV), and
Planning Target Volume (PTV). GTV represents the visible tumor, aligning with the goal of tradi-
tional segmentation to delineate observable image regions. CTV, while sometimes derived directly
from GTV when a gross tumor is present, typically encompasses areas at risk for microscopic
disease. Defining CTV requires consideration of multiple clinical factors, including tumor type,
histological findings, cancer stage according to the Tumor, Node, Metastasis (TNM) classification
system, and, in specific cases, patient age and performance status. PTV further extends CTV to
account for uncertainties in patient setup and positioning.
Radiotherapy for prostate cancer is utilized with definitive, salvage, or palliative intent.
Definitive radiotherapy is applied as a curative option when surgery is not feasible due to ad-
vanced age, comorbidities, or patient preference. Even in cases where surgery is possible, some
patients may choose definitive radiotherapy over radical prostatectomy. For patients who undergo
surgery, salvage radiotherapy is administered when rising PSA levels suggest recurrence or when
recurrent disease is confirmed. Palliative radiotherapy is often used to manage metastatic disease,
such as bone metastases, though target volume delineation in these cases is highly variable due to
differing clinical scenarios 22.
18
In definitive radiotherapy, the target volume typically includes the prostate, seminal vesi-
cles, and areas of suspected extracapsular extension 31. For postoperative radiotherapy, the target
encompasses the prostate bed and the seminal vesicle bed, with consideration of the surrounding
anatomical structures based on the surgical field 32. If lymph node involvement is confirmed, or
if the patient falls within the high-risk or very high-risk groups according to NCCN guidelines
22, pelvic nodal irradiation (PNI) is recommended, even in the absence of radiographic evidence
of lymph node metastasis. Despite the existence of general principles guiding target volume de-
lineation, there is considerable variability in practice, particularly in the specifics of margin sizes
and the inclusion of adjacent structures in cases of suspected locoregional invasion. Intermediate-
risk patients with unfavorable factors may also receive PNI depending on institutional protocols,
although this is not universally applied.
To reduce variability in target volume delineation, multiple consensus guidelines have been
developed for both definitive and postoperative radiotherapy in prostate cancer 22,31. However,
significant inter-institutional variability persists, influenced by multifactorial elements including
clinician-specific preferences, established institutional protocols, diverse educational paradigms,
and region-specific practices that reflect local population demographics and disease characteristics.
Complete standardization remains challenging, emphasizing the need for strategies that can adapt
to the specific practices of individual institutions while maintaining high standards of care.
Dataset characteristics and clinical context In this study, we utilized datasets from five differ-
ent centers. Detailed information regarding the number of patients, tumor stage, histopathological
grading, PSA levels, surgical status, treatment intent, and imaging acquisition protocols for each
center is provided in Supplementary Table 1. Centers A, B, and C are located in South Korea,
and while the patient characteristics vary based on the size and location of the centers, they share
similar ethnic backgrounds. In contrast, Centers D and E, located in the United States, have a more
diverse racial composition compared to the Korean centers (A–C). To address the potential limi-
tations of data sharing between countries, we simulated a closed center environment for Centers
D and E. In this scenario, direct data sharing is restricted, and only model weights are transferred.
This allowed us to evaluate the feasibility of fine-tuning the MoME model in an in-house setting
without exchanging sensitive patient data.
For model training, we utilized the largest dataset from Center A (Yonsei Cancer Center,
Seoul, South Korea). A total of 943 primary prostate cancer patients were randomly split, with 774
patients used for training and 169 for internal validation. Center B (Yongin Severance Hospital,
19
Yongin, South Korea) contributed data from 137 patients. For fine-tuning, 10, 15, or 20 patients
were used under different experimental conditions, with the remaining 117 patients reserved for
external validation. Similarly, Center C (Gangnam Severance Hospital, Seoul, South Korea) pro-
vided data from 149 patients. We used 10, 15, or 20 patients for fine-tuning, while the remaining
129 were used for external validation. For Center D (MGH, Boston, MA, USA), a total of 65 pa-
tients were collected, with 10, 15, or 20 patients used for fine-tuning in the closed center setting,
and the remaining 45 used for external validation. Finally, Center E (Mayo Clinic, Phoenix, AZ,
USA) contributed data from 93 patients, with 10, 15, or 20 patients used for fine-tuning in the
closed center environment, and the remaining 73 patients utilized for external validation.
In terms of clinical characteristics, Center A had a higher proportion of locally advanced
cases, with a higher tendency towards elevated T stages. In contrast, Centers B and C showed
fewer cases with high T stages. This trend was even more pronounced in the U.S. centers (D and
E), where T stages were generally even lower than those observed in Centers B and C. Across
all institutions, N stage showed minimal variation, with most cases being node-negative, which
provided an ideal setting to evaluate institutional policies regarding prophylactic nodal irradia-
tion (PNI). Similar to the T stage trend, the Korean centers (A–C) generally had higher Gleason
scores, indicating a greater prevalence of advanced tumors. This was also reflected in the initial
PSA values (iPSA), where the Korean institutions reported higher values compared to the U.S.
centers. Among them, Center A had the highest iPSA values overall, while the U.S. centers exhib-
ited comparatively lower values. There were also notable differences in the rates of prostatectomy
between the Korean and U.S. centers. In the Korean centers, 40% to 80% of patients underwent
surgery, whereas approximately more than 70% of patients in the U.S. centers received definitive
radiotherapy without surgery. These differences in surgical rates influenced the treatment intent.
In the Korean centers, around 50% to over 80% of patients received adjuvant or salvage radiother-
apy after surgery, while in the U.S. centers, most patients received definitive radiotherapy without
undergoing surgery. Regarding imaging acquisition settings, Centers A and B used similar devices
and followed comparable protocols. While Centers C and E employed different settings from A and
B, they were closely aligned with each other in their imaging acquisition approaches. In contrast,
Center D utilized a distinct combination of devices and protocols, further differentiating it from
the other centers. These similarities and differences in imaging acquisition settings, patient demo-
graphics (e.g., the similarity between Centers A and B), and clinical practices (e.g., the notable
differences between the remaining centers) provided a structured environment to systematically
evaluate the effectiveness of MoME in adapting the model to various national and institutional
treatment strategies.
20
To provide relevant clinical context, we extracted key clinical factors essential for prostate
cancer radiotherapy from the electronic medical records (EMRs) of each center. These factors
were chosen based on their importance for treatment planning and their availability across all
institutions. The curated data were then standardized into a formatted clinical dataset, which was
used as input for both the multi-modal and MoME models, as shown in Supplementary Table 2.
Multimodal MoME framework The detailed schematic of our proposed network is illustrated in
Supplementary Fig. 1. For the image encoder/decoder for each organ extractor and PTV delineation
network, we employed the 3D Residual U-Net 24. For the large language model (LLM), we uti-
lized the pre-trained Llama3-8B-chat 33 model. Our proposed multimodal MoME is composed of
four key steps: (i) prostate segmentation, (ii) interactive multimodal alignment, (iii) center-specific
MoME training, (iv) center-specific MoME inference, and (v) closed-center MoME fine-tuning.
(i) Prostate segmentation To guide PTV delineation based on the prostate region, we first use a
3D Residual U-Net 24 as the organ extraction module, as illustrated in Supplementary Fig. 1 (a).
We trained prostate segmentation module in a supervised manner with the Center A dataset. In the
subsequent PTV training phase, we freeze the prostate segmentation module and use the predicted
prostate mask m ∈RB×HWS×C, where B denotes batch size, H, W, S, C correspond to height,
width, slice, and channel of prostate mask, as an additional input channel, concatenating it with the
input CT scans x ∈RB×HWS×C along the channel dimension.
(ii) Interactive multimodal alignment For our multimodal AI training, we adapted a large lan-
guage model (LLM) to extract clinical data. As illustrated in Supplementary Fig. 1 (b), to efficiently
transfer the extensive knowledge of the LLM, we employed text prompt tuning, as previously in-
troduced in our context-aware 3D segmentation network, LLMSeg 4. By expanding this approach
for fine-grained multimodal alignment, we embeds the entire sentence of clinical data through the
frozen LLM into L token-wise context embeddings g ∈RL×D, where D is the LLM embedding
dimension. To align these context embeddings g with the image embeddings fl ∈RHlWlSl×Chl,
where fl is the output of the l-th layer of the 3D image encoder, with Hl, Wl, and Sl representing
the height, width, and slice of the image embeddings, and Chl the intermediate channel dimension,
we first project g to match the dimensions of each fl using a layer-wise linear transformation. Then,
these linearly projected context embeddings ¯gl ∈RL×Chl are subsequently processed through self-
attention and cross-attention mechanisms with the image embeddings fl within two-way trans-
former modules of SAM 34 structure, resulting in multimodal image embeddings ¯fl ∈RHlWlSl×Chl.
In contrast to the original LLMSeg, for multicenter training, we further introduced a mixture of
21
multicenter experts module within the interactive multimodal alignment module.
(iii) Center-specific MoME training We adopt the sparse MoE training mechanisms18 as the
backbone structure of our proposed center-specific, multicenter training framework. The proposed
MoME module consists of multiple center-specific router network Gc and shallow multi-layer
perceptron neural networks (MLP)-based expert modules. During training, we utilized center flag
c, where c ∈{A, B, C} for representing each Center A, B, and C. Given each data along with this
center flag c, the corresponding center-specific router network Gc is selectively activated during
both training and inference, and it selects the top-k expert modules E, and outputs of each expert
module are then weighted by the output of each center-specific router network.
As illustrated in Supplementary Fig. 1 (c), the multimodal image embeddings ¯fl from each
interactive multimodal alignment module are fed to MoME modules. Then, the outputs of the top-
k experts with produced center-specific weights W c are fused, yielding producing center-specific
outcomes. Then, the weighted summation of each expert output, which represents the center-
specific router path, are combined with ¯fl, which represents the shared path, to yield the final
multimodal MoME image embedding ˜fl, which is computed as follows:
˜fl = ¯fl +
k
X
i=1
Gc
i(¯fl) · Ei(¯fl),
(1)
where Gc() prioritizes each expert’s contribution, and the output ˜fl from each MoME module has
the same shape as ¯fl. We follow the sparse MoE training mechanism 18, where the gating network
G computes sparse weight H by adding Gaussian noise, as follows:
G(x) = Softmax(KeepTop-k(H(x), k))
(2)
H(x)i = (x · W)i + StandardNormal() · Softplus(x · W noise)i
(3)
KeepTop-k(v, k)i =



vi
if vi is in the top k elements of v
−∞
otherwise
(4)
where, Softmax(·) function then normalizes these weights, emphasizing the contribution of the
selected experts, KeepTop-k(·) function selects the k most relevant experts while setting the rest to
−∞, and each W and W noise are trainable weight matrices. We set the hyperparameter k as 2.
After the MoME modules, the final mulimodal MoME embeddings ˜fl for each layer be-
22
come inputs for the decoder module. The final predicted decoder output ˆy is then calculated the
combination of the Cross-entropy (CE) loss and the Dice coefficient (Dice) loss by following:
min
M L = λceLce(ˆy, y) + λdiceLdice(ˆy, y),
(5)
where L(ˆy, y) = −Ex∼PX [yi log p(ˆyi)] ,
(6)
where M denotes our proposed multimodal MoME network, λce and λdice are hyper-parameters
for each CE loss and Dice loss, respectively. y ∈RB×HWS×C is the ground-truth PTV mask.. p(ˆyi)
denotes softmax probability of the i-th pixel within the final predicted output ˆy ∈RB×HWS, which
is defined as:
ˆy = M([x, m]C, g, c)
(7)
where x is the input CT scan, m is the predicted prostate mask, [, ]C denotes the channel-wise
concatenation operator, g is the context embedding corresponds to the input patient x, and c denotes
the center information, where c ∈{A, B, C}.
(iv) Center-specific MoME inference To perform inference using the trained multimodal MoME
network, we conducted center-specific inference based on the center flag c ∈{A, B, C} for the
corresponding datasets from Centers A, B, and C, respectively. For external datasets, such as those
from Centers D or E, we collected sample test datasets from each center and inferenced them
multiple times with every center flag c to activate center-specific router Gc to identify the most
relevant router network for zero-shot inference. By comparing the results from each center-specific
inference, we selected the best-performing router network using the center flag c.
(v) Closed-center MoME fine-tuning To further fine-tune the multimodal MoME network in an
in-house setting, we trained the model with each closed center dataset by utilizing closed center
flag c′ ∈{D, E} for representing each Center D and E, based on the pre-trained model weights
as the starting point. For fine-tuning the model, we followed the center-specific MoME training
approach by utilizing few-shot dataset from each center. For efficiently transferring the pre-trained
knowledge, we further froze the image encoder module.
Data pre-processing and model training For data pre-processing, all chest CT images, prostate
labels, and PTV labels were resampled to a uniform voxel spacing of 1.0 × 1.0 × 3.0 mm3. Image
intensities were truncated between -200 and 250 Hounsfield units (HU) and linearly normalized
to a range between 0 and 1.0. During network training, 3D patches of 384 × 384 × 128 pixels
23
were randomly cropped to include the entire pelvic region, along with the corresponding clini-
cal data, using a batch size of 2. For evaluation, the full 3D CT volumes were processed with
a sliding window approach, using the same patch size of 384 × 384 × 128 pixels. Throughout
training, the entire large language model (LLM) was frozen, while the image encoder/decoder
modules, interactive alignment modules, and text prompts were set as trainable parameters. The
loss function combined binary cross-entropy and Dice loss, with equal weights of 1.0. The net-
work was optimized using the AdamW optimizer 35, with an initial learning rate of 0.0001, for
100 training epochs. For fine-tuning the network, we optimized only the image decoder, interac-
tive multimodal alignment modules, and text prompts, while keeping the image encoder module
and LLM frozen. The learning rate was reduced to 0.00001, and the network parameters were op-
timized for up to 500 fine-tuning epochs. The network was implemented using the open-source
library MONAI2. All experiments were conducted using PyTorch 36 in Python, leveraging CUDA
11.4 on a single NVIDIA RTX A6000 48GB GPU. For in-house model fine-tuning, we further
utilized a single NVIDIA A100 40GB GPU. For multicenter training, we utilized the entire Center
A training dataset, combined with 1-shot, 2-shot, and 3-shot samples for each trial from Centers
B and C with 1-shot validation samples. The few-shot samples were randomly selected based on
Prostate-Specific Antigen (PSA) clusters, where the clusters were categorized as follows: 0 – PSA
values below 5.0; 1 – PSA values below 10.0; 2 – PSA values below 20.0; 3 – PSA values below
30.0; and 4 – PSA values above 30.0. For fine-tuning the pre-trained multimodal MoME to adapt
to each external center, we utilized 1-shot, 2-shot, and 3-shot samples from each center D or E
within their server with 1-shot validation samples.
Evaluation To quantitatively assess PTV delineation performance, we calculated the Dice coeffi-
cient (Dice) and Intersection over Union (IoU) for each patient’s PTV delineation result. We further
calculated the 95th percentile of the Hausdorff Distance (95-HD) 37 to evaluate spatial discrepan-
cies between the ground-truth and predicted contours. For reporting 95-HD, all measured distances
in pixel units were adjusted according to the original pixel resolution and reported in centimeters
(cm). To further evaluate center-specific PTV delineation strategies, we defined the gross tumor
volume (GTV)-to-planning target volume (PTV) ratio (GPR) as the total volume of GTV divided
by the total volume of PTV.
Statistics & reproducibility For statistical analysis, we employed the non-parametric bootstrap
method to estimate confidence intervals (CIs) for each metric. We performed 1,000 resampling it-
erations with replacement from the original dataset to generate bootstrap samples. The mean values
2https://monai.io/
24
and 95% CIs were then derived from the relative frequency distributions of these bootstrap sam-
ples. Statistical comparisons between groups were conducted using a two-tailed Student’s paired
t-test. Sample size determination was not based on statistical methods. No data were excluded from
the analyses, and the experiments were not randomized. The investigator was not blinded to the
allocation during the experiments or outcome assessment.
Data availability
Data availability may be considered by contacting one of the corresponding authors, Kyungsang
Kim (kkim24@mgh.harvard.edu), who will assess compliance with these legal provisions. If deemed
appropriate, data sharing will proceed following formal inter-institutional collaboration agree-
ments. Initial requests will receive a response within 10 working days. Data usage is restricted
to research purposes only, and redistribution is prohibited. No additional documents, such as study
protocols or statistical analysis plans, will be provided, and individual patient data will not be
directly shared.
Code availability
The Pytorch codes for the proposed Multimodal AI used in this study is available at the following
GitHub repository at https://github.com/tvseg/MoME-RO
1. E. Huynh, A. Hosny, C. Guthier, D. S. Bitterman, S. F. Petit, D. A. Haas-Kogan, B. Kann, H. J.
Aerts, and R. H. Mak, “Artificial intelligence in radiation oncology,” Nature Reviews Clinical Oncology,
vol. 17, no. 12, pp. 771–781, 2020.
2. C. Liu, Z. Liu, J. Holmes, L. Zhang, L. Zhang, Y. Ding, P. Shu, Z. Wu, H. Dai, Y. Li, D. Shen, N. Liu,
Q. Li, X. Li, D. Zhu, T. Liu, and W. Liu, “Artificial general intelligence for radiation oncology,” 2023.
3. K. Harrison, H. Pullen, C. Welsh, O. Oktay, J. Alvarez-Valle, and R. Jena, “Machine learning for auto-
segmentation in radiotherapy planning,” Clinical Oncology, vol. 34, no. 2, pp. 74–88, 2022.
4. Y. Oh, S. Park, H. K. Byun, Y. Cho, I. J. Lee, J. S. Kim, and J. C. Ye, “Llm-driven multimodal target
volume contouring in radiation oncology,” Nature Communications, vol. 15, no. 1, p. 9186, 2024.
25
5. P. Rajendran, Y. Chen, L. Qiu, T. Niedermayr, W. Liu, M. Buyyounouski, H. Bagshaw, B. Han, Y. Yang,
N. Kovalchuk et al., “Auto-delineation of treatment target volume for radiation therapy using large
language model-aided multimodal learning,” International Journal of Radiation Oncology* Biology*
Physics, 2024.
6. K. Zhang, R. Zhou, E. Adhikarla, Z. Yan, Y. Liu, J. Yu, Z. Liu, X. Chen, B. D. Davison, H. Ren et al.,
“A generalist vision–language foundation model for diverse biomedical tasks,” Nature Medicine, pp.
1–13, 2024.
7. H.-Y. Zhou, S. Adithan, J. N. Acosta, E. J. Topol, and P. Rajpurkar, “A generalist learner for multifaceted
medical image interpretation,” arXiv preprint arXiv:2405.07988, 2024.
8. K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis,
S. Pfohl et al., “Large language models encode clinical knowledge,” arXiv preprint arXiv:2212.13138,
2022.
9. I. Fotina, C. L¨utgendorf-Caucig, M. Stock, R. P¨otter, and D. Georg, “Critical discussion of evaluation
parameters for inter-observer variability in target definition for radiation therapy,” Strahlentherapie und
Onkologie, vol. 188, no. 2, p. 160, 2012.
10. S. K. Vinod, M. Min, M. G. Jameson, and L. C. Holloway, “A review of interventions to reduce inter-
observer variability in volume delineation in radiation oncology,” Journal of medical imaging and ra-
diation oncology, vol. 60, no. 3, pp. 393–406, 2016.
11. L. Caravatta, G. Macchia, G. C. Mattiucci, A. Sainato, N. L. Cernusco, G. Mantello, M. Di Tommaso,
M. Trignani, A. De Paoli, G. Boz et al., “Inter-observer variability of clinical target volume delineation
in radiotherapy treatment of pancreatic cancer: a multi-institutional contouring experience,” Radiation
oncology, vol. 9, pp. 1–9, 2014.
12. M. Barkati, D. Simard, D. Taussky, and G. Delouya, “Magnetic resonance imaging for prostate bed
radiotherapy planning: an inter-and intra-observer variability study,” Journal of Medical Imaging and
Radiation Oncology, vol. 60, no. 2, pp. 255–259, 2016.
13. R. K. Valicenti, J. W. Sweet, W. W. Hauck, R. S. Hudes, T. Lee, A. P. Dicker, F. M. Waterman, P. R.
Anne, B. W. Corn, and J. M. Galvin, “Variation of clinical target volume definition in three-dimensional
conformal radiation therapy for prostate cancer,” International Journal of Radiation Oncology* Biol-
ogy* Physics, vol. 44, no. 4, pp. 931–935, 1999.
14. F. Shi, W. Hu, J. Wu, M. Han, J. Wang, W. Zhang, Q. Zhou, J. Zhou, Y. Wei, Y. Shao et al., “Deep
learning empowered volume delineation of whole-body organs-at-risk for accelerated radiotherapy,”
Nature Communications, vol. 13, no. 1, p. 6566, 2022.
26
15. L. Zhang, Z. Liu, L. Zhang, Z. Wu, X. Yu, J. Holmes, H. Feng, H. Dai, X. Li, Q. Li et al., “Segment
anything model (sam) for radiation oncology,” arXiv preprint arXiv:2306.11730, 2023.
16. K. Chang, N. Balachandar, C. Lam, D. Yi, J. Brown, A. Beers, B. Rosen, D. L. Rubin, and J. Kalpathy-
Cramer, “Distributed deep learning networks among institutions for medical imaging,” Journal of the
American Medical Informatics Association, vol. 25, no. 8, pp. 945–954, 2018.
17. P. Rajpurkar, E. Chen, O. Banerjee, and E. J. Topol, “Ai in health and medicine,” Nature medicine,
vol. 28, no. 1, pp. 31–38, 2022.
18. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously large
neural networks: The sparsely-gated mixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.
19. G. M. Van de Ven, H. T. Siegelmann, and A. S. Tolias, “Brain-inspired replay for continual learning
with artificial neural networks,” Nature communications, vol. 11, no. 1, p. 4069, 2020.
20. G. Rype´s´c, S. Cygert, V. Khan, T. Trzci´nski, B. Zieli´nski, and B. Twardowski, “Divide and not forget:
Ensemble of selectively trained experts in continual learning,” arXiv preprint arXiv:2401.10191, 2024.
21. J. Yu, Y. Zhuge, L. Zhang, P. Hu, D. Wang, H. Lu, and Y. He, “Boosting continual learning of vision-
language models via mixture-of-experts adapters,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2024, pp. 23 219–23 230.
22. National Comprehensive Cancer Network, NCCN Clinical Practice Guidelines in Oncology: Prostate
Cancer (Version 4.2024), 2024, https://www.nccn.org/professionals/physician gls/pdf/prostate.pdf.
23. Z. Huemann, J. Hu, and T. Bradshaw, “Contextual net: A multimodal vision-language model for seg-
mentation of pneumothorax,” arXiv preprint arXiv:2303.01615, 2023.
24. ¨O. C¸ ic¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger, “3d u-net: learning dense vol-
umetric segmentation from sparse annotation,” in Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Pro-
ceedings, Part II 19.
Springer, 2016, pp. 424–432.
25. S. Benjamens, P. Dhunnoo, and B. Mesk´o, “The state of artificial intelligence-based fda-approved med-
ical devices and algorithms: an online database,” NPJ digital medicine, vol. 3, no. 1, p. 118, 2020.
26. D. Shen, G. Wu, and H.-I. Suk, “Deep learning in medical image analysis,” Annual review of biomedical
engineering, vol. 19, pp. 221–248, 2017.
27. R. Daneshjou, M. P. Smith, M. D. Sun, V. Rotemberg, and J. Zou, “Lack of Transparency and Potential
Bias in Artificial Intelligence Data Sets and Algorithms: A Scoping Review,” JAMA Dermatology, vol.
157, no. 11, pp. 1362–1369, 11 2021.
27
28. D. Wen, S. M. Khan, A. J. Xu, H. Ibrahim, L. Smith, J. Caballero, L. Zepeda, C. de Blas Perez, A. K.
Denniston, X. Liu, and R. N. Matin, “Characteristics of publicly available skin cancer image datasets:
a systematic review,” The Lancet Digital Health, vol. 4, no. 1, pp. e64–e74, 2022.
29. S. Ganapathi, J. Palmer, J. E. Alderman, M. Calvert, C. Espinoza, J. Gath, M. Ghassemi, K. Heller,
F. McKay, A. Karthikesalingam, S. Kuku, M. Mackintosh, S. Manohar, B. A. Mateen, R. Matin, M. Mc-
Cradden, L. Oakden-Rayner, J. Ordish, R. Pearson, S. R. Pfohl, N. Rostamzadeh, E. Sapey, N. Sebire,
V. Sounderajah, C. Summers, D. Treanor, A. K. Denniston, and X. Liu, “Tackling bias in ai health
datasets through the standing together initiative,” Nature Medicine, vol. 28, no. 11, pp. 2232–2233,
2022.
30. A. S. Tejani, T. A. Retson, L. Moy, and T. S. Cook, “Detecting common sources of ai bias: Questions
to ask when procuring an ai solution,” Radiology, vol. 307, no. 3, p. e230580, 2023, pMID: 36943081.
31. C. Salembier, G. Villeirs, B. De Bari, P. Hoskin, B. R. Pieters, M. Van Vulpen, V. Khoo, A. Henry,
A. Bossi, G. De Meerleer et al., “Estro acrop consensus guideline on ct-and mri-based target volume
delineation for primary radiation therapy of localized prostate cancer,” Radiotherapy and Oncology,
vol. 127, no. 1, pp. 49–61, 2018.
32. A. Dal Pra, P. Dirix, V. Khoo, C. Carrie, C. Cozzarini, V. Fonteyne, P. Ghadjar, A. Gomez-Iturriaga,
V. Panebianco, A. Zapatero et al., “Estro acrop guideline on prostate bed delineation for postoperative
radiotherapy in prostate cancer,” Clinical and translational radiation oncology, vol. 41, p. 100638,
2023.
33. AI@Meta, “Llama 3 model card,” 2024. [Online]. Available: https://github.com/meta-llama/llama3/
blob/main/MODEL CARD.md
34. A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg,
W.-Y. Lo et al., “Segment anything,” arXiv preprint arXiv:2304.02643, 2023.
35. D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in International Conference on
Learning Representations (ICLR), San Diega, CA, USA, 2015.
36. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga et al., “Pytorch: An imperative style, high-performance deep learning library,” Advances in
neural information processing systems, vol. 32, 2019.
37. W. R. Crum, O. Camara, and D. L. Hill, “Generalized overlap measures for evaluation and validation in
medical image analysis,” IEEE transactions on medical imaging, vol. 25, no. 11, pp. 1451–1461, 2006.
28
Acknowledgment
This research was supported by the National Research Foundation of Korea (NRF) grant funded
by the Korea government (MSIT) (RS-2024-00345854) to Y.O. and (No. 2022R1A2C2008623)
to J.S.K., and also supported by Basic Science Research Program through the NRF funded by
the Ministry of Education under Grant RS-2023-00242164 to S.P., and by the Hanim Precision
Medicine Center of Yonsei University Health System under Grant number (6-2021-0214) to W.S.K.
Author contributions
Y.O. designed the study, extended the code, conducted all experiments, analyzed data, and con-
tributed to manuscript preparation. S.P. conceptualized the study, gathered and labeled the data,
analyzed data, and also contributed to manuscript preparation. X.L. designed the study, provided
supervision throughout the project, analyzed data, and contributed to manuscript preparation. W.Y.,
J.P., J.E. and A.C. were responsible for data collection of Massachusetts General Hospital and
manuscript preparation. W.S.K., J.W.K., H.K.B., I.J.L., J.C. and C.W.W were responsible for data
collection of South Korea and manuscript preparation. P.S., P.W., N.Y., and J.H. were responsible
for data collection of Mayo Clinic and manuscript preparation. J.C.Y. provided supervision for
conceptualization of the study. Q.L. provided supervision throughout the project, and contributed
to manuscript preparation. W.L., W.S.K., J.S.K., and K.K. provided supervision throughout the
project, from conception to discussion, and assisted in preparing the manuscript.
Competing interests
There is no competing interest.
29
Center A
(n=169 patients)
Center B
(n=117 patients)
Center C
(n=129 patients)
Dice
1.0
0.8
0.6
0.4
0.2
0
p=0.003
p=0.0009
p=3.5E-6
p=9.1E-11
×
Mean
a
Vision-only AI
Multimodal AI
Multimodal MoME
R
PTV delineation performance
f
e
Prostate
Planning Target Volume (PTV)
b
MoME
Multimodal
GT
Vision-only
Input CT
Input Text : <Grade> 8 (4+4) <Stage> cT1b, N0 <Metastasis> Negative <Age> 75 <PSA> 9.5
c
Input Text : <Grade> 7 (4+3) <Stage> cT2a, N0 <Metastasis> Negative <Age> 69 <PSA> 4.5
MoME
Multimodal
GT
Vision-only
Input CT
Inference on Center A (n=150 N0 patients)
Inference on Center B (n=98 N0 patients)
Inference on Center C (n=114 N0 patients)
g
GPR (%) = 100 × Gross tumor volume (GTV) / Planning target volume (PTV)
d
Higher peak
Lower peak
Higher: Radiotherapy targets only the prostate with a narrow margin
Lower: Radiotherapy includes the prostate and pelvic lymph nodes with a generous margin
GPR (%)
Figure 2: Multicenter AI training comparison on radiotherapy target volume delineation. (a) Quantitative
comparison under the multicenter AI training setting. The center line indicates the median, the bounds of
the box represent the interquartile range (from the lower quartile to the upper quartile), and the x mark in-
dicates the mean. The p-values indicate the statistically significant superiority of the proposed multimodal
MoME to baseline methods. (b) In an intermediate-risk patient case, the institution typically does not per-
form prophylactic nodal irradiation, yet both vision-only and multimodal AI erroneously included nodes in
the delineation. In contrast, the multimodal MoME correctly focused on the prostate, excluding the nodes.
(c) In another intermediate-risk, N0 case where none of the models included elective nodal irradiation, the
institution generally defines a larger PTV margin around the prostate. The MoME model delineated the
target volume with a larger margin, consistent with institutional practice, while both vision-only and mul-
timodal AI applied smaller margins. (d) GTV-to-PTV ratio (GPR) distribution. (e-g) GPR for each Centers
A, B, and C patients under different training strategies. The center line indicates the median, and the bounds
of the box represent the interquartile range (from the lower quartile to the upper quartile).
30
Experts
Average center-specific inference
b
Frequency of expert activation
Layer #0 
E0
E1
E2
E5
E4
E3
E6
E7
Layer #1 
Layer #2 
Layer #3
d
Ground truth (Center C)
Center C-specific Inference
Center A-specific Inference
Input Text: <Grade> 7 (4+3) <Stage> cT2a, N0 
<Metastasis> Negative <Age> 79 <PSA> 11.9
c
A (n=169 patients)
B (n=117 patients)
C (n=129 patients)
Router
A (n=169 patients)
B (n=117 patients)
C (n=129 patients)
Router
Individual center-specific inference
Experts
Layer #0 
E0
E1
E2
E5
E4
E3
E6
E7
Layer #1 
Layer #2 
Layer #3
A 
B 
C 
Router
A 
B
C
Router
A 
B
C
Router
A
B
C 
Router
Prostate
Planning Target Volume (PTV)
Center B-specific Inference
A (n=169 patients)
B (n=117 patients)
C (n=129 patients)
Router
A (n=169 patients)
B (n=117 patients)
C (n=129 patients)
Router
Frequency of expert activation
0.00
0.00
0.00
0.56
0.44
0.00
0.00
0.00
0.04
0.02
0.13
0.39
0.09
0.22
0.03
0.07
0.05
0.66
0.07
0.05
0.03
0.10
0.01
0.02
0.40
0.08
0.44
0.01
0.01
0.06
0.00
0.00
0.10
0.11
0.09
0.07
0.09
0.07
0.09
0.38
0.16
0.17
0.17
0.19
0.21
0.03
0.04
0.04
0.00
0.08
0.16
0.68
0.05
0.00
0.00
0.01
0.15
0.10
0.18
0.19
0.11
0.19
0.03
0.04
0.04
0.09
0.26
0.12
0.10
0.06
0.07
0.26
0.05
0.21
0.00
0.07
0.04
0.08
0.12
0.44
0.01
0.07
0.02
0.21
0.14
0.27
0.26
0.01
0.20
0.13
0.17
0.10
0.06
0.16
0.05
0.12
0.00
0.00
0.00
0.42
0.34
0.18
0.00
0.05
0.07
0.02
0.12
0.35
0.14
0.19
0.04
0.06
0.05
0.63
0.08
0.06
0.03
0.08
0.05
0.02
0.05
0.25
0.00
0.07
0.04
0.10
0.10
0.39
0.01
0.06
0.01
0.20
0.16
0.27
0.28
0.01
0.19
0.14
0.18
0.12
0.07
0.14
0.04
0.11
0.01
0.04
0.34
0.53
0.04
0.02
0.01
0.01
0.18
0.11
0.12
0.13
0.11
0.25
0.04
0.05
0.05
0.05
0.18
0.09
0.09
0.17
0.12
0.25
0.35
0.05
0.11
0.03
0.02
0.40
0.03
0.01
0.09
0.07
0.03
0.12
0.11
0.11
0.18
0.29
0.11
0.18
0.10
0.21
0.23
0.05
0.08
0.04
Center A
Center B
Center C
a
Center-specific inference on Center B 
(n=98 N0 patients) 
Center-specific inference on on Center C 
(n=114 N0 patients) 
Center-specific inference on Center A 
(n=150 N0 patients) 
Figure 3: Center-specific inference with the proposed multimodal MoME model. (a) Radiotherapy planning
strategy change with different center-specific inference in GTV-to-PTV ratio (GPR) distribution. Given each
center dataset as input, the corresponding center inference mostly reflects the planning target strategy. (b)
Visualization of average frequency of activated top-k experts for each center dataset with corresponding
center-specific router. (c) Visualization of frequency of activated top-k experts for an individual case. For
(a-c), the center line indicates the median, and the bounds of the box represent the interquartile range (from
the lower quartile to the upper quartile). (d) Qualitative comparison of center-specific inference. Given a
CT scan from Center C dataset, activating Center A and B-specific router results in a slightly enlarged PTV
margin with prophylactic nodal irradiation, while the correct Center C-specific router aligns closely with the
ground truth.
31
0.2
0.3
0.4
0.5
0.6
0.7
0-shot
1-shot
2-shots
3-shots
Dice
Few-shot Fine-tuning Dataset Size
0.2
0.3
0.4
0.5
0.6
0.7
0-shot
1-shot
2-shots
3-shots
Dice
Few-shot Fine-tuning Dataset Size
a
b
Closed Center D Evaluation (n=45)
Closed Center E Evaluation (n=73)
Vision-only
Multimodal
MoME
c
MoME Fine-tuning
Zero-shot Inference
Inference on Closed Center D (n=36 N0 patients)
Vision-only AI
Multimodal AI
Multimodal MoME
Ground Truth
MoME Fine-tuning
(95% CI)
(95% CI)
(95% CI)
d
MoME Fine-tuning
Zero-shot Inference
Inference on Closed Center E (n=64 N0 patients)
Figure 4: Closed center dataset evaluation. (a) Closed Center D evaluation in Dice metric based on varying
few-shot fine-tuning dataset sizes. (b) GTV-to-PTV ratio (GPR) distribution for each method, and MoME
fine-tuning result with varying number of few-shot fine-tuning of the closed center D dataset. For (a-b),
the Dice metric for each trial is presented as mean values (center lines) with 95th percentile of confidence
intervals calculated with the non-parametric bootstrap method (shaded areas). (c) Closed Center E evaluation
in Dice metric based on varying few-shot fine-tuning dataset sizes. (d) GPR distribution comparison on
closed center E dataset. For (c-d), the center line indicates the median, and the bounds of the box represent
the interquartile range (from the lower quartile to the upper quartile).
32
Supplementary Information
Supplementary Table 1: Details of data partitioning and characteristics for each center.
Center
Center A
Center B
Center C
Center D
Center E
Hospital
Yonsei Cancer Center
Yongin Severance
Gangnam Severance
MGH
MAYO Clinic
Data split
Train (n=774)
Test (n=169)
Train (n=10/15/20†)
Train (n=10/15/20†)
Fine-tune (n=10/15/20†)
Fine-tune (n=10/15/20†)
Test (n=117)
Test (n=129)
Test (n=45)
Test (n=73)
T stage
T1
31 (4.1%)
5 (3.0%)
1 (0.7%)
10 (6.7%)
36 (55.3%)
30 (32.3%)
T2
231 (30.6%)
58 (34.3%)
55 (40.1%)
78 (52.3%)
12 (18.5%)
28 (30.1%)
T3
435 (57.7%)
100 (59.2%)
67 (48.9%)
49 (32.9%)
17 (26.2%)
32 (34.4%)
T4
57 (7.6%)
6 (3.6%)
14 (10.2%)
12 (8.1%)
0 (0%)
3 (3.2%)
N stage
N0
676 (89.7%)
150 (89.9%)
118 (86.1%)
137 (91.9%)
55 (84.6%)
86 (92.5%)
N1
78 (10.3%)
19 (10.1%)
19 (13.9%)
12 (8.1%)
10 (15.4%)
7 (7.5%)
Gleason score
5 (2+3)
20 (2.6%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
0 (0%)
6 (3+3)
42 (5.4%)
6 (3.2%)
17 (12.4%)
17 (11.4%)
6 (9.2%)
9 (9.6%)
7 (3+4, 4+3)
318 (41.1%)
58 (36.0%)
57 (41.6%)
70 (47.0%)
38 (58.5%)
53 (57.0%)
8 (3+5, 4+4, 5+3)
150 (19.4%)
43 (25.4%)
22 (16.1%)
22 (14.8%)
8 (12.3%)
15 (16.1%)
9 (4+5, 5+4)
225 (29.1%)
58 (33.3%)
35 (25.5%)
35 (23.5%)
13 (20.0%)
15 (16.1%)
10 (5+5)
19 (2.5%)
4 (2.1%)
6 (4.4%)
5 (3.4%)
0 (0%)
1 (1.1%)
Initial PSA
39.3 (0.3-3865.0)
39.3 (0.6-682.0)
27.7 (0.9-217.0)
22.2 (2.99-281.67)
12.5 (0.2-126.0)
9.5 (0-156.0)
Prostatectomy
Yes
511 (66.0%)
129 (76.3%)
55 (40.1%)
70 (47.0%)
9 (13.8%)
30 (32.3%)
No
263 (34.0%)
40 (23.7%)
82 (59.9%)
79 (53.0%)
56 (86.2%)
63 (67.7%)
Therapy purpose
Definitive
270 (34.9%)
30 (17.8%)
82 (59.9%)
79 (53.0%)
56 (86.2%)
63 (67.7%)
Postoperative
74 (9.6%)
19 (11.2%)
14 (10.2%)
3 (2.0%)
3 (4.6%)
20 (21.5%)
Salvage
431 (55.7%)
120 (71.0%)
41 (29.9%)
67 (45.0%)
6 (9.2%)
10 (10.8%)
CT Scanner
Manufacturer
Canon
Canon
Canon
SIEMENS
GE
SIEMENS
Model
Aquilion LB
Aquilion LB
Aquilion LB
SOMATOM
Discovery RT
SOMATOM
Scan mode
Helical
Helical
Helical
Helical
Helical
Helical
Filter type
LARGE
LARGE
LARGE
FLAT
BODY
FLAT
kVp
120
120
120
120
140
120
Spatial pixel size (mm)
0.977
0.977
1.367
1.269
0.977
1.269
Slice thickness (mm)
2
2
3
5
1.25
2
Note. † indicates utilized samples for each 1-shot / 2-shots / 3-shots training for each prostate specific antigen (PSA) cluster. PSA clusters (0-4) are categorized as:
0 - PSA values below 5.0, 1 - PSA values below 10.0, 2 - PSA values below 20, 3 - PSA values below 30, and 4 - PSA values above 30.
‘
33
Supplementary Table 2: Examples of the curated input clinical data from EMR data.
Center
EMR Data (Parsed information)
Input Clinical Data
A,B,C
61-years old patient.
#1. Prostate, Adenoca, GS7(4+3), pT3aN0M0, Stage IIIB
- Tumor location: Both lobes [Index tumor: right, posterior, volume (1.44cc)]
- Extraprostatic extension: Present, focal (right posterior, width: 3.0mm, depth: 0.5mm)
- Intraglandular tumor volume: V2 (2.64cc)
- Lymphovascular invasion: Not identified
- Prostatic intraepithelial neoplasia, high grade: Present
...
- Vas deferens, right: Free of ca
- Vas deferens, left: Free of ca Seminal vesicle, right: Free of ca Seminal vesicle
- LN (-), Bone (-) ** iPSA : 8.31
** Roach score : ECE 52.47 SV 18.31 LN 15.54
s/p Prostate biopsy
s/p RALRP
#2. Recurrence, prostate PSA elevation
@ Prostate MRI No evidence of local recurrence No enlarged LNs on both iliac chain
@ PSA 0.72 - 0.43 - 0.08 - 0.01
<Grade> 7 (4+3)
<Stage> pT3a, N0
<Metastasis> negative
<Age> 61
<PSA> 8.31
D
Tumor markers: Gleason 4+3 = 7
Clinical staging: cT1c N0 M0 11.63 IIC
Notes: 69 y.o. male with HTN/HLD, orthostatic hypotension, currently on Midodrine
...
PSA 11.63 prostate cancer, with MRI showing a 73 cc prostate and stable 13 mm index
area (PIRADS 3 previously) in the right anterior transition zone at apex and PET CT
<Grade> 7 (4+3)
<Stage> cT1c, N0
<Metastasis> unknown
<Age> 69
<PSA> 11.63
E
diagnosis details: 78-year-old male with a history of rectal cancer status post neoadjuvant
chemoradiation.
: Gleason 5+4 prostate cancer, PSA 38.4, cT3aN0M0
(rectal stenosis unable to do DRE but no T3 per MRI).
...
Plan PBT 79.2Gy/44fx +18 mo ADT
<Grade> 9 (5+4)
<Stage> cT3a, N0
<Metastasis> unknown
<Age> 78
<PSA> 38.4
34
Large 
Language 
Model
Image 
Encoder
(3D)
Multi-modal
Decoder
(3D)
×2
Self-attention
Context-to-Image Cross-attention
Image-to-Context Cross-attention
MLP
Image
Embeddings
𝒇𝒍∈ ℝ𝑯𝒍𝑾𝒍×𝑫𝒍×𝑪𝒉𝒍
Text 
Embeddings
𝒈∈ ℝ𝟏×𝑪
𝒇𝒍'
M
Clinical  
Data
Local 
LLM
EMR
Text Prompts  
M
M
M
𝒈'𝒍∈ ℝ𝟏×𝑪𝒉𝒍
Linear
Image 
Encoder
(3D)
Image 
Decoder
(3D)
Cat
Multimodal 
Alignment (M)
MoME
(a) Prostate Segmentation
(b) Interactive Multimodal Alignment
(c) MoME Training
Prostate
Planning Target Volume (PTV)
Trainable
Frozen
E
E
E
E
R
R
R
Center-specific router path
Mixture of Multicenter Experts
Shared path 
Multimodal MoME
Image Embeddings
𝒇𝒍( ∈ ℝ𝑯𝒊𝑾𝒊×𝑫𝒊×𝑪𝒉𝒊
E
Experts Module
R
Dectivated Router
R
Activated Router
Input CT Scan
𝒙∈ℝ𝑯𝑾𝑺×𝑪
Predicted PTV
𝒚+ ∈ℝ𝑯𝑾𝑺
Supplementary Figure 1: Schematic of multimodal mixture of multicenter experts (MoME) network. (a)
Prostate segmentation network is pre-trained the resulting prostate mask processed through the frozen
prostate segmentation network is added as an additional input to the multimodal MoME network. (b) For
our multimodal AI training, we adapted a large language model (LLM) to extract the formatted clinical data
curated from electronic medical records (EMRs). This context embeddings are interactively aligned with
image embeddings to yield multimodal image embeddings. (c) MoME training splits the input multimodal
image embeddings into a shared path and a center-specific router path, which are combined at the end of the
MoME module to produce multimodal MoME image embeddings.
35
a
b
c
d
e
f
Supplementary Figure 2: Comparison of GTV-to-PTV ratio (GPR) across risk groups for each center by
model. The vision-only model consistently demonstrates lower GPR values across all risk groups, showing
significant deviation from the ground truth distribution and failing to capture the clinical context. In Centers
A and B, the multi-modal and MoME models exhibit similar trends for both (a, c) low/intermediate-risk
and (b, d) high/very high-risk groups. However, in Center C, where differences in data acquisition settings,
patient distribution, and clinical practice patterns are pronounced, the MoME model more closely aligns
with the ground truth GPR distribution compared to the multi-modal model, a trend observed in both (e)
low/intermediate-risk and (f) high/very high-risk groups. For (a-f), the center line indicates the median, and
the bounds of the box represent the interquartile range (from the lower quartile to the upper quartile).
36
a
c
e
b
d
f
Supplementary Figure 3: Comparison of GTV-to-PTV ratio (GPR) across risk groups when using each
center-specific router. For Center A, both (a) low/intermediate-risk and (b) high/very high-risk groups show
the closest alignment with the ground truth GPR distribution when using the Center A expert router, while
the Center B expert router produces similar results. In contrast, using the Center C expert router leads to the
largest deviation, effectively highlighting the similarities and differences in practice patterns across institu-
tions. For Center B, the (c) low/intermediate-risk and (d) high/very high-risk groups also show consistent
alignment with Center B’s original practice when using the Center B expert router, with a similarly close
match from the Center A expert router, whereas the Center C expert router again leads to a significant
increase in GPR. In Center C, both (e) low/intermediate-risk and (f) high/very high-risk groups display a
distinct pattern, with higher GPR values that reflect the center’s less frequent use of PNI and tighter PTV
margins. For (a-f), the center line indicates the median, and the bounds of the box represent the interquartile
range (from the lower quartile to the upper quartile).
37
a
b
Supplementary Figure 4: Comparison of GTV-to-PTV ratio (GPR) across risk groups for the closed center
D. In both (a) low/intermediate-risk and (b) high/very high-risk groups, the MoME model demonstrates a
closer alignment with the ground truth distribution compared to the multi-modal as well as the vision only
models. This trend becomes more pronounced as the number of examples increases with 1-shot, 2-shot, and
3-shot learning. Notably, in the high-risk group, the GPR distribution produced by the MoME model nearly
matches the ground truth with just three-shot fine-tuning. For (a-b), the center line indicates the median, and
the bounds of the box represent the interquartile range (from the lower quartile to the upper quartile).
38
a
b
Supplementary Figure 5: Comparison of GTV-to-PTV ratio (GPR) across risk groups for the closed center
E. (a) low/intermediate-risk and (b) high/very high-risk groups, the MoME model demonstrates the most
similar distribution with the ground truth distribution compared to the multi-modal as well as the vision
only models. The distribution gets more similar to the ground truth as the few-shot tuning samples get
increased to 3-shot learning, specifically in the low & intermediate group. For (a-b), the center line indicates
the median, and the bounds of the box represent the interquartile range (from the lower quartile to the upper
quartile).
39
