FIDELITY-IMPOSED DISPLACEMENT EDITING FOR THE LEARN2REG 2024 SHG-BF
CHALLENGE
Jiacheng Wang‚ãÜ
Xiang Chen‚Ä†
Renjiu Hu‚Ä°
Rongguang Wang¬ß
Min Liu‚Ä†
Yaonan Wang‚Ä†
Jiazheng Wang‚Ä†
Hao Li‚ãÜ
Hang Zhang‚Ä°
‚ãÜVanderbilt University, Nashville, USA
‚Ä† Hunan University, Hunan, China
‚Ä° Cornell University, Ithaca, USA
¬ß University of Pennsylvania, Philadelphia, USA
ABSTRACT
Co-examination of second-harmonic generation (SHG) and
bright-field (BF) microscopy enables the differentiation of tis-
sue components and collagen fibers, aiding the analysis of hu-
man breast and pancreatic cancer tissues. However, large dis-
crepancies between SHG and BF images pose challenges for
current learning-based registration models in aligning SHG to
BF. In this paper, we propose a novel multi-modal registration
framework that employs fidelity-imposed displacement edit-
ing to address these challenges. The framework integrates
batch-wise contrastive learning, feature-based pre-alignment,
and instance-level optimization. Experimental results from
the Learn2Reg COMULISglobe SHG-BF Challenge validate
the effectiveness of our method, securing the 1st place on the
online leaderboard.
Index Terms‚Äî Second-harmonic generation, Image reg-
istration, Contrastive learning.
1. INTRODUCTION
Image registration is a fundamental task in medical imag-
ing, crucial for aligning images from different modalities or
time points. Second-harmonic generation (SHG) microscopy
provides high-resolution images sensitive to collagen fibers,
while bright-field (BF) microscopy with hematoxylin and
eosin (H&E) staining highlights various tissue components
[1, 2]. Accurate registration of SHG and BF images is essen-
tial for comprehensive cancer tissue analysis, offering deeper
insights into tissue structure and pathology.
SHG-BF registration presents two primary challenges.
First, the two modalities differ significantly: SHG images
emphasize collagen fibers, while BF images highlight stained
tissue components, resulting in large visual discrepancies.
Second, the sparse distribution of highlighted structures in
SHG creates a severe foreground-background imbalance,
making it difficult for learning-based methods, including con-
volutional neural networks [3, 4, 5, 6], vision transformers
[7, 8], and keypoint-based methods [9], to perform effectively.
To address these challenges, we propose a novel SHG-BF
multimodal registration method with the following key con-
tributions:
1. Batch-wise contrastive loss (B-NCE): We introduce a
batch-wise noise contrastive estimation loss to effectively
capture shared features between SHG and BF images.
2. Feature-based prealignment and instance optimiza-
tion: A prealignment step using descriptor matching is
followed by instance-level optimization to refine the reg-
istration.
3. Flexible transition of similarity metrics: We combine
local normalized cross-correlation (LNCC) and cross mu-
tual information function (CMIF) as similarity metrics,
balancing global and local alignment.
The novel contrastive learning loss addresses the modal-
ity gap, while the instance optimization overcomes the
foreground-background imbalance.
Quantitative and qual-
itative results demonstrate considerable improvements in
registration accuracy and robustness, earning us 1st place on
the online leaderboard of the Learn2Reg COMULISglobe
SHG-BF Challenge.
2. RELATED WORK
Classical methods.
Traditional registration methods typ-
ically formulate the problem as an iterative energy op-
timization, employing similarity criteria such as LNCC,
mean-squared error (MSE), or mutual information (MI). Ap-
proaches like ANTS [10] and ELASTIX [11] utilize gradient
descent for optimization but are prone to local minima, par-
ticularly when displacements exceed feature scales. Discrete
optimization techniques based on Markov Random Fields
(MRF) [12] and local cost aggregation [13, 14] have been
proposed to mitigate these issues.
Regression-Based Learning.
Regression-based methods
[15] train convolutional neural networks (ConvNets) to di-
rectly estimate the affine matrix. However, ConvNets lack
inherent coordinate information, making affine matrix regres-
arXiv:2410.20812v1  [cs.CV]  28 Oct 2024
neg
pos
pos
neg
modal 1
modal 2
InfoNCE
B-NCE
patch-wise
stacked 
pixel-wise
Fig. 1: Illustration of our proposed batch-wise noise con-
trastive estimation (B-NCE) loss, which aggregates pixel-
level information across patches, compared to traditional In-
foNCE loss that operates directly on patch-level information.
Moving
Fixed
Tiramisu
Feature
Extraction
XFeat
Keypoint
Matching
Feature Fixed
Feature Moving
Initial Affine
Initial Warped
ConvexAdam
Refinement
Iterated
Displacement
Field
Final Warped
Warped
Moving
Overlay
Fixed
Matched Keypoints
Sampling
+ Least Square
ùìê
ùìê
Fig. 2: Overview of our proposed framework, which follows
three main steps: feature extraction, XFeat feature matching,
and ConvexAdam fine-tuning.
sion challenging. Vision transformers [7] and keypoint-based
methods like KeyMorph [9] attempt to overcome these limita-
tions by leveraging invariant feature representations, but still
struggle with the high foreground-background imbalance.
Descriptor Matching Learning. Descriptor matching meth-
ods [16, 17, 18, 19] effectively address the high foreground-
background imbalance by relying on feature descriptors and
feature matching, utilizing both handcrafted and learning-
based descriptors.
Traditional RANSAC [20] and modern
graph- and attention-based approaches [21] are commonly
used for matching. However, these approaches lack a fidelity
loss, leaving the output predictions unconstrained.
3. METHODS
Our proposed registration framework, illustrated in Fig. 2,
consists of two main stages: a feature-based prealignment
using contrastive representation learning and a test-time
instance-level optimization for final registration. Initially, we
learn a shared representation between SHG and BF images
using our specially designed Batch-wise Noise Contrastive
Estimation (B-NCE) loss. This shared latent space simpli-
fies the multimodal registration problem into a monomodal
one, making alignment more tractable.
During inference,
we refine the registration using ConvexAdam optimization,
leveraging both cross-mutual information function (CMIF)
and local normalized cross-correlation (LNCC) as similarity
metrics to achieve accurate alignment.
Let IA, IB ‚ààR2 denote the fixed and moving images,
respectively. Our goal is to construct affine transformations
Œ¶(IA, IB) for each image pair, mapping R2 ‚ÜíR2. We aim
to align the transformed moving image to the fixed image,
such that Œ¶(IA, IB) ‚äôIB ‚àºIA where ‚äôdenotes the spatial
transformation of IB using Œ¶(IA, IB).
3.1. Features prealignment with contrastive learning
Due to the significant differences between SHG and BF im-
ages, direct registration is challenging. To address this, we
map both images into a common latent space where shared
features can be effectively captured. We achieve this using
contrastive representation learning with a specially designed
Batch-wise Noise Contrastive Estimation (B-NCE) loss.
Inspired by CoMIR [22], we employ two identical Dense
Tiramisu networks [23], DŒ∏, to extract feature representa-
tions from IA and IB: CA = DŒ∏(IA) and CB = DŒ∏(IB),
where CA, CB ‚ààRH√óW √óC and C is the number of feature
channels. And we demonstrated the trained features shown in
Fig. 3 second row.
Batch-wise noise contrastive estimation (B-NCE) loss. Our
B-NCE loss is designed to enhance the network‚Äôs ability to
capture shared features between modalities by focusing on
position-level similarities across the batch dimension. We ex-
tract patches from CA and CB of size p √ó p, resulting in sets
of patches {Xk} and {Yk} for k = 1, . . . , N, where N is
the number of patches. To compute the loss, we consider the
features at corresponding spatial locations across the batch:
xi,j = {Xk(i, j)}N
k=1,
yi,j = {Yk(i, j)}N
k=1,
(1)
where (i, j) indexes the spatial positions within each patch.
We define the similarity between features using the cosine
similarity:
s(xi,j, yi,j) =
x‚ä§
i,jyi,j
‚à•xi,j‚à•‚à•yi,j‚à•.
(2)
The B-NCE loss for each spatial position is:
Li,j
B-NCE = ‚àílog
exp(s(xi,j, yi,j)/œÑ)
PN
k=1 exp(s(xi,j, zk)/œÑ)
,
(3)
where zk includes both positive and negative samples, and œÑ
is a temperature parameter.
The total loss is averaged over all spatial positions:
LB-NCE =
1
HpWp
Hp
X
i=1
Wp
X
j=1
Li,j
B-NCE,
(4)
Fixed
Moving
Tiramisu-Fixed
Tiramisu-Moving
Keypoints Alignment
Keypoints Alignment
Fig. 3:
XFeat feature detection/description/matching in
SHG/BF images and the Tiramisu feature images.
where Hp and Wp are the height and width of the patches.
By minimizing LB-NCE, the network learns to bring corre-
sponding features from IA and IB closer in the latent space
while pushing apart non-corresponding features, effectively
capturing shared structures across modalities.
Feature-based prealignment. With the learned representa-
tions CA and CB, we perform feature-based prealignment.
We detect keypoints and extract descriptors using a method
inspired by XFeat [24], adapted to our context. We match fea-
tures between CA and CB to compute an initial affine trans-
formation Œ¶init. We also demonstrated the matching keypoints
in Fig. 3 in blue dots and connected with green lines.
This prealignment step reduces large discrepancies be-
tween the images, providing a good starting point for the sub-
sequent optimization.
3.2. Test-time Instance Optimization
We adopt the core principles of the ConvexAdam method
[25], enabling flexible instance optimization between affine
and deformable transformations. Unlike the original Convex-
Adam, which optimizes flow parameters using hand-crafted
features, our approach leverages learned features from the
preceding stage and employs a pyramid structure for multi-
resolution alignment across scales 1, 1
2, 1
4, 1
8, 1
16. This hierar-
chical setup allows for efficient alignment at both large and
fine scales.
Following the prealignment performed during the train-
ing stage, we use a convolutional network with large kernel
sizes to generate the displacement fields. Displacement as-
signment is achieved by evaluating patch similarity costs and
identifying the minimal cost positions. Smooth regulariza-
tion is then applied through Gaussian smoothing, as described
in [26]. Our fidelity loss combines Local Normalized Cross-
Correlation (LNCC), which handles local similarities to ad-
dress the foreground-background imbalance, with Cross Mu-
tual Information Function (CMIF), which considers global in-
tensity relationships to bridge the modality gap.
Since the primary features in SHG images are sparsely
distributed, we avoid using the deformation field directly.
SHG
BF
Warped BF
Overlay
Fig. 4: Qualitative results on the validation set. From left to
right: warped BF image overlaid on SHG image, SHG image
(fixed) shown in Virdis colormap, BF image (moving), and
warped BF image.
Instead, we sample displacements based on SHG intensity,
followed by least-squares analysis on these sparse displace-
ments. This results in an over-determined system of equations
that we solve to obtain the final affine transformation. This
approach allows for a smooth transition between deformable
and affine transformations, enhancing the precision of the
registration results.
3.3. Multimodal Similarities: Mutual Information
We compute mutual information between fixed image IA ‚äÜ
XA and moving image IB ‚äÜXB based on cross-mutual in-
formation function (CMIF) [27, 28]. Given images IA and IB
intersecting on IAB and IAB Ã∏= ‚àÖ, we firstly cluster each im-
age IA and IB into level sets A ‚ààZ and B ‚ààZ accordingly
using K-means. For each level a and b from the level sets A
and B, we compute the marginal and joint histogram entries
separately, Then, based on the normalized histograms, we de-
rive the marginal and joint Shannon entropies as follows,
HA = ‚àí
X
a‚ààA
Ia
A ¬∑ IB
NAB
log Ia
A ¬∑ IB
NAB
;
(5)
HB = ‚àí
X
b‚ààB
IA ¬∑ Ib
B
NAB
log IA ¬∑ Ib
B
NAB
;
(6)
HAB = ‚àí
X
a‚ààA
X
b‚ààB
Ia
A ¬∑ Ib
B
NAB
log Ia
A ¬∑ Ib
B
NAB
,
(7)
where Ia
A and Ib
B represent images where the pixel values
equal to level set value a and b respectively; NAB is the num-
ber of total pixels in both image IA and IB.
Finally, we
can compute the mutual information using MI(IA, IB) =
HA + HB ‚àíHAB.
Table 1: TRE comparison of teams on the Learn2Reg 2024
Challenge validation leaderboard. Rankings are based on a
snapshot of the leaderboard as of Sep 20, 2024, excluding
challenge organizer entries.
Method
TRE (LM)
Best Validation Place
Team VROC
2.620 ¬± 1.206
36th
Team Yangzhao
2.578 ¬± 1.377
34th
Team IWM
2.077 ¬± 1.182
8th
Team meeem
2.032 ¬± 1.026
6th
XFeat Only
5.939 ¬± 8.198
39th
XFeat + Ours (CMIF)
9.128 ¬± 1.211
72th
XFeat + Ours (LNCC)
2.361 ¬± 0.812
20th
XFeat + Ours (LNCCC+CMIF)
1.943 ¬± 0.765
1st
4. EXPERIMENTS AND RESULTS
4.1. Dataset
We evaluated our method on the dataset from the Learn2Reg
2024 Challenge Task 3: COMULISglobe SHG-BF [1], which
includes 156 training, 10 validation, and 40 test cases of
SHG and H&E-stained BF microscopy images of breast and
pancreatic cancer tissues.
Acquired at the University of
Wisconsin-Madison, these images present significant chal-
lenges due to differences in imaging modalities: SHG images
emphasize collagen fibers, while BF images highlight H&E-
stained tissue structures. Reliable expert annotations provide
landmarks for validation and testing.
4.2. Data Preprocessing and Evaluation Metrics
For preprocessing, we applied stochastic intensity transfor-
mations using B¬¥ezier curves [29] and random affine augmen-
tations, including scaling, rotation, and shearing, to enhance
the robustness of our model to intensity variations and geo-
metric distortions. During validation and testing, we omitted
stochastic augmentations but maintained normalization and
intensity adjustments to ensure consistency.
We adopted the evaluation metrics established by the
Learn2Reg 2024 Challenge, specifically using Target Regis-
tration Error (TRE) as the primary metric to assess registra-
tion accuracy. TRE measures the average Euclidean distance
between corresponding anatomical landmarks in the fixed and
moving images after registration. Lower TRE values indicate
better alignment. All reported results on the validation set are
based on the challenge leaderboard.
4.3. Quantitative Results
Table 1 presents the performance of our method compared
to top-performing methods from the challenge. Our approach
achieved a mean TRE of 1.943 mm on the validation set, rank-
ing first. Notably, our method demonstrated the lowest stan-
dard deviation (0.765 mm) among all submissions, indicating
high robustness and consistency across different cases.
15
30
50
Iterations
1
2
3
4
5
TRE
ns
ns
ns
Fig. 5: Impact of the number of optimization iterations on
the mean TRE. Our method achieves consistent performance,
with the best result at 30 iterations.
To evaluate the effectiveness of our proposed components,
we conducted ablation studies summarized in Table 1 (second
part). Introducing instance-level optimization using LNCC
reduced the TRE by approximately 60%, effectively mitigat-
ing the imbalance caused by sparse SHG features. Using only
CMIF as the optimization metric increased the TRE, suggest-
ing that CMIF alone may lead to local minima due to its
global nature and the modality gap. Combining CMIF with
LNCC in our fidelity loss improved performance, reducing
the TRE from 2.361 to 1.943. This demonstrates that our
fidelity-imposed displacement editing, which balances global
and local alignment, enhances registration accuracy.
Additionally, we tested different iteration counts during
instance-level optimization (15, 30, and 50 iterations) using
both CMIF and LNCC (Fig. 5).
Our method consistently
achieved low TRE values, with the best performance at 30
iterations (mean TREs of 2.759, 1.943, and 2.168, respec-
tively). This indicates our approach‚Äôs stability without exten-
sive computation.
4.4. Qualitative Results
Two validation examples are presented in Fig. 4. Despite sig-
nificant structural differences due to the modality gap, our
method successfully aligned the moving BF images to the
fixed SHG images. The overlaid warped BF images show
main structures aligned with minimal misalignment, demon-
strating effective multimodal registration.
Figure 3 shows feature matching with our learned repre-
sentations. Despite large modality discrepancies, the features
are visually similar, enabling robust matching. The sufficient
matching pairs confirm the effectiveness of B-NCE loss in
bridging the modality gap for feature-based prealignment.
5. CONCLUSION
In this paper, we propose a new affine registration framework
that integrates a novel contrastive learning loss (B-NCE)
with rich contextual information from modern neural net-
works inside fidelity-imposed iterative instance optimization.
Our fidelity-imposed network editing framework overcomes
modality gap between SHG and BF images, providing robust
and accurate SHG-BF registration performance. Experimen-
tal results from the Learn2Reg COMULISglobe SHG-BF
Challenge demonstrate the effectiveness of our approach,
earning the 1st place on the online leaderboard.
6. REFERENCES
[1] Kevin Eliceiri et al., ‚ÄúMultimodal biomedical dataset for
evaluating registration methods (full-size tma cores),‚Äù
Zenodo, Feb, 2021.
[2] Adib Keikhosravi et al.,
‚ÄúIntensity-based registration
of bright-field and second-harmonic generation images
of histopathology tissue sections,‚Äù Biomedical Optics
Express, vol. 11, no. 1, pp. 160‚Äì173, 2019.
[3] Hang Zhang et al.,
‚ÄúMemwarp:
Discontinuity-
preserving cardiac registration with memorized anatom-
ical filters,‚Äù in MICCAI. Springer, 2024, pp. 671‚Äì681.
[4] Xiang Chen et al., ‚ÄúSpatially covariant image registra-
tion with text prompts,‚Äù IEEE Transactions on Neural
Networks and Learning Systems, 2024.
[5] Guha Balakrishnan et al.,
‚ÄúVoxelmorph: a learning
framework for deformable medical image registration,‚Äù
IEEE transactions on medical imaging, vol. 38, no. 8,
pp. 1788‚Äì1800, 2019.
[6] Hang Zhang et al., ‚ÄúSlicer networks,‚Äù arXiv preprint
arXiv:2401.09833, 2024.
[7] Tony CW Mok and Albert Chung, ‚ÄúAffine medical im-
age registration with coarse-to-fine vision transformer,‚Äù
in CVPR, 2022, pp. 20835‚Äì20844.
[8] Junyu Chen et al., ‚ÄúTransmorph: Transformer for un-
supervised medical image registration,‚Äù Medical image
analysis, vol. 82, pp. 102615, 2022.
[9] M Yu Evan et al.,
‚ÄúKeymorph: Robust multi-modal
affine registration via unsupervised keypoint detection,‚Äù
in MIDL, 2022.
[10] B B Avants et al.,
‚ÄúAdvanced normalization tools
(ants),‚Äù Insight j, vol. 2, no. 365, pp. 1‚Äì35, 2009.
[11] Stefan Klein et al.,
‚ÄúElastix: a toolbox for intensity-
based medical image registration,‚Äù IEEE transactions
on medical imaging, vol. 29, no. 1, pp. 196‚Äì205, 2009.
[12] Mattias P Heinrich et al., ‚ÄúMrf-based deformable reg-
istration and ventilation estimation of lung ct,‚Äù IEEE
transactions on medical imaging, vol. 32, no. 7, pp.
1239‚Äì1248, 2013.
[13] Mattias P Heinrich et al.,
‚ÄúMulti-modal multi-
atlas segmentation using discrete optimisation and self-
similarities.,‚Äù ISBI, vol. 1390, pp. 27, 2015.
[14] Frank Steinbr¬®ucker et al., ‚ÄúLarge displacement optical
flow computation without warping,‚Äù in ICCV, 2009, pp.
1609‚Äì1614.
[15] Xu Chen et al.,
‚ÄúLearning unsupervised parameter-
specific affine transformation for medical images reg-
istration,‚Äù in MICCAI. Springer, 2021, pp. 24‚Äì34.
[16] Daniel DeTone et al., ‚ÄúSuperpoint: Self-supervised in-
terest point detection and description,‚Äù in CVPR work-
shops, 2018, pp. 224‚Äì236.
[17] Prune Truong et al., ‚ÄúGlampoints: Greedily learned ac-
curate match points,‚Äù in ICCV, 2019, pp. 10732‚Äì10741.
[18] Jiacheng Wang et al., ‚ÄúNovel oct mosaicking pipeline
with feature-and pixel-based registration,‚Äù
in ISBI.
IEEE, 2024, pp. 1‚Äì5.
[19] Jiacheng Wang et al.,
‚ÄúRetinal ipa:
Iterative key-
points alignment for multimodal retinal imaging,‚Äù arXiv
preprint arXiv:2407.18362, 2024.
[20] Robert C Bolles and Martin A Fischler, ‚ÄúA ransac-based
approach to model fitting and its application to finding
cylinders in range data.,‚Äù in IJCAI, 1981, vol. 1981, pp.
637‚Äì643.
[21] Paul-Edouard Sarlin et al., ‚ÄúSuperglue: Learning feature
matching with graph neural networks,‚Äù in CVPR, 2020,
pp. 4938‚Äì4947.
[22] Nicolas Pielawski et al.,
‚ÄúComir: Contrastive multi-
modal image representation for registration,‚Äù Advances
in neural information processing systems, vol. 33, pp.
18433‚Äì18444, 2020.
[23] Simon J¬¥egou et al., ‚ÄúThe one hundred layers tiramisu:
Fully convolutional densenets for semantic segmenta-
tion,‚Äù in CVPR workshops, 2017, pp. 11‚Äì19.
[24] Guilherme Potje et al., ‚ÄúXfeat: Accelerated features for
lightweight image matching,‚Äù in CVPR, 2024, pp. 2682‚Äì
2691.
[25] Hanna Siebert et al., ‚ÄúFast 3d registration with accurate
optimisation and little learning for learn2reg 2021,‚Äù in
MICCAI. Springer, 2021, pp. 174‚Äì179.
[26] Tom Vercauteren et al., ‚ÄúDiffeomorphic demons: Effi-
cient non-parametric image registration,‚Äù NeuroImage,
vol. 45, no. 1, pp. S61‚ÄìS72, 2009.
[27] Bernd Pompe et al., ‚ÄúUsing mutual information to mea-
sure coupling in the cardiorespiratory system,‚Äù
IEEE
Engineering in Medicine and Biology Magazine, vol.
17, no. 6, pp. 32‚Äì39, 1998.
[28] Johan ¬®Ofverstedt et al., ‚ÄúFast computation of mutual in-
formation in the frequency domain with applications to
global multimodal image alignment,‚Äù Pattern Recogni-
tion Letters, vol. 159, pp. 196‚Äì203, 2022.
[29] Ken Kobayashi et al., ‚ÄúB¬¥ezier simplex fitting: Describ-
ing pareto fronts of simplicial problems with small sam-
ples in multi-objective optimization,‚Äù
in AAAI, 2019,
vol. 33, pp. 2304‚Äì2313.
