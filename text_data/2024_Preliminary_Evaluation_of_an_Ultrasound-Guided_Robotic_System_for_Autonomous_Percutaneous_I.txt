Preliminary Evaluation of an Ultrasound-Guided Robotic System for
Autonomous Percutaneous Intervention
Pratima Mohan, Aayush Agrawal and Niravkumar A. Patel Member IEEE
Abstract— Cancer cases have been rising globally, resulting
in nearly 10 million deaths in 2023. Biopsy, crucial for di-
agnosis, is often performed under ultrasound (US) guidance,
demanding precise hand coordination and cognitive decision-
making. Robot-assisted interventions have shown improved
accuracy in lesion targeting by addressing challenges such as
noisy 2D images and maintaining consistent probe-to-surface
contact. Recent research has focused on fully autonomous
robotic US systems to enable standardized diagnostic proce-
dures and reproducible US-guided therapy. This study presents
a fully autonomous system for US-guided needle placement
capable of performing end-to-end clinical workflow. The system
autonomously: 1) identifies the liver region on the patient’s
abdomen surface, 2) plans and executes the US scanning path
using impedance control, 3) localizes lesions from the US images
in real-time, and 4) targets the identified lesions, all without
human intervention. This study evaluates both position and
impedance-controlled systems. Validation on agar phantoms
demonstrated a targeting error of 5.741±2.70 mm, highlighting
its potential for accurately targeting tumors larger than 5 mm.
Achieved results show its potential for a fully autonomous
system for US-guided biopsies.
Keywords-
Autonomous,
percutaneous
interventions,
ultrasound-guided robot, biopsy
I. INTRODUCTION
Biopsy, a common method for cancer diagnosis, involves
needle insertion to extract tissue samples for pathological
examination. It is often guided by ultrasound (US) due to
its cost-effectiveness, portability, and lack of radiation. The
clinician holds the US probe in one hand while inserting the
needle with the other, interpreting real-time US images based
on the understanding of general human anatomy. Maintaining
needle alignment within the imaging plane while ensuring
accuracy is difficult, especially for less experienced surgeons,
leading to potential errors, prolonged procedures, and risks
like internal bleeding. The procedure’s reliance on operator
skill also increases the risk of fatigue and musculoskeletal
injuries [1], [2]. Accurate access to small targets is essential
to avoid critical structures like blood vessels and nerve
plexuses.
Robot-assisted minimally invasive interventions [2]–[4]
have been shown to outperform human capabilities by pro-
viding enhanced dexterity and precision, leading to more
accurate procedures and improved outcomes. Over the past
decade, research has focused on robotic US screening to
*This work was funded by the Science and Engineering Research
Board, Department of Science and Technology, Government of India, grant
SRG/2021/002086.
Authors are with the Indian Institute of Technology Madras, Chennai,
TN 600036, India [nirav.robotics@gmail.com]
Fig. 1.
Ultrasound-guided robotic system for autonomous percutaneous
interventions showing: (a) a KUKA iiwa7 with Intelligent Needle Guidance
system, an ultrasound probe and mannequin setup for the experiments, and
(b) top view of the mannequin showing key points for organ localization
and the desired safe scan region for the liver interventions.
improve accuracy and reduce hand-eye coordination chal-
lenges in manual percutaneous procedures [5]–[7]. However,
robot-assisted percutaneous needle placement faces signifi-
cant challenges including low signal-to-noise ratio of ultra-
sound images, echo artifacts, and difficulties in maintaining
proper probe-to-surface contact. Optimal US image quality
is achieved when the probe is positioned perpendicular to
the contact surface [8], [9]. Traditional 2D ultrasound (US)
images lack the spatial information needed for clinicians
to fully visualize the 3D anatomy, making it cognitively
challenging to localize lesions. This limitation has led to the
adoption of 3D US technology, which can be achieved either
through a specialized 3D probe with a 2D transducer array
or by capturing multiple 2D images with a standard probe
and combining them into a 3D volume. This process uses
tracking systems like optical, electromagnetic, or mechanical
trackers to combine pose information with each 2D image,
allowing accurate alignment and reconstruction into a 3D
volume. The quality of 3D volumes generated by a 3D
US probe is comparable to those created by compounding
tracked B-mode images from a 2D probe. These factors
motivated the use of tracked robotic 3D US built from 2D
images instead of commercial 3D US probes [7].
Recent research has increasingly focused on addressing
the challenges of ultrasound (US) scanning by shifting from
robot-assisted to fully autonomous robotic systems. This
transition aims to reduce or eliminate human involvement,
improving both consistency and accuracy while minimizing
arXiv:2410.10299v1  [cs.RO]  14 Oct 2024
Depth Camera
 Denspose 
3D Point
Cloud
Inital
 Transform 
Keypoints
CT-Atlas
Organ Localisation
Registration and
Definition of ROI
 Region of  
Interest
Tumor 
 Location 
Intelligent Needle
Guidance System
Position Robot
Perpendicular to
Target
Needle Insertion
Execution
Move to Next Target
Scanning & Tumor
Detection
Scanning and
Ultrasound Image
Acquisation
Circular Intensity
Rejection & Tumor
Clustering
Target Localisation in
Robot Base
Coordinate System
Fig. 2.
System architecture for US-guided autonomous percutaneous intervention showing its modules for: (1) organ localization, (2) trajectory panning
and lesion detection, and (3) needle manipulation.
dependence on operator skill and experience. US imaging
remains highly user-dependent and lacks reproducibility,
making it difficult to standardize diagnostic procedures. This
issue is further complicated by anatomical variability, uneven
body surface geometry, and skin elasticity, all of which affect
the quality and reliability of US imaging [10]. These factors
also hinder the automation of volumetric data analysis, which
is crucial for integrating with global diagnostic systems and
computer-assisted interventions.
Mustafa et al. developed an algorithm for autonomous
liver scanning by detecting the umbilicus and estimating
the epigastric region [6]. Huang et al. developed an RGB-
based color segmentation technique combined with a zig-
zag ultrasound scanning approach [5], [11]. Virga et al.
demonstrated a fully autonomous US scanning framework
with MRI-based atlas registration and force-controlled scan-
ning, validating on human volunteers [4]. Tan et al. de-
veloped an autonomous breast scanning system using sta-
tistical modeling for chest localization, but its complexity
and reliance on 3D path computations limit clinical viability
[12]. Suligoj et al. advanced US automation with multi-
scan line generation, impedance control, and automatic data
analysis, but it still requires manual region selection in
MRI/CT volumes, hindering full autonomy [10]. Kojcev’s
dual-robot system addresses autonomous needle placement
but requires a complex calibration setup and a large operating
room [13]. Chen et al. developed a portable puncture robot
that improves accuracy by converting manually planned
ultrasound paths into robotic coordinates, validated on ex-
vivo tissues [2]. However, pre-interventional assessment and
post-interventional decision-making still lack the level of
autonomy needed for practical clinical application [12],
[14]. Ongoing research into autonomous US-guided needle
placement aims to achieve reproducible diagnostic outcomes
with clinical applicability. Li et al. and Haxthausen et al.
have classified US-guided robotic systems into levels of
robot autonomy (LORA), ranging from levels 1-2 for assisted
teleoperation to levels 3-4 for collaborative manipulation by
robot and clinician, and levels 5-9 for assisted autonomy
based on clinician involvement [15], [16]. The highest LORA
for ultrasound therapy systems is level 7 [13]. However, no
US-guided robotic system for percutaneous intervention has
yet validated fully autonomous end-to-end needle placement
without any human involvement.
This paper presents a fully autonomous system for
ultrasound-guided robotic percutaneous interventions, en-
abling needle placement without human involvement. To the
best of our knowledge, this is the first time that a fully
autonomous US-guided robotic system for needle placement
has been demonstrated. The system uses a depth camera to
create a point cloud of the patient, which is registered to a
CT-based atlas to localize the anatomical region of interest
(ROI). The robot autonomously plans and executes a multi-
sweep trajectory for scanning the ROI, maintaining con-
tinuous probe-to-surface contact using impedance control.
During the scan, US images are generated and processed in
real time to detect the lesions using CIR (Circular Intensity
Rejection) algorithm. All the images are stacked to produce
3D volume, which is processed using a clustering algorithm
to identify lesions, calculating the centroid of each target.
The robot then targets the centroid of each detected lesion,
and finally, the needle guidance module inserts the needle
precisely into the target tissue for sampling.
The key contributions of the paper are as follows:
1) A fully autonomous robotic system for US-guided
percutaneous interventions is demonstrated in anthro-
pomorphic phantom studies as shown in Fig. 1.
2) ATIR (Atlas-based Transformation and Iterative Reg-
Fig. 3.
Showing coordinate system transformations between various
components of the system.
istration)A fully autonomous multi-organ localization
has been demonstrated by registering real-time point
cloud and MRI/CT.
3) Autonomous robot trajectory control for US-imaging
to produce a 3D volumetric representation of the iden-
tified organ and lesion segmentation has been achieved.
4) End-to-end clinical workflow is demonstrated with
fully autonomous needle placement in anthropomor-
phic phantom studies.
II. MATERIAL AND METHODS
This section gives an overview of the proposed robotic
system for autonomous liver biopsy procedures. Figure 2
shows the system framework and its modules. The rest of
the section is organized as follows: (A) Hardware Setup (B)
ATIR: Atlas-based Transformation and Iterative Registration
(C) Robot path planning and execution (D) Lesion segmen-
tation and 3D localization, (E) INGS: Intelligent Needle
Guidance System.
A. Hardware Setup
The presented system comprises the following compo-
nents: (1) a 7-degree-of-freedom (DOF) robotic arm (KUKA
LBR IIWA7 R800) with 7 Kg payload capacity for precise
trajectory execution, (2) a Telemed (TELEMED MEDICAL
SYSTEMS S.r.l., Milano, Italia) MicrUs EXT-1H with a
convex array probe C5-2R60S-3 for real-time ultrasound
imaging of the abdomen, (3) a Windows computer (Mini PC)
equipped with a 12th Gen Alder Lake N100 processor (Quad-
core, 1.7GHz-3.4GHz, 16GB RAM) and a Ubuntu Computer
(host PC) with 32GB RAM for managing the sensor data and
biopsy workflow respectively, (4) an RGB-D camera (Intel
Realsense D435i) for the reconstruction of the 3D surface
of the phantom, and (5) Intelligent Needle Guidance System
(INGS) (1.6 kg weight) for autonomous needle placement.
Fig. 4.
ATIR: Atlas-based Transformation and Iterative Registration: (a)
Initial point clouds with keypoints (b) Point clouds aligned with the initial
transform generated by keypoint matching (c) Point clouds aligned after
refining the initial transform using ICP and corresponding 3D organs (d)
Corresponding ROI for Ultrasound scanning on the surface of the subject’s
point cloud *Only the liver is visualized, as scanning was limited to this
organ for experiments
The INGS module can be attached to any robot with an end-
effector holder and payload capacity of 2 kg. The INGS has
a needle guide for autonomous needle insertion. The whole
setup can be seen in Fig. 1 The open-source KST-Kuka-
Sunrise Toolbox [17] and ROS (Robot Operating System)
are used to control the KUKA LBR IIWA R800/R820).
The World Coordinate System (WCS) is defined at the
robot’s base. The 3D camera (C) frame T C
E and US probe (U)
frame T U
E are defined with respect to the robot’s end effector
frame E based on the CAD model. 2D image I is converted
to the point cloud (by pyrealsense library) using a camera
matrix (T I
C). The relationship between these transformation
frames is shown in Fig. 3.
The RGB image (I) can be represented in WCS using the
following equation:
T I
B = (T E
B ) · (T C
E ) · (T I
C)
(1)
Where (T I
C) is provided by Intel RealSense, (T C
E ) is a
fixed translation matrix described using CAD model, and
(T E
B ) is calculated using KDL, an inverse kinematic solver.
Similarly, the 3D voxels in WCS for the 2D US image
pixels are calculated using the following equation:
T F
B = (T E
B ) · (T U
E ) · (T F
U )
(2)
Where (T U
E ) is a fixed transformation between the robot
end effector and ultrasound probe derived from the CAD
model. The resolution of the ultrasound image is 512 x 512.
Physical dimensions of every pixel in US image are produced
using the pixel spacing [Sx, Sy] = [0.2, 0.2] mm, provided
by Telemed Ultrasound Software Development Kit (SDK)
and open-source PlusToolkit [18]. Using the pixel spacing
and transformation T F
B , the 2D coordinates of pixels in the
US image (x, y) are converted to the 3D voxel coordinates
defined as (xF , yF , zF ) using the following equations:
xF = xEE + (256 −x) · Sx
yF = yEE
zF = zEE + y · Sy
(xF , yF , zF ) represents the 3D voxel that lies in the US
coordinate system. This point can be represented in WCS
using:
(xB, yB, zB) = (T F
B ) · (xF , yF , zF )
(3)
Using this 3D voxel information, multiple image frames
are converted into 3D US slices and stacked to produce the
3D reconstruction of the phantom. This 3D reconstruction is
used further for lesion localization and targeting.
B. ATIR: Atlas-based Transformation and Iterative Registra-
tion
ATIR estimates the position of target anatomical organs
within the patient’s body. A generic CT-Atlas with segmented
organs is prepared and registered to the patient, creating
a detailed 3D anatomical map inside the anthropomorphic
phantom. Fig. 4 shows the different stages of the registration
procedure.
The CT-Atlas organs and body structures are generated by
segmenting from a CT scan using 3D Slicer software. The
abdomen CT data was segmented using the TotalSegmentor
plugin [19]. Separate point clouds are created for each
organ and the abdomen skin surface point cloud. For the
registration, DensePose [20] has been used to filter the RGB-
D point cloud such that only the subject’s abdomen point
cloud is retained.
An accurate transformation matrix is estimated before
applying ICP (Iterative Closest Point) to minimize axis
mismatches between point clouds. Point clouds are typically
initialized randomly, but using an estimated transformation
matrix reduces randomness, accelerating the convergence
process. The initial matrix is derived by aligning key points
(left chest, right chest, navel) from the CT-Atlas with corre-
sponding points on the anthropomorphic phantom, detected
via an RGB-D camera, as shown in Figures 4 and 1.
Finally, after ICP (Iterative Closest Point) is applied, A
convex hull is built around 3D points for the specific organ
to be scanned. Points from the live abdomen point cloud
within the convex hull are identified as part of the ROI,
highlighting the region to be scanned on the human body
for that particular organ.
C. Robot Path Planning and Execution
The path planning algorithm autonomously generates a se-
quence of robot poses on the phantom surface for ultrasound
image acquisition. This is used to image and reconstruct the
underlying volume beneath this area.
1) Path Planning Algorithm and 3D Reconstruction:
The ROI generated on the patient body surface by the
perception module as described in Section II-B is used to
generate robot trajectory for ultrasound image acquisition.
Point Cloud Library [21] is used to compute normal ⃗zlocal
of the local surface around the point ρij. The unit normal
vector ⃗zlocal for each point ρij can be used to calculate the
orientation Oij as follows :
⃗xlocal =
⃗zlocal × ⃗yglobal
∥⃗zlocal × ⃗yglobal∥
⃗ylocal =
⃗zlocal × ⃗xlocal
∥⃗zlocal × ⃗xlocal∥
Oij =
⃗xlocal
⃗ylocal
⃗zlocal

(4)
where ⃗yglobal is the unit vector of the robot base in WCS
(World Coordinate System). The unit vectors ⃗xlocal, ⃗ylocal,
and ⃗zlocal at point ρij are used to form a rotation matrix
Oij of the robot’s end effector at point ρij. The complete
trajectory that contains a set of robot’s end effector poses
can be represented as:
T = {ρij{(xij, yij, zij), Oij}, | 1 ≤i ≤n, 1 ≤j ≤m} .
(5)
Moveit [22] is used to execute the above-generated task
space trajectory. Moveit generates a joint space trajectory
corresponding to the task space trajectory using KDL as an
inverse kinematic solver; the generated joint space trajectory
is executed by the robot using the KST-Kuka-Sunrise Tool-
box [17]. Two different methods are used for the trajectory
following, position and impedance control. The position
control method moves the end effector to scan through a
set of points without using the end effector forces. While,
impedance control regulates a robot’s interaction with its
environment by dynamically adjusting its compliance, or
how it responds to external forces. This control strategy
mimics a virtual spring-mass-damper system, where the
robot’s interaction forces are modulated based on three key
parameters: stiffness, damping, and mass. By fine-tuning
these parameters, impedance control allows the robot to
maintain constant probe-to-surface contact and ensure safety.
The force safety threshold for impedance control was set to
12 Nm; the robot will automatically stop if the forces go
beyond this threshold.
The robot acquires 2D US images at each point in the
planned trajectory. The robot moves to the next point and
repeats the process until the entire trajectory is executed.
A 3D volume is constructed by stacking these 2D US
images. Each 2D image is segmented to find the target (as
described in Section II-D), and transformed into a 3D slice
(as described in Section II-A).
D. Lesion Segmentation and 3D Localisation
To identify targets within the reconstructed ultrasound vol-
ume, the incoming 2D US image is pre-processed to reduce
noise. The Circle Hough Transform (CHT) detects high-
contrast circular targets in the pre-processed image. However,
speckle noise and high-intensity echo waves could result
in false target detection. A circular-intensity-based rejection
(CIR) algorithm is applied to the resultant circular targets
to check radial reduction in lesion intensity and discard the
false target detection by CHT, as shown in equation 6.
Fig. 5.
Needle driver assembly and insertion angle calculation for
autonomous needle insertion.
I(r + 2δ) < I(r + δ) < I(r)
(6)
Where I(x) represents the average intensity of the pixels
in the circle of radius x around a detected target and r is the
radius of the detected tumor. δ is a threshold value that can
be tuned based on the echo noise. If a particular detected
target satisfies this equation it is considered as a valid target
tumor since there is a continuous intensity drop over circles
of larger radii. All target pixels inside the circle with an
intensity greater than I0 are converted into 3D target regions
T t
B as mentioned in Section II-A
The produced 3D volume is input to the density-based
clustering non-parametric algorithm (DBSCAN). Its param-
eters, ϵ and γ, determine the maximum distance between
points for clustering and the minimum points needed for a
cluster, respectively. These parameters are tuned based on
target density and neighboring distances to ensure consistent
clustering.
E. INGS: Intelligent Needle Guidance System
The 2 DOF needle driver assembly shown in Fig. 5
consists of a needle driver module and a fixed mount. The
standard biopsy needle can be attached to the needle guide.
The needle guide can align the needle insertion axis to the
desired needle insertion trajectory and insert up to a depth
of 100 mm.
The insertion point is calculated using target centers
identified by the DBSCAN algorithm. The insertion vector
and the tool vector, denoted as ⃗vinsertion and ⃗vtool respectively,
are obtained as follows, ⃗vinsertionand⃗ptarget −⃗pinsertion and
⃗vtooland⃗ptarget −⃗ptool
Here, ⃗ptarget represents the coordinates of the target point.
The insertion angle, denoted as θ, is determined using the
dot product between the insertion vector and the tool vector
and is calculated using the following equation:
θ = arccos

⃗vinsertion · ⃗vtool
∥⃗vinsertion∥· ∥⃗vtool∥

(7)
The calculated angle is sent to the needle driver system
to align the needle guide and perform needle insertion.
The needle driver system uses two Dynamixel XC330-
T288 motors for autonomous needle guide control, integrated
Fig. 6.
a) shows the needle targeting error calculation in which user
manually selects point on the visible needle path in US image. b) shows the
CIR condition where case (i) shows the valid target as the average intensity
of CIR output is higher than the surrounding region, and case (ii) shows an
invalid target as the average intensity of CIR output is comparable to the
neighborhood region.
with a Python GUI built using Tkinter and the Dynamixel
library. The GUI continuously reads motor encoder positions,
allowing real-time system monitoring. It also offers manual
control for tasks such as testing, calibration, and homing.
Homing ensures motors return to a zero reference point using
mechanical stops. For US image to needle guide calibra-
tion, linear regression was used to establish the relationship
between the needle guide angle and the pixel position of
the needle entry point on the ultrasound image. The needle
guide angle was varied from 30 to 40 degrees in 1-degree
steps, and the corresponding pixel values were recorded.
The relationship between the angle θ and pixel position y
is expressed as θ = β0y + β1
where β0 = 0.03682 and β1 = 47.3703. This allows the
calculation of the needle guide angle based on the pixel
position of the needle. Post-calibration, the average error
decreased from 0.69 ± 0.37 mm to 0.35 ± 0.24 mm.
III. EXPERIMENTS AND RESULTS
The targeting accuracy of the presented autonomous
robotic biopsy system is evaluated using an anthropomorphic
phantom. Three foam targets ranging from 15 to 20 mm in
size were embedded in an agar-water-based phantom. This
agar-water mixture with the ratio of 2 g agar to every 100
ml of water was boiled for 15 minutes. This phantom was
fixed at an approximate liver location in the anthropomorphic
phantom.
The autonomous targeting procedure begins with the hom-
ing of the needle guide; then, the robot moves to a fixed
camera position to capture the surgical scene using the
RGBD camera. The captured RGB image is used to identify
the abdomen region, static key points, and the rectangular
safety workspace. ATIR (Sec. II-B) processes the detected
key points and the abdomen region to detect the organ of
interest from the abdomen point cloud. Following this, a
path is generated, and the robot executes the trajectory in
a point-to-point manner, skipping any points outside the
safety workspace. During the execution, the system captures
an ultrasound (US) image at each point on the trajectory,
identifies potential targets, and converts the segmented image
TABLE I
COMPARISON OF MEAN TARGETING ERROR, MEAN TIME, AND TARGET LOCATIONS FOR POSITION AND IMPEDANCE CONTROL MODES
Target
Control Type
Target Point X
Target Point Y
Target Point Z
Mean Error (mm)
Mean Time (s)
Target 1
Position
-603.59
223.81
155.81
4.582
29.682
Target 2
Position
-622.97
222.04
175.03
3.168
12.641
Target 3
Position
-576.68
226.54
168.54
2.163
23.826
Overall
Position
-
-
-
3.304
22.049
Target 1
Impedance
-581.15
225.76
166.80
7.530
32.979
Target 2
Impedance
-553.72
231.30
171.16
4.330
15.947
Target 3
Impedance
-604.97
222.67
180.23
5.365
31.218
Overall
Impedance
-
-
-
5.741
26.715
into a 3D US slice in the robot’s base coordinate system.
These slices are then concatenated to form a US volume,
which is processed using a clustering algorithm to localize
all targets and their centroid. Once targets are localized, the
robot plans and executes a trajectory to reach each target’s
centroid, calculating the insertion angle and depth for precise
needle insertion and retrieval.
Experiments were performed with two robot control
modes (1) only position-controlled trajectory execution and
(2) impedance-controlled trajectory execution. For each of
these control modes, experiments were repeated ten times,
resulting in a total of 30 targeting attempts for each of the
control modes.
A. Accuracy Assessment
The positional accuracy of autonomous needle targeting
was assessed by manually selecting the needle pixels on
ultrasound (US) images saved after each needle insertion, as
shown in Figure 6. These selected needle pixel positions on
the US image were then converted to 3D needle points using
the saved robot pose. To avoid user bias, the needle trajectory
was manually assessed three times. Principal Component
Analysis (PCA) was used to fit a line to the 3D needle points.
The targeting error is calculated as the minimum distance
between the 3D line and the target point.
B. Results
As shown in Table I, the mean positional accuracy of the
autonomous percutaneous system is 3.304
±
2.70 mm
in position-controlled mode and 5.74
±
2.70 mm in
impedance-controlled mode. The increase in targeting error
in impedance-controlled mode can be attributed to force
compliance adapted to maintain probe-to-surface contact,
resulting in adjustments in the probe’s position while per-
forming US scan and needle placement. The total time to
reach the target in position-controlled mode was 22.04 sec-
onds, while impedance-controlled mode took 26.71 seconds.
Although the needle targeting time should be comparable, the
difference in time is due to fewer needle insertions with ef-
fective needle visibility in position-controlled mode. A lower
error and lower sampling time for position control can be
attributed to the fact that the position control system missed
targets during the scanning procedure in a few attempts.
Each target was supposed to be sampled 10 times each
(a total of 30 attempts for 3 targets); however, in position
control mode, only 18 attempts were successful, while in the
impedance control model, all 30 attempts were successful.
This discrepancy is due to uneven probe-to-surface contact
on the phantom, caused by deformation from the robot-
held US probe and slight mechanical misalignment of the
needle guide, which tilted the needle off the US imaging
plane in the case of position control. However, impedance-
controlled percutaneous operation maintained consistent and
safe surface contact throughout the procedure. This is a clear
indication of the requirement of an impedance-controlled
system for clinical translation. As corroborated in [23] un-
even force on the surface by the ultrasound probe can cause
several errors. These results highlight the potential of an
autonomous impedance-controlled percutaneous system for
safe and effective liver biopsy with tumor size larger than 5
mm.
IV. CONCLUSION AND DISCUSSION
This paper presented a fully autonomous robotic sys-
tem for ultrasound-guided percutaneous interventions. The
system operates through a multi-step process designed to
achieve full autonomy. Firstly, target organs are localized
using CT-point cloud registration using the ATIR algorithm,
followed by generating a region of interest for the organ on
the subject’s skin. The system then plans and executes a path
for ultrasound scanning. Two robot control modes have been
evaluated, position and impedance control for trajectory exe-
cution. During this scanning process, 2D ultrasound images
are acquired, with tumor detection facilitated by the CIR
algorithm. These 2D images are stacked to produce a 3D
volumetric representation of the scanned organ and lesions.
Finally, the calibrated needle guidance system is used to
perform needle insertion. Using the impedance control mode,
the experiments showed a targeting accuracy of 5.74 ± 2.7
mm, lesion detection and targeting success rate of 100%,
and a mean procedure time of 26.7 seconds. These results
demonstrate the potential for autonomous ultrasound-guided
interventions. With the achieved accuracy, lesion detection
rate, and successful needle placements, the presented system
shows promise for achieving full autonomy in performing
US-guided percutaneous interventions. Future work will fo-
cus on ex vivo studies with nonhomogeneous phantoms and
organ motion compensation.
REFERENCES
[1] C. Graumann, B. Fuerst, C. Hennersperger, F. Bork, and N. Navab,
“Robotic ultrasound trajectory planning for volume of interest cover-
age,” in 2016 IEEE international conference on robotics and automa-
tion (ICRA).
IEEE, 2016, pp. 736–741.
[2] S. Chen, F. Wang, Y. Lin, Q. Shi, and Y. Wang, “Ultrasound-
guided needle insertion robotic system for percutaneous puncture,”
International Journal of Computer Assisted Radiology and Surgery,
vol. 16, pp. 475–484, 2021.
[3] A. I. Chen, M. L. Balter, T. J. Maguire, and M. L. Yarmush, “Deep
learning robotic guidance for autonomous vascular access,” Nature
Machine Intelligence, vol. 2, no. 2, pp. 104–115, 2020.
[4] S. Virga, O. Zettinig, M. Esposito, K. Pfister, B. Frisch, T. Neff,
N. Navab, and C. Hennersperger, “Automatic force-compliant robotic
ultrasound screening of abdominal aortic aneurysms,” in 2016
IEEE/RSJ international conference on intelligent robots and systems
(IROS).
IEEE, 2016, pp. 508–513.
[5] Q. Huang, J. Lan, and X. Li, “Robotic arm based automatic ultra-
sound scanning for three-dimensional imaging,” IEEE Transactions
on Industrial Informatics, vol. 15, no. 2, pp. 1173–1182, 2018.
[6] A. S. B. Mustafa, T. Ishii, Y. Matsunaga, R. Nakadate, H. Ishii,
K. Ogawa, A. Saito, M. Sugawara, K. Niki, and A. Takanishi,
“Development of robotic system for autonomous liver screening using
ultrasound scanning device,” in 2013 IEEE international conference
on robotics and biomimetics (ROBIO).
IEEE, 2013, pp. 804–809.
[7] E. M. Boctor, M. A. Choti, E. C. Burdette, and R. J. Web-
ster Iii, “Three-dimensional ultrasound-guided robotic needle place-
ment: an experimental evaluation,” The International Journal of Med-
ical Robotics and Computer Assisted Surgery, vol. 4, no. 2, pp. 180–
191, 2008.
[8] Z. Jiang, Y. Zhou, Y. Bi, M. Zhou, T. Wendler, and N. Navab,
“Deformation-aware robotic 3d ultrasound,” IEEE Robotics and Au-
tomation Letters, vol. 6, no. 4, pp. 7675–7682, 2021.
[9] Z. Jiang, H. Wang, Z. Li, M. Grimm, M. Zhou, U. Eck, S. V. Brecht,
T. C. Lueth, T. Wendler, and N. Navab, “Motion-aware robotic 3d
ultrasound,” in 2021 IEEE International Conference on Robotics and
Automation (ICRA).
IEEE, 2021, pp. 12 494–12 500.
[10] F. Suligoj, C. M. Heunis, J. Sikorski, and S. Misra, “Robust–an
autonomous robotic ultrasound system for medical imaging,” IEEE
Access, vol. 9, pp. 67 456–67 465, 2021.
[11] Q. Huang, B. Wu, J. Lan, and X. Li, “Fully automatic three-
dimensional ultrasound imaging based on conventional b-scan,” IEEE
transactions on biomedical circuits and systems, vol. 12, no. 2, pp.
426–436, 2018.
[12] J. Tan, B. Li, Y. Li, B. Li, X. Chen, J. Wu, B. Luo, Y. Leng,
Y. Rong, and C. Fu, “A flexible and fully autonomous breast ultrasound
scanning system,” IEEE Transactions on Automation Science and
Engineering, 2022.
[13] R. Kojcev, B. Fuerst, O. Zettinig, J. Fotouhi, S. C. Lee, B. Frisch,
R. Taylor, E. Sinibaldi, and N. Navab, “Dual-robot ultrasound-guided
needle placement: closing the planning-imaging-action loop,” Interna-
tional journal of computer assisted radiology and surgery, vol. 11, pp.
1173–1181, 2016.
[14] X. Ma, Z. Zhang, and H. K. Zhang, “Autonomous scanning target
localization for robotic lung ultrasound imaging,” in 2021 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE, 2021, pp. 9467–9474.
[15] F. von Haxthausen, S. B¨ottger, D. Wulff, J. Hagenah, V. Garc´ıa-
V´azquez, and S. Ipsen, “Medical robotics for ultrasound imaging:
current systems and future trends,” Current robotics reports, vol. 2,
pp. 55–71, 2021.
[16] K. Li, Y. Xu, and M. Q.-H. Meng, “An overview of systems and
techniques for autonomous robotic ultrasound acquisitions,” IEEE
Transactions on Medical Robotics and Bionics, vol. 3, no. 2, pp. 510–
524, 2021.
[17] M. Safeea and P. Neto, “Kuka sunrise toolbox: Interfacing collabora-
tive robots with matlab,” IEEE Robotics Automation Magazine, vol. 26,
no. 1, pp. 91–96, March 2019.
[18] A.
Lasso,
T.
Heffter,
A.
Rankin,
C.
Pinter,
T.
Ungi,
and
G.
Fichtinger,
“Plus:
Open-source
toolkit
for
ultrasound-guided
intervention
systems,”
IEEE
Transactions
on
Biomedical
Engineering,
no.
10,
pp.
2527–2537,
Oct
2014.
[Online].
Available:
http://perk.cs.queensu.ca/contents/
plus-open-source-toolkit-ultrasound-guided-intervention-systems
[19] J. Wasserthal, H.-C. Breit, M. T. Meyer, M. Pradella, D. Hinck,
A. W. Sauter, T. Heye, D. T. Boll, J. Cyriac, S. Yang, M. Bach, and
M. Segeroth, “Totalsegmentator: Robust segmentation of 104 anatomic
structures in ct images,” Radiology: Artificial Intelligence, vol. 5, no. 5,
Sep. 2023. [Online]. Available: http://dx.doi.org/10.1148/ryai.230024
[20] R. A. G¨uler, N. Neverova, and I. Kokkinos, “Densepose: Dense human
pose estimation in the wild,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2018, pp. 7297–7306.
[21] R. B. Rusu and S. Cousins, “3D is here: Point Cloud Library (PCL),” in
IEEE International Conference on Robotics and Automation (ICRA),
Shanghai, China, May 9-13 2011.
[22] D. Coleman, I. Sucan, S. Chitta, and N. Correll, “Reducing the barrier
to entry of complex robotic software: a moveit,” Case Study, pp. 1–14,
2014.
[23] S. Ødegaard, M. Kimmey, R. Martin, H. Yee, A. Cheung, and
F. Silverstein, “The effects of applied pressure on the thickness,
layers, and echogenicity of gastrointestinal wall ultrasound images,”
Gastrointestinal endoscopy, vol. 38, no. 3, pp. 351–356, 1992.
