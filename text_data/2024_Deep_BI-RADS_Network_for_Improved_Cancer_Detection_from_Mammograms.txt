Deep BI-RADS Network for Improved Cancer
Detection from Mammograms
Gil Ben-Artzi1,2, Feras Daragma1,3, and Shahar Mahpod1,2
1 School of Computer Science, Ariel University, Ariel, Israel
2 {gilba,mahpods}@ariel.ac.il
3 daragma.feras@gmail.com
Abstract. While state-of-the-art models for breast cancer detection
leverage multi-view mammograms for enhanced diagnostic accuracy, they
often focus solely on visual mammography data. However, radiologists
document valuable lesion descriptors that contain additional information
that can enhance mammography-based breast cancer screening. A key
question is whether deep learning models can benefit from these expert-
derived features. To address this question, we introduce a novel multi-
modal approach that combines textual BI-RADS lesion descriptors with
visual mammogram content. Our method employs iterative attention lay-
ers to effectively fuse these different modalities, significantly improving
classification performance over image-only models. Experiments on the
CBIS-DDSM dataset demonstrate substantial improvements across all
metrics, demonstrating the contribution of handcrafted features to end-
to-end.
Keywords: Cancer Detection, BI-RADS, Deep Learning, Mammograms, Breast
Cancer, Attention, Transformer, Multi-Modal
1
Introduction
In recent years, deep learning techniques have emerged as a powerful tool for
breast cancer detection, demonstrating significant potential in enhancing the ac-
curacy of mammography interpretation. State-of-the-art models [?, 6, 28] have
achieved impressive results by leveraging information from different mammo-
gram views (craniocaudal (CC) and mediolateral oblique (MLO)) to enhance
diagnostic accuracy. However, these approaches often focus solely on end-to-end
extracted visual features.
Radiologists use the Breast Imaging Reporting and Data System (BI-RADS)
lexicon [1] to document specific lesion descriptors such as size, shape, and margin
characteristics during mammogram interpretation. These descriptors can offer
crucial insights that aid in distinguishing between benign and malignant lesions.
In this paper we investigate whether incorporating BI-RADS descriptors can
improve deep learning for cancer detection.
arXiv:2411.10894v1  [cs.CV]  16 Nov 2024
2
G. Ben-Artzi et al.
Integrating these descriptors with mammograms poses challenges due to dif-
ferences in modalities, scales, importance levels, and inconsistencies across ra-
diology reports. To address these challenges and answer our research question,
we propose a multi-modal dual-branch architecture. Each branch, corresponding
to CC/MLO views, encodes the mammogram in a multi-resolution manner. We
introduce a dedicated iterative attention mechanism [10] that processes input
from the previous layer, the current encoded resolution of the mammogram, and
processed information from the other branch. By processing information from
these sources at each level using the attention mechanism, our model effectively
overcomes the differences in modalities and inconsistencies.
We conduct experiments using the CBIS-DDSM dataset, which includes both
mammograms and BI-RADS descriptors as metadata. Our results indicate that
our multi-modal iterative attention-based approach effectively integrates both
visual and textual modalities, outperforming image-only models for benign vs.
malignant classification. We achieve performance improvements across all met-
rics compared to image-only models, with an AUC score of 0.87. Our results
demonstrate the significant potential of incorporating handcrafted features with
deep learning models, suggesting a promising direction for future research in
medical image analysis.
2
Related Work
2.1
Handcrafted Features for Cancer Detection
The Breast Imaging Reporting and Data System (BI-RADS) [1], developed by
the American College of Radiology (ACR), acts as a standardized language for
describing and classifying breast lesions identified through mammograms, ultra-
sounds, and MRIs. This system plays a crucial role in improving the consistency,
clarity, and accuracy of breast imaging reports. Unlike our suggested features,
BI-RADS descriptors are based on the grayscale level of the pixels in the lesions.
A similar lexicon, the Thyroid Imaging Reporting and Data System (TI-RADS),
has been proposed for thyroid lesions [2].
2.2
Multi-View Cancer Detection
Liu et al. [14] presented a cross-view correspondence reasoning method based on
a bipartite graph convolutional network for mammogram mass detection. This
approach effectively addresses the challenge of inherent view alignment between
different views by learning geometric constraints. Tulder et al. [24] proposed
a multi-view analysis method for unregistered medical images using cross-view
transformers, addressing the challenge of effectively combining features from
unregistered mammogram views (CC/MLO) with perspective differences. Shen
et al. [20] presented an interpretable classifier for high-resolution breast cancer
screening images utilizing weakly supervised localization. This approach effec-
tively addresses the challenge of interpretability in deep learning models for
Deep BI-RADS
3
Table 1: The BI-RADS descriptors and descriptors classes in the CBIS-DDSM
dataset.
Mass
Calcifications
Margin
Morphology
Circumscribed
Pleomorphic
Ill-defined
Amorphous
Spicular
Linear
Obscured
Punctate
Shape
Distribution
Round
Clustered
Oval
Scattered
Irregular
Diffuse
mammogram analysis. Chen et al. [4] proposed a multi-view local co-occurrence
and global consistency learning method for mammogram classification general-
ization, addressing the challenge of effectively combining features from unregis-
tered mammogram views (CC/MLO) with perspective differences. While these
methods address multi-view analysis, they do not utilize the textual lesion at-
tribute data and cross-view information at each analysis stage - key capabilities
of our architecture.
2.3
Incorporating Handcarfted Features
In the field of mammogram-based deep learning for breast cancer detection,
current research primarily focuses on predicting BI-RADS descriptors as model
outputs. The integration of both these descriptors and visual features in mam-
mogram analysis remains an open research question.
Zhang et al. introduced BI-RADS-NET [29], an explainable deep learning ap-
proach for breast cancer diagnosis that outputs BI-RADS descriptors to better
explain predictions, although their model was designed for ultrasound images.
[16, 22] investigated a deep learning method that utilizes multi-view mammo-
gram images to enhance BI-RADS and breast density assessment, rather than
integrating them as in our approach. Liu et al. [13] explored the potential of
combining mammography-based deep learning with clinical factors such as age
and family history of breast cancer, demonstrating the potential benefits of in-
tegrating additional features with visual data in the prediction process.
3
Model Architecture
Our model consists of two branches. Each branch is composed of N = 6 stacked
identical attention-based layers. An overview of our dual-branch architecture
using stacked multi-attention layers (gray background) is presented in Fig. 1.
4
G. Ben-Artzi et al.
Fig. 1: Our model takes mammograms from the MLO and CC views along with
a varying number of textual descriptors classes describing one or more lesions
as input. The multi-attention layers (grayed blocks) processes these descriptors
along with visual features extracted from mammogram images in different reso-
lutions.
The attention based layers progressively fuse and process the multi-modal
inputs. The input to the first layer is textual attributes with a skip connection
to the last layer. In the first layer, learnable query vectors ˆX and ˆY are used
since no feature queries exist yet. The input to subsequent layers is the extracted
image features at different resolutions using the Big Transfer (BiT) blocks [11],
the input from the preceding layer, and the latent features from the other branch.
The output of the final attention layer in each branch is aggregated by av-
eraging to obtain a unified vector z ∈RL×1. This representation encodes the
joint contribution of images and text. The vector is then layer-normalized and
reduced into a labeling vector using a fully-connected layer for the benign and
malignant classes.
In the following we present a detailed description of each component of our
model.
Deep BI-RADS
5
3.1
BI-RADS Descriptors Encoding
Our model utilizes the textual metadata associated with each mammogram,
which contains the classes of the lesion and breast descriptors of the Breast
Imaging Reporting and Data System (BI-RADS) lexicon [1]. We do not use
subjective assessments reflecting radiologist suspicion, like BI-RADS scores, but
only the descriptive physical lesion and breast characteristics annotated during
routine screening. Table 1 presents examples of the descriptors and descriptors
classes that are incorporated by our model. Both calcifications and mass lesions
can have combinations of these descriptors. For instance, a mass lesion can have
a "Circumscribed-Obscured" margin or a "Round-Oval" shape. Our approach
allows for the integration of a variable number of classes, as well as their combi-
nations.
We assign a unique index i = 1, . . . , N to enumerate the possible values of
the descriptors classes across all categories. The input to our model is a binary
vector ϕ ∈RL (we use L = 256), defined as:
ϕ(i) =
(
1,
if descriptor class i exists in the description
0,
otherwise.
(1)
The encoded input vector ϕ represents the BI-RADS descriptors for a single
lesion. However, there can be cases with more than one lesion. Our architecture
supports a dynamic number of input vectors, so the input is Φ = {ϕj}K
j=1 where
K is the number of lesions in the mammogram.
3.2
Feature Extraction
We use BiT layers as our feature extractor, pre-trained on the PatchCamelyon
dataset [26]. Our BiT layer F is based on ResNet50-V2 [7,8] with modifications
made by [11] to Group Normalization [27] instead of Layer Normalization [3],
and the use of Weight Standardization [18] for all convolution layers. The output
of each of these blocks is the input to our multi-attention layer. Each BiT layer F
reduces the resolution and increases the number of channels using the following
formulation:
F0 = F0(I)
Fk = Fk(Fk−1),
∀
k = 1, . . . , N −1,
(2)
I ∈R1×H×W is the input image of height H and width W, Fk ∈Rd0·2k×H′×W ′
where H′ =
H
4·2k , W ′ =
W
4·2k and d0 = 64.
3.3
Multi-Attention Layer
The multi-attention layer has three attention-based [25] sub-layers. The first
is a cross-attention mechanism, the second is self-attention and the third is
view-attention. They enable the model to establish connections between different
6
G. Ben-Artzi et al.
resolutions and the attributes, between patches within the same image, and
between images from different views.
The utilization of attention enables the exploration of connections between a
provided query Q, pre-existing key data K, while representing these relationships
using V . It is stated as follows:
Attn(Q, K, V ) = Softmax(QKT
√
d
)V,
(3)
where d is the scaling factor corresponding to the dimensionality of the key
vectors.
Cross-Attention The first sub-layer, referred to as cross-attention, allows ef-
ficient processing of multi-modal inputs including attributes, latent features
and images, without relying on domain-specific assumptions. It takes a high-
dimensional input and projects it into a lower-dimensional latent bottleneck [10].
It then applies Transformer-style self-attention on this latent space. It combines
the preceding latent features with either the attributes or image features at a
given resolution.
The cross-attention in layer k, denoted as AC
k , is defined as:
AC
k := Attn(Ck−1, Fk−1, Fk−1),
(4)
where Fk−1 is the extracted features from the previous layer (Section 3.2)
and Ck−1 is the output of the previous multi-attention layer.
Positional encoding vectors are employed to encode the feature vector Fk. In
the first Cross-attention sub-layer, where no preceding input exists, the query is
learnable parameters X:
Attn(X, Φ, Φ).
(5)
Self-Attention The self-attention sub-layer is placed right after the cross-
attention sub-layer. Similar to [10], the goal is to model both short-range and
long-range dependencies within the features and capture the global context.
The inputs for the self-attention in layer k, denoted as AS
k , are the output of
the cross-attention:
AS
k := Attn(AC
k , AC
k , AC
k ).
(6)
View-Attention The view-attention sub-layer combines the latent features
from the current view with the latent features from the other view, enabling
the expansion of the context to both MLO and CC. The values V of the view-
attention block are the output of the preceding self-attention block, while the
query Q and keys K are the output of the view-attention from the other branch
at the corresponding level:
Deep BI-RADS
7
Attn( ˆ
AS
k , ˆ
AS
k , AS
k ),
(7)
where ˆ· denotes the output of the self-attention sub-layer in the other branch.
Input-Output To ensure that our multi-attention layer receives input with the
same number of channels, we reshape the feature tensor Fk to have dimensions
Fk ∈Rd′×Nk, where Nk =
H′·W ′
2(n−k−1) and d′ = 4 · d0 represents the desired length
of the feature vectors inserted into the multi-attention layer.
The output tensors of the multi-attention layer at level k have dimensions
RL×Nk, where L denotes the length of the multi-attention latent vector. The
query parameters ˆX and ˆY are learnable parameters with dimensions RL×NQ,
where NQ is a hyperparameter set by the user.
3.4
Sub-Layer Attention Computation
Given an input sequence X = (x1, x2, . . . , xN), each attention sub-layer com-
putes a weighted sum of the values at all positions in the sequence. This is
achieved through the following steps:
Positional Encoding. To provide positional information to the model, we
apply a Fourier feature encoding to the input sequences. Similar to [10], we
utilize the Fourier feature positional encodings introduced in [21].
Given N input vectors xi ∈RL each associated with a position index i, we
first normalize the index as:
pi = 2 · i
N −1
(8)
We then define a set of sinusoidal frequency bands:
Sb = {S | S = b · mfreq
nbands
, 1 ≤b < nbands, b ∈Z+}
(9)
where mfreq and nbands determine the maximum frequency and number of
bands.
The sinusoidal position encoding vectors are then calculated as:
PE1b(pi) = sin(pi · Sb · π),
(10)
PE2b(pi) = cos(pi · Sb · π).
(11)
Finally, we concatenate the normalized index pi and encoding vectors PE1b,
PE2b to the original input xi, expanding it to xi ∈RL+2nbands+1. This injects
positional information through sinusoidal functions of different frequencies, al-
lowing the model to utilize the order of the input vectors.
Linear Transformation. The input sequence X is linearly projected into
the query (Q), key (K), and value (V ) matrices using learnable weight matrices
WQ, WK, and WV :
8
G. Ben-Artzi et al.
Q = XWQ
(12)
K = XWK
(13)
V = XWV
(14)
where Q, K, V ∈RL×dmodel, and dmodel is the dimensionality of the model.
This transformation projects the input into distinct query, key, and value
spaces. The query and key matrices are used to compute attention weights in-
dicating the relevance between inputs. The value matrix holds the input repre-
sentations that will be aggregated according to the attention weights.
Attention Unit. We compute the attention function (Equation (3)).
Position-wise Feed-Forward Network. After the attention unit, a position-
wise feed-forward network is applied to each position independently. The feed-
forward network consists of two linear transformations with a ReLU activation
function in between:
FFN(x) = max(0, xW1 + b1)W2 + b2
(15)
where x is the input, W1, W2 are weight matrices, and b1, b2 are bias vectors.
4
Experimental Setup
4.1
Dataset
We use the Curated Breast Imaging Subset of DDSM (CBIS-DDSM) dataset [12]
that contains valuable metadata providing additional clinical information about
each mammogram and associated lesions. It is a widely used mammography
image collection annotated by radiologists, derived from the original DDSM [9]
dataset and contains a diverse range of breast abnormalities, including benign
and malignant lesions. The images are provided in the Digital Imaging and
Communications in Medicine (DICOM) format, along with detailed annotation
files. These files specify lesion locations, types, ROI crops, and binary masks
across the craniocaudal (CC) and mediolateral oblique (MLO) views. It includes
1566 patients with total of 3,568 abnormalities, 1696 mass and 1872 calcification.
In our experiments, we employ five-fold stratified cross-validation to maintain
class balance across folds.
4.2
Implementation Details
The training was done in mini-batches, with each mini-batch size set to 16. For
each image in our training data, the image’s content is scaled to 1024 × 1024
pixels. As data augmentation, we used vertical and horizontal flips, as well as
elastic deformation. We set a total of 1000 training iterations for each fold. We
utilized Cross-Entropy as our loss [5]. An initial learning rate was set to 0.001
and we employed a decaying factor of 10 after 500 iterations. We used the SGD
optimizer [19] with momentum set to 0.9. Dropout value was set to 0.25. We
implemented our model in PyTorch [17].
Deep BI-RADS
9
Model
AUC Accuracy Specificity Precision Recall F1-Score
[15]
0.680
0.661
0.670
0.638
0.651
0.644
[23]
0.811
0.723
0.750
0.686
0.698
0.692
Ours - no descriptors 0.711
0.664
0.650
0.676
0.619
0.634
Ours
0.872
0.760
0.773
0.760
0.743
0.751
Table 2: Quantitative performance analysis to detect abnormality using CBIS-
DDSM dataset.
Configuration Q K V AUC Accuracy Specificity Precision Recall F1-Score
0
C O O 0.835
0.738
0.731
0.643
0.750
0.692
1
O C O 0.850
0.760
0.790
0.762
0.727
0.744
2
O O C 0.878
0.796
0.773
0.816
0.786
0.780
3
C C O 0.852
0.760
0.768
0.714
0.750
0.732
4
C O C 0.843
0.764
0.792
0.762
0.733
0.747
5
O C C 0.848
0.771
0.803
0.778
0.737
0.757
Table 3: The effect of different configurations for the inputs (Query, Keys, Value)
to the view attention sub-layer in the multi-attention layer. The inputs can come
from either the current view (C) or the opposite view (O) of the mammogram.
Configuration 2, in which the Query and Keys inputs are from the opposite view,
achieved the best overall performance.
5
Results
We compare our multi-modal descriptor-based model ("Deep BI-RADS") against
several baselines: a descriptor-excluded variant of our own model, a multi-view
Transformer baseline [23], and an advanced recent single-view Transformer ap-
proach with four branches [15]. The descriptor-excluded variant includes the
multi-attention layers, allowing us to evaluate the specific contribution of the
BI-RADS descriptors. Comparing with a multi-view architecture helps assess
the contribution of both the attention layers and the BI-RADS descriptors. The
multi-view baseline employs a Transformer architecture to analyze pairs of unreg-
istered mammograms from different views and achieves state-of-the-art results
on the CBIS-DDSM. Comparison with a single-view Transformer evaluates the
contribution of the multi-view architecture.
To ensure a fair comparison, we trained all models from scratch following their
respective provided training protocols. We evaluate the models for classifying
mass lesions, following common practice. The results are obtained using five-
fold stratified cross-validation to maintain class balance across folds.
Table 2 presents the quantitative performance analysis. Our multi-modal ap-
proach achieves a higher AUC of 0.872 compared to 0.711 without incorporating
the BI-RADS descriptors, demonstrating the benefits of integrating textual infor-
mation. We also attain an AUC of 0.872 versus 0.811 for the baseline multi-view
10
G. Ben-Artzi et al.
model, showcasing the advantages of our multi-attention fusion approach over
prior multi-view only techniques.
Beyond AUC, utilizing BI-RADS descriptors enables consistent gains across
accuracy, specificity, precision, recall, and F1-score on both tasks. Our approach
increases recall from 0.619 to 0.743 compared to the baseline without BI-RADS.
This demonstrates improved sensitivity in detecting true positive cases by incor-
porating textual descriptor classes.
Notably, the high F1 scores demonstrate that our model balances improved
sensitivity with precision, rather than sacrificing one metric for the other. This
indicates that our multi-modal methodology incorporates the radiologist context
to enhance interpretation without introducing additional false positives.
Figure 2 presents the ROC curve for a single fold, summarizing the trade-
off between the true positive rate and false positive rate for our model using
different probability thresholds. Overall, our multi-modal method shows promise
for generalized breast abnormality detection by effectively combining visual and
textual information.
Fig. 2: ROC curve for our approach
5.1
Input to Multi-Attention Layer
Table 3 represents different configurations (Figure 3) for wiring the query (Q),
and keys (K), and values (V). In both branches, the input to the multi-attention
layer can be either from the current view (C) or from the opposite view (O).
There are six possible configurations, as we always wire at least one input from
the other view. Based on the results in Table 3, we conclude that wiring the
query (Q) and keys (K) inputs to the attention layer from the opposite view
(configuration 2) leads to the best performance, with the highest metrics.
Deep BI-RADS
11
Configuration
Average
3 layers
0.69 ± 0.015
5 layers
0.73 ± 0.019
6 layers
0.76 ± 0.010
7 layers
0.67 ± 0.018
Table 4: Accuracy for different numbers of multi-attention layers. Results are
across five folds.
Some observations:
– Wiring Q and K from the opposite view consistently outperforms wiring
them from the current view (e.g., compare configurations 2 vs 3). This might
suggest that the attention mechanism benefits from fusing information be-
tween the two views via the value input specifically.
– Based on how the inputs are interconnected between the two views, there is
a noticeable difference in performance. This emphasizes the importance of
effectively leveraging the two views.
– Wiring Q and K from the same view (configurations 2,3) performs better
than wiring them from different views.
– Specificity is highest when wiring Q from the opposite view and K and V
from the current view (configuration 5). However, other metrics like recall
are lower in this configuration.
5.2
Number of Multi-Attention Layers
The number of multi-attention layers primarily influences our model size. We
trained configurations with 3, 5, 6, and 7 layers.
Table 4 presents the accuracy for each model across five folds. The 3-layer
model underperformed, while the 5-layer model achieved the second-best results
overall. The 6-layer configuration yielded the highest average accuracy, outper-
forming 5 layers. However, further increasing layers to 7 degraded performance,
likely due to overfitting given the limited dataset size. In our implementation,
we deploy 6 layers which achieved the optimal trade-off between model capacity
and overfitting on this dataset.
5.3
Augmentations
We tested various data augmentation strategies to improve model generalization
of our model. Table 5 presents test set performance for different augmentation
configurations. The baseline with no augmentations underperformed all aug-
mented models, indicating augmentations are beneficial. Adding random hor-
izontal/vertical flips or elastic deformations to the baseline improved average
accuracy. Resizing the images to 1024x1024 pixels achieved the best overall re-
sults. Interpolation sizes of 2048x2048 and 384x384 underperformed. Gaussian
12
G. Ben-Artzi et al.
Fig. 3: The possible configurations of the inputs for the view attention sublayer
in our multi-attention layer. Q, K, and V represent the Query, Keys, and Value
respectively.
Configuration
Average
Baseline w/o aug.
0.656 ± 0.025
Baseline + 384
0.702 ± 0.014
Baseline + 1024
0.72 ± 0.008
Baseline + 2048
0.646 ± 0.019
Baseline + h/vflip
0.666 ± 0.02
Baseline + elastic
0.688 ± 0.017
Baseline + Gaussian 0.612 ± 0.018
Table 5: Accuracy for different augmentation strategies.
noise augmentation degraded performance, likely due to occluding meaningful
mammographic details. The optimal configuration utilized interpolation upsam-
pling to 1024x1024 pixels, which seems to balance overfitting and underfitting
effects based on model capacity.
6
Conclusion
In this study we ask whether incorporating BI-RADS descriptors can improve
deep learning for cancer detection. Our results provide a clear affirmative answer
to this question. We presented a multi-modal approach that combines visual
mammogram data with textual BI-RADS descriptors, utilizing a dual-branch
architecture with iterative attention layers. Experiments on the CBIS-DDSM
dataset demonstrated significant improvements over image-only models. These
Deep BI-RADS
13
findings suggests that the fusion of features based on human expertise and auto-
matically extracted features can lead to superior outcomes in cancer detection.
References
1. American College of Radiology: ACR BI-RADS® Atlas — Mammography. Amer-
ican College of Radiology, Reston, VA (2013)
2. American College of Radiology: ACR TI-RADS® Atlas. American College of Ra-
diology, Reston, VA (2073)
3. Ba, L.J., Kiros, J.R., Hinton, G.E.: Layer normalization. CoRR abs/1607.06450
(2016), http://arxiv.org/abs/1607.06450
4. Chen, Y., Wang, H., Wang, C., Tian, Y., Liu, F., Liu, Y., et al.: Multi-view lo-
cal co-occurrence and global consistency learning improve mammogram classifica-
tion generalisation. In: International Conference on Medical Image Computing and
Computer-Assisted Intervention. pp. 3–13. Springer Nature Switzerland (2022)
5. De Boer, P.T., Kroese, D.P., Mannor, S., Rubinstein, R.Y.: A tutorial on the cross-
entropy method. Annals of operations research 134(1), 19–67 (2005)
6. Falconi, L.G., Maria Perez, W.G.A., Conci, A.: Transfer learning and fine tuning
in breast mammogram abnormalities classification on cbis-ddsm database 5(2),
154–165
7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770–778 (2016)
8. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks
(2016),
http://arxiv.org/abs/1603.05027,
cite
arxiv:1603.05027Comment:
ECCV 2016 camera-ready
9. Heath, M., Bowyer, K., Kopans, D., Kegelmeyer, P., Moore, R., Chang, K., Mun-
ishkumaran, S.: Current status of the digital database for screening mammography.
In: Digital mammography, pp. 457–460. Springer (1998)
10. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., Carreira, J.: Per-
ceiver: General perception with iterative attention. In: International conference on
machine learning. pp. 4651–4664. PMLR (2021)
11. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., Houlsby, N.:
Big transfer (bit): General visual representation learning. In: Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part V 16. pp. 491–507. Springer (2020)
12. Lee, R.S., Gimenez, F., Hoogi, A., Miyake, K.K., Gorovoy, M., Rubin, D.L.: A
curated mammography data set for use in computer-aided detection and diagnosis
research. Scientific data 4(1), 1–9 (2017)
13. Liu, H., Chen, Y., Zhang, Y., Wang, L., Luo, R., Wu, H., Wu, C., Zhang, H.,
Tan, W., Yin, H., et al.: A deep learning model integrating mammography and
clinical factors facilitates the malignancy prediction of bi-rads 4 microcalcifications
in breast cancer screening. European Radiology 31, 5902–5912 (2021)
14. Liu, Y., Zhang, F., Zhang, Q., Wang, S., Wang, Y., Yu, Y.: Cross-view correspon-
dence reasoning based on bipartite graph convolutional network for mammogram
mass detection. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 3811–3821 (2020)
15. Mo, Y., Han, C., Liu, Y., Liu, M., Shi, Z., Lin, J., Zhao, B., Huang, C., Qiu, B., Cui,
Y., et al.: Hover-trans: Anatomy-aware hover-transformer for roi-free breast cancer
diagnosis in ultrasound images. IEEE Transactions on Medical Imaging (2023)
14
G. Ben-Artzi et al.
16. Nguyen, H.T.X., Tran, S.B., Nguyen, D.B., Pham, H.H., Nguyen, H.Q.: A novel
multi-view deep learning approach for bi-rads and density assessment of mam-
mograms. In: 2022 44th Annual International Conference of the IEEE Engineer-
ing in Medicine and Biology Society (EMBC). pp. 2144–2148 (2022). https:
//doi.org/10.1109/EMBC48229.2022.9871564
17. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,
Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,
Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.:
Pytorch: An imperative style, high-performance deep learning library. In: Wallach,
H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., Garnett, R. (eds.)
Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran
Associates, Inc. (2019)
18. Qiao, S., Wang, H., Liu, C., Shen, W., Yuille, A.L.: Weight standardization.
CoRR abs/1903.10520 (2019), http://dblp.uni-trier.de/db/journals/corr/
corr1903.html#abs-1903-10520
19. Robbins, H., Monro, S.: A stochastic approximation method. The annals of math-
ematical statistics pp. 400–407 (1951)
20. Shen, Y., Wu, N., Phang, J., Park, J.C., Liu, K., Tyagi, S., et al.: An interpretable
classifier for high-resolution breast cancer screening images utilizing weakly super-
vised localization. Medical Image Analysis 68, 101908 (2021)
21. Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Sing-
hal, U., Ramamoorthi, R., Barron, J., Ng, R.: Fourier features let networks learn
high frequency functions in low dimensional domains. Advances in Neural Infor-
mation Processing Systems 33, 7537–7547 (2020)
22. Tsai, K.J., Chou, M.C., Li, H.M., Liu, S.T., Hsu, J.H., Yeh, W.C., Hung, C.M.,
Yeh, C.Y., Hwang, S.H.: A high-performance deep neural network model for bi-rads
classification of screening mammography. Sensors 22(3), 1160 (2022)
23. Tulder, G.v., Tong, Y., Marchiori, E.: Multi-view analysis of unregistered medical
images using cross-view transformers. In: International Conference on Medical Im-
age Computing and Computer-Assisted Intervention. pp. 104–113. Springer (2021)
24. van Tulder, G., Tong, Y., Marchiori, E.: Multi-view analysis of unregistered medical
images using cross-view transformers. In: International Conference on Medical Im-
age Computing and Computer-Assisted Intervention. pp. 104–113. Springer (2021)
25. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
L., Polosukhin, I.: Attention is all you need. Advances in Neural Information Pro-
cessing Systems 30, 5998–6008 (2017)
26. Veeling, B.S., Linmans, J., Winkens, J., Cohen, T., Welling, M.: Rotation equiv-
ariant CNNs for digital pathology (Jun 2018)
27. Wu, Y., He, K.: Group normalization. In: Ferrari, V., Hebert, M., Sminchisescu,
C., Weiss, Y. (eds.) Computer Vision - ECCV 2018 - 15th European Conference,
Munich, Germany, September 8-14, 2018, Proceedings, Part XIII. Lecture Notes
in Computer Science, vol. 11217, pp. 3–19. Springer (2018). https://doi.org/10.
1007/978-3-030-01261-8_1, https://doi.org/10.1007/978-3-030-01261-8_1
28. Yan, Y., Conze, P.H., Lamard, M., Quellec, G., Cochener, B., Coatrieux, G.: To-
wards improved breast mass detection using dual-view mammogram matching.
Medical Image Analysis 71, 102083 (2021)
29. Zhang, B., Vakanski, A., Xian, M.: Bi-rads-net: an explainable multitask learning
approach for cancer diagnosis in breast ultrasound images. In: 2021 IEEE 31st
International Workshop on Machine Learning for Signal Processing (MLSP). pp. 1–
6. IEEE (2021)
