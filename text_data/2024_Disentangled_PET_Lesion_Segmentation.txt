DISENTANGLED PET LESION SEGMENTATION
Tanya Gatsak1, Kumar Abhishek1, Hanene Ben Yedder1, Saeid Asgari Taghanaki1,2, Ghassan Hamarneh1
1Medical Image Analysis Lab, School of Computing Science, Simon Fraser University, Canada
2Autodesk AI Research, Canada
ABSTRACT
PET imaging is an invaluable tool in clinical settings as it
captures the functional activity of both healthy anatomy and
cancerous lesions. Developing automatic lesion segmentation
methods for PET images is crucial since manual lesion seg-
mentation is laborious and prone to inter- and intra-observer
variability. We propose PET-Disentangler, a 3D disentangle-
ment method that uses a 3D UNet-like encoder-decoder ar-
chitecture to disentangle disease and normal healthy anatomi-
cal features with losses for segmentation, reconstruction, and
healthy component plausibility. A critic network is used to
encourage the healthy latent features to match the distribu-
tion of healthy samples and thus encourages these features to
not contain any lesion-related features. Our quantitative re-
sults show that PET-Disentangler is less prone to incorrectly
declaring healthy and high tracer uptake regions as cancerous
lesions, since such uptake pattern would be assigned to the
disentangled healthy component.
Index Terms— Positron Emission Tomography (PET),
Image Segmentation, Disentangled Representations
1. INTRODUCTION
Positron emission tomography (PET) is a medical imaging
modality that captures functional, metabolic activity within
the body. PET is used in many areas of medicine, especially
in oncology for cancer staging, diagnosis, and monitoring [1].
Due to the nature of the modality, areas of high metabolic ac-
tivity, including both healthy anatomy and cancerous lesions,
correspond to high intensity values. Expert clinicians’ analy-
sis of PET images is laborious and can suffer from inter- and
intra-observer variability, neccessitating automated methods.
Over the past few decades, automatic PET lesion seg-
mentation methods have evolved from various intensity
thresholding-based approaches, active contours, and region
growing algorithms [2] to the current state-of-the-art methods
that rely on deep learning (DL) to optimize models to learn
disease features [3]. A promising DL-based approach that
has yet to be explored in PET lesion segmentation is image
disentanglement, which attempts to separate distinct sources
of variation within images into independent representations.
Image disentanglement has been shown to be beneficial in
a wide variety of medical image analysis applications [4].
However, there is no work on evaluating the utility of an
image disentanglement-based approach for PET images, a
challenging modality owing to the presence of noise, low
signal-to-contrast ratio, and high activity regions correspond-
ing to both healthy and disease activity.
In this work, we leverage lesion segmentation with a dis-
entanglement framework and introduce PET-Disentangler, a
novel PET segmentation method that disentangles 3D PET
images in the latent space into disease features and normal
healthy anatomical features for a robust and explainable seg-
mentation method. PET-Disentangler uses the disease fea-
tures for lesion segmentation prediction, the healthy features
to estimate pseudo-healthy images per input, and re-entangles
both healthy and disease features for a full reconstruction.
PET-Disentangler enhances the lesion segmentation task by
providing explainability in the form of a pseudo-healthy im-
age as to what the model expects the lesion-free image to
look like per given input.
Furthermore, PET-Disentangler
shows that learning the healthy component provides a solu-
tion to a critical challenge in PET lesion segmentation [5],
where segmentation models can incorrectly segment healthy,
high-intensity areas as disease. This challenge has seen solu-
tions that focus on localizing healthy, high intensity regions
[6] whereas the proposed PET-Disentangler can both capture
the healthy anatomy features and delineate lesions through
the learned disentangled representations.
2. METHOD
An overview of the proposed architecture is shown in Fig. 1,
including a critic network that is used to ensure that healthy
features do not contain lesion features.
PET-Disentangler
adopts a UNet-like architecture that is extended from 2D to
3D and further modified to have one encoder and two de-
coders for segmentation prediction and for reconstruction.
The encoder takes as input a PET image X and outputs two
bottleneck latent vectors zh and zd, which encode the healthy
and disease features of the input image, respectively. The
disease features are passed to the segmentation decoder that
predicts a segmentation mask M. The healthy features are
passed along with the segmentation mask to the image de-
coder to re-entangle the healthy and disease features and
arXiv:2411.01758v1  [eess.IV]  4 Nov 2024
Fig. 1.
The proposed disentanglement architecture of
PET-Disentangler facilitates the learning of disease fea-
tures through two pathways:
a segmentation prediction
path and image reconstruction path.
By re-entangling
healthy and disease-specific features, this design enables
PET-Disentangler to effectively capture disease characteris-
tics while maintaining an accurate representation of healthy
anatomy. The black arrows indicate feature flow throughout
the network, the blue arrows represent skip connections, and
the green arrow represents the use of the mask prediction in
the image decoder.
produce a reconstruction of the input R. Skip connections are
used between each encoder and segmentation decoder block,
where the number of skip connections between the encoder
and the image decoder are tuned to prevent the introduction
of disease features. This is because the disease features, if
any exist, should be introduced via the segmentation mask
during image decoding.
2.1. Modified Encoder Architecture
The encoder takes in a 3D PET image and applies a series of
encoding blocks, consisting of a 3D convolution, batch nor-
malization and ReLU, to produce the latent vectors zh and
zd. The first encoding block takes X as input and produces
an output vector that is then split into healthy and disease fea-
tures via the application of two separate encoding blocks. For
the encoding, separate encoding blocks are applied to healthy
and disease features.
2.2. Segmentation Decoder and Prediction
The disease features zd are passed to the segmentation de-
coder with the corresponding skip connections to generate a
segmentation mask prediction M. Each decoding block con-
sists of a transposed 3D convolution to upsample the spatial
dimensions of the feature vector, concatenation between the
feature vector and skip connection from the encoder, followed
by a sequence of 3D convolution, batch normalization and
ReLU.
To optimize the segmentation mask prediction, Com-
boLoss [7], LCombo, is used between the predicted mask and
the ground truth mask, MGT :
Lseg = LCombo(M, MGT ) = LDice + LCE,
(1)
where MGT is binary such that voxels labelled as 0 belong
to either background or healthy anatomy whereas voxels la-
belled as 1 belong to cancerous lesions.
2.3. Image Decoder and Reconstruction
The image decoder uses the healthy features zh and corre-
sponding skip connections along with the segmentation pre-
diction M during decoding to generate image reconstructions.
When the segmentation mask prediction contains lesion fea-
tures, the image decoder is optimized to predict R, the full
reconstruction of input X. When the mask prediction is set to
empty such that it contains all zeros, M0, the image decoder
is optimized to leverage only the healthy features to generate
P, a pseudo-healthy estimate of the input X. For inputs X
that have no tumours, R and P should be identical.
The image decoder re-entangles the healthy and disease
features using spatially-adaptive normalization (SPADE)
blocks [8] to combine the healthy features at each resolu-
tion during image decoding with a downsampled version of
the segmentation mask of the same spatial dimensions. The
image decoder follows a similar architecture as the segmen-
tation decoder, with the key difference being the introduction
of SPADE blocks to combine healthy and disease features.
Another difference is that the skip connections are removed
from the last three decoding blocks to prevent disease features
from potentially being introduced from outside of the mask
prediction.
To optimize the model to learn the reconstruction R, L1
and L2 reconstruction losses are used:
Lrecon = ||X −R||1 + ||X −R||2.
(2)
2.4. Critic Network for Healthy Distribution Matching
To ensure that zh only contains features relating to healthy
anatomy, regardless of whether the input image has lesions, a
Wasserstein GAN (WGAN) is used as a critic network with
gradient penalty to align healthy features to a healthy distri-
bution [9].
A set of images can be partitioned into those without any
tumour lesions (i.e., negative findings) X−and those with
tumour lesions (i.e., positive findings) X+, where the cor-
responding healthy feature vectors are z−
h and z+
h , respec-
tively. Ideally, the healthy feature vectors should only contain
features for healthy anatomy regardless of whether the input
image has disease, and the distributions corresponding to the
sets of healthy feature vectors should match. The process of
distribution matching will eliminate any leakage of disease
in zh, as disease features will be outside of the distribution
of healthy anatomy, thus ensuring that zh from all examples
only contain healthy features. The critic network is optimized
using the Lcritic loss:
Lcritic = wc
 −
 C(z−
h ) −C(z+
h )

+ λGP (||∇zmC(zm)||2 −1)2
,
(3)
where the two terms correspond to the Wasserstein distance
and the gradient penalty, respectively. The gradient penalty
uses an interpolated healthy vector zm between z−
h and z+
h ,
that is weighted by α and calculated as follows:
zm = αz−
h + (1 −α)z+
h .
(4)
As the critic network learns to identify the difference be-
tween z−
h and z+
h , the encoder tries to produce healthy feature
vectors that appear from the same distribution. As such, the
corresponding WGAN term used in the overall loss is referred
to as the pseudo-healthy loss Lpseudo-healthy, described by:
Lpseudo-healthy = −C(z+
h ).
(5)
2.5. Overall Objective Function
The critic network is optimized separately from the rest of the
network components, whereas the overall objective function
to optimize the encoder, segmentation decoder, and image de-
coder is:
Loverall = ws Lseg + wr Lrecon + wph Lpseudo-healthy. (6)
The parameters ws, wr, and wph refer to the weights of
the contribution for the segmentation, reconstruction, and
pseudo-healthy losses to the overall loss, Loverall.
3. EXPERIMENTS
Dataset: We evaluate PET-Disentangler on TCIA whole-
body FDG-PET/CT dataset [10] that consists of 900 patients
and a total of 1014 scans, where 513 scans have no cancerous
lesions and 501 scans have lesions from either lymphoma,
melanoma, or lung cancer.
As a core contribution of our
method is the ability to model and learn the healthy anatomy
component to ultimately distinguish disease features when
they are present, we require volumes of the same relative
area between all healthy and disease examples to learn from.
TotalSegmentator [11] is leveraged to obtain anatomical seg-
mentations for each whole-body PET-aligned CT scan, in
which these segmentations act as landmarks to center spatial
croppings in the PET scans and obtain a set of aligned sub-
volumes for the dataset. We utilize the bladder segmentation
to center a lower torso 128 × 128 × 128 cropped PET volume
that is further resized to 64×64×64 using bilinear interpola-
tion. We then clip PET SUV values to [0, 15] and normalize
Table 1. Lesion segmentation Dice on lower torso
Method
Healthy (71)
Disease (31)
Overall (102)
SegOnly
0.0007 ± 0.0026
0.1864 ± 0.2474
0.0572 ± 0.1598
SegRecon
0.0013 ± 0.0048
0.1847 ± 0.2474
0.0570 ± 0.1593
SegReconHealthy
0.0008 ± 0.0012
0.1791 ± 0.2403
0.0550 ± 0.1547
PET-Disentangler
0.7174 ± 0.4200
0.5153 ± 0.2843
0.6560 ± 0.3937
to [0, 1]. The healthy and disease examples are split into
80:10:10 splits for training, validation, and testing sets.
Implementation Details:
PET-Disentangler was imple-
mented using PyTorch and MONAI. We used the Adam
optimizer with a learning rate of 1e-3 and a batch size of 4
volumes of 643 voxels, consisting of 2 healthy and 2 disease
examples per batch, to train the models. Each model was
trained for 300 epochs using a NVIDIA A5000 GPU (24
GB) and the model with the lowest LCombo on the validation
set was used for evaluation. The weights ws, wr, wph, and
wc were empirically found to be 100, 10, 1e-3, and 1e-2,
respectively. The Dice coefficient was used for evaluating the
segmentations.
Baseline and Ablation Experiments: To investigate whether
disentanglement of healthy and disease features can improve
lesion segmentation in PET images, PET-Disentangler is
compared to baseline non-disentanglement variations that use
3D UNet: (i) SegOnly performs segmentation only, learning
only disease features, (ii) SegRecon performs simultaneous
segmentation and reconstruction learning both disease and
image features, and (iii) SegReconHealthy learns disease and
healthy features by performing segmentation on all examples
and reconstruction on only healthy examples, providing a
simpler approach to learn disjoint healthy and disease fea-
tures compared to disentanglement.
4. RESULTS
Table 1 presents the Dice coefficients (mean ± standard devi-
ation) obtained on the test set for healthy, disease, and overall
examples for each experiment.
We observe that SegOnly,
SegRecon, and SegReconHealthy have significantly lower
values compared to PET-Disentangler in each set of exam-
ples.
Fig.
2 presents segmentation prediction examples for
SegOnly and PET-Disentangler, where we observe SegOnly
consistently misclassifies the bladder as lesions given its
high, yet normal, activity. Additionally, the second and third
rows indicate that high intensity activity, that corresponds
to healthy kidney uptake, is also incorrectly segmented as
lesions. The red markers in Fig. 2 highlight the incorrectly
segmented normal activity by SegOnly.
In contrast, PET-
Disentangler has learned the healthy anatomy component
of the images and can identify these high uptake regions
as healthy activity and focus on the remaining activity to
accurately delineate the lesions.
Fig. 2. Comparison of PET-Disentangler to SegOnly, with red
markers highlighting false positives for healthy uptake pat-
terns produced by SegOnly, which are not present in PET-
Disentangler.
Fig.
2 visualizes PET-Disentangler-generated pseudo-
healthy image and the corresponding image reconstruction.
A comparison between these images shows an absence of
lesion features, characterized by high intensity and abnormal
uptake patterns, in the pseudo-healthy images. The pseudo-
healthy images fill in these lesion regions with the expected
“healthy” appearance.
These lesion features re-appear in
the full reconstructions of the images, which are obtained
by re-entangling the healthy and disease components via the
SPADE blocks in the image decoder.
5. CONCLUSIONS
In this work, we proposed the use of image disentanglement
for lesion segmentation by decomposing a PET image into
healthy and disease features. Our method, PET-Disentangler,
is a modified UNet architecture that learns to disentangle
PET images into healthy and disease features in the latent
space, producing segmentation predictions using the disease
features, and re-entangles healthy and disease features to pro-
duce reconstructed images. A critic network is used to ensure
the healthy features match the same distribution to prevent
leakage of disease features. To the best of our knowledge, our
work is the first to apply healthy and disease disentanglement
to PET images and to extend this concept to 3D images.
Our evaluations on a whole-body FDG PET/CT dataset
show PET-Disentangler greatly reduces the false positive
segmentation of healthy uptake patterns, including but not
limited to the bladder, compared to non-disentanglement
methods by learning the healthy anatomy component. Future
work can investigate the use of an additional modality (i.e.,
CT, MRI) in combination with PET.
6. COMPLIANCE WITH ETHICAL STANDARDS
This research study was conducted retrospectively using hu-
man subject data made available in open access by TCIA [10].
Ethical approval was not required as confirmed by the license
attached with the open access data.
7. ACKNOWLEDGMENTS
This work was funded by the National Institutes of Health
(NIH) / Canadian Institutes of Health Research (CIHR) Quan-
titative Imaging Network (QIN) (OQI-137993), and NSERC
Discovery (RGPIN-06795). The authors are also grateful to
the computational resources provided by NVIDIA Corpora-
tion, Digital Research Alliance of Canada, and SFU’s Solar.
8. REFERENCES
[1] R. R. Beichel et al., “Multi-site quality and variability
analysis of 3D FDG PET segmentations based on phan-
tom and clinical image data,” Med Phys, 2017.
[2] B. Foster et al., “A review on segmentation of positron
emission tomography images,” Comput Biol Med, 2014.
[3] F. Yousefirizi et al.,
“Toward High-Throughput Arti-
ficial Intelligence-Based Segmentation in Oncological
PET imaging,” PET Clin, 2021.
[4] X. Liu et al., “Learning disentangled representations in
the imaging domain,” Med Image Anal, 2022.
[5] P. Ahmadvand et al., “Tumor Lesion Segmentation from
3D PET Using a Machine Learning Driven Active Sur-
face,” in MICCAI MLMI, 2016.
[6] S. Afshari et al., “Automatic localization of normal ac-
tive organs in 3D PET scans,” Comput Med Imaging
Graph, 2018.
[7] S. A. Taghanaki et al., “Combo loss: Handling input and
output imbalance in multi-organ segmentation,” Comput
Med Imaging Graph, 2019.
[8] T. Park et al., “Semantic Image Synthesis with Spatially-
Adaptive Normalization,” in CVPR, 2019.
[9] K. Kobayashi et al., “Decomposing normal and abnor-
mal features of medical images for content-based image
retrieval of glioma imaging,” Med Image Anal, 2021.
[10] S. Gatidis et al., “A whole-body FDG-PET/CT Dataset
with manually annotated Tumor Lesions,”
Sci Data,
2022.
[11] J. Wasserthal et al.,
“TotalSegmentator: Robust Seg-
mentation of 104 Anatomic Structures in CT Images,”
Radiol Artif Intell, 2023.
