A convex formulation of covariate-adjusted Gaussian
graphical models via natural parametrization
Ruobin Liu and Guo Yu
Department of Statistics and Applied Probability, University of California, Santa Barbara, Santa Barbara, CA, USA
October 10, 2024
Abstract
Gaussian graphical models (GGMs) are widely used for recovering the conditional inde-
pendence structure among random variables. Recently, several key advances have been made
to exploit an additional set of variables for better estimating the GGMs of the variables of
interest. For example, in co-expression quantitative trait locus (eQTL) studies, both the mean
expression level of genes as well as their pairwise conditional independence structure may
be adjusted by genetic variants local to those genes. Existing methods to estimate covariate-
adjusted GGMs either allow only the mean to depend on covariates or suffer from poor scaling
assumptions due to the inherent non-convexity of simultaneously estimating the mean and
precision matrix. In this paper, we propose a convex formulation that jointly estimates the
covariate-adjusted mean and precision matrix by utilizing the natural parametrization of the
multivariate Gaussian likelihood. This convexity yields theoretically better performance as
the sparsity and dimension of the covariates grow large relative to the number of samples. We
verify our theoretical results with numerical simulations and perform a reanalysis of an eQTL
study of glioblastoma multiforme (GBM), an aggressive form of brain cancer.
1
Introduction
Graphical models are used to represent the distribution of a random vector X = (X1, . . . , Xp) by
relating its conditional independence structure to a graph. This correspondence is particularly
salient when X is Gaussian. Letting ‚Ñ¶= Œ£‚àí1 be the precision matrix of a Gaussian random vector
X, for i Ã∏= j, the components Xi and Xj are conditionally independent given X{1,...,p}\{i,j} if and
only if ‚Ñ¶ij = 0 [Lauritzen, 1996]. Models that estimate the conditional independence structure by
imposing sparsity on ‚Ñ¶are known as Gaussian graphical models (GGMs).
There has been considerable research on GGMs and their estimation in the high-dimensional
setting [Yuan and Lin, 2007, Friedman et al., 2007, Meinshausen and B√ºhlmann, 2006]. GGMs find
major applications in genomics where the network structure of genes is of interest. Such a setting is
typically high-dimensional such that the number of genes exceeds the number of samples [Sch√§fer
and Strimmer, 2005]. Of particular interest in genomics are co-expression quantitative trait locus
(eQTL) studies. In these studies, gene expression data are analyzed alongside information about
1
arXiv:2410.06326v1  [stat.ME]  8 Oct 2024
external genetic markers that are known to confound the expression levels of genes. In order
to use GGMs for eQTL studies, one must allow the GGM framework to incorporate covariate
information.
Let X ‚ààRp be the vector of responses and U ‚ààRq the associated vector of covariates. We
consider a general model
X | U = u ‚àºN(¬µ(u), ‚Ñ¶‚àí1(u))
(1)
and a specification given by
¬µ(u) = Œìu,
‚Ñ¶(u) = B0.
(2)
Note that in this specification, the error covariance matrix does not depend on covariates. Multi-
variate regression models specify (2) with the goal to improve estimation of Œì by also estimating
B0; see for example Yuan et al. [2007] and references therein for this well-studied setting. By
contrast, the focus of the covariate-adjusted GGM framework is to estimate the sparsity pattern of
the precision matrix while accounting for covariate information through B0. Several methods exist
in this framework that employ sparse penalization to the negative log-likelihood of (1) [Rothman
et al., 2010, Yin and Li, 2011, Cai et al., 2013, Chen et al., 2016, Wang, 2015, Chen et al., 2018]. These
models also are termed ‚Äúconditional Gaussian graphical models‚Äù in Yin and Li [2011].
Estimating (1) with (2) is challenging because the negative log-likelihood of (1) is not jointly
convex in Œì and B0. Therefore, covariate-adjusted GGM methods use either alternating [Rothman
et al., 2010, Yin and Li, 2011, Chen et al., 2018] or two-stage [Cai et al., 2013, Chen et al., 2016]
estimation algorithms. Despite these challenges, minimax-optimal error rates in jointly estimating
(2) have been established [Chen et al., 2018, Lv et al., 2022].
Although there has been progress in allowing heterogeneous means in GGMs, there is little
work on allowing the graph structure of ‚Ñ¶to also depend on covariates; indeed the aforementioned
methods all assume that B0 is fixed across subjects. Yet subject-specific graph structures are
plausible in applications such as eQTL analyses. For example, gene C may mediate the co-expression
of genes A and B, but only in the presence of a single-nucleotide polymorphism (SNP) local to
gene C. In other words, the expression of genes A and B may be conditionally independent given
the rest of the network unless a genetic variant is present near gene C [Fehrmann et al., 2011,
Kolberg et al., 2020, Rockman and Kruglyak, 2006]. To capture this additional structure, Zhang
and Li [2022] extends (2) to allow both the mean and the network structure of the responses to be
adjusted by covariates. Their model specifies (1) by taking
¬µ(u) = Œìu,
‚Ñ¶(u) = B0 +
q
X
h=1
Bhuh
(3)
where Bh, h = 0, 1 . . . , q are sparse, symmetric matrices. Like previous works in covariate-adjusted
GGMs, the joint estimation of the parameters (Œì, B0, B1, . . . , Bh) in (3) involves a non-jointly-
convex objective. Therefore Zhang and Li [2022] use a two-stage estimation procedure like in
preceding methods.
In this work, we contribute a jointly convex formulation of (1) such that both the mean and the
graph structure may depend on covariates. Like Zhang and Li [2022], our method uses nodewise
regression to estimate the covariate-adjusted graph structure [Meinshausen and B√ºhlmann, 2006].
However, we base our formulation upon a natural parametrization of the multivariate Gaussian
likelihood such that each nodewise regression is a convex optimization problem.
2
1.1
Previous work
Our method is inspired by the penalized likelihood-based method of Zhang and Li [2022]. Bayesian
approaches to subject-level precision matrix adjustment have been considered before. In Niu et al.
[2024], a Bayesian method is developed where a discrete set of precision matrices ‚Ñ¶1, . . . , ‚Ñ¶K
is determined for a partition of n samples into K sets, with the partition depending on external
covariates. However, this approach does not immediately allow for a covariate-adjusted mean,
nor is it easy to interpret how each covariate affects the overall graph structure through the ‚Ñ¶i.
Wang et al. [2022] develops a Bayesian approach to solve similar nodewise regression problems as
Zhang and Li [2022], utilizing sparsity-inducing priors to estimate the sparse component matrices.
Their method also does not allow a covariate-adjusted mean, nor does it identify group structures.
Convex formulations of covariate-adjusted GGMs have been proposed before. Wang [2015]
remarks on the non-convex nature of existing covariate-adjusted GGM methods and suggests a
convex formulation of (1) with (2). However, their method achieves convexity by relying on an
initial estimate of Œì, thereby placing it in the category of two-stage methods. A related line of
work is the conditional Gaussian random field, which is also sometimes called the conditional
Gaussian graphical model [Sohn and Kim, 2012, Yuan and Zhang, 2014]. This class of methods
investigates estimating a sparse B0 in (1) and (2) by penalizing a convex negative log-likelihood.
This parametrization is made possible by the additional assumption that (X, U) is jointly Gaussian.
In Zhu [2020], the matrix Œì in (2) is written as Œì = ÀúŒì‚Ñ¶leading to a negative log-likelihood of
X that is jointly convex in ÀúŒì and ‚Ñ¶. However, their method does not immediately allow for
covariate-adjusted network structures.
Our proposed method differs from these works as it allows both the mean and the precision
matrix to depend on external covariates while also offering the advantage of solving a convex
optimization problem.
1.2
Outline
We recall the Gaussian graphical regression framework of Zhang and Li [2022] and motivate
the natural parametrization in Section 2. In Section 3 we provide the model specification and
algorithm to induce a sparse-group structure in the graph. Our theoretical results are discussed in
Section 4, namely that the natural parametrization allows for better theoretical scaling of p and q
relative to n under the same assumptions as in Zhang and Li [2022]. This is demonstrated through
extensive simulations in Section 5. Finally we apply our method to perform a reanalysis of data
from glioblastoma microforme (GBM) tissue samples in Section 6.
2
Parametrizations for Covariate-adjusted Graphical Models
Let X ‚ààRp be a random vector of responses and U ‚ààRq the corresponding covariates. In
Zhang and Li [2022], the dependence of X on U is given by (1) and (3) so that both the mean
and precision matrix are covariate-dependent. Write Œì = (Œ≥1, . . . , Œ≥p)‚ä§and let ‚Ñ¶jk(u) denote
the (j, k)-th element of ‚Ñ¶(u), keeping in mind its dependence on u. Then (3) may be estimated
via a neighborhood regression method [Meinshausen and B√ºhlmann, 2006], which amounts to
3
estimating p separate linear regression models of the form
Xj = u‚ä§Œ≥j +
p
X
kÃ∏=j
Œ≤jk0(Xk ‚àíu‚ä§Œ≥k) +
p
X
kÃ∏=j
q
X
h=1
Œ≤jkhuh(Xk ‚àíu‚ä§Œ≥k) + Œµj
(4)
where Œ≤jkh = ‚àí[Bh]jk/‚Ñ¶jj and Œµj ‚àºN(0, 1/‚Ñ¶jj) for all j, k, and h. This is termed Gaussian
graphical regression in Zhang and Li [2022].
Note that a least squares criterion based on (4) is not jointly convex owing to the cross term
Œ≤jkh√óu‚ä§Œ≥k. Hence, Zhang and Li [2022] uses a two-stage estimation method. First, Œ≥j is estimated
for all j via the model
Xj = u‚ä§Œ≥j + Œæj,
E Œæj = 0,
(5)
and ‚Ñì1 penalization is added so that ÀÜŒ≥j is sparse. Second, the observed response vectors xj are
centered using ÀÜŒ≥j and the coefficients Œ≤jkh in (4) are estimated with ÀÜŒ≥j in place of Œ≥j for all j ‚àà[p].
As with other two-stage methods, the above procedure incurs an estimation error in the first stage
because (5) ignores the dependence on the Œ≤jkh terms in (4). Assumptions on the scaling of the
ambient dimensions p and q and the sparsity of the coefficient vectors are needed to suppress the
model misspecification errors. Next, we will present a formulation of (1) so that the corresponding
least squares criterion is convex, obviating the need for a two-stage procedure.
2.1
Convex formulation
Recall the form of the p-dimensional multivariate Gaussian likelihood expressed in terms of the
natural parameters (ÀúŒ∏, ÀúŒò) where ÀúŒ∏ = ‚Ñ¶¬µ and ÀúŒò = ‚àí‚Ñ¶:
L(ÀúŒ∏, ÀúŒò | x) = exp

ÀúŒ∏
‚ä§x ‚àí1
2x‚ä§ÀúŒòx ‚àíA(ÀúŒ∏, ÀúŒò)

.
Our formulation is motivated by the fact that the cumulant function A is jointly convex in (ÀúŒ∏, ÀúŒò)
and therefore so is ‚àílog L(ÀúŒ∏, ÀúŒò | x). Define
Œ∏ = diag(‚Ñ¶)‚àí1‚Ñ¶¬µ
and
Œò = ‚àídiag(‚Ñ¶)‚àí1‚Ñ¶,
(6)
where diag(‚Ñ¶) is the p √ó p diagonal matrix of ‚Ñ¶. Our model is to incorporate covariates as in (3)
to (Œ∏, Œò) instead of (¬µ, ‚Ñ¶), namely
Œ∏(u) = Œìu,
Œò(u) = B0 +
q
X
h=1
Bhuh.
(7)
To see that this leads to jointly convex nodewise regression problems, fix j ‚àà[p] and consider
the partial regression of component Xj against the covariates U and the remaining components
X‚àíj in the general setting (1). For a matrix M and sets of indices I and J , denote by MI,J the
sub-matrix of M consisting of rows indexed by I and columns indexed by J . Letting ‚àíj indicate
all indices excluding j, we have the conditional distribution
Xj ‚àí¬µj | X‚àíj, U ‚àºN
 Œ£j,‚àíjŒ£‚àí1
‚àíj,‚àíj(X‚àíj ‚àí¬µ‚àíj), Œ£jj ‚àíŒ£j,‚àíjŒ£‚àí1
‚àíj,‚àíjŒ£‚àíj,j

,
(8)
4
suppressing the dependence of Œ£ and ¬µ on the covariates u for notational convenience. Defining
the error terms Œµj ‚àºN(0, œÉ2
Œµj) where œÉ2
Œµj = 1/‚Ñ¶jj, by the matrix block inversion formula we may
write (8) as the linear model
Xj = ¬µj ‚àí‚Ñ¶‚àí1
jj ‚Ñ¶j,‚àíj(X‚àíj ‚àí¬µ‚àíj) + Œµj = Œ∏j + Œòj,‚àíjX‚àíj + Œµj,
which is readily seen to be jointly convex in the parameters Œ∏j and Œòj,‚àíj as defined in (6). In light
of the specification of covariate dependence (7) and with Œì = (Œ≥1, . . . , Œ≥p)‚ä§and Œ≤jkh = [Bh]jk,
the above leads to the nodewise regression model
Xj = u‚ä§Œ≥j +
p
X
kÃ∏=j
Œ≤jk0Xk +
p
X
kÃ∏=j
q
X
h=1
Œ≤jkhuhXk + Œµj,
Œµj ‚àºN(0, œÉ2
Œµj)
(9)
which allows for simultaneous estimation of the parameters Œ≥j and Œ≤jkh in a jointly convex setting.
Implicit in (6) is that Œòjj = ‚àí1 for all j ‚àà[p]. This is satisfied in (7) by letting [B0]jj = ‚àí1 and
[Bh]jj = 0 for all h ‚àà[q] and j ‚àà[p]. By doing so, we assume as in Zhang and Li [2022] that the
residual variance œÉ2
Œµj is not covariate-dependent, allowing us to write ‚Ñ¶jj = ‚Ñ¶jj(u) for j ‚àà[p].
3
Estimation
Suppose we collect n independent observations {(x(i), u(i))}n
i=1 of responses and covariates that
follow the joint distribution in (1) with (6) and (7). Let U =

u(1), . . . , u(n)‚ä§‚ààRn√óq be the matrix
of covariates and X =

x(1), . . . , x(n)‚ä§the matrix of responses with xj ‚ààRn denoting the j-th
column of X and uj ‚ààRn the j-th column of U. For j ‚àà[p], let W‚àíj be the n √ó (p ‚àí1)(q + 1)
matrix of interactions of the remaining responses with the covariates. Concretely,
W‚àíj,0 = [x1, . . . , xj‚àí1, xj+1, . . . , xp] ‚ààRn√ó(p‚àí1),
W‚àíj,h = [x1 ‚äôuh, . . . , xj‚àí1 ‚äôuh, xj+1 ‚äôuh, . . . , xp ‚äôuh] for h ‚àà[q],
W‚àíj = [W‚àíj,0, W‚àíj,1, . . . , W‚àíj,q] ‚ààRn√ó(p‚àí1)(q+1),
(10)
where ‚äôdenotes the elementwise product of two vectors. By writing
Œ≤j,h = (Œ≤j1h, . . . , Œ≤j,j‚àí1,h, Œ≤j,j+1,h, . . . , Œ≤jph)‚ä§‚ààRp‚àí1 and Œ≤j = (Œ≤j,0, Œ≤j,1, . . . , Œ≤j,q)‚ä§‚ààR(p‚àí1)(q+1),
we can view Œ≤j as q + 1 blocks of (p ‚àí1)-element vectors Œ≤j,h for h = {0, 1, . . . q} so that (9)
may be written in the block form
Xj = u‚ä§Œ≥j + X‚ä§
‚àíjŒ≤j,0 +
q
X
h=1
(uhX‚àíj)‚ä§Œ≤j,h + Œµj,
Œµj ‚àºN(0, œÉ2
Œµj).
(11)
Figure 1 relates the blocks Œ≤j,h to Œò in (7).
The joint convexity of (11) in (Œ≥j, Œ≤j) allows for simultaneous estimation of the penalized
problem
minimize
Œ≥j, Œ≤j
1
2n
xj ‚àíUŒ≥j ‚àíW‚àíjŒ≤j
2
2 + gj(Œ≥j, Œ≤j)
(12)
where gj is an arbitrary convex penalty function. Moreover, by the relationship between Œ≤j and
rows of Œò, sparsity-inducing penalties gj in (12) will give us sparse estimates of Œò. From (6),
we see that Œò has the same sparsity pattern as ‚Ñ¶. Hence we keep the conditional independence
interpretation of the sparsity in Œò as in ‚Ñ¶.
5
Figure 1: Decomposition of Œò into components Bh according to (7). The block Œ≤j,h corresponds
to the effects of covariate uh on the partial correlations of response Xj while Œ≤j,0 describes the
population effect.
3.1
Estimating covariate-adjusted graphs
In the nodewise regression approach to GGMs, a symmetrization step is needed to ensure that
the estimate of ‚Ñ¶is symmetric because each regression is fit independently [Meinshausen and
B√ºhlmann, 2006]. In our approach, the nodewise regressions target Œò, which is not necessarily
symmetric due to the different scaling factor for each row in (6). To ensure symmetry in the estimate
of ‚Ñ¶, we perform a similar symmetrization step after performing all nodewise regressions. First,
specify an estimate ÀÜœÉ2
Œµj of the error variance in (11). Then for an estimate ÀÜŒ≤j, set ÀúŒ≤j = ‚àíÀÜŒ≤j/ÀÜœÉ2
Œµj
and define for all h = 0, 1, . . . , q the symmetric matrix
[ ÀúBh]jk = [ ÀúBh]kj = ÀúŒ≤jkh
h
|ÀúŒ≤jkh| < |ÀúŒ≤kjh|
i
+ ÀúŒ≤kjh
h
|ÀúŒ≤jkh| > |ÀúŒ≤kjh|
i
,
(13)
where the expression [P] is equal to 1 if P is true and 0 otherwise. The mean vector and precision
matrix may then be estimated by (6) via
ÀÜ‚Ñ¶(u(i)) = ÀúB0 +
q
X
h=1
ÀúBhu(i)
h
and
ÀÜ¬µ(u(i)) = ( ÀÜ‚Ñ¶(u(i)))‚àí1 diag( ÀÜ‚Ñ¶)ÀÜŒìu(i).
(14)
The preceding steps are summarized in Algorithm 1 in Section S1.
Equation (13) is the ‚Äúand-rule‚Äù to symmetrize the matrices ÀúBh for h = 0, 1, . . . , q; [ ÀúBh]jk is
nonzero if both ÀúŒ≤jkh and ÀúŒ≤kjh are nonzero. A less conservative estimate would be given by the
‚Äúor-rule‚Äù, namely
[ ÀúBh]jk = [ ÀúBh]kj = ÀúŒ≤jkh
h
|ÀúŒ≤jkh| ‚â•|ÀúŒ≤kjh|
i
+ ÀúŒ≤kjh
h
|ÀúŒ≤jkh| ‚â§|ÀúŒ≤kjh|
i
,
so that [ ÀúBh]jk is nonzero if either ÀúŒ≤jkh or ÀúŒ≤kjh is nonzero. We elect to use the more conservative
rule and note that both approaches are asymptotically equivalent [Meinshausen and B√ºhlmann,
2006] and that the and-rule has been considered before in covariate-adjusted graphical models
[Cai et al., 2013, Zhang and Li, 2022].
3.2
Sparsity structure
With the application to eQTL studies in mind, we will focus on a particular sparsity-inducing
penalty gj in (15). We wish to identify elementwise sparsity within a group h, amounting to sparsity
in the coefficient vector Œ≤j,h. We may interpret sparse entries in Œ≤j,h to mean that the covariate
uh affects the conditional independence between Xj and some, but not all other responses Xk.
6
Meanwhile, we wish to identify groupwise sparsity, where Œ≤j,h = 0 for a group h. This means
that uh has no effect on the conditional independence between Xj and the other responses.
We consider the following convex problem:
minimize
Œ≥j, Œ≤j
1
2n
xj ‚àíUŒ≥j ‚àíW‚àíjŒ≤j
2
2 + Œª‚à•Œ≥j‚à•1 + Œª‚à•Œ≤j‚à•1 + Œªg‚à•Œ≤j,‚àí0‚à•1,2
(15)
where Œª ‚â•0 and Œªg ‚â•0 are tuning parameters and ‚à•Œ≤j,‚àí0‚à•1,2 = Pq
h=1‚à•Œ≤j,h‚à•2 is the group lasso
penalty. Together, the penalty on Œ≤j is the sparse-group lasso penalty developed in Simon et al.
[2013]. The group lasso penalty is not applied Œ≤j,0 nor to Œ≥j.
We considered the following two estimates of œÉ2
Œµj,
ÀÜœÉ2
1 = ‚à•xj ‚àíUÀÜŒ≥j ‚àíW‚àíj ÀÜŒ≤j‚à•2
2
n ‚àíÀÜsŒ≤j ‚àíÀÜsŒ≥j
and
ÀÜœÉ2
2 = ‚à•xj ‚àíUÀÜŒ≥j ‚àíW‚àíj ÀÜŒ≤j‚à•2
2
n ‚àíÀÜsŒ≤j ‚àí1
,
where ÀÜsŒ≥j and ÀÜsŒ≤j are the number of nonzero elements of ÀÜŒ≥j and ÀÜŒ≤j. Following the lasso literature
[Reid et al., 2016, Yu and Bien, 2019], a straightforward estimate is given by ÀÜœÉ2
1 as the value ÀÜsŒ≤j +ÀÜsŒ≥j
roughly approximates the degrees of freedom of (15). In our experimentation, we have found ÀÜœÉ2
2
to be slightly more consistent.
3.3
Implementation
We use block coordinate descent to solve (15) where the blocks are given by {Œ≥j, Œ≤j,0, . . . , Œ≤j,q}.
Since (15) is convex and separable in these blocks, this approach is guaranteed to converge to the
optimal solution [Tseng, 2001]. Our R implementation uses the sparsegl package to solve (15)
[Liang et al., 2024]. We use k-fold cross-validation to select the tuning parameter pair (Œª, Œªg). To
do so, we set a parameter Œª0 > 0 and mixture parameter Œ±s ‚àà[0, 1] so that the penalty in (15)
may be written as
Œ±sŒª0(‚à•Œ≥j‚à•1 + ‚à•Œ≤j‚à•1) + (1 ‚àíŒ±s)Œª0‚à•Œ≤j,‚àí0‚à•1,2.
(16)
Then for each Œ±s, we run the method on a path of Œª0.
4
Theoretical Properties
In this section we will analyze the estimation error and support recovery of (15). For two sequences
of real numbers an and bn, we write an ‚âæbn if an = O(bn), i.e. there exists constants C > 0 and
N > 0 so that an < Cbn for all n ‚â•N. If an ‚âæbn and bn ‚âæan, we write an ‚âçbn. We write
an = o(bn) if limn‚Üí‚àûan/bn = 0. With the understanding that j is fixed, we will suppress the
subscript j when referring to Œ≥j, Œ≤j and Œµj for notational convenience.
Let (ÀÜŒ≥, ÀÜŒ≤) be the solution to (15) and let (Œ≥, Œ≤) be the true parameters. Denote by SŒ≥ and
SŒ≤ the support sets of Œ≥ and Œ≤ respectively. Further let SŒ≤,g index the active groups of Œ≤, that
is SŒ≤,g =

h : Œ≤j,h Ã∏= 0, h ‚àà[q]
	
. Denote by sŒ≥, sŒ≤ and sŒ≤,g the cardinalities of these sets. For
a square matrix M, denote by Œªmin(M) and Œªmax(M) its minimum and maximum eigenvalue,
respectively.
We require the following assumptions on the model (1) with specification (7).
7
Assumption 1. The covariates {u(i)}n
i=1 are i.i.d. mean zero random vectors with covariance matrix
satisfying
œï0 ‚â§Œªmin(Cov(u(i))) ‚â§Œªmax(Cov(u(i))) ‚â§œï1
for some constants 0 < œï0 ‚â§œï1 < ‚àû. Furthermore, there exists a constant M > 0 such that
|u(i)
j | ‚â§M for all j ‚àà[q] and i ‚àà[n].
Assumption 1 is the same as Assumptions 1 and 5 in Zhang and Li [2022] without requiring a
bound on ‚à•Œ≤‚à•1. The boundedness of the covariates is not restrictive in the eQTL setting since
SNPs are often binary coded.
Denote by [U, W‚àíj] the n √ó (p(q + 1) ‚àí1) matrix that is the concatenation of the covariate
matrix U with the interaction matrix W‚àíj. Define the following Gram matrix of covariates,
responses, and interactions:
Œ£UW = E
[U, W‚àíj]‚ä§[U, W‚àíj]
n

.
Our next assumption bounds the eigenvalues of Œ£UW, which is needed to characterize the joint
distribution of u(i), x(i), and their interactions.
Assumption 2. We assume there exist positive constants m0, M0 such that
m0 ‚â§Œªmin(Œ£UW) ‚â§Œªmax(Œ£UW) ‚â§M0.
Elementwise boundedness in Assumption 1 implies that u(i) is elementwise sub-Gaussian. Our
theoretical analysis further requires that our design matrix [U, W‚àíj] is elementwise sub-Gaussian.
Assumption 3 ensures this since each entry of W‚àíj is the product of a sub-Gaussian and a bounded
random variable.
Assumption 3. The marginal distribution of X is elementwise sub-Gaussian with bounded sub-
Gaussian norm.
The following assumption controls how the sparsity of the true parameters may scale with
the sample size n.
Assumption 4. Let p and q be the number of responses and covariates, respectively. The true sparsities
sŒ≥ and sŒ≤ satisfy
1. (sŒ≥ + sŒ≤)
p
log(pq) = o(‚àön),
2. (sŒ≥ + sŒ≤) log(pq) = o(n/ log n).
In practice we consider estimates (ÀÜŒ≥, ÀÜŒ≤) of varying sparsities and select from this pool of
candidate models via cross-validation. Define ÀÜsmax
Œ≥
and ÀÜsmax
Œ≤
to be the maximum sparsity of Œ≥
and Œ≤ out of all candidate models, chosen so that sŒ≥ < smax
Œ≥
and sŒ≤ < smax
Œ≤
. Our first theorem
describes the ‚Ñì2 estimation error of the nodewise solution.
8
Theorem 1. Under Assumptions 1-4, with (ÀÜsmax
Œ≥
+ ÀÜsmax
Œ≤
) log(pq) = O(‚àön), and with
Œª = C œÉŒµ
‚àön
sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep)
sŒ≥ + sŒ≤
+
sŒ≤,g
sŒ≥ + sŒ≤
log(eq/sŒ≤,g)
1/2
and
Œªg = Œª
s
sŒ≥ + sŒ≤
sŒ≤,g
(17)
we have
‚à•ÀÜŒ≥ ‚àíŒ≥‚à•2
2 + ‚à•ÀÜŒ≤ ‚àíŒ≤‚à•2
2 ‚âæœÉ2
Œµ
n (sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g)).
with probability at least 1 ‚àíC1 exp{‚àíC2(sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g))} where
C, C1, and C2 are positive constants.
The bound on the sparsity of candidate models helps to bound the errors in Theorem 1 and is
also assumed in Zhang and Li [2022].
Our second result proves that our method achieves support recovery with high probability
under some additional conditions. Denote the (k, ‚Ñì)-th entry of Œ£UW by Œ£UW(k, ‚Ñì).
Assumption 5. Define œÑj = 1+
p
(sŒ≥ + sŒ≤)/sŒ≤,g. We assume that there exists a constant c0 > 2/m0
such that
max
kÃ∏=‚Ñì|Œ£UW(k, ‚Ñì)| ‚â§
1
c0(1 + 8œÑj)(sŒ≤ + sŒ≥).
Theorem 2. Suppose Assumptions 1-5 hold. Additionally, suppose log p ‚âçlog q and that
n ‚â•A1(sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g))
for some constant A1 > 0. Then with Œª and Œªg defined as in Theorem 1, we have
max
n
‚à•ÀÜŒ≥ ‚àíŒ≥‚à•‚àû, ‚à•ÀÜŒ≤ ‚àíŒ≤‚à•‚àû
o
‚â§3
m0
(Œª + Œªg)

1 +
6(1 + 4œÑj)2
(1 + 8œÑj)(c0m0 ‚àí2)

with probability at least 1 ‚àíC3 exp(‚àíC4 log p) for some positive constants C3, C4.
Theorem 2 implies that our method achieves support recovery with high probability if Œ≥ and
Œ≤ satisfy a minimal signal strength condition.
Corollary 1. Define
Œ∫0 = 3
m0
(Œª + Œªg)

1 +
6(1 + 4œÑj)2
(1 + 8œÑj)(c0m0 ‚àí2)

,
ÀúSŒ≥ = {k : |ÀÜŒ≥k| > Œ∫0},
and
ÀúSŒ≤ =
n
k : |ÀÜŒ≤k| > Œ∫0
o
.
If it holds that
min

min
‚Ñì‚ààSŒ≥|Œ≥‚Ñì|, min
k‚ààSŒ≤|Œ≤k|

‚â•2Œ∫0,
we have that P(SŒ≥ = ÀúSŒ≥ and SŒ≤ = ÀúSŒ≤) ‚â•1 ‚àíC3 exp(‚àíC4 log p).
9
Zhang and Li [2022] provides two theorems related to the estimation error of ÀÜŒ≤; an oracle rate
where the true Œì in (3) is known and a rate where ÀÜŒì is estimated via independent regressions in
stage 1 of the procedure. It is shown that the oracle rate, given by
‚à•ÀÜŒ≤ ‚àíŒ≤‚à•2
2 ‚âæ
œÉ2
Œµj
n (sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g)),
can be achieved by the two-stage rate under the assumptions log(pq) = O(n1/6), sŒ≤ = o(n1/6),
and sŒ≥ = o(n1/3). By comparison, Theorem 1 shows that our method achieves the oracle rate
under milder assumptions; more explicitly, Assumption 4 is satisfied when log(pq) = O(n1/3) and
sŒ≥ + sŒ≤ = o(n1/3). The milder scaling assumptions in log(pq) and sŒ≤ in our formulation stems
from the fact that (15) is jointly convex in (Œ≥, Œ≤), allowing for their simultaneous estimation. By
comparison, a more stringent scaling assumption is needed to control the estimation error of Œ≥j
incurred in stage 1 of Zhang and Li [2022]. These comments apply as well to the support recovery
results in Theorem 2.
5
Simulation Study
In this section, we compare our method Covariate-adjusted Sparse Precision with Natural Estimation
(cspine) with the Gaussian graphical model regression method developed in Zhang and Li [2022]
(RegGMM) under extensive simulations. We are interested in the behavior of the two methods under
data generated according to both models.
For each simulation setting, we generate a sparse Œì ‚ààRp√óq by independently determining
if Œìjk is non-zero with probability 0.3. The non-zero entries are sampled from N(0, 1). Then
Œì is scaled by a constant to ensure that the signal-to-noise ratio is approximately 1 for each
observation.
We generate the population graph B0 using a preferential attachment algorithm [Barab√°si
and Albert, 1999] with power 1. We select qe = 5 out of q covariates to have nonzero effects
and generate the covariate-adjusted components Bh for h ‚àà[qe] as Erd≈ës-Renyi graphs [Erd≈ës
and R√©nyi, 1959] with edge probability ve = 0.01. This choice of graph structure ensures that
the covariate-dependent graphs are more sparse than the population graph; see Clauset et al.
[2009] in particular for a discussion of power law graph models in applications. After determining
the graph structure, we generate entries in {Bh} by independently and uniformly sampling
from [‚àí0.5, ‚àí0.35] ‚à™[0.35, 0.5] and then dividing each row j by P
h
P
kÃ∏=j|Œ≤jkh| √ó 1.5 before
symmetrizing each {Bh} by taking the average of Œ≤jkh and Œ≤kjh. These steps help ensure a
diagonally dominant precision matrix, and mirror the data model of Zhang and Li [2022].
We select half of the q covariates to be binary and half to be continuous from Unif(0, 1) and
then standardize the continuous covariates across the n observations to have zero mean and unit
variance. With Œì and {Bh} in hand, we generate a precision matrix ‚Ñ¶(i) = B0 + Pq
h=1 Bhu(i)
h .
Then we generate a dataset under the original model (3) by setting ¬µ(i) = Œìu(i) and and a second
dataset under (7), termed the ‚Äúnatural model‚Äù, via ¬µ(i) = Œ£(i)Œìu(i). Finally, we sample a vector of
responses x(i) ‚àºNp(¬µ(i), Œ£(i)) for i ‚àà[n]. In both models, we set diag(‚Ñ¶(i)) = 1 so that œÉ2
Œµj = 1.
For each of the settings p = 25, q = 50 and p = 25, q = 100, we generate 100 independent
data sets of n = 200 and n = 400 samples and run both RegGMM and cspine on each data set. For
a fair comparison, both methods have two tuning parameters selected via 5-fold cross-validation
10
on a path of 100 Œª parameters and 10 mixture parameters. With the notation in (16), this means
cross-validating over 100 values of Œª0 and Œ±s = 0.1, 0.2, . . . , 1.
We report the mean and standard error of the following metrics after applying (13): TPR, the
true positive rate of detected edges across all {Bh}; TPRpop, the true positive rate of detected
edges in the population graph B0; FPRpop, the false positive rate of detected edges in B0; TPRcov,
the true positive rate of detected edges in covariate graphs Bh, h ‚àà[q]; the nodewise estimation
error given by Œ≤err = Pp
j=1‚à•ÀÜŒ≤j ‚àíŒ≤j‚à•2; and the average error of the estimated precision matrix
given by ‚Ñ¶err = Pn
i=1‚à•ÀÜ‚Ñ¶
(i) ‚àí‚Ñ¶(i)‚à•2
F,off/n, where the norm is taken over off-diagonal entries. The
overall false positive rate of detected edges is less than 0.005 in all cases.
In Table 1, we see that cspine outperforms RegGMM under the natural model setting across all
metrics. Table 2 compares the two methods under the setting (3) where RegGMM is correctly specified
and cspine is misspecified. We see that despite the misspecification, cspine is competitive with
RegGMM in edge detection and estimation error due to the convex formulation of the nodewise
regressions. Both methods are conservative in their detection of edges in the covariate networks.
n
(p, q)
Method
TPR
TPRpop
FPRpop
TPRcov
Œ≤err
‚Ñ¶err
200
(25, 50)
cspine
0.73 (0.08)
0.89 (0.07)
0.07 (0.02)
0.45 (0.21)
5.66 (0.50)
1.09 (0.15)
RegGMM
0.62 (0.09)
0.80 (0.10)
0.16 (0.06)
0.30 (0.18)
7.43 (0.61)
1.82 (0.26)
(25, 100)
cspine
0.66 (0.10)
0.82 (0.09)
0.05 (0.02)
0.40 (0.24)
5.98 (0.55)
1.22 (0.17)
RegGMM
0.55 (0.08)
0.71 (0.08)
0.14 (0.05)
0.28 (0.19)
7.73 (0.56)
1.95 (0.28)
400
(25, 50)
cspine
0.77 (0.10)
0.96 (0.04)
0.09 (0.02)
0.52 (0.24)
4.50 (0.59)
0.66 (0.08)
RegGMM
0.67 (0.08)
0.86 (0.07)
0.17 (0.07)
0.41 (0.20)
5.99 (0.55)
1.22 (0.17)
(25, 100)
cspine
0.72 (0.09)
0.94 (0.05)
0.08 (0.02)
0.44 (0.21)
4.96 (0.60)
0.76 (0.09)
RegGMM
0.62 (0.08)
0.85 (0.08)
0.22 (0.07)
0.32 (0.17)
6.80 (0.58)
1.40 (0.20)
Table 1: Mean and standard error of performance metrics over 100 data sets.
n
(p, q)
Method
TPR
TPRpop
FPRpop
TPRcov
Œ≤err
‚Ñ¶err
200
(25, 50)
cspine
0.53 (0.06)
0.77 (0.08)
0.12 (0.02)
0.09 (0.10)
7.70 (0.29)
2.18 (0.24)
RegGMM
0.50 (0.07)
0.70 (0.10)
0.14 (0.05)
0.12 (0.11)
8.02 (0.41)
2.20 (0.25)
(25, 100)
cspine
0.44 (0.07)
0.67 (0.09)
0.09 (0.02)
0.04 (0.06)
7.97 (0.32)
2.43 (0.25)
RegGMM
0.39 (0.07)
0.59 (0.12)
0.19 (0.07)
0.05 (0.06)
8.80 (0.45)
2.69 (0.32)
400
(25, 50)
cspine
0.60 (0.05)
0.92 (0.05)
0.18 (0.03)
0.12 (0.10)
6.71 (0.28)
1.54 (0.18)
RegGMM
0.59 (0.07)
0.80 (0.10)
0.14 (0.05)
0.28 (0.15)
6.64 (0.42)
1.49 (0.19)
(25, 100)
cspine
0.55 (0.06)
0.88 (0.07)
0.15 (0.02)
0.12 (0.10)
6.83 (0.26)
1.69 (0.22)
RegGMM
0.55 (0.07)
0.81 (0.09)
0.22 (0.07)
0.19 (0.14)
7.15 (0.42)
1.69 (0.23)
Table 2: Performance metrics over 100 data sets under specification (3).
Next, we explore the error bound in Theorem 1 through simulations. We fixed p = 25, q = 10,
and n = 300. In the first set of simulations, we fixed a collection {Bh} and generated 300 data
sets from Œì matrices of varying sparsities. In the second set of simulations, we fixed a sparse
Œì and generated data sets from 300 different collections of {Bh} with varying sparsity levels.
In Figure 2, we plot the error Œ≥err + Œ≤err where Œ≥err = Pp
j=1‚à•ÀÜŒ≥j ‚àíŒ≥j‚à•2 against the number of
non-zero entries in Œì and {Bh} The shape of the plot verifies the theoretical results of Section 4,
that the ‚Ñì2 error scales with the square-root of the sparsity levels.
11
(a)
(b)
Figure 2: Estimation error of cspine over 300 data sets plotted against the number of non-zero
entries in (a) Œì and (b) {Bh}. In both cases, the ‚Ñì2 estimation error bound increases at a rate that
is roughly the square-root of the sparsity level.
6
GBM eQTL Reanalysis
Glioblastoma multiforme (GBM) is the most malignant type of brain cancer and patient prognosis is
typically very poor. Although there has been research on the genetic signaling pathways involved
in the proliferation of GBM, it remains largely incurable; see Hanif et al. [2017] for a survey. It
is important to understand the conditional independence structure of genes involved in GBM in
order to discover new drug therapies [Kwiatkowska et al., 2013]. Our estimated graphs describe
the conditional independence of co-expressions in a gene network; hence, we refer to estimated
networks and effects of SNPs on this network.
We reanalyze a GBM eQTL data set that was reported in Zhang and Li [2022]. The data set
contains microarray and SNP profiling data of n = 401 GBM patients from the REMBRANDT trial
(GSE108476) We use the expression levels of p = 73 genes known to belong to the human glioma
pathway according to the Kyoto Encyclopedia of Genes and Genomes (KEGG) database [Kanehisa
and Goto, 2000]; the genes and pathways are detailed in Table S3. We also consider q = 118 SNPs
that are local to these 73 genes. The SNPs are binary-coded, with 0 indicating homozygous major
alleles at that locus and 1 otherwise. Our data set is slightly different from the one used in Zhang
and Li [2022]; we have a larger cohort size (compared to n = 178 in Zhang and Li 2022) and do
not include age and sex as covariates.
We ran cspine on the data set using 5-fold cross-validation over Œ±s = 0.1, 0.2, . . . , 1, and
100 values of Œª0. Our method identified 56 SNPs that potentially modify expressions in the
network. However, many of the identified edges in these networks have small weights. Since
cross-validation tends to select dense models, we focus on interpreting those entries of { ÀúBh} above
0.005 in magnitude. With this threshold, 10 SNPs are estimated to have effects on the network;
see Table S4.
We also ran RegGMM on the data set for comparison, using 5-fold cross-validation for a path of
100 Œª parameters and 10 mixture parameters. RegGMM identified 16 SNPs with nonzero effects on
12
the network. However, similarly to cspine, many of the estimated edges have very small weights.
We choose to threshold these edges as well.
Figure 3 shows the estimated population (covariate-independent) network from cspine and
RegGMM. It can be seen that many estimated edges overlap and that cspine estimates a somewhat
denser network. For example, SHC4 and CALML4 are highly connected nodes in both population
networks.
Figure 3: Population network from GBM eQTL data estimated by cspine (left) and RegGMM (right).
The graph structure is determined by the estimates of B0 in (3) and (7). Solid blue lines and dashed
red lines indicate positive and negative edge weights, respectively.
Four SNPs are identified by RegGMM to affect the network of co-expressions. Out of the four,
rs1267622 is also identified by cspine to have a nonzero effect. It is interesting to look at the
estimated effect of rs1267622, a variant of the BRAF gene, by the two methods, shown in Figure 4.
The results from both methods suggest that this SNP may modify the co-expressions of PDGFRA
and PDGFB as well as PDGFRA and SOS2. This is plausible since all three of these genes lie
in the Ras-Raf-MEK-ERK pathway along with BRAF (Table S3). There is evidence that variants
on this pathway are associated with the proliferation of certain cancers [Gonzalez-Hormazabal
et al., 2019]. Beyond this, the two methods differ; cspine estimates this variant to modify the
co-expressions of GADD45A and GADD45B while RegGMM suggests that the co-expressions of
PDGFRA and CAMK1 are modified.
We now focus on the connections between four genes in particular, namely PIK3CA, CALML5,
E2F1, and E2F3. PIK3CA is one of the most highly mutated oncogenes in a variety of cancers and
resides in the PI3K/Akt/mTOR signaling pathway [Samuels and Velculescu, 2004]. CALML5 is a
calcium-binding protein that is part of the calcium (Ca+2) signaling pathway, which is known to
have diverse roles in explaining GBM biology and is a topic of active research [Azab et al., 2020,
Cheng et al., 2021]. E2F1 and E2F3 are oncogenic transcription factors. The over-expression of
13
Figure 4: Estimated covariate networks for the SNP rs1267622, a variant local to the BRAF gene.
The graph structure corresponds to the sparsity pattern of the matrix Bh corresponding to this
SNP in (3) and (7). Solid blue lines and dashed red lines indicate positive and negative effects of
this SNP on the partial correlation of co-expressions. An effect on the co-expression of GADD45A
and GADD45B is detected by cspine while RegGMM detects an effect between PDGFRA and SOS2.
The remaining two edges, between PDGRFA and PDGFB and PDGRFA and SOS2, are detected by
both methods.
14
E2F3 is known to be vital in the development of various types of cancers including GBM [Zhang
et al., 2019, Wu et al., 2021, Feng et al., 2018].
The estimated connections among these four genes in the population network are shown in
Figure 5. While RegGMM and cspine detect mostly the same edges in the population, only cspine
finds that these co-expressions are modified by the presence of SNPs. These effects are summarized
in Figure 5 (right). For instance, it is estimated that a variant local to the CALML4 gene may
mediate the induction of the PI3K/Akt/mTOR pathway by expressions in E2F1. PIK3CA and E2F1
are part of interconnected signaling pathways implicated in cancer progression; E2F1 is involved
in regulating the expression of PIK3CA and downstream signaling components [Ladu et al., 2008].
Hence, this discovery may give a clue for the role of CALML4 on regulating this pathway.
Figure 5: Estimated population network from cspine (left) and RegGMM (middle) highlighting
four select genes. An edge indicates the partial correlation between the expressions of two genes
conditional on the rest of the network (not shown). Right: the estimated effect of SNPs on co-
expressions among the selected genes according to cspine. The gene to which the SNP is local is
given in parentheses. Solid blue lines and dashed red lines indicate positive and negative edges,
respectively. RegGMM did not detected any effects of SNPs on these co-expressions.
7
Discussion
In this work, we contributed to the covariate-adjusted Gaussian graphical model literature by
developing a framework allowing for jointly convex optimization of the mean and precision
matrix. Our theoretical work implies that the convex formulation allows for more relaxed scaling
assumptions in the sparsities of Œ≤j in relation to the sample size n and this is confirmed by
our simulation results. Our method relies on tuning the pair (Œ±s, Œª0) over a grid as described
in 3.3, which may be too computationally intensive for large datasets. If the practitioner has
preconceptions about how many covariates are relevant in explaining the network structure of
the responses, he may fix Œ±s to specify a desired group sparsity level. Alternatively, adapting
tuning-free methods such as the square-root lasso [Belloni et al., 2011] to our method would be an
important contribution for large datasets. A direction for theoretical work would be to establish
the minimax rate of (15). Although our estimator achieves the same rate as in Zhang and Li [2022],
15
the optimality of this rate has yet to be established. Our R package cspine is available on GitHub1.
Acknowledgements
We are grateful to Emma Zhang for generously making the RegGMM software and GBM eQTL data
set available to us.
Use was made of computational facilities purchased with funds from the National Science
Foundation (CNS-1725797) and administered by the Center for Scientific Computing (CSC). The
CSC is supported by the California NanoSystems Institute and the Materials Research Science and
Engineering Center (MRSEC; NSF DMR 2308708) at UC Santa Barbara.
References
Steffen L. Lauritzen. Graphical Models. Oxford University Press, 1996. ISBN 0-19-852219-3.
Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. Biometrika,
94(1):19‚Äì35, 2007. ISSN 00063444, 14643510. URL http://www.jstor.org/stable/20441351.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation
with the graphical lasso. Biostatistics, 9(3):432‚Äì441, 12 2007. ISSN 1465-4644. doi: 10.1093/
biostatistics/kxm045. URL https://doi.org/10.1093/biostatistics/kxm045.
Nicolai Meinshausen and Peter B√ºhlmann. High-dimensional graphs and variable selection with
the Lasso. The Annals of Statistics, 34(3):1436 ‚Äì 1462, 2006. doi: 10.1214/009053606000000281.
URL https://doi.org/10.1214/009053606000000281.
Juliane Sch√§fer and Korbinian Strimmer. A shrinkage approach to large-scale covariance matrix
estimation and implications for functional genomics. Statistical Applications in Genetics and
Molecular Biology, 4(1), 2005. doi: doi:10.2202/1544-6115.1175. URL https://doi.org/10.
2202/1544-6115.1175.
Ming Yuan, Ali Ekici, Zhaosong Lu, and Renato Monteiro. Dimension reduction and coefficient
estimation in multivariate linear regression. Journal of the Royal Statistical Society. Series B
(Statistical Methodology), 69(3):329‚Äì346, 2007. ISSN 13697412, 14679868.
Adam J. Rothman, Elizaveta Levina, and Ji Zhu. Sparse multivariate regression with covariance
estimation. Journal of Computational and Graphical Statistics, 19(4):947‚Äì962, 2010. ISSN 10618600.
URL http://www.jstor.org/stable/25765382.
Jianxin Yin and Hongzhe Li. A sparse conditional gaussian graphical model for analysis of genetical
genomics data. The Annals of Applied Statistics, 5(4):2630‚Äì2650, 2011. ISSN 19326157, 19417330.
URL http://www.jstor.org/stable/23069344.
1https://github.com/roobnloo/cspine
16
T. Tony Cai, Hongzhe Li, Weidong Liu, and Jichun Xie. Covariate-adjusted precision matrix
estimation with an application in genetical genomics. Biometrika, 100(1):139‚Äì156, 2013. ISSN
00063444. URL http://www.jstor.org/stable/43304542.
Mengjie Chen, Zhao Ren, Hongyu Zhao, and Harrison Zhou. Asymptotically normal and efficient
estimation of covariate-adjusted gaussian graphical model. Journal of the American Statistical
Association, 111(513):394‚Äì406, 2016. doi: 10.1080/01621459.2015.1010039.
Junhui Wang. Joint Estimation of Sparse Multivariate Regression and Conditional Graphical
Models. Statistica Sinica, 25(3):831‚Äì851, 2015. ISSN 1017-0405. Publisher: Institute of Statistical
Science, Academia Sinica.
Jinghui Chen, Pan Xu, Lingxiao Wang, Jian Ma, and Quanquan Gu. Covariate adjusted precision
matrix estimation via nonconvex optimization. In Jennifer Dy and Andreas Krause, editors,
Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 922‚Äì931. PMLR, July 2018.
Xiao Lv, Wei Cui, and Yulong Liu. A sharp analysis of covariate adjusted precision matrix estimation
via alternating projected gradient descent. IEEE Signal Processing Letters, 29:877‚Äì881, 2022. doi:
10.1109/LSP.2022.3159402.
Rudolf S. N. Fehrmann, Ritsert C. Jansen, Jan H. Veldink, Harm-Jan Westra, Danny Arends,
Marc Jan Bonder, Jingyuan Fu, Patrick Deelen, Harry J. M. Groen, Asia Smolonska, Rinse K.
Weersma, Robert M. W. Hofstra, Wim A. Buurman, Sander Rensen, Marcel G. M. Wolfs, Mathieu
Platteel, Alexandra Zhernakova, Clara C. Elbers, Eleanora M. Festen, Gosia Trynka, Marten H.
Hofker, Christiaan G. J. Saris, Roel A. Ophoff, Leonard H. van den Berg, David A. van Heel,
Cisca Wijmenga, Gerard J. te Meerman, and Lude Franke. Trans-eqtls reveal that independent
genetic variants associated with a complex phenotype converge on intermediate genes, with
a major role for the hla. PLoS Genetics, 7(8):e1002197, August 2011. ISSN 1553-7404. doi:
10.1371/journal.pgen.1002197. URL http://dx.doi.org/10.1371/journal.pgen.1002197.
Liis Kolberg, Nurlan Kerimov, Hedi Peterson, and Kaur Alasoo. Co-expression analysis reveals
interpretable gene modules controlled by trans-acting genetic variants. eLife, 9, September 2020.
ISSN 2050-084X. doi: 10.7554/elife.58705. URL http://dx.doi.org/10.7554/eLife.58705.
Matthew Rockman and Leonid Kruglyak. Genetics of global gene expression. Nature reviews.
Genetics, 7:862‚Äì72, 12 2006. doi: 10.1038/nrg1964.
Jingfei Zhang and Yi Li. High-dimensional gaussian graphical regression models with covariates.
Journal of the American Statistical Association, 0(0):1‚Äì13, 2022. doi: 10.1080/01621459.2022.
2034632. URL https://doi.org/10.1080/01621459.2022.2034632.
Yabo Niu, Yang Ni, Debdeep Pati, and Bani K. Mallick. Covariate-Assisted Bayesian Graph
Learning for Heterogeneous Data. Journal of the American Statistical Association, 119(547):
1985‚Äì1999, July 2024. ISSN 0162-1459. doi: 10.1080/01621459.2023.2233744. URL https:
//doi.org/10.1080/01621459.2023.2233744.
17
Zeya Wang, Veerabhadran Baladandayuthapani, Ahmed O. Kaseb, Hesham M. Amin, Manal M.
Hassan, Wenyi Wang, and Jeffrey S. Morris. Bayesian Edge Regression in Undirected Graphical
Models to Characterize Interpatient Heterogeneity in Cancer. Journal of the American Statistical
Association, 117(538):533‚Äì546, April 2022. ISSN 0162-1459. doi: 10.1080/01621459.2021.2000866.
URL https://doi.org/10.1080/01621459.2021.2000866. Publisher: ASA Website _eprint:
https://doi.org/10.1080/01621459.2021.2000866.
Kyung-Ah Sohn and Seyoung Kim. Joint Estimation of Structured Sparsity and Output Structure
in Multiple-Output Regression via Inverse-Covariance Regularization. In Proceedings of the
Fifteenth International Conference on Artificial Intelligence and Statistics, pages 1081‚Äì1089. PMLR,
March 2012. ISSN: 1938-7228.
Xiao-Tong Yuan and Tong Zhang. Partial Gaussian Graphical Model Estimation. IEEE Transactions
on Information Theory, 60(3):1673‚Äì1687, March 2014. ISSN 1557-9654. doi: 10.1109/TIT.2013.
2296784. Conference Name: IEEE Transactions on Information Theory.
Yunzhang Zhu.
A convex optimization formulation for multivariate regression.
In Ad-
vances in Neural Information Processing Systems, volume 33, pages 17652‚Äì17661. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/
hash/ccd2d123f4ec4d777fc6ef757d0fb642-Abstract.html.
Noah Simon, Jerome Friedman, Trevor Hastie, and Robert Tibshirani. A sparse-group lasso. Journal
of Computational and Graphical Statistics, 22(2):231‚Äì245, 2013. doi: 10.1080/10618600.2012.681250.
URL https://doi.org/10.1080/10618600.2012.681250.
Stephen Reid, Robert Tibshirani, and Jerome Friedman. A study of error variance estimation in
lasso regression. Statistica Sinica, 26(1):35‚Äì67, 2016. ISSN 10170405, 19968507.
Guo Yu and Jacob Bien.
Estimating the error variance in a high-dimensional linear model.
Biometrika, 106(3):533‚Äì546, May 2019. ISSN 1464-3510. doi: 10.1093/biomet/asz017.
P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization.
Journal of Optimization Theory and Applications, 109(3):475‚Äì494, June 2001. ISSN 1573-2878.
doi: 10.1023/a:1017501703105.
Xiaoxuan Liang, Aaron Cohen, Anibal S√≥lon Heinsfeld, Franco Pestilli, and Daniel J. McDonald.
sparsegl : An R Package for Estimating Sparse Group Lasso. Journal of Statistical Software,
110(6), 2024. ISSN 1548-7660. doi: 10.18637/jss.v110.i06. URL https://www.jstatsoft.org/
v110/i06/.
Albert-L√°szl√≥ Barab√°si and R√©ka Albert. Emergence of scaling in random networks. Science, 286
(5439):509‚Äì512, 1999. doi: 10.1126/science.286.5439.509. URL https://www.science.org/
doi/abs/10.1126/science.286.5439.509.
P. Erd≈ës and A. R√©nyi. On random graphs. i. Publicationes Mathematicae Debrecen, 6(3‚Äì4):290‚Äì297,
1959. ISSN 0033-3883. doi: 10.5486/pmd.1959.6.3-4.12.
18
Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J. Newman. Power-law distributions in empirical
data. SIAM Review, 51(4):661‚Äì703, 2009. ISSN 00361445, 10957200. URL http://www.jstor.
org/stable/25662336.
Farina Hanif, Kanza Muzaffar, kahkashan Perveen, Saima Malhi, and Shabana Simjee. Glioblastoma
multiforme: A review of its epidemiology and pathogenesis through clinical presentation and
treatment. Asian Pacific Journal of Cancer Prevention, 18(1), January 2017. doi: 10.22034/APJCP.
2017.18.1.3. URL https://doi.org/10.22034/APJCP.2017.18.1.3.
Aneta Kwiatkowska, Mohan Nandhu, Prajna Behera, E. Chiocca, and Mariano Viapiano. Strategies
in gene therapy for glioblastoma. Cancers, 5(4):1271‚Äì1305, October 2013. ISSN 2072-6694. doi:
10.3390/cancers5041271. URL http://dx.doi.org/10.3390/cancers5041271.
Minoru Kanehisa and Susumu Goto. KEGG: Kyoto Encyclopedia of Genes and Genomes. Nucleic
Acids Research, 28(1):27‚Äì30, 01 2000. ISSN 0305-1048. doi: 10.1093/nar/28.1.27.
Patricio Gonzalez-Hormazabal, Maher Musleh, Marco Bustamante, Juan Stambuk, Raul Pisano,
Hector Valladares, Enrique Lanzarini, Hector Chiong, Jorge Rojas, Jose Suazo, V. Gonzalo Castro,
Lilian Jara, and Zoltan Berger. Polymorphisms in ras/raf/mek/erk pathway are associated
with gastric cancer. Genes, 10(1), 2019. ISSN 2073-4425. doi: 10.3390/genes10010020. URL
https://www.mdpi.com/2073-4425/10/1/20.
Yardena Samuels and Victor E. Velculescu. Oncogenic mutations of pik3ca in human cancers.
Cell Cycle, 3(10):1221‚Äì1224, October 2004. ISSN 1551-4005. doi: 10.4161/cc.3.10.1164. URL
http://dx.doi.org/10.4161/cc.3.10.1164.
Mohammed A Azab, Abdulraheem Alomari, and Ahmed Y Azzam.
Featuring how calcium
channels and calmodulin affect glioblastoma behavior. a review article. Cancer Treatment
and Research Communications, 25:100255, 2020.
ISSN 2468-2942.
doi: https://doi.org/10.
1016/j.ctarc.2020.100255. URL https://www.sciencedirect.com/science/article/pii/
S2468294220300903.
Quan Cheng, Anliu Tang, Zeyu Wang, Ning Fang, Zhuojing Zhang, Liyang Zhang, Chuntao Li,
and Yu Zeng. Cald1 modulates gliomas progression via facilitating tumor angiogenesis. Cancers,
13(11), 2021. ISSN 2072-6694. doi: 10.3390/cancers13112705. URL https://www.mdpi.com/
2072-6694/13/11/2705.
Guoxin Zhang, Zhen Dong, Briana C. Prager, Leo J.K. Kim, Qiulian Wu, Ryan C. Gimple, Xiuxing
Wang, Shideng Bao, Petra Hamerlik, and Jeremy N. Rich. Chromatin remodeler hells maintains
glioma stem cells through e2f3 and myc. JCI Insight, 4(7), 4 2019. doi: 10.1172/jci.insight.126140.
URL https://insight.jci.org/articles/view/126140.
Lei Wu, Jinfan Li, Yiying Xu, Xiaoli Lou, Maomin Sun, and Shouli Wang. Expression and prognostic
value of e2f3 transcription factor in non-small cell lung cancer. Oncology letters, 21:411, 05 2021.
doi: 10.3892/ol.2021.12672.
Zhicai Feng, Cheng Peng, Daojiang Li, Danhua Zhang, Xu Li, Fengran Cui, Yanhong Chen, and
Quanyong He. E2f3 promotes cancer growth and is overexpressed through copy number
variation in human melanoma. OncoTargets and therapy, pages 5303‚Äì5313, 2018.
19
Sara Ladu, Diego F. Calvisi, Elizabeth A. Conner, Miriam Farina, Valentina M. Factor, and Snorri S.
Thorgeirsson. E2f1 inhibits c-myc-driven apoptosis via pik3ca/akt/mtor and cox-2 in a mouse
model of human liver cancer. Gastroenterology, 135(4):1322‚Äì1332, 2008. ISSN 0016-5085. doi:
https://doi.org/10.1053/j.gastro.2008.07.012.
A. Belloni, V. Chernozhukov, and L. Wang. Square-root lasso: pivotal recovery of sparse signals
via conic programming. Biometrika, 98(4):791‚Äì806, November 2011. ISSN 1464-3510. doi:
10.1093/biomet/asr043. URL http://dx.doi.org/10.1093/biomet/asr043.
Cancer Genome Atlas Research Network. Comprehensive genomic characterization defines human
glioblastoma genes and core pathways. Nature, 455(7216):1061‚Äì1068, September 2008. ISSN
1476-4687. doi: 10.1038/nature07385. URL http://dx.doi.org/10.1038/nature07385.
Cameron W. Brennan, Roel G.W. Verhaak, Aaron McKenna, Benito Campos, Houtan Noushmehr,
Sofie R. Salama, Siyuan Zheng, Debyani Chakravarty, J. Zachary Sanborn, Samuel H. Berman,
Rameen Beroukhim, Brady Bernard, Chang-Jiun Wu, Giannicola Genovese, Ilya Shmulevich,
Jill Barnholtz-Sloan, Lihua Zou, Rahulsimham Vegesna, Sachet A. Shukla, Giovanni Ciriello,
W.K. Yung, and Zhang. The somatic genomic landscape of glioblastoma. Cell, 155(2):462‚Äì477,
October 2013. ISSN 0092-8674. doi: 10.1016/j.cell.2013.09.034. URL http://dx.doi.org/10.
1016/j.cell.2013.09.034.
Ahmed Maklad, Anjana Sharma, and Iman Azimi. Calcium signaling in brain cancers: Roles
and therapeutic targeting. Cancers, 11(2):145, January 2019. ISSN 2072-6694. doi: 10.3390/
cancers11020145. URL http://dx.doi.org/10.3390/cancers11020145.
Pierre C. Bellec, Arnak S. Dalalyan, Edwin Grappin, and Quentin Paris. On the prediction loss of
the lasso in the partially labeled setting. Electronic Journal of Statistics, 12(2):3443 ‚Äì 3472, 2018.
doi: 10.1214/18-EJS1457. URL https://doi.org/10.1214/18-EJS1457.
Franklin A. Graybill and George Marsaglia. Idempotent matrices and quadratic forms in the
general linear hypothesis. The Annals of Mathematical Statistics, 28(3):678‚Äì686, 1957. ISSN
00034851. URL http://www.jstor.org/stable/2237227.
B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection.
The Annals of Statistics, 28(5):1302‚Äì1338, 2000. ISSN 00905364. URL http://www.jstor.org/
stable/2674095.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, 2011.
Arun Kuchibhotla and Abhishek Chakrabortty.
Moving beyond sub-gaussianity in high-
dimensional statistics: Applications in covariance estimation and linear regression. Information
and Inference A Journal of the IMA, 11:1389‚Äì1456, 06 2022. doi: 10.1093/imaiai/iaac012.
Po-Ling Loh and Martin J. Wainwright. High-dimensional regression with noisy and missing data:
Provable guarantees with nonconvexity. The Annals of Statistics, 40(3):1637 ‚Äì 1664, 2012. doi:
10.1214/12-AOS1018. URL https://doi.org/10.1214/12-AOS1018.
20
Supplemental Materials
A convex formulation of covariate-adjusted Gaussian graphical
models via natural parametrization
S1
Algorithm outline
Algorithm 1: Covariate-adjusted Sparse Precision with Natural Estimation (cspine)
Input
: U ‚ààRn√óq matrix of covariates.
X ‚ààRn√óp matrix of responses.
gj : Rq √ó R(p‚àí1)(q+1) ‚ÜíR convex penalty functions for j ‚àà[p].
H(ÀÜŒ≥j, ÀÜŒ≤j) a function to estimate the error variance œÉ2
Œµj.
Output
: ÀÜŒì ‚ààRp√óq, ÀúB0, ÀúB1, . . . ÀúBq ‚ààRp√óp, and ÀÜœÉ2
Œµj for j ‚àà[p].
for j = 1, 2, . . . , p do
Define W‚àíj as in (10) and solve
(ÀÜŒ≥j, ÀÜŒ≤j) = arg min
Œ≥j, Œ≤j
1
2n
xj ‚àíUŒ≥j ‚àíW‚àíjŒ≤j
2
2 + gj(Œ≥j, Œ≤j).
Set ÀÜœÉ2
Œµj = H(ÀÜŒ≥j, ÀÜŒ≤j) and ÀúŒ≤jkh = ‚àíÀÜŒ≤jkh/ÀÜœÉ2
Œµj for k Ã∏= j, h = 0, 1, . . . , q.
end
Set ÀÜŒì = [ÀÜŒ≥1, . . . , ÀÜŒ≥p]‚ä§.
Initialize ÀúB0 = ÀúB1 = ¬∑ ¬∑ ¬∑ = ÀúBq = 0.
for h = 0, 1, . . . , q do
Apply the and-rule:
[ ÀúBh]jk = [ ÀúBh]kj = ÀúŒ≤jkh1{|ÀúŒ≤jkh|<|ÀúŒ≤kjh|} + ÀúŒ≤kjh1{|ÀúŒ≤jkh|>|ÀúŒ≤kjh|},
j Ã∏= k
end
return ÀÜŒì, ÀúB0, ÀúB1, . . . , ÀúBq, ÀÜœÉ2
Œµ1, . . . , ÀÜœÉ2
Œµp.
S2
Additional Simulation Results
In addition to the metrics reported in Table 1, we report here the support recovery metrics for the
estimated precision matrix and the estimation error of the mean given by ¬µerr = Pn
i=1‚à•ÀÜ¬µi‚àí¬µi‚à•2
2/n
where ÀÜ¬µi is computed from (14). The results are shown in Table S1 for data generated under the
natural model (7) and in Table S2 for data generated under the original model (3). We see that
cspine underperforms in mean prediction error in both data models. This is a reasonable finding
since our method does not focus on estimating the mean. Instead, our method targets the precision
matrix and its components {Bh} while incorporating the covariate effects on the mean.
21
n
(p, q)
Method
‚Ñ¶TPR
‚Ñ¶FPR
‚Ñ¶err
¬µerr
200
(25, 50)
cspine
0.84 (0.06)
0.10 (0.03)
1.09 (0.15)
10.14 (1.49)
RegGMM
0.74 (0.08)
0.18 (0.06)
1.82 (0.26)
8.28 (1.13)
(25, 100)
cspine
0.79 (0.07)
0.08 (0.02)
1.22 (0.17)
11.94 (1.12)
RegGMM
0.68 (0.07)
0.17 (0.04)
1.95 (0.28)
10.11 (1.01)
400
(25, 50)
cspine
0.93 (0.04)
0.12 (0.03)
0.66 (0.08)
5.80 (1.05)
RegGMM
0.83 (0.06)
0.20 (0.07)
1.22 (0.17)
4.96 (0.72)
(25, 100)
cspine
0.89 (0.04)
0.11 (0.03)
0.76 (0.09)
7.98 (1.05)
RegGMM
0.79 (0.06)
0.24 (0.07)
1.40 (0.20)
6.76 (0.91)
Table S1: Additional metrics under the natural model setting (7).
n
(p, q)
Method
‚Ñ¶TPR
‚Ñ¶FPR
‚Ñ¶err
¬µerr
200
(25, 50)
cspine
0.69 (0.07)
0.14 (0.03)
2.18 (0.24)
6.20 (0.67)
RegGMM
0.65 (0.09)
0.16 (0.05)
2.20 (0.25)
5.89 (0.66)
(25, 100)
cspine
0.54 (0.07)
0.12 (0.02)
2.43 (0.25)
9.95 (1.16)
RegGMM
0.51 (0.09)
0.21 (0.06)
2.69 (0.32)
9.58 (1.12)
400
(25, 50)
cspine
0.79 (0.06)
0.20 (0.03)
1.54 (0.18)
3.34 (0.36)
RegGMM
0.75 (0.07)
0.16 (0.05)
1.49 (0.19)
3.07 (0.31)
(25, 100)
cspine
0.73 (0.06)
0.16 (0.03)
1.69 (0.22)
5.83 (0.63)
RegGMM
0.74 (0.07)
0.24 (0.07)
1.69 (0.23)
5.58 (0.54)
Table S2: Additional metrics under specification (3).
S3
Additional Results from GBM eQTL Analysis
Table S3 displays the genes belonging to known signaling pathways associated with GBM. The
table is copied from Zhang and Li [2022]. Table S4 lists the SNPs that are detected by cspine to
affect network co-expression.
22
name
genes
references
PI3K/Akt/mTOR
PIK3CA, PIK3CB, PIK3CD,
Network [2008]
signaling pathway
PIK3R3, PTEN, AKT1, AKT2, AKT3
MTOR, IGF1, PRKCA
EGF, EGFR, GRB2,
Brennan et al. [2013]
SOS1, SOS2, IGF1
Ras-Raf-MEK-ERK
SHC1, SHC2, SHC3, SHC4
signaling pathway
MAPK1, MAPK3, MAP2K1, MAP2K2
HRAS, KRAS, NRAS,
RAF1, ARAF, BRAF, PRKCA
CALM1,CALML3, CALML4, CALML5,
Maklad et al. [2019]
calcium (Ca+2)
CALML6, CAMK1,CAMK4, CAMK1D,
signaling pathway
CAMK1G,CAMK2A, CAMK2B,
CAMK2D,CAMK2G, PRKCA
p53
TP53, MDM2, DDB2, PTEN, IGF1
Network [2008]
signaling pathway
CDK4, CDK6, CDKN1A, CDKN2A
Table S3: Gene signaling pathways related to GBM.
SNP
co-expressed genes
rs1267622
(PDGFRA, PDGFB), (PDGRFA, SOS2), (GADD45A, GADD45B)
rs10519201
(E2F3, PIK3CA), (E2F3, BAX)
rs10488141
(SOS2, E2F1), (SOS2, GADD45B)
rs2076655
(E2F1, CALML5)
rs10509346
(E2F3, CALML5)
rs10518759
(E2F1, PIK3CA)
rs10512510
(E2F3, BRAF)
rs9303504
(E2F1, CALML5)
rs4834352
(E2F1, E2F3)
rs2075109
(PIK3CA, CALML5)
Table S4: Estimated SNP effects on gene co-expressions according to cspine.
S4
Proofs
Our proof strategies for Theorems 1 and 2 follow that of Zhang and Li [2022], with modifications
made to accommodate our concatenated design matrix [U, W]. First we state a few lemmas.
Lemma 1 (Bellec et al. [2018] Lemma 1). Let g : Rd ‚ÜíR be any convex function and let
ÀÜŒ≤ ‚ààarg min
Œ≤‚ààRd

‚à•y ‚àíHŒ≤‚à•2
2 + g(Œ≤)
	
where H ‚ààRn√ód and y ‚ààRn. Then for all Œ≤ ‚ààRd,
1
2n‚à•y ‚àíHÀÜŒ≤‚à•2
2 + g(ÀÜŒ≤) + 1
2n‚à•H(ÀÜŒ≤ ‚àíŒ≤)‚à•2
2 ‚â§1
2n‚à•y ‚àíHŒ≤‚à•2
2 + g(Œ≤).
Lemma 2 (Graybill and Marsaglia [1957] Theorem F). Let Œµ ‚àºNp(0, œÉ2Ip) and let A be a p √ó p
idempotent matrix with rank r ‚â§p. Then Œµ‚ä§AŒµ/œÉ2 ‚àºœá2
r.
23
Lemma 3 (Laurent and Massart [2000] Lemma 1). Suppose that U ‚àºœá2
r. For any x > 0 it holds
that
P(U ‚àír ‚â•2‚àörx + 2x) ‚â§e‚àíx.
Lemma 4 (Vershynin [2011] Proposition 5.16). Let X1, . . . , Xn be independent, mean zero sub-
exponential random variables. Let v1 = maxi‚à•Xi‚à•œà1 where ‚à•¬∑‚à•œà1 is the sub-exponential norm. Then
there exists a constant c such that for any t > 0 we have
P
 
n
X
i=1
Xi
 ‚â•t
!
‚â§2 exp

‚àíc min
 t2
v2
1n, t
v1

.
Lemma 5 comes from Theorem 4.1 in Kuchibhotla and Chakrabortty [2022] applied to marginally
sub-Gaussian random vectors.
Lemma 5 (Kuchibhotla and Chakrabortty [2022] Theorem 4.1). Let Z1, . . . , Zn be independent
random vectors in Rp. Assume each element of Zi is sub-Gaussian with bounded sub-Gaussian norm
for all i ‚àà[n]. Let ÀÜŒ£Z = Z‚ä§Z/n and Œ£Z = E[Z‚ä§Z/n]. Define
Œ•n = max
j,k
1
n
n
X
i=1
Var(ZijZik).
Then we have
sup
‚à•v‚à•0‚â§k,‚à•v‚à•2‚â§1
v‚ä§( ÀÜŒ£Z ‚àíŒ£Z)v
 ‚âæk
r
Œ•n log p
n
+ k log n log p
n
with probability at least 1 ‚àíO(1/p).
We may apply Lemma 5 because by Assumption 3, [U, W‚àíj] is elementwise sub-Gaussian,
each entry being the product of a sub-Gaussian and a bounded random variable. Furthermore, we
have by the Cauchy-Schwarz inequality that
max
‚Ñì,k
1
n
n
X
i=1
Var([U, W‚àíj]i‚Ñì[U, W‚àíj]ik) ‚â§
max
‚Ñì1,‚Ñì2,‚Ñì3,‚Ñì4 E

X(1)2
‚Ñì1 X(1)2
‚Ñì2 U (1)2
‚Ñì3
U (1)2
‚Ñì4

= O(1)
since the entries of X(1) and U(1) have bounded moments. Thus Œ•n = O(1) in our setting.
Lemma 6 (Loh and Wainwright [2012] Lemma 12). Let Œ£ ‚ààRp√óp be a symmetric matrix such
that |v‚ä§Œ£v| ‚â§Œ¥1 for all v ‚ààRp with ‚à•v‚à•2 = 1 and ‚à•v‚à•0 ‚â§2s. It holds for all v ‚ààRp that
|v‚ä§Œ£v| ‚â§27Œ¥1

‚à•v‚à•2
2 + 1
s‚à•v‚à•2
1

.
Define the matrices:
ÀÜŒ£UW = 1
n[U, W‚àíj]‚ä§[U, W‚àíj],
Œ£UW = E( ÀÜŒ£U,W).
24
Lemma 7. For a set of indices S ‚äÇ[p(q + 1) ‚àí1], denote by [U, W‚àíj]S the submatrix of [U, W‚àíj]
with columns indexed by S. Let ‚à•¬∑‚à•op denote the matrix operator norm. Under Assumptions 1-4, there
exist constants Muw and C0 such that with probability at least 1 ‚àíC0 exp(‚àílog(pq)) we have
1
n‚à•[U, W‚àíj]S‚à•2
op ‚â§Muw
for all S satisfying |S| ‚â§ÀÜsmax
Œ≥
+ ÀÜsmax
Œ≤
, provided that (ÀÜsmax
Œ≥
+ ÀÜsmax
Œ≤
) log(pq) = O(‚àön), as assumed
in Theorem 1.
Proof. Letting k = |S|, it suffices to show that sup‚à•v‚à•0‚â§k, ‚à•v‚à•2‚â§1 v‚ä§ÀÜŒ£UWv is bounded. We may
write
sup
‚à•v‚à•0‚â§k, ‚à•v‚à•2‚â§1
v‚ä§ÀÜŒ£UWv =
sup
‚à•v‚à•0‚â§k, ‚à•v‚à•2‚â§1
n
v‚ä§
ÀÜŒ£UW ‚àíŒ£UW

v + v‚ä§Œ£UWv
o
.
By Assumption 2, the second term is bounded. For the first term, by Lemma 5 we have
sup
‚à•v‚à•0‚â§k, ‚à•v‚à•2‚â§1
n
v‚ä§
ÀÜŒ£UW ‚àíŒ£UW

v
o
‚âæ
k2 log(pq)
n
1/2
+ k log(pq)
n/ log n
with probability at least 1 ‚àíC0 exp(‚àílog(pq)) and we see that the right-hand side is bounded by
Assumption 4.
S4.1
Proof of Theorem 1
For ease of notation, we will drop the dependence of Œ≥j, Œ≤j, Œµj and W‚àíj on j. Let SŒ≤, SŒ≥, ÀÜSŒ≤, ÀÜSŒ≥
be the support sets of Œ≤, Œ≥, ÀÜŒ≤, ÀÜŒ≥, respectively. Let SŒ≤,g = {h ‚àà[q] : Œ≤h Ã∏= 0} index the blocks
Œ≤h of Œ≤ that are not identically zero and let ÀÜSŒ≤,g be the corresponding block indices for ÀÜŒ≤. For
any vector v and set of block indices S, let v(S) denote the sub-vector containing blocks in S. Let
sŒ≤, sŒ≥, sŒ≤,g, ÀÜsŒ≤, ÀÜsŒ≥, ÀÜsŒ≤,g be the number of elements in SŒ≤, SŒ≥, SŒ≤,g, ÀÜSŒ≤, ÀÜSŒ≥, ÀÜSŒ≤,g, respectively. Our
proof occurs in three steps.
Step 1
In this step we bound the error ‚à•UŒΩ +W‚àÜ‚à•2
2/n by the stochastic term ‚ü®Œµ, UŒΩ +W‚àÜ‚ü©/n, which
is then bounded by a projection of Œµ onto the columns of [U, W].
Since our penalty function
g(Œ≥, Œ≤) = Œª‚à•Œ≥‚à•1 + Œª‚à•Œ≤‚à•1 + Œªg‚à•Œ≤‚àí0‚à•1,2
is convex, by Lemma 1 we have
1
2n‚à•x ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤‚à•2
2 + g(ÀÜŒ≥, ÀÜŒ≤) + 1
2n‚à•UŒΩ + W‚àÜ‚à•2
2 ‚â§1
2n‚à•x ‚àíUŒ≥ ‚àíWŒ≤‚à•2
2 + g(Œ≥, Œ≤)
where ŒΩ = ÀÜŒ≥ ‚àíŒ≥ and ‚àÜ= ÀÜŒ≤ ‚àíŒ≤. Since Œµ = x ‚àíUŒ≥ ‚àíWŒ≤ we may write
1
2n‚à•x ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤‚à•2
2 = 1
2n‚à•Œµ ‚àíUŒΩ ‚àíW‚àÜ‚à•2
2
= 1
2n‚à•Œµ‚à•2
2 ‚àí1
n‚ü®Œµ, UŒΩ + W‚àÜ‚ü©+ 1
2n‚à•UŒΩ + W‚àÜ‚à•2
2.
25
Plugging this into the previous expression and substituting the penalty expression then yields
1
n‚à•UŒΩ + W‚àÜ‚à•2
2 + Œª‚à•ÀÜŒ≥‚à•1 + Œª‚à•ÀÜŒ≤‚à•1 + Œªg‚à•ÀÜŒ≤‚àí0‚à•1,2
‚â§1
n‚ü®Œµ, UŒΩ + W‚àÜ‚ü©+ Œª‚à•Œ≥‚à•1 + Œª‚à•Œ≤‚à•1 + Œªg‚à•Œ≤‚àí0‚à•1,2.
Notice that ‚à•‚àÜSc
Œ≤‚à•1 = ‚à•ÀÜŒ≤Sc
Œ≤‚à•1, ‚à•ÀÜŒ≤‚à•1 = ‚à•ÀÜŒ≤SŒ≤‚à•1 + ‚à•ÀÜŒ≤Sc
Œ≤‚à•1, and ‚à•Œ≤‚à•1 = ‚à•Œ≤SŒ≤‚à•1. Hence we can
express the above as
1
n‚à•UŒΩ + W‚àÜ‚à•2
2 + Œª‚à•ÀÜŒ≥‚à•1 + Œª‚à•ÀÜŒ≤SŒ≤‚à•1 + Œª‚à•‚àÜSc
Œ≤‚à•1
|
{z
}
Œª‚à•ÀÜŒ≤‚à•1
+Œªg‚à•ÀÜŒ≤‚àí0‚à•1,2
‚â§1
n‚ü®Œµ, UŒΩ + W‚àÜ‚ü©+ Œª‚à•Œ≥‚à•1 + Œª‚à•Œ≤SŒ≤‚à•1
|
{z
}
Œª‚à•Œ≤‚à•1
+Œªg‚à•Œ≤‚àí0‚à•1,2.
Thus we have
1
n‚à•UŒΩ + W‚àÜ‚à•2
2 + Œª‚à•ÀÜŒ≥‚à•1 + Œª‚à•‚àÜSc
Œ≤‚à•1 + Œªg‚à•ÀÜŒ≤‚àí0‚à•1,2
‚â§1
n‚ü®Œµ, UŒΩ + W‚àÜ‚ü©+ Œª‚à•Œ≥‚à•1 + Œª(‚à•Œ≤SŒ≤‚à•1 ‚àí‚à•ÀÜŒ≤SŒ≤‚à•1) + Œªg‚à•Œ≤‚àí0‚à•1,2
‚â§1
n‚ü®Œµ, UŒΩ + W‚àÜ‚ü©+ Œª‚à•Œ≥‚à•1 + Œª‚à•‚àÜSŒ≤‚à•1 + Œªg‚à•Œ≤‚àí0‚à•1,2
using the triangle inequality for the ‚Ñì1 norm. The same development holds for Œª‚à•ÀÜŒ≥‚à•1. Finally,
notice that
‚à•‚àÜ(Sc
Œ≤,g)‚à•1,2 = ‚à•ÀÜŒ≤(Sc
Œ≤,g)‚à•1,2, ‚à•ÀÜŒ≤‚àí0‚à•1,2 = ‚à•ÀÜŒ≤(SŒ≤,g)‚à•1,2 + ‚à•ÀÜŒ≤(Sc
Œ≤,g)‚à•1,2, and ‚à•Œ≤‚àí0‚à•1,2 = ‚à•Œ≤(SŒ≤,g)‚à•1,2.
Hence the previous development holds for the Œªg‚à•ÀÜŒ≤‚àí0‚à•1,2 term by the triangle inequality of ‚à•¬∑‚à•1,2.
All in all we have
1
n‚à•UŒΩ + W‚àÜ‚à•2
2 + Œª‚à•ŒΩScŒ≥‚à•1 + Œª‚à•‚àÜSc
Œ≤‚à•1 + Œªg‚à•‚àÜ(Sc
Œ≤,g)‚à•1,2
‚â§1
n‚ü®Œµ, UŒΩ + W‚àÜ‚ü©+ Œª‚à•ŒΩSŒ≥‚à•1 + Œª‚à•‚àÜSŒ≤‚à•1 + Œªg‚à•‚àÜ(SŒ≤,g)‚à•1,2.
(S1)
Now let I and J be arbitrary index sets of the columns of U and W respectively. Denote by
PI,J the orthogonal projection onto the columns of [U, W] indexed by (I, J ). Let I0 = SŒ≥ ‚à™ÀÜSŒ≥
and J0 = SŒ≤ ‚à™ÀÜSŒ≤ denote the unions of the true and estimated support sets of Œ≥ and Œ≤. We seek
to bound the stochastic term
‚ü®Œµ, UŒΩ + W‚àÜ‚ü©= ‚ü®PI0,J0(Œµ), UŒΩ + W‚àÜ‚ü©
‚â§‚à•PI0,J0(Œµ)‚à•2‚à•UŒΩ + W‚àÜ‚à•2 ‚â§
1
2a1
‚à•UŒΩ + W‚àÜ‚à•2
2 + a1
2 ‚à•PI0,J0(Œµ)‚à•2
2. (S2)
The last inequality follows from the fact that 2xy ‚â§ax2 + y2/a holds for any constant a > 0 and
real numbers x and y.
26
Following Zhang and Li [2022], we first bound the term ‚à•PI0,J0(Œµ)‚à•2
2 with a counting argument.
For fixed s‚Ä≤
Œ≥, s‚Ä≤
Œ≤, and s‚Ä≤
Œ≤,g, we will bound the cardinality of the set
H(s‚Ä≤
Œ≥, s‚Ä≤
Œ≤, s‚Ä≤
Œ≤,g) = {(I, J ) ‚äÇ[q] √ó [(p ‚àí1)(q + 1)] : |I| = s‚Ä≤
Œ≥, |J | = s‚Ä≤
Œ≤, |g(J )| = s‚Ä≤
Œ≤,g}
where g(J ) is the number of nonzero groups of Œ≤J . For ease of notation, we will write H while
keeping in mind its dependence on s‚Ä≤
Œ≥, s‚Ä≤
Œ≤, and s‚Ä≤
Œ≤,g. We will show that
log|H| ‚â§s‚Ä≤
Œ≥ log eq
s‚Ä≤
Œ≥
+ s‚Ä≤
Œ≤,g log eq
s‚Ä≤
Œ≤,g
+ s‚Ä≤
Œ≤ log(ep)
by considering the two cases s‚Ä≤
Œ≤ = s‚Ä≤
Œ≤,g and s‚Ä≤
Œ≤ > s‚Ä≤
Œ≤,g separately. These are the only cases since
we cannot have more nonzero groups than nonzero elements.
1. s‚Ä≤
Œ≤ = s‚Ä≤
Œ≤,g: In this case, we have |H| ‚â§
  q
s‚Ä≤Œ≥
  q
s‚Ä≤
Œ≤,g

(p ‚àí1)s‚Ä≤
Œ≤. Hence
log|H| ‚â§log
 q
s‚Ä≤
Œ≥

+ log
 q
s‚Ä≤
Œ≤,g

+ s‚Ä≤
Œ≤ log(p ‚àí1)
‚â§s‚Ä≤
Œ≥ log eq
s‚Ä≤
Œ≥
+ s‚Ä≤
Œ≤,g log eq
s‚Ä≤
Œ≤,g
+ s‚Ä≤
Œ≤ log(ep)
where we use log
 n
k

‚â§k log(en/k) which follows from Stirling‚Äôs approximation.
2. s‚Ä≤
Œ≤ > s‚Ä≤
Œ≤,g: In this case, the cardinality is bounded by
|H| ‚â§
 q
s‚Ä≤
Œ≥
 q
s‚Ä≤
Œ≤,g
(p ‚àí1)(s‚Ä≤
Œ≤,g + 1)
s‚Ä≤
Œ≤

.
Since by Stirling‚Äôs approximation
log
(p ‚àí1)(s‚Ä≤
Œ≤,g + 1)
s‚Ä≤
Œ≤

‚â§s‚Ä≤
Œ≤ log e(p ‚àí1)(s‚Ä≤
Œ≤,g + 1)
s‚Ä≤
Œ≤
‚â§s‚Ä≤
Œ≤ log(ep),
we have
log|H| ‚â§s‚Ä≤
Œ≥ log eq
s‚Ä≤
Œ≥
+ s‚Ä≤
Œ≤,g log eq
s‚Ä≤
Œ≤,g
+ s‚Ä≤
Œ≤ log(ep)
as desired.
Define k0 to be the exponential of the right hand side of the above inequality, so that |H| ‚â§k0.
For any (I, J ) ‚ààH, since PI,J is idempotent, Lemma 2 implies
‚à•PI,J (Œµ)‚à•2
2/œÉ2
Œµ ‚àºœá2
d
where d ‚â§|I| + |J | = s‚Ä≤
Œ≥ + s‚Ä≤
Œ≤ is the rank of PI,J . By Lemma 3 we have for arbitrary t‚Ä≤ > 0 that
P

‚à•PI,J (Œµ)‚à•2
2 ‚â•œÉ2
Œµ(2
‚àö
dt‚Ä≤ + d + 2t‚Ä≤)

‚â§e‚àít‚Ä≤.
27
Since 2
‚àö
dt‚Ä≤ ‚â§d + t‚Ä≤ and d ‚â§s‚Ä≤
Œ≥ + s‚Ä≤
Œ≤ ‚â§log k0, we have
P
 ‚à•PI,J (Œµ)‚à•2
2 ‚â•œÉ2
Œµ(2 log k0 + 3t‚Ä≤)

‚â§e‚àít‚Ä≤.
Taking the supremum over H and applying the union bound yields
P
 
sup
(I,J )‚ààH
‚à•PI,J (Œµ)‚à•2
2 ‚â•œÉ2
Œµ(2 log k0 + 3t‚Ä≤)
!
‚â§|H|e‚àít‚Ä≤.
Now set t‚Ä≤ = t/3 + log k0 for t > 0. Then |H|e‚àít‚Ä≤ = |H|/k0 ¬∑ e‚àít/3 ‚â§e‚àít/3 since |H| ‚â§k0.
Substituting these expressions into the previous bound (and recalling that we have fixed s‚Ä≤
Œ≥, s‚Ä≤
Œ≤,
and s‚Ä≤
Œ≤,g) yields
P
 
sup
(I,J )‚ààH(s‚Ä≤Œ≥,s‚Ä≤
Œ≤,s‚Ä≤
Œ≤,g)
‚à•PI,J (Œµ)‚à•2
2 ‚â•5œÉ2
Œµ
"
s‚Ä≤
Œ≥ log eq
s‚Ä≤
Œ≥
+ s‚Ä≤
Œ≤,g log eq
s‚Ä≤
Œ≤,g
+ s‚Ä≤
Œ≤ log(ep)
#
+ œÉ2
Œµt
!
‚â§e‚àít/3,
(S3)
recalling that
log k0 = s‚Ä≤
Œ≥ log eq
s‚Ä≤
Œ≥
+ s‚Ä≤
Œ≤,g log eq
s‚Ä≤
Œ≤,g
+ s‚Ä≤
Œ≤ log(ep).
This gives a concentration bound of ‚à•PI,J (Œµ)‚à•2
2 over all possible subsets I of columns of U and
J of columns of W satisfying |I| = s‚Ä≤
Œ≥, |J | = s‚Ä≤
Œ≤, and |g(J )| = s‚Ä≤
Œ≤,g. Recalling that I0 and J0 are
the support sets of ŒΩ and ‚àÜ, we can now bound ‚à•PI0,J0(Œµ)‚à•2
2. Define
r(s‚Ä≤
Œ≥, s‚Ä≤
Œ≤, s‚Ä≤
Œ≤,g) =
 
sup
(I,J )‚ààH(s‚Ä≤Œ≥,s‚Ä≤
Œ≤,s‚Ä≤
Œ≤,g)
‚à•PI,J (Œµ)‚à•2
2 ‚àí5œÉ2
Œµ
(
s‚Ä≤
Œ≥ log eq
s‚Ä≤
Œ≥
+ s‚Ä≤
Œ≤,g log eq
s‚Ä≤
Œ≤,g
+ s‚Ä≤
Œ≤ log(ep)
)!
+
and
r =
sup
s‚Ä≤Œ≥,s‚Ä≤
Œ≤,s‚Ä≤
Œ≤,g
r(s‚Ä≤
Œ≥, s‚Ä≤
Œ≤, s‚Ä≤
Œ≤,g).
It is clear that
|I0| ‚â§sŒ≥ + ÀÜsŒ≥, |J0| ‚â§sŒ≤ + ÀÜsŒ≤, and |g(J0)| ‚â§sŒ≤,g + ÀÜsŒ≤,g.
Thus we have
‚à•PI0,J0(Œµ)‚à•2
2 ‚â§5œÉ2
Œµ

(sŒ≥ + ÀÜsŒ≥) log eq
sŒ≥
+ (sŒ≤ + ÀÜsŒ≤) log(ep) + (sŒ≤,g + ÀÜsŒ≤,g) log eq
sŒ≤,g

+ r.
(S4)
We also have the following concentration inequality on r for t > 0, which comes from the
definition of r(s‚Ä≤
Œ≥, s‚Ä≤
Œ≤, s‚Ä≤
Œ≤,g) along with (S3):
P(r ‚â•tœÉ2
Œµ) ‚â§
X
s‚Ä≤Œ≥,s‚Ä≤
Œ≤,s‚Ä≤
Œ≤,g
P(r(s‚Ä≤
Œ≥, s‚Ä≤
Œ≤, s‚Ä≤
Œ≤,g) ‚â•tœÉ2
Œµ) ‚â§
X
s‚Ä≤Œ≥,s‚Ä≤
Œ≤,s‚Ä≤
Œ≤,g
e‚àít/3 ‚â§q3p c1e‚àít/3,
where c1 > 0 is a constant and the sum is taken over s‚Ä≤
Œ≥ ‚àà[q], s‚Ä≤
Œ≤ ‚àà[(p ‚àí1)(q + 1)], s‚Ä≤
Œ≤,g ‚àà[q].
Then for a sufficiently large constant Àú
M and some c2 > 0, by letting
t = Àú
M(sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g))
we have
P
n
r ‚â•Àú
MœÉ2
Œµ(sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g))
o
‚â§c1 exp{‚àíc2(sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g))}.
(S5)
28
Step 2
We now use the KKT optimality conditions to bound ‚à•PI0,J0(Œµ)‚à•2
2 in terms of ‚à•UŒΩ + W‚àÜ‚à•2
2.
This takes care of the stochastic term after plugging back into (S1) and (S2). To ease the notation
when describing the conditions, define the vectors
rŒ≥ = 1
nU‚ä§(xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤),
rŒ≤ = 1
nW‚ä§(xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤)
Let ÀÜŒ≤h be the h-th block of ÀÜŒ≤. By the KKT conditions, we know that an optimizer (ÀÜŒ≥, ÀÜŒ≤) of (15)
must satisfy
(rŒ≥)‚Ñì= Œª sign(ÀÜŒ≥‚Ñì)
for ÀÜŒ≥‚ÑìÃ∏= 0
(S6)
(rŒ≤)‚Ñì= Œª sign((ÀÜŒ≤0)‚Ñì)
for (ÀÜŒ≤0)‚ÑìÃ∏= 0
(S7)
(rŒ≤)‚Ñì= Œª sign((ÀÜŒ≤h)‚Ñì) + Œªg
(ÀÜŒ≤h)‚Ñì
‚à•ÀÜŒ≤h‚à•2
for (ÀÜŒ≤h)‚ÑìÃ∏= 0, h ‚àà[q].
(S8)
Squaring both sides of (S6) and summing over ‚Ñìgives
Œª2ÀÜsŒ≥ = 1
n2‚à•U‚ä§
ÀÜSŒ≥(xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤)‚à•2
2
and doing the same for (S7) and (S8) gives
Œª2ÀÜsŒ≤ + Œª2
gÀÜsŒ≤,g ‚â§1
n2‚à•W‚ä§
ÀÜSŒ≤(xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤)‚à•2
2
since in (S8) the cross term sign((ÀÜŒ≤j,h)‚Ñì) √ó (ÀÜŒ≤j,h)‚Ñìis nonnegative. We have shown that
Œª2ÀÜsŒ≥ + Œª2ÀÜsŒ≤ + Œª2
gÀÜsŒ≤,g ‚â§1
n2
[U ÀÜSŒ≥, W ÀÜSŒ≤]‚ä§(xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤)

2
2
= 1
n2
[U ÀÜSŒ≥, W ÀÜSŒ≤]‚ä§(Œµ ‚àíUŒΩ ‚àíW‚àÜ)

2
2.
(S9)
Using that ‚à•a ‚àíb‚à•2
2 ‚â§2‚à•a‚à•2
2 + 2‚à•b‚à•2
2 for any vectors a and b along with Lemma 7, we have
with probability at least 1 ‚àíC0 exp(‚àílog(pq)) that
Œª2ÀÜsŒ≥ + Œª2ÀÜsŒ≤ + Œª2
gÀÜsŒ≤,g ‚â§2
n2
[U ÀÜSŒ≥, W ÀÜSŒ≤]‚ä§Œµ

2
2 + 2
n2
[U ÀÜSŒ≥, W ÀÜSŒ≤]‚ä§(UŒΩ + W‚àÜ)

2
2
‚â§2
nMuw‚à•PI0,J0(Œµ)‚à•2
2 + 2
nMuw‚à•UŒΩ + W‚àÜ‚à•2
2,
(S10)
where the last inequality uses that ÀÜSŒ≥ ‚äÇI0 and ÀÜSŒ≤ ‚äÇJ0.
Now let
Œª = C œÉŒµ
‚àön
sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep)
sŒ≥ + sŒ≤
+
sŒ≤,g
sŒ≥ + sŒ≤
log(eq/sŒ≤,g)
1/2
,
Œªg = C œÉŒµ
‚àön
sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep)
sŒ≤,g
+ log(eq/sŒ≤,g)
1/2
,
29
where C = ‚àö5Muwa2 for some a2 > 2. Combining (S10) and (S4) gives

1 ‚àí2
a2

‚à•PI0,J0(Œµ)‚à•2
2
‚â§5œÉ2
Œµ(sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g)) + 2
a2
‚à•UŒΩ + W‚àÜ‚à•2
2 + r
(S11)
It is then straightforward to multiply both sides by a1a2/(2(a2 ‚àí2)) and plug into (S1) and (S2)
to get
1
n‚à•UŒΩ + W‚àÜ‚à•2
2 + Œª‚à•ŒΩScŒ≥‚à•1 + Œª‚à•‚àÜSc
Œ≤‚à•1 + Œªg‚à•‚àÜSc
Œ≤,g‚à•1,2
‚â§
1
2a1
¬∑ 1
n‚à•UŒΩ + W‚àÜ‚à•2
2 +
5a1a2
2(a2 ‚àí2) ¬∑ Ej +
a1
a2 ‚àí2 ¬∑ 1
n‚à•UŒΩ + W‚àÜ‚à•2
2
+
a1a2
2(a2 ‚àí2) ¬∑ r
n + Œª‚à•ŒΩSŒ≥‚à•1 + Œª‚à•‚àÜSŒ≤‚à•1 + Œªg‚à•‚àÜSŒ≤,g‚à•1,2
(S12)
where Ej = (œÉ2
Œµ/n) ¬∑ (sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g)).
We also have that
‚à•ŒΩSŒ≥‚à•1 + ‚à•‚àÜSŒ≤‚à•1
‚àösŒ≥ + sŒ≤
+ ‚à•‚àÜSŒ≤,g‚à•1,2
‚àösŒ≤,g
‚â§

ŒΩ
‚àÜ

2
+ ‚à•‚àÜSŒ≤,g‚à•2
‚â§2

ŒΩ
‚àÜ

2
‚â§2m‚àí1/2
0
Œ£1/2
UW
ŒΩ
‚àÜ

2
where the last inequality follows from Assumption 2. With our chosen values of Œª and Œªg, we
have after multiplying through by C
p
Ej that
Œª‚à•ŒΩSŒ≥‚à•1 + Œª‚à•‚àÜSŒ≤‚à•1 + Œªg‚à•‚àÜSŒ≤,g‚à•1,2 ‚â§2Cm‚àí1/2
0
p
Ej
Œ£1/2
UW
ŒΩ
‚àÜ

2
‚â§a3
C2
m0
Ej + 1
a3
Œ£1/2
UW
ŒΩ
‚àÜ

2
2
for any a3 > 0. Plugging this into (S12) gives

1 ‚àí1
2a1
‚àí
a1
a2 ‚àí2
1
n‚à•UŒΩ + W‚àÜ‚à•2
2
‚â§

5a1a2
2(a2 ‚àí2) + a3
C2
m0

Ej + 1
a3
Œ£1/2
UW
ŒΩ
‚àÜ

2
2
+
a1a2
2(a2 ‚àí2) ¬∑ r
n.
(S13)
Step 3
We now bound the difference between ‚à•UŒΩ + W‚àÜ‚à•2
2/n and
Œ£1/2
UW
ŒΩ
‚àÜ

2
2
. Let L > 0 be an
arbitrarily large constant. By Assumption 4 and Lemma 5, we have with probability at least
1 ‚àíC‚Ä≤ exp(‚àílog(pq)) that
sup
‚à•v‚à•0‚â§2(sŒ≥+sŒ≤), ‚à•v‚à•2=1
v‚ä§
[U, W]‚ä§[U, W]
n
‚àíŒ£UW

v
 ‚â§
1
27L
30
for sufficiently large n. By Lemma 6, it holds with probability at least 1 ‚àíC‚Ä≤ exp(‚àílog(pq)) that
(ŒΩ‚ä§, ‚àÜ‚ä§)
[U, W]‚ä§[U, W]
n
‚àíŒ£UW
 ŒΩ
‚àÜ

‚â§1
L

‚à•ŒΩ‚à•2
2 + ‚à•‚àÜ‚à•2
2 + ‚à•ŒΩ‚à•2
1 + ‚à•‚àÜ‚à•2
1
sŒ≥ + sŒ≤

(S14)
for sufficiently large n. Plugging this into (S13) gives

1 ‚àí1
2a1
‚àí
a1
a2 ‚àí2 ‚àí1
a3
Œ£1/2
UW
ŒΩ
‚àÜ

2
2
‚â§

5a1a2
2(a2 ‚àí2) + a3
C2
m0

Ej +
a1a2
2(a2 ‚àí2) ¬∑ r
n
+

1 ‚àí1
2a1
‚àí
a1
a2 ‚àí2
 Œ£1/2
UW
ŒΩ
‚àÜ

2
2
‚àí1
n‚à•UŒΩ + W‚àÜ‚à•2
2
!
‚â§

5a1a2
2(a2 ‚àí2) + a3
C2
m0

Ej +
a1a2
2(a2 ‚àí2) ¬∑ r
n
+

1 ‚àí1
2a1
‚àí
a1
a2 ‚àí2

¬∑ 1
L

‚à•ŒΩ‚à•2
2 + ‚à•‚àÜ‚à•2
2 + ‚à•ŒΩ‚à•2
1 + ‚à•‚àÜ‚à•2
1
sŒ≥ + sŒ≤

.
Recalling that the above holds for any a1 > 0, a2 > 2, and a3 > 0, choose a1 = 2, a2 = 6, and
a3 = 6 to get
1
2
Œ£1/2
UW
ŒΩ
‚àÜ

2
2
‚âæEj + 1
L

‚à•ŒΩ‚à•2
2 + ‚à•‚àÜ‚à•2
2 + ‚à•ŒΩ‚à•2
1 + ‚à•‚àÜ‚à•2
1
sŒ≥ + sŒ≤

+ r
n.
By the concentration bound on r in (S5), we in fact have
1
2
Œ£1/2
UW
ŒΩ
‚àÜ

2
2
‚âæEj + 1
L

‚à•ŒΩ‚à•2
2 + ‚à•‚àÜ‚à•2
2 + ‚à•ŒΩ‚à•2
1 + ‚à•‚àÜ‚à•2
1
sŒ≥ + sŒ≤

(S15)
with probability at least 1 ‚àíc1 exp{‚àíc2(sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g))}. Then
by Assumption 2 we have with the same high probability
m0
2 (‚à•ŒΩ‚à•2
2 + ‚à•‚àÜ‚à•2
2) ‚âæEj + 1
L

‚à•ŒΩ‚à•2
2 + ‚à•‚àÜ‚à•2
2 + ‚à•ŒΩ‚à•2
1 + ‚à•‚àÜ‚à•2
1
sŒ≥ + sŒ≤

.
(S16)
Next, taking a1 = 2 ‚àí
‚àö
2, a2 = 6 in (S12) cancels out the ‚à•UŒΩ + W‚àÜ‚à•2
2/n terms. Appealing
again to (S5) yields
‚à•ŒΩScŒ≥‚à•1 + ‚à•‚àÜSc
Œ≤‚à•1
‚àösŒ≥ + sŒ≤
+
‚à•‚àÜ(Sc
Œ≤,g)‚à•1,2
‚àösŒ≤,g
‚â§
p
Ej + ‚à•ŒΩSŒ≥‚à•1 + ‚à•‚àÜSŒ≤‚à•1
‚àösŒ≥ + sŒ≤
+ ‚à•‚àÜ(SŒ≤,g)‚à•1,2
‚àösŒ≤,g
.
with probability at least 1 ‚àíc1 exp{‚àíc2(sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g))}.
31
Adding (‚à•ŒΩSŒ≥‚à•1 + ‚à•‚àÜSŒ≤‚à•1)/‚àösŒ≥ + sŒ≤ to both sides and using
‚à•ŒΩSŒ≥‚à•1 + ‚à•‚àÜSŒ≤‚à•1 ‚â§
p
sŒ≥ + sŒ≤(‚à•ŒΩ‚à•2 + ‚à•‚àÜ‚à•2)
and
‚à•‚àÜ(SŒ≤,g)‚à•1,2 ‚â§‚àösŒ≤,g‚à•‚àÜ‚à•2
yields
‚à•ŒΩ‚à•1 + ‚à•‚àÜ‚à•1
‚àösŒ≥ + sŒ≤
‚â§
p
Ej + 2‚à•ŒΩ‚à•2 + 3‚à•‚àÜ‚à•2
and after squaring both sides we have
‚à•ŒΩ‚à•2
1 + ‚à•‚àÜ‚à•2
1
sŒ≥ + sŒ≤
‚â§k0Ej + k1‚à•ŒΩ‚à•2
2 + k2‚à•‚àÜ‚à•2
2
(S17)
for some absolute constants k0, k1, k2 > 0. Plugging (S17) into the right-hand side of (S16) yields
m0
2 (‚à•ŒΩ‚à•2
2 + ‚à•‚àÜ‚à•2
2) ‚âæ

1 + k0
L

Ej + k1 + 1
L
‚à•ŒΩ‚à•2
2 + k2 + 1
L
‚à•‚àÜ‚à•2
2
Finally, plugging in the expression for Ej and recalling that L is arbitrarily large yields
‚à•ŒΩ‚à•2
2 + ‚à•‚àÜ‚à•2
2 ‚âæœÉ2
Œµ
n (sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g))
with probability at least 1 ‚àíC1 exp{‚àíC2(sŒ≥ log(eq/sŒ≥) + sŒ≤ log(ep) + sŒ≤,g log(eq/sŒ≤,g))} for
some positive constants C1, C2 as desired. ‚ñ°
S4.2
Proof of Theorem 2
The proof of Theorem 2 occurs in three steps.
Step 1
Recall that ÀÜŒ£UW = [U, W]‚ä§[U, W]/n. We wish to show that with high probability
 ÀÜŒ£UW
ŒΩ
‚àÜ

‚àû
‚â§3
2(Œª + Œªg).
(S18)
To ease the notation for this step, define the vectors
rŒ≥ = 1
nU‚ä§(xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤),
rŒ≤ = 1
nW‚ä§(xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤)
Let ÀÜŒ≤h indicate the h-th block of ÀÜŒ≤. By the KKT conditions, we know that an optimizer (ÀÜŒ≥, ÀÜŒ≤) of
(15) must satisfy
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
(rŒ≥)‚Ñì= Œª sign(ÀÜŒ≥‚Ñì)
ÀÜŒ≥‚ÑìÃ∏= 0
|(rŒ≥)‚Ñì| ‚â§Œª
ÀÜŒ≥‚Ñì= 0
(rŒ≤)‚Ñì= Œª sign((ÀÜŒ≤0)‚Ñì)
(ÀÜŒ≤0)‚ÑìÃ∏= 0
(rŒ≤)‚Ñì= Œª sign((ÀÜŒ≤h)‚Ñì) + Œªg
(ÀÜŒ≤h)‚Ñì
‚à•ÀÜŒ≤h‚à•2
(ÀÜŒ≤h)‚ÑìÃ∏= 0, h ‚àà[q]
|(rŒ≤)‚Ñì| ‚â§Œª + Œªg
(ÀÜŒ≤h)‚Ñì= 0, h ‚àà[q]
32
so we have for all ‚Ñì‚àà[p(q + 1) ‚àí1]


[U, W]‚ä§(xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤)/n

‚Ñì
 ‚â§Œª + Œªg.
Recalling that
xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤ = xj ‚àíUŒΩ ‚àíW‚àÜ‚àíUŒ≥ ‚àíWŒ≤ = Œµ ‚àíUŒΩ ‚àíW‚àÜ,
it follows by the triangle inequality that
 ÀÜŒ£UW
ŒΩ
‚àÜ

‚àû
‚àí

1
n[U, W]‚ä§Œµ

‚àû
‚â§

1
n[U, W]‚ä§Œµ ‚àí1
n[U, W]‚ä§[U, W]
ŒΩ
‚àÜ

‚àû
=

1
n[U, W]‚ä§(xj ‚àíUÀÜŒ≥ ‚àíWÀÜŒ≤)

‚àû
‚â§Œª + Œªg.
Thus to show (S18) it suffices to show that with high probability

1
n[U, W]‚ä§Œµ

‚àû
‚â§Œª + Œªg
2
.
(S19)
To see this, let MŒ£ denote the bound on the sub-Gaussian norm of elements of X as assumed in
Assumption 3. Recall that each element of U is bounded by a constant M by Assumption 1. Then
the products of random variables U‚Ñìand X‚Ñìwith the random noise Œµ are sub-exponential with
norm satisfying
max
‚Ñì‚à•U‚ÑìŒµ‚à•œà1 ‚â§max
‚Ñì‚à•U‚Ñì‚à•œà2‚à•Œµ‚à•œà2 ‚â§MœÉŒµ
max
‚Ñì‚à•X‚ÑìŒµ‚à•œà1 ‚â§max
‚Ñì‚à•U‚Ñì‚à•œà2 max
‚Ñì‚à•X‚Ñì‚à•œà2‚à•Œµ‚à•œà2 ‚â§MMŒ£œÉŒµ.
Define the vector v = [U, W]‚ä§Œµ/n, where each element vi is a sum of sub-exponential functions
with bounded norm. By Lemma 4 we have
P

|vi| > 1
2(Œª + Œªg)

‚â§2 exp

‚àíc min
n(Œª + Œªg)2
4M 2M 2
Œ£œÉ2
Œµ
, n(Œª + Œªg)
2MMŒ£œÉŒµ

.
where c > 0 is a constant coming from Lemma 4. Since by (17)
Œª + Œªg ‚â•CœÉŒµ
r
log p
n
‚áê‚áí(Œª + Œªg)2n
C2œÉ2
Œµ
‚â•log p,
we have
(Œª + Œªg)2n
C2œÉ2
Œµ
¬∑ C‚Ä≤‚Ä≤
0 ‚â•C‚Ä≤‚Ä≤
0 log p
where C‚Ä≤‚Ä≤
0 = C2/(4M 2M 2
Œ£). Similarly, the inequality
(Œª + Œªg)n
CœÉŒµ
‚â•
p
n log p
33
implies
(Œª + Œªg)n
CœÉŒµ
¬∑
p
C‚Ä≤‚Ä≤
0 ‚â•
p
C‚Ä≤‚Ä≤
0 ¬∑
p
n log p ‚â•ÀúA‚àí1/2p
C‚Ä≤‚Ä≤
0 log p
provided that log p ‚â§ÀúAn for some constant ÀúA > 0, which we assumed in the hypothesis of
Theorem 2. The above shows that we can pick either argument of the minimum in Lemma 4 to
develop and hence
P

|vi| > 1
2(Œª + Œªg)

‚â§2 exp(‚àíC‚Ä≤
0 log p).
for some constant C‚Ä≤
0 that increases with C, recalling that C comes from (17). Then by the union
bound we have
P
1
n
[U, W]‚ä§Œµ

‚àû‚â•Œª + Œªg
2

= P

max
i |vi| ‚â•Œª + Œªg
2

‚â§(p(q + 1)) ¬∑ 2 exp(‚àíC‚Ä≤
0 log p)
‚â§2 exp(‚àíC‚Ä≤
0 log p + log p + log(q + 1))
‚â§2 exp(‚àíc3 log p)
for some c3 > 0. In the last step we used log p ‚âçlog q. This shows (S19) holds with high probability.
Next we will show the cone condition
‚à•ŒΩScŒ≥‚à•1 + ‚à•‚àÜSc
Œ≤‚à•1 ‚â§4œÑj(‚à•ŒΩSŒ≥‚à•1 + ‚à•‚àÜSŒ≤‚à•1)
(S20)
holds with high probability, where œÑj = 1 +
p
(sŒ≥ + sŒ≤)/sŒ≤,g as defined in Assumption 5. Since
(ÀÜŒ≥, ÀÜŒ≤) is optimal, we have that
1
2n‚à•xj ‚àíUÀÜŒ≥‚àíWÀÜŒ≤‚à•2
2+Œª‚à•ÀÜŒ≥‚à•1+Œª‚à•ÀÜŒ≤‚à•1+Œªg‚à•ÀÜŒ≤‚àí0‚à•1,2 ‚â§1
2n‚à•Œµj‚à•2
2+Œª‚à•Œ≥‚à•1+Œª‚à•Œ≤‚à•1+Œªg‚à•Œ≤‚àí0‚à•1,2.
Rearranging the above leads to
Œª‚à•ÀÜŒ≥‚à•1 + Œª‚à•ÀÜŒ≤‚à•1 + Œªg‚à•ÀÜŒ≤‚àí0‚à•1,2 ‚â§Œª‚à•Œ≥‚à•1 + Œª‚à•Œ≤‚à•1 + Œªg‚à•Œ≤‚àí0‚à•1,2 + 1
n‚ü®UŒΩ + W‚àÜ, Œµ‚ü©.
(S21)
By the same argument leading to (S19), the events
A1 =
1
n
U‚ä§Œµ

‚àû‚â§Œª
2

and
A2 =
1
n
W‚ä§Œµ

‚àû‚â§Œª
2

hold with probability at least 1 ‚àí2 exp(‚àíc4 log p) and 1 ‚àí2 exp(‚àíc5 log p) respectively for some
constants c4 > 0 and c5 > 0.
Conditional on A1 and A2, we have by H√∂lder‚Äôs inequality
1
n‚ü®UŒΩ + W‚àÜ, Œµ‚ü©= 1
n‚ü®UŒΩ, Œµ‚ü©+ 1
n‚ü®W‚àÜ, Œµ‚ü©
‚â§1
n‚à•U‚ä§Œµ‚à•‚àû‚à•ŒΩ‚à•1 + 1
n‚à•W‚ä§Œµ‚à•‚àû‚à•‚àÜ‚à•1
‚â§Œª
2‚à•ŒΩ‚à•1 + Œª
2‚à•‚àÜ‚à•1
34
so that (S21) becomes
Œª‚à•ÀÜŒ≥‚à•1 + Œª‚à•ÀÜŒ≤‚à•1 + Œªg‚à•ÀÜŒ≤‚àí0‚à•1,2 ‚â§Œª‚à•Œ≥‚à•1 + Œª‚à•Œ≤‚à•1 + Œªg‚à•Œ≤‚àí0‚à•1,2 + Œª
2‚à•ŒΩ‚à•1 + Œª
2‚à•‚àÜ‚à•1.
Since Œªg/Œª =
p
(sŒ≥ + sŒ≤)/sŒ≤,g, we have
2‚à•ÀÜŒ≥‚à•1 + 2‚à•ÀÜŒ≤‚à•1 + 2
s
sŒ≥ + sŒ≤
sŒ≤,g
‚à•ÀÜŒ≤‚àí0‚à•1,2 ‚â§2‚à•Œ≥‚à•1 + 2‚à•Œ≤‚à•1 + 2
s
sŒ≥ + sŒ≤
sŒ≤,g
‚à•Œ≤‚àí0‚à•1,2 + ‚à•ŒΩ‚à•1 + ‚à•‚àÜ‚à•1.
Now subtract by the left-hand side and add ‚à•ŒΩ‚à•1, ‚à•‚àÜ‚à•1, and
p
(sŒ≥ + sŒ≤)/sŒ≤,g‚à•‚àÜ‚àí0‚à•1,2 to find
‚à•ŒΩ‚à•1 + ‚à•‚àÜ‚à•1 +
s
sŒ≥ + sŒ≤
sŒ≤,g
‚à•‚àÜ‚àí0‚à•1,2 ‚â§2(‚à•Œ≥‚à•1 ‚àí‚à•ÀÜŒ≥‚à•1 + ‚à•ŒΩ‚à•1)
+ 2

‚à•Œ≤‚à•1 ‚àí‚à•ÀÜŒ≤‚à•1 + ‚à•‚àÜ‚à•1

+ 2
s
sŒ≥ + sŒ≤
sŒ≤,g

‚à•Œ≤‚àí0‚à•1,2 ‚àí‚à•ÀÜŒ≤‚àí0‚à•1,2 + ‚à•‚àÜ‚àí0‚à•1,2

,
Then using the triangle inequality and the fact that the terms in parentheses vanish outside of the
respective support sets SŒ≥, SŒ≤, and SŒ≤,g, we have
‚à•ŒΩ‚à•1 + ‚à•‚àÜ‚à•1 ‚â§4‚à•ŒΩSŒ≥‚à•1 + 4‚à•‚àÜSŒ≤‚à•1 + 4
s
sŒ≥ + sŒ≤
sŒ≤,g
‚à•‚àÜ(SŒ≤,g)‚à•1,2.
Since ‚à•‚àÜ(SŒ≤,g)‚à•1,2 ‚â§‚à•‚àÜSŒ≤‚à•1, the above yields
‚à•ŒΩScŒ≥‚à•1 + ‚à•‚àÜSc
Œ≤‚à•1 ‚â§4œÑj(‚à•ŒΩSŒ≥‚à•1 + ‚à•‚àÜSŒ≤‚à•1)
as desired, recalling again that œÑj = 1 +
p
(sŒ≥ + sŒ≤)/sŒ≤,g as defined in Assumption 5
Step 2
In this step, we bound the diagonal difference | ÀÜŒ£UW(‚Ñì, ‚Ñì) ‚àíŒ£UW(‚Ñì, ‚Ñì)|. Notice that ÀÜŒ£UW(‚Ñì, ‚Ñì)
is a sum of sub-exponential random variables with sub-exponential norm bounded by M 2M 2
Œ£,
where M is from Assumption 1 and MŒ£ from Assumption 3. We have by Lemma 4
P

| ÀÜŒ£UW(‚Ñì, ‚Ñì) ‚àíŒ£UW(‚Ñì, ‚Ñì)| > 1
2Œ£UW(‚Ñì, ‚Ñì)

‚â§2 exp

‚àícn min
Œ£UW(‚Ñì, ‚Ñì)2
4M 2M 2
Œ£
, Œ£UW(‚Ñì, ‚Ñì)
2MMŒ£

‚â§2 exp

‚àícn min

m2
0
4M 2M 2
Œ£
,
m0
2MMŒ£

‚â§2 exp(‚àíc6n)
where we used m0 ‚â§Œ£UW(‚Ñì, ‚Ñì) by Assumption 2. Since
| ÀÜŒ£UW(‚Ñì, ‚Ñì) ‚àíŒ£UW(‚Ñì, ‚Ñì)| ‚â§1
2Œ£UW(‚Ñì, ‚Ñì)
35
implies
1
2Œ£UW(‚Ñì, ‚Ñì) ‚â§ÀÜŒ£UW(‚Ñì, ‚Ñì) ‚â§2Œ£UW(‚Ñì, ‚Ñì)
which by Assumption 2 implies
m0
2 ‚â§ÀÜŒ£UW(‚Ñì, ‚Ñì) ‚â§2M0,
it follows from the above concentration bound that
P
m0
2 ‚â§ÀÜŒ£UW(‚Ñì, ‚Ñì) ‚â§2M0

‚â•1 ‚àí2 exp(‚àíc6n).
(S22)
Next we bound the off-diagonal terms. Again by Lemma 4
P

| ÀÜŒ£UW(‚Ñì, k) ‚àíŒ£UW(‚Ñì, k)| ‚â•2Œ£UW(‚Ñì, k)

‚â§2 exp

‚àícn min
4Œ£UW(‚Ñì, k)2
M 2M 2
Œ£
, 2Œ£UW(‚Ñì, k)
MMŒ£

‚â§2 exp

‚àícn min
 4M 2
0
M 2M 2
Œ£
, 4M0
MMŒ£

‚â§2 exp(‚àíc7n).
By Assumption 5, we have
‚àí2Œ£UW(k, ‚Ñì) ‚â§ÀÜŒ£UW(k, ‚Ñì) ‚àíŒ£UW(k, ‚Ñì) ‚â§2Œ£UW(k, ‚Ñì)
=‚áí‚àíŒ£UW(k, ‚Ñì) ‚â§ÀÜŒ£UW(k, ‚Ñì) ‚â§3Œ£UW(k, ‚Ñì)
=‚áí‚àí
1
c0(1 + 8œÑj)(sŒ≤ + sŒ≥) ‚â§ÀÜŒ£UW(k, ‚Ñì) ‚â§
3
c0(1 + 8œÑj)(sŒ≤ + sŒ≥)
Combining this with the above concentration bound yields
P

‚àí
1
c0(1 + 8œÑj)(sŒ≤ + sŒ≥) ‚â§Œ£UW(k, ‚Ñì) ‚â§
3
c0(1 + 8œÑj)(sŒ≤ + sŒ≥)

‚â•1 ‚àí2 exp(‚àíc7n).
(S23)
Step 3
Define the index set ÀúS = SŒ≥ ‚à™{q + i | i ‚ààSŒ≤}. The set {q + i | i ‚ààSŒ≤} indexes the columns of
the submatrix W of the matrix [U, W] corresponding to the support set of Œ≤. With slight abuse
of notation, for a vector v ‚ààRp(q+1)‚àí1 let v ÀúS ‚ààRp(q+1)‚àí1 be the vector that equals v on the set ÀúS
and is zero on ÀúSc, so that it has sŒ≥ + sŒ≤ nonzero elements. Define v ÀúSc analogously.
We will show that conditional on A1, A2, and the bounds
m0
2 ‚â§ÀÜŒ£UW(‚Ñì, ‚Ñì) ‚â§2M0,
‚àí
1
c0(1 + 8œÑj)(sŒ≤ + sŒ≥) ‚â§ÀÜŒ£UW(k, ‚Ñì) ‚â§
3
c0(1 + 8œÑj)(sŒ≤ + sŒ≥),
(S24)
we have
inf
v‚ààV
‚à•[U, W]v‚à•2
‚àön‚à•v ÀúS‚à•2
‚â•
r
m0
2 ‚àí1
c0
> 0.
36
where V = {v ‚ààRp(q+1)‚àí1 | ‚à•v ÀúSc‚à•1 ‚â§4œÑj‚à•v ÀúS‚à•1}
First we have that
‚à•[U, W]v ÀúS‚à•2
2
n‚à•v ÀúS‚à•2
2
=
v‚ä§
ÀúS diag( ÀÜŒ£UW)v ÀúS
‚à•v ÀúS‚à•2
2
+
v‚ä§
ÀúS ( ÀÜŒ£UW ‚àídiag( ÀÜŒ£UW))v ÀúS
‚à•v ÀúS‚à•2
2
‚â•m0
2 ‚àí
1
c0(1 + 8œÑj)(sŒ≤ + sŒ≥)
‚à•v ÀúS‚à•2
1
‚à•v ÀúS‚à•2
2
.
Then by ‚à•a + b‚à•2
2 = ‚à•a‚à•2
2 + 2a‚ä§b + ‚à•b‚à•2
2 we have
‚à•[U, W]v‚à•2
2
n‚à•v ÀúS‚à•2
2
‚â•‚à•[U, W]v ÀúS‚à•2
2
n‚à•v ÀúS‚à•2
2
+ 2
v‚ä§
ÀúS ÀÜŒ£UWv ÀúSc
n‚à•v ÀúS‚à•2
2
‚â•m0
2 ‚àí
1
c0(1 + 8œÑj)(sŒ≤ + sŒ≥)
‚à•v ÀúS‚à•2
1
‚à•v ÀúS‚à•2
2
‚àí
2
c0(1 + 8œÑj)(sŒ≤ + sŒ≥)
‚à•v ÀúS‚à•1‚à•v ÀúSc‚à•1
‚à•v ÀúS‚à•2
2
‚â•m0
2 ‚àí
1
c0(1 + 8œÑj)(sŒ≤ + sŒ≥)
‚à•v ÀúS‚à•2
1
‚à•v ÀúS‚à•2
2
‚àí
8œÑj
c0(1 + 8œÑj)(sŒ≤ + sŒ≥)
‚à•v ÀúS‚à•2
1
‚à•v ÀúS‚à•2
2
‚â•m0
2 ‚àí
1 + 8œÑj
c0(1 + 8œÑj)(sŒ≤ + sŒ≥)
‚à•v ÀúS‚à•2
1
‚à•v ÀúS‚à•2
2
‚â•m0
2 ‚àí(1 + 8œÑj)(sŒ≤ + sŒ≥)
c0(1 + 8œÑj)(sŒ≤ + sŒ≥) = m0
2 ‚àí1
c0
> 0.
In the last step we used that c0 > 2/m0 in Assumption 5. We have shown that
1
n
[U, W]
ŒΩ
‚àÜ

2
2
‚â•
m0
2 ‚àí1
c0

ŒΩ
‚àÜ

ÀúS

2
2
(S25)
Final step
It is true that for ‚Ñì‚àà[p(q + 1) ‚àí1] we have

ÀÜŒ£UW
ŒΩ
‚àÜ

‚Ñì
= ÀÜŒ£UW(‚Ñì, ‚Ñì)
ŒΩ
‚àÜ

‚Ñì
+
X
kÃ∏=‚Ñì
ÀÜŒ£UW(k, ‚Ñì)
ŒΩ
‚àÜ

k
.
Then by (S24) and the triangle inequality we have
 ÀÜŒ£UW(‚Ñì, ‚Ñì)
ŒΩ
‚àÜ

‚Ñì
 ‚àí


ÀÜŒ£UW
ŒΩ
‚àÜ

‚Ñì
 ‚â§
 ÀÜŒ£UW(‚Ñì, ‚Ñì)
ŒΩ
‚àÜ

‚Ñì
‚àí

ÀÜŒ£UW
ŒΩ
‚àÜ

‚Ñì

‚â§

X
kÃ∏=‚Ñì
ÀÜŒ£UW(k, ‚Ñì)
ŒΩ
‚àÜ

k

‚â§
3
c0(1 + 8œÑj)(sŒ≥ + sŒ≤)
X
kÃ∏=‚Ñì

ŒΩ
‚àÜ

k
.
Rearranging terms and applying (S24) yields
m0
2

ŒΩ
‚àÜ

‚Ñì
 ‚â§


ÀÜŒ£UW
ŒΩ
‚àÜ

‚Ñì
 +
3
c0(1 + 8œÑj)(sŒ≥ + sŒ≤)

ŒΩ
‚àÜ

1
.
37
Since this holds for all ‚Ñì‚àà[p(q + 1) ‚àí1], we have shown that

ŒΩ
‚àÜ

‚àû
‚â§2
m0
 ÀÜŒ£UW
ŒΩ
‚àÜ

‚àû
+
6
c0m0(1 + 8œÑj)(sŒ≥ + sŒ≤)

ŒΩ
‚àÜ

1
.
(S26)
Combining (S18) and (S20) we have
1
n
[U, W]
ŒΩ
‚àÜ

2
2
‚â§
 ÀÜŒ£UW
ŒΩ
‚àÜ

‚àû

ŒΩ
‚àÜ

1
‚â§3
2(Œª + Œªg) ¬∑ (1 + 4œÑj)

ŒΩ
‚àÜ

ÀúS

1
‚â§3
2(Œª + Œªg) ¬∑ (1 + 4œÑj)
p
sŒ≥ + sŒ≤

ŒΩ
‚àÜ

ÀúS

2
and combining this with (S25) gives us

ŒΩ
‚àÜ

ÀúS

2
2
‚â§3
2(Œª + Œªg) ¬∑ (1 + 4œÑj)

2c0
c0m0 ‚àí2
p
sŒ≥ + sŒ≤

ŒΩ
‚àÜ

ÀúS

2
and therefore

ŒΩ
‚àÜ

ÀúS

2
‚â§3(Œª + Œªg)(1 + 4œÑj)

c0
c0m0 ‚àí2
p
sŒ≥ + sŒ≤.
On the other hand, by (S20) we have

ŒΩ
‚àÜ

1
‚â§(1 + 4œÑj)

ŒΩ
‚àÜ

ÀúS

1
‚â§(1 + 4œÑj)
p
sŒ≤ + sŒ≥

ŒΩ
‚àÜ

ÀúS

2
so combining this with the above yields

ŒΩ
‚àÜ

1
‚â§3(Œª + Œªg)(1 + 4œÑj)2

c0
c0m0 ‚àí2

(sŒ≥ + sŒ≤)
Finally, plugging this into (S26) and with (S18) yields

ŒΩ
‚àÜ

‚àû
‚â§3
m0
(Œª + Œªg) + 18(Œª + Œªg)(1 + 4œÑj)2
c0m0(1 + 8œÑj)

c0
c0m0 ‚àí2

= 3
m0
(Œª + Œªg)

1 +
6(1 + 4œÑj)2
(1 + 8œÑj)(c0m0 ‚àí2)

as desired. ‚ñ°
38
