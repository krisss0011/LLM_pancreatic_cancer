NUTRIVISION: A SYSTEM FOR AUTOMATIC DIET MANAGEMENT
IN SMART HEALTHCARE
Madhumita Veeramreddy
Department of Computer Science & Engineering
SRM University AP, Amaravati, India
madhumita_v@srmap.edu.in
Ashok Kumar Pradhan
Department of Computer Science & Engineering
SRM University AP, Amaravati, India
ashokkumar.p@srmap.edu.in
Swetha Ghanta
Department of Computer Science & Engineering
SRM University AP, Amaravati, India
swetha_ghanta@srmap.edu.in
Laavanya Rachakonda
Department of Computer Science
University of North Carolina, Wilmington, USA
rachakondal@uncw.edu
Saraju P Mohanty
Department of Computer Science & Engineering
University of North Texas, Denton, USA
saraju.mohanty@unt.edu
September 29, 2024
ABSTRACT
Maintaining excellent health and fitness depends on Maintaining health and fitness through a balanced
diet is essential for preventing non-communicable diseases such as heart disease, diabetes, and
cancer. NutriVision combines smart healthcare with computer vision and machine learning to address
the challenges of nutrition and dietary management. This paper introduces a novel system that
can identify food items, estimate quantities, and provide comprehensive nutritional information.
NutriVision employs the Faster Region-based Convolutional Neural Network (Faster R-CNN), a
deep learning algorithm that improves object detection by generating region proposals and then
classifying those regions, making it highly effective for accurate and fast food identification even
in complex and disorganized meal settings. Through smartphone-based image capture, NutriVision
delivers instant nutritional data, including macronutrient breakdown, calorie count, and micronutrient
details. One of the standout features of NutriVision is its personalized nutritional analysis and diet
recommendations, which are tailored to each user’s dietary preferences, nutritional needs, and health
history. By providing customized advice, NutriVision helps users achieve specific health and fitness
goals, such as managing dietary restrictions or controlling weight. In addition to offering precise food
detection and nutritional assessment, NutriVision supports smarter dietary decisions by integrating
user data with recommendations that promote a balanced, healthful diet. This system presents a
practical and advanced solution for nutrition management and has the potential to significantly
influence how people approach their dietary choices, promoting healthier eating habits and overall
well-being. This paper discusses the design, performance evaluation, and prospective applications of
the NutriVision system.
Keywords Smart Healthcare, Smart Agriculture, Diet Management, Diet Estimation, Food Quality, Food Nutrition,
Convolutional Neural Networks (CNN)
arXiv:2409.20508v1  [cs.CV]  30 Sep 2024
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
I
Introduction
Indeed, the saying "wealth is nothing without health" underscores the pivotal role of nutrition in our overall well-being.
Nutrients derived from food are essential for the proper functioning of our bodies, and deviations from a balanced
diet can lead to various health issues. Fig. 1 illustrates the critical impact of both overeating and undernutrition on
the development of chronic illnesses such as diabetes, heart disease, kidney disease, hypertension, obesity, and even
life-threatening conditions like cancer ([52],[53]). Obesity and inflammation emerge as primary catalysts for many
diseases, and the consumption of certain food elements, including color additives, chemicals, trans fats, refined sugar,
salt, and processed foods, is linked to these health concerns ([54]). Consequently, individuals are modifying their
dietary habits to address these challenges, prompting a widespread focus on diet regulation. Achieving a balance
between dietary intake and monitoring is crucial for effective diet management.
Obesity
Reproductive
Disease
Insulin
Secretion
Blood Pressure
Cardiovascular
Diseases
Immune System
Problems
Kidney Disease
Inflammatory
Diseases
Figure 1: Impact of Unbalanced Diet
Despite the proliferation of nutritional tracking devices in the market, many rely heavily on user input, necessitating
manual entry of food intake ([55],[56]). This manual entry process can be cumbersome and time-consuming, leading
to users hesitating to utilize these systems consistently. Additionally, dependence on user input raises the risk of
inaccurate data entry, resulting in unreliable results. Commonly, these systems provide data on calorie quantities in
food, consumption levels, and remaining allowances for the day, aligning with their primary goals of promoting a
healthy weight and fitting into a particular diet framework. However, a crucial point emerges: being healthy doesn’t
always equate to adhering strictly to a low-calorie diet. A holistic approach to health considers various factors beyond
mere calorie counts, emphasizing the importance of balanced nutrition and overall lifestyle choices.
In response to these challenges, there is a growing need for innovative solutions that offer accurate nutri-
tional tracking without placing undue burdens on users, fostering sustained adherence and promoting a more
comprehensive understanding of healthful dietary practices ([57]). Individuals who have high blood pressure ought to
stick to a low-sodium diet, diabetics need to avoid sugar, and so on. Thus, monitoring sugar, sodium, saturated fat,
protein, carbs, and other nutrients is crucial.
The research presented here explores the intersection of personalized healthcare, nutritional science, and
computer vision, presenting a comprehensive system known as "NutriVision". The goal of the NutriVision system is
to address the growing demand for a more informed and health-conscious approach to food selection. Essentially,
NutriVision recognizes and detects food ingredients in user-provided photos using image recognition and AI-driven
algorithms. After that, it embarks on a complicated journey of nutritional evaluation, quantifying the macronutrients,
vitamins, and minerals present in the foods it has identified. However, what truly sets NutriVision apart is not just its
capacity to deliver an accurate nutritional analysis but also its capacity to deliver personalized dietary suggestions and
health guidance according to each user’s unique health reports and goals.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
Moreover, the adaptability of the NutriVision system to accommodate various dietary preferences and restrictions
ensures that it can serve a wide audience, including vegetarians, vegans, and individuals with food allergies. By
providing customizable options, NutriVision empowers users to make choices that align with their personal health
needs and lifestyle preferences. This versatility not only enhances user engagement but also promotes a more inclusive
approach to nutrition, ultimately fostering a supportive environment for diverse dietary practices.
Improve Recipe
Recommendations
BMI Calculation
User
Recipe
Recommendation
Queries Resolved
BMI Category
Height,Weight,
Gender
Food Preference
Feedback
Diet/Health
Queries
Processing 
Hub
Patient
Hub
Connectivity 
Hub
ECG
 Patch
Fitness
Tracker
Smart 
Contact
Lenses
Glucometer
Blood 
Pressure
Monitor
Cochlear
Implant
Smart Healthcare
GPS
Bluetooth
WiFi
5G
RFID
Satellite
Communication
NFC
Doctor
Cloud
Patient
User 
Interface
NutriVision
Database
Processor
Monitoring 
Vitals
Figure 2: Smart Healthcare as a Healthcare Cyber-Physical System (H-CPS)
Fig. 2 illustrates the Healthcare Cyber-Physical System (H-CPS) as a multi-layered model integrating various devices
and technologies to monitor and enhance patient health in real time. At the core is the Patient Hub, comprising devices
like fitness trackers, glucometers, and blood pressure monitors that directly track vital signs. Surrounding it, the
Processing Hub includes AI algorithms, cloud computing, and data processors that analyze and interpret the collected
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
data. NutriVision integrates within this hub as a nutritional management system, utilizing image recognition and
machine learning to assess food intake and provide personalized dietary guidance.
NutriVision’s role in H-CPS focuses on nutrition as a key element of preventive healthcare. By offering real-
time insights into food choices, it helps users make healthier decisions tailored to their unique health needs. The
Connectivity Hub ensures smooth communication between devices via 5G, Wi-Fi, and other networks, enabling
NutriVision to provide instant feedback and seamlessly integrate its recommendations into broader healthcare
monitoring. Through this smart integration, NutriVision supports healthier eating habits, reducing the risk of diet-related
diseases and contributing to overall health and wellness within the smart healthcare system.
As society increasingly recognizes the importance of nutrition in disease prevention and overall health, tools
like NutriVision stand to play a transformative role in empowering individuals to make informed dietary choices. By
leveraging advanced technologies such as image recognition and machine learning, NutriVision not only simplifies the
process of tracking nutritional intake but also enhances users’ awareness of their eating habits. This intuitive approach
fosters a deeper connection between individuals and their food choices, encouraging them to embrace healthier eating
patterns. Furthermore, by providing personalized insights and recommendations tailored to specific health conditions,
NutriVision aims to bridge the gap between dietary knowledge and practical application, ultimately supporting users in
achieving their health goals and improving their quality of life. This integration of technology and nutrition is poised to
contribute significantly to public health initiatives aimed at reducing diet-related diseases.
This is how the other parts of the paper are arranged.
The innovative aspects of the suggested solution are
discussed in Section II. We review several related works and provide background information for our work in Section
III. Together with the test findings, a thorough explanation of the recommended solution is given in Section IV. Section
V contains the results and some comparative analysis. The paper is concluded in Section VI with a discussion of
upcoming projects.
II
Novel Contributions of the Current Paper
A)
Problem Addressed
Current food nutrition monitoring systems often lack personalization and real-time estimation capabilities. Users
frequently face challenges with generic dietary advice that doesn’t account for individual health data. Additionally, there
is a need for systems that provide immediate feedback on the nutritional content of food and tailor recommendations
based on daily nutrient requirements.
B)
Solution Proposed by NutriVision
NutriVision addresses these issues with a state-of-the-art system that offers several significant advantages. First, it
provides automated, real-time nutritional content estimation, allowing users to understand the nutritional value of their
food before consumption. This feature ensures that users receive instant feedback on their meals, enhancing their
awareness of nutritional intake. Second, NutriVision integrates user-specific health data to deliver personalized dietary
recommendations tailored to individual needs, based on metrics such as BMI and dietary preferences. The system also
considers daily nutrient intake requirements when suggesting meals, ensuring that advice is relevant and promotes a
balanced diet. By eliminating the need for manual entry of food data, NutriVision streamlines the monitoring process,
reducing user effort and minimizing errors. Additionally, the system’s real-time adaptation feature allows it to adjust
recommendations based on continuous feedback and data, providing ongoing improvement in dietary guidance. This
combination of real-time data, personalization, and adaptability enhances user convenience, making NutriVision an
ideal solution for managing nutritional intake with minimal effort and maximum efficiency.
C)
Significance of the Proposed Solution
NutriVision’s innovative approach revolutionizes food nutrition monitoring by offering an interactive platform that
improves dietary habits. Its real-time estimation and personalized recommendations provide users with actionable
insights that are directly applicable to their daily lives. The system’s ability to adapt recommendations based on
feedback ensures continuous improvement and relevance. This combination of real-time data, personalization, and
adaptability makes NutriVision an ideal solution for those seeking to enhance their dietary health with minimal effort
and maximum efficiency.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
III
Related Work
In light of the contemporary surge in advanced hardware technologies, particularly in high-performance computing
processors and devices, coupled with the evolution of diverse computing paradigms such as the Internet of Things (IoT),
edge-based computing platforms, and tailored artificial intelligence (AI) models for edge computing, innovative models
like smart healthcare and intelligent agriculture have materialized into practical applications. Notably, the realm of
automatic dietary intake estimations has witnessed notable advancements, predominantly manifesting through mobile
app-centric solutions. This segment delves into pertinent research endeavors within this domain.
A systematic classification of diet monitoring approaches is shown in Fig.
3, which differentiates between
automated and manual techniques. Manual diet monitoring methods require individuals to actively engage in tracking
their food intake, with users being responsible for recording their dietary habits. One of the simplest forms is
maintaining a food journal, where individuals document what they eat throughout the day, either on paper or digitally
through spreadsheets or text files. This approach heavily relies on user memory and honesty, making it prone to
inconsistencies. In recent years, various smartphone apps have emerged to streamline this process, offering a digi-
tal interface for meal input and integrating nutritional databases to estimate caloric intake and macronutrient breakdowns.
Calorie counting is another widely used manual approach, involving the calculation of the caloric value of
consumed foods through nutritional labels, online resources, or databases. While some apps assist with this, users
remain responsible for ensuring the accuracy of their entries, introducing potential for human error. Issues such as
overestimating portions or misidentifying food types can lead to inaccurate caloric calculations, which may derail
dietary goals. These manual methods are often time-consuming and heavily dependent on user motivation, discipline,
and consistency. This reliance on active participation increases the likelihood of under- or over-reporting, as human
error—such as forgetting to log certain meals or incorrectly estimating portion sizes—significantly affect the results of
manual diet monitoring.
Diet Monitoring
Manual
Food Journals
Mobile Apps 
(User input 
based)
Calorie Counting
Automatic
Sensor-Based 
Monitoring
Hand and Mouth 
Movement 
Tracking
Food-Weight 
Sensing
Image-Based 
Monitoring
Physiological 
Monitoring
Glucose 
Monitoring
Biomarker 
Tracking
Figure 3: Diet Monitoring Approaches
On the other hand, automated approaches utilize cutting-edge technology to reduce user burden, making the
process more efficient, precise, and less prone to human error. One key approach in the automated category involves
sensor-based systems. For instance, food-weight sensing utilizes smart plates, bowls, or scales that can automatically
quantify the weight of the food. These systems are often integrated with AI to recognize food types and calculate
nutritional values. Another type of sensor-based approach tracks hand-to-mouth movement through wearable devices
like smartwatches or smart glasses. These wearables detect movement patterns and infer eating episodes by monitoring
hand gestures and bite counts. This method can automatically log meals and estimate intake frequency, reducing
reliance on self-reporting.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
Another advanced technique for automated diet tracking is image-based monitoring.
This approach lever-
ages artificial intelligence (AI) and image recognition technologies to analyze photos of food and estimate the portion
size and nutritional content. Users simply take a picture of their meal, and AI algorithms trained on large food datasets
identify the food items and generate estimates for calories, macronutrients, and portion sizes.
In addition to visual and sensor-based methods, physiological monitoring offers another layer of precision in
automated diet tracking. These techniques include continuous glucose monitoring (CGM) devices and biomarker
analysis. For example, CGM systems monitor blood sugar levels in real-time, providing insights into how specific
foods impact an individual’s glucose levels. Similarly, biomarker tracking analyzes bodily fluids such as saliva, blood,
or urine to detect key nutritional markers. These physiological indicators help to track metabolic responses and offer a
more comprehensive picture of how the body processes consumed foods.
An edge-cloud procedure has been used and food in the cloud is classified using a convolutional neural net-
work (CNN) in [1]. Although food has been identified, this work has not estimated nutritional value. The user is
informed whether or not he is stress eating, and some exercise is advised. Only the number of calories has been
estimated. It does not give any personalized diet advice to the users. While in [2], 19 classes of Food-101 dataset
has been identified. The user does not need to provide any kind of manual input , given it is fully automated. It uses
a CNN(convolutional neural network) approach for food detection. It provides estimation of nutrients and not just
calories. It has difficulty in handling customized food orders. But any kind of personalized advice and suggestions are
not given.
An edge-cloud method for dietary assessment has been used in [8]. The food images have been processed at
the edge devices using image textural features, and the food in the cloud is classified using a convolutional neural
network (CNN). Despite the fact that food has been identified in the Food-101 dataset, no nutritional value has been
estimated in this work. Real-time estimations of food attributes are presented in [9]. A textual corpus has been
employed for the identification of food attributes, integrating deep learning methodologies for the recognition of food
items. While the context of our work is the same, the nutrients are estimated based on the image.
Food nutrients were analyzed and calories were estimated using mask-RCNN and union post processing in
[10].
The estimation of food weight involves the utilization of both the pixel count within the mask and the
implementation of a linear regression model. Food serving sizes and their nutritional and calorific contents are
estimated in [20]. There has been no quantification in this case.
A deep autoencoder network has been used to determine the vitamin A content based on the color of the
pureed food in [21]. Only one particular nutrient has been found in this work. To detect food for diabetic patients,
researchers have implemented the bag-of-features model, as referenced in [18]. It calculates dense local features in
the HSV color space through scale-invariant feature transforms to generate a visual dictionary consisting of 10,000
words. Subsequently, food photos are classified using linear support vector machine classifiers. Here, no nutrient value
is computed. In a different investigation, the Food-Ingredient Joint Learning module was employed for ingredient
recognition, and the Attention Fusion Network was utilized for fine-grained food recognition, as documented in [22]. Al-
though the accuracy in food recognition is substantial, the accuracy in ingredient recognition varies from moderate to low.
For ingredient recognition and food classification, a deep CNN has been employed [19]. However, this work
does not show any nutritional value. [5] is dispersed across multiple computer platforms. There is a suggested food
calorie estimation app that is mobile and cloud-based. Here, the food is identified using a cloud-based Support Vector
Machine using the MapReduce technique. Calorie counting is done both before meals and after eating leftovers. For
that, image graph cut segmentation has been employed. To establish a reference point for measurement, it is essential to
have a thumb present within the frame of the food photo. The calorific value has also been determined here. Vari-
ations in thumb size are another problem. The person’s height affects the thumb size, which can lead to inaccurate results.
The work [11] introduces a model for food recognition based on Convolutional Neural Networks (CNN) and
utilizes text2vec for attribute estimation. The model is client-server based. But the system does not include cooked
foods, mixed foods, or liquid foods. The calorific value of the food is the main focus here as well. A wearable system
based on piezoelectricity was used in [23] to identify food and calculate caloric intake. When eating, skin movement
from the lower trachea is detected by the piezoelectric sensors. The food’s weight was determined, and then the final
calorie count was carried out. Here, no additional nutritional value has been tallied.
Diverse methodologies have been documented across several publications for determining calorific values
from food volume. Notably, a multilayer perceptron model is presented in [24], an image analysis-based approach is
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
outlined in [25], and a CNN-based method is detailed in [26]. Additionally, some works, including [27], extend their
focus beyond calorific values to encompass ingredient lists and cooking instructions, employing multitask CNN for this
purpose. In a distinct context, the work [28] categorizes various food types. This observation underscores that, while
numerous papers address the computation of calorific values, a comparatively limited number delve into the broader
spectrum of food nutritional value.
Several studies have utilized various methodologies for dietary intake estimation.
In the work [45], an inno-
vative solution using inertial sensors was introduced. This system tracks eating gestures and combines them with
image-based food identification algorithms to estimate calorie intake. However, it lacks real-time feedback and nutrition
advice tailored to the user. Similarly, [42] employed a multimodal approach that integrates visual and acoustic data for
food intake monitoring, though this system also focuses solely on calorific estimation rather than complete nutritional
analysis.
In an effort to automate dietary tracking, [49] proposed a smartphone-based system that detects food using
deep learning models. This system emphasizes ease of use, eliminating the need for user input, but suffers from
accuracy issues when it comes to complex food mixtures. Nutritional value estimation is limited to caloric content, with
no focus on personalized diet suggestions.
In [48], a cloud-based mobile app that allows users to capture images of their meals is used.
The food
recognition is powered by CNNs, and the app estimates macronutrient content. However, this approach is limited by its
reliance on cloud resources, which might hinder its performance in low-connectivity environments. Another study [46]
explored an AI-driven mobile app that utilizes food diaries and manual data entry to track food intake. While this tool
offers a holistic nutritional breakdown, it lacks automation, as users must input data manually.
Other researchers have focused on ingredient recognition as an intermediary step toward dietary analysis.
For instance, [47] introduced a model that uses a CNN to recognize ingredients in complex dishes. Their work
emphasizes ingredient classification rather than a complete nutritional breakdown. Similarly, [39] employed a
CNN-based approach for ingredient recognition in mixed dishes, but like [47], their system struggles with accurate
nutritional estimation.
A unique contribution is made by [44], where a system using a graph-based approach to food recognition is
discussed. This method segments the food image into portions and then uses those segments for caloric and nutritional
estimation. However, the system is prone to inaccuracies when dealing with complex meals. [50] tackled this issue
by proposing a method that integrates portion size estimation with ingredient recognition, though their model is still
primarily concerned with calorie counting rather than a full nutritional assessment.
In [38], a hybrid system that combines image analysis with manual data entry for diet tracking is introduced.
Although effective in estimating calories and basic nutrients, it lacks the level of automation seen in other state-of-the-art
systems. Another notable mention is the research conducted by [43], which explored deep learning algorithms for
estimating the nutritional value of food in a fully automated way. However, the model was trained on a limited dataset,
and its real-world application may be hindered by a lack of diversity in the food items it can recognize.
In a similar way, [41] presented a dietary tracking system that uses a knowledge-based approach, drawing
on food ontologies to estimate nutrients and suggest personalized meal plans. While innovative, the system relies
heavily on an external database, which may not cover all local food variations. Another edge-cloud system designed
by [51] focused on identifying food in real-time but offered limited nutritional data, focusing primarily on calorie
estimation. The study conducted by [40] uses a mask-RCNN approach to segment food images and estimate portion
sizes for caloric calculation. While their system is effective for identifying meal portions, it does not extend to broader
nutritional analyses, leaving a gap in personalized dietary recommendations.
Table 1 offers an overview of several studies for a more comprehensive context, highlighting significant re-
sults and insights from a variety of research work. By comparing these systems side by side, the table underscores the
unique contributions of NutriVision, particularly in areas where other systems fall short, such as nutritional analysis and
the provision of an interactive platform.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
Table 1: Comparative Analysis of Self-Tracking Systems
Tracking System
Input
Analysis
Fully-
Automated
System
Nutritional
Value Es-
timation
Personalized
Advice
Interactive
Platform
Harrison,
et
al.
(2010) [29]
Image
Not Feasible
No
No
No
No
O. Beijbom, et al.
(2015) [30]
Image
Not Entirely Possible
to be Done
Semi-
Automated
No
No
No
Jiang, et al. (2018)
[31]
Image
Not Very Beneficial
Semi-
Automated
No
No
No
Pouladzadeh, et al.
(2015) [34]
Image
Not Very Beneficial
Semi-
Automated
No
No
No
L. Rachakonda, et al.
(2019) [35]
Image
Yes
Semi-
Automated
No
No
No
Taichi, et al. (2009)
[32]
Image
No Nutritional Info
No
No
No
No
L. Rachakond, et.al
(2020) [1]
Image
Yes, Calorie Estima-
tion and Stress Detec-
tion
Yes
No
No
No
A.
Mitra,
et
al.
(2022) [2]
Image
Yes, Nutrition Esti-
mation
Yes
Yes
No
No
M.-L.Chian,
et al.
(2019) [10]
Image
Yes, Nutritional Esti-
mation
Semi-
Automated
Yes
No
No
P. Pouladzadeh, et al.
(2014) [5]
Image
Yes, Calorie Estima-
tion
Semi-
Automated
No
No
No
NutriVision
(Current-Paper)
Image
Yes, Nutrition Es-
timation and Per-
sonalized Nutrition
Analysis
Yes
Yes
Yes,
Food
recommen-
dations
based
on
health
his-
tory
and
BMI.
Yes,
it
answers
diet
and
health
queries.
IV
The Innovative Framework: NutriVision
A)
Framework
This section details the intricacies of food nutritional value estimation and personalization, mirroring the process
illustrated in Fig. 4. The user leverages her smartphone to capture an image of the food alongside a reference object,
facilitating the determination of the nutritional value of the food. In this section, comprehensive details regarding the
reference object are expounded. Subsequent to capturing the image, features specific to the food are extracted and
employed for food detection. The nutritional value is then determined by comparing the identified food object with
entries in a nutrition database, representing average serving sizes. Discrepancies between the serving size and stored
values may exist, necessitating food quantification. Hence, the reference object plays a crucial role. Following the
quantification process outlined in this section, the nutritional values are presented. Fig. 5 illustrates the developmental
workflow of NutriVision, spanning from dataset collection to inference, showcasing the comprehensive journey of the
system. This workflow highlights the importance of accuracy in both image processing and database comparisons, as
these steps are vital for ensuring reliable nutritional assessments.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
Object Detection
Photo Taken
Food Quantification
Nutritional Estimation
Nutritional Analysis
Figure 4: Process Workflow of NutriVision
Dataset Collection
Reference
Detection
Image
Preprocessing
Image
Augmentation
Food Detection
Model Training
Trained Model
Object Detection
Model
User
Nutrition
Estimation
Food
Quantification
Feedback
Nutritional Analysis
Output
User
Image
Nutrition Database
User 
Health 
History
Figure 5: Development Workflow of NutriVision
1)
Acquisition of Dataset
Food classification employs a tailored and customized dataset, incorporating images sourced from the COCO Dataset[15]
and customized dataset [14]. In contrast to COCO’s extensive set of 91 classes, our customized dataset is streamlined to
encompass a total of 10 specific classes. The annotation of images with bounding boxes is conducted through Roboflow
[33]. The dataset comprises a total of 500 images, with 450 designated for training and 50 for testing, encompassing
the 10 defined classes: Apples, Oranges, Pizza, Broccoli, Cake, Donut, Sandwich, Carrot, Banana, and Hotdog. It is
noteworthy that, for addressing nutritional aspects, a distinct custom dataset has been employed.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
2)
Reference Detection
In food quantification, a critical consideration is the selection of a reference for accurate measurement. The size of
the food plate in a given picture can vary based on the proximity of the smartphone camera to the table during photo
capture. Quantifying food can be achieved through various methods, including measuring the distance between the
plate and the phone, employing image segmentation, or utilizing a reference object. For simplicity, we opt for the third
method, involving the use of a reference object, in this instance. Specifically, a one rupee coin with a diameter of 21.93
millimeters is employed as the reference given in Fig. 6. Positioned on the same table next to the plate, the coin and the
food are captured within the same frame during photo capture. The detection of the coin, facilitated by Algorithm 1,
establishes it as a reference. As outlined in Section V, the portion size of the identified food is determined by leveraging
the dimensions of the detected coin.
Algorithm 1: Detection of Reference Coin
1. Colour of the coin is set.
2. The image is transformed from RGB to HSV colour space.
3. The area of the coin is calculated.
4. Determine the ratio between the image and the coin’s actual sizes.
5. Locate all the contours.
6. Coin’s contour is selected.
7. Minimum Bounding Box is determined.
8. Note the Bounding Box’s Length and Width.
21.93mm
Figure 6: Reference Selection of Nutrivision
3)
Image Preprocessing and Augmentation
The images have been subjected to preprocessing, involving size and normalization adjustments before undergoing data
augmentation. In an effort to enrich the diversity of the training dataset, additional data was introduced. This involved
cropping the images with a minimum scale size of 0.1 and a maximum scale size of 2.0, along with horizontal flipping.
The setting for maximum dimension padding has been retained as True.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
4)
Model Training
The feature extractor for the object detection model utilizes a pre-trained Faster R-CNN model [16] on the COCO
dataset [7]. The application of transfer learning not only streamlined the training process but also significantly improved
accuracy. Faster R-CNN is well-regarded for its effective object detection capabilities [17], incorporating a Region
Proposal Network (RPN) with a convolutional neural network (CNN) backbone, such as ResNet. The Region Proposal
Network (RPN) within Faster R-CNN is integral to the model’s functionality, handling the localization of objects.
Specifically, the RPN is responsible for generating region proposals, representing potential bounding boxes around
objects within the image. Subsequently, these region proposals contribute to the precise localization of objects. The
collaborative operation of the RPN and the feature maps extracted from the ResNet-50 backbone [6] plays a crucial role
in proposing regions of interest within the image.
5)
Food Quantification and Nutrition Estimation
After identifying the food and reference coin in the image, a comparison is made between the names of the food items
in the nutritional database and the detected food label. The nutritional values from the dataset that have been stored
correspond to predefined values for a specific portion size. Algorithm 2 is used to properly quantify the food that has
been detected.
Algorithm 2: Nutritional Value Estimation from Food Image
Input: Image to be tested
Output: Nutritional Value of the food
1. Take a picture of a plate of food from above with a one rupee coin next to it.
2. To generate bounding boxes of recognized food on the plate, run the image through the food detection model.
3. Use Algorithm 1 to measure the coin area in relation to the image.
4. Determine the ratios between the 1 rupee coin’s actual diameter (21.93 millimeters) and the identified coin’s
pixel diameter in the picture.
5. To determine the true dimensions of the observed food, multiply the bounding box dimensions by the
corresponding ratios.
6. Use the food’s preset generalized height based on the class of food that has been detected.
7. Determine the identified food’s total volume in cubic centimeters by calculating its length, width, and height.
8. To take into consideration the additional area that comes from the rectangular boundary boxes, multiply this
volume by a constant factor (such as 0.8).
9. Calculate the number of grams of the particular food on the plate by using the cubic centimeter to gram
converter.
10. Lastly, use the nutrition database to derive the associated nutritional value.
Here, the food’s actual dimension is determined by contrasting it with the object of reference. To account for the excess
space in the rectangular bounding box, the computed volume is multiplied by 0.8. In the rectangular bounding box,
there are empty spaces where there is no food because most of the food is served in circular or oval plates. Following
the estimation of the number of pixels within the bounding box’s empty space in comparison with the bounding box as
a whole, the value of 0.8 was carefully selected and confirmed across multiple images.
Finally, the nutritional values of the food on the plate are approximated by converting the volume into weight (using
data from the Food-A-Pedia[12] database). After the calculation of the food quantity, the nutritional value is estimated
by using the custom nutrition database which contains the nutrient information of different foods for a single serving
(100 grams). The nutritional content of the detected food is calculated accordingly.
6)
Nutritional Analysis
In this research study, we use the latest technologies and sophisticated algorithms to present a technically sophisticated
method to individualized nutritional analysis by integrating collaborative filtering approaches with content-based
filtering techniques. Important information is extracted from consumers’ health reports via the content-based filtering
component using machine learning methods, notably natural language processing (NLP).
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
A thorough user profile is created using this data, which also includes nutritional objectives, dietary limita-
tions, and medical concerns. To convert text data into numerical vectors for the NLP model, we use the TF-IDF
(Term Frequency-Inverse Document Frequency) vectorizer from the scikit-learn module. By allocating weights
based on word frequency in relation to the complete document collection, TF-IDF efficiently captures the meaning
of each word within the context of a given document.
Each word in the user’s health history correlates to
a feature, and the history is handled like a document.
After processing this text data, the TF-IDF vectorizer
generates numerical vectors for each user that indicate the significance of different health-related phrases. Then,
cosine similarity between these user vectors and the meal descriptions is computed in order to find the closest
matches and produce customized food recommendations. We use matrix factorization and Singular Value De-
composition (SVD) methods for the collaborative filtering component to uncover latent patterns in user-item interactions.
By using these methods, the algorithm may find minute relationships between food items and users, which
helps to improve the recommendations even further. To guarantee effective model training and suggestion production,
we include these algorithms into sophisticated programming frameworks like PyTorch or TensorFlow. This hybrid
strategy combines the best features of both filtering techniques, enhancing the accuracy and applicability of dietary
advice by combining cutting-edge technology and complex algorithms.
An intelligent chatbot that converses with users to obtain information about their dietary preferences, past
medical conditions, and food selections facilitates the entire process. Along with gathering this data, the chatbot also
provides real-time, customized recommendations, according to user reaction and continued communication.
Improve Recipe
Recommendations
BMI Calculation
Warning is given
Recipe
Recommendation
If 
carbs OR
sugar>advised
limit 
User
Recipe
Recommendation
Queries Resolved
BMI
Category
No
User  Health 
History
Yes
Height,Weight,
Gender
Food
Preference
Feedback
Diet/Health
Queries
Figure 7: Workflow of Nutritional Analysis
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
The nutritional analysis module’s workflow is depicted in Fig. 7, which also emphasizes the chatbot’s contribution to
personalized recipe recommendations based on user health data. First, based on the user’s medical history, a recipe
recommendation is generated. A warning is sent if the suggested amounts of sugar or carbohydrates are higher than the
safe thresholds. Following that, users offer feedback on the recommendations, allowing for ongoing improvement of
the list. In order to update the suggestions in light of changing food patterns, the system also computes BMI. Some
more recipe recommendations are given according to the BMI category and nutritional content of previously consumed
meals. Lastly, users can ask the chatbot more questions to make sure their decisions support their health objectives.
This technical connection demonstrates our commitment to offering a robust and cutting-edge personalized nutritional
analysis solution, ensuring that users receive timely and accurate dietary recommendations tailored to their individual
health needs.
V
Validation of NutriVision
A)
Validation
A carefully chosen set of 200 photos was used in the model evaluation process to assess how well the categorization
system worked; the outcomes are shown in Fig. 8. Even after localization, the model’s ability to categorize food
products has shown to be very accurate, producing exact results almost all of the time. The model classifies and
recognizes various food products with remarkable accuracy. But the assessment also identified several difficulties. In
particular, when the photos show food items next to one another with little space between them on the plate, it may
compromise the accuracy of the model.
Figure 8: Classification by NutriVision
In these situations, the model’s confidence scores typically lean toward the middle rather than the high end, suggesting
that the model is less certain of its classifications. More diversity in the dataset is considered to be crucial in order to solve
these issues and raise the model’s confidence scores in complex food plating scenarios. The model would be better able
to manage various meal presentations and increase its accuracy under difficult circumstances with a more diverse dataset.
Using Algorithm 2 for food quantification on our dataset, Fig. 9 displays the nutritional estimates for two example food
plates. This shows that the model can infer nutritional values from the detected and quantified food items in the pictures.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
Food 
item
Quantity
(grams)
Calories 
(kcal)
Carbs
(grams)
Protein 
(grams)
Saturated 
Fat 
(grams)
Sugar 
(grams)
Potassium 
(mg)
Iron 
(mg)
Fiber 
(grams)
Calcium 
(mg)
Pizza
790.91
2182.9
237.27
94.91
39.55
23.73
1273.36
7.91
7.91
1660.9
Cake
500.59
1835.8
291.23
20.9
23.97
153.82
1003.91
11.4
14.67
215.59
Pizza(99.32)
Cake(99.5)
Figure 9: Nutritional Value Estimation by NutriVision
Table 2: Performance Metrics of NutriVision
Types
Metric
Value
Classification (Training)
Accuracy
95%
Precision
84%
Recall
82%
Localization
IoU
61%
Classification (Testing)
Accuracy
92%
Precision
82%
Recall
79%
Table 2 provides a comprehensive overview of the NutriVision system’s performance data, emphasizing the
main functionalities. Several metrics, such as accuracy, precision, recall, and intersection-over-union (IoU), have been
calculated for classification purposes, offering a thorough evaluation of the system’s performance.
Detailed graphical representations of the classification loss, localization loss, and overall total loss evolution
during the object detector’s training phase are shown in Fig. 10. Comprehension of the model’s performance dynamics
requires a comprehension of these visuals, which provide a thorough grasp of how the object detector changes over
time in order to handle classification jobs, achieve localization precision, and control the combined loss metrics.
The classification loss graph, which decreases during training, illustrates the model’s growing accuracy in
differentiating between object classes. In the same way, the localization loss graph shows increased accuracy in
identifying food items in photos—even in intricate situations. The overall total loss graph shows how well the model
locates and recognizes food items by combining the losses from localization and classification by plotting the total loss
versus number of steps using Faster RCNN. These visual aids demonstrate NutriVision’s present performance as well as
opportunities for improvement. The model appears to be becoming more reliable, as evidenced by the gradual decrease
in overall loss during training.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
Figure 10: a) Total Loss of Faster RCNN, b) Localization Loss of Faster RCNN, c) Classification Loss of Faster RCNN
The two main visual representations of the data produced by the NutriVision system are the macronutrient
distribution graph in Fig. 11 and the macronutrient composition in grams graph in Fig. 12. These graphs, which give
both a general summary and a thorough breakdown of the macronutrient composition, are essential for comprehending
the nutritional value of the food products that are being studied.
Carbohydrates
60%
Proteins
24%
Fats
10%
Sugars
6%
Figure 11: Macronutrient Distribution
The macronutrient distribution graph (Fig. 11) breaks down the four main macronutrients (carbohydrates, proteins,
sugars, and fats) in the dietary items under analysis as percentages. Users can rapidly ascertain if a meal is strong in
carbohydrates, high in protein, or balanced in terms of all macronutrients by referring to this graph, which provides
a clear and short assessment of the nutritional balance of a meal. For individuals looking to control their intake of
macronutrients, the visual representation is a priceless tool as it provides an instantaneous evaluation of whether a meal
fits with their dietary objectives.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
On the other hand, the macronutrient composition in grams graph (Fig. 12) gives an accurate assessment of every
macronutrient present in the food items. This graph provides precise numbers for the amount of carbs, proteins, carbs,
and sugars consumed, measured in grams, in contrast to the percentage-based distribution. Users can precisely trace
their consumption by performing nutritional evaluations and dietary planning with this specific composition. This
graph helps users make better meal choices by providing information on the precise amounts of each macronutrient,
making it easier for them to accurately meet their nutritional needs. When combined, these graphs provide a thorough
understanding of meal nutritional composition. Combining these visual aids improves the user’s comprehension and
control over dietary intake by enabling them to assess food patterns and make educated decisions.
0
100
200
Carbohydrates
237
Proteins
45
Sugars
95
Fats
40
Figure 12: Macronutrient Composition in grams
Figure 13: Personalized Diet Advice
Fig.
13 presents an intuitive user interface that walks users through a customized dietary analysis procedure,
greatly improving the user experience. Users are required to enter their unique user ID, which is connected to their
dietary and personal health information, when they first use the interface. After the user ID is entered, the system
processes the data using sophisticated algorithms to provide the user with individualized nutritional recommendations
based on their health history. Next, based on the user’s individual dietary requirements, the interface shows the top five
food recipes that are suggested. Based on the user’s dietary preferences, nutritional objectives, and medical history,
these recommendations have been carefully selected. Users can easily follow along and replicate the dishes at home
with the help of the interface, which also offers a link to a corresponding recipe video on YouTube.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
Furthermore, an essential component of the interface raises user awareness of possible nutritional concerns. A text
alert alerts the user if a suggested meal has more sugar or carbohydrates than is healthy for them. Because each user
has different limits on how much sugar and carbohydrates they can consume based on their dietary preferences and
medical history, this alarm system is tailored specifically for them. With customized restrictions kept in a separate
database connected to user IDs, these notifications are customized for each individual user, guaranteeing feedback and
monitoring that is specific to them.
Through the integration of individualized alerts, the NutriVision system guarantees users are aware of any
possible nutritional issues, empowering them to make healthier and safer choices. Overall, the NutriVision system’s
dedication to offering a thorough, customized, and user-friendly nutritional analysis tool that enables users to make
educated and health-focused dietary decisions is highlighted by the combination of these visual representations and the
intuitive interface in Fig. 13.
Figure 14: Feedback System of NutriVision
A simple feedback exchange is shown in Fig.
14, where the user is asked about their experience using the
recommended recipes. This exchange facilitates the collection of basic information regarding user participation and
meal completion. It also offers insightful information to enhance the recommendation system. By analyzing user
feedback, the system can adapt to individual preferences and improve the quality of future recommendations. This it-
erative process not only increases user satisfaction but also fosters a more personalized and engaging culinary experience.
The utilization of input data, such as height, weight, and gender, to calculate a user’s Body Mass Index
(BMI) is another feature of NutriVision. Following calculation, this BMI is divided into four groups: underweight,
normal weight, overweight, and obese. NutriVision creates customized meal suggestions based on a user’s BMI
category, taking into account their dietary preferences, whether they are vegan, non-vegetarian, or vegetarian. The
suggestion algorithm of the system is made to guarantee a diet rich in variety.
In addition to taking into account the user’s BMI and dietary preferences, it assesses the nutritional value of
meals the user has previously eaten. NutriVision can detect any potential nutritional gaps, such as an inadequate intake
of carbohydrates, proteins, or fats, by examining the nutritional values of these previous meals. Subsequently, the
algorithm customizes its meal recommendations to compensate for these inadequacies, encouraging a well-rounded diet
that enhances the user’s general health and wellbeing.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
Figure 15: Food Recommendation System-Case(i)
Fig.
15 presents a detailed interaction between the chatbot and a female user who maintains an average
weight and strictly follows a vegan diet. This figure highlights how NutriVision adeptly customizes its meal recommen-
dations to cater to the user’s particular dietary preferences and overall health condition. By doing so, NutriVision ensures
that the recommendations provided are not only aligned with her specific needs but also promote a well-balanced and
nutritious diet, demonstrating its capability to adapt to various dietary restrictions while maintaining nutritional integrity.
Figure 16: Food Recommendation System-Case(ii)
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
On the other hand, Fig.
16 depicts NutriVision’s tailored suggestions for a female user classified as under-
weight. This figure illustrates the system’s capability to adjust its recommendations based on the user’s specific BMI
category. By offering carefully considered nutritional guidance, NutriVision focuses on helping the user achieve a
healthier weight through a strategic selection of foods. The system’s ability to adapt its recommendations according
to the user’s unique health profile underscores its effectiveness in providing personalized dietary advice aimed at
promoting overall well-being. This personalized approach not only enhances user satisfaction but also empowers
individuals to make informed choices that support their health goals.
Figure 17: Food Recommendation System-Case(iii)
Fig.
17 illustrates the dietary recommendations provided by NutriVision for an overweight male user, fur-
ther emphasizing the system’s remarkable versatility. NutriVision’s method is designed to deliver personalized meal
recommendations that actively support the user in achieving a healthier weight, considering the distinct dietary needs
and weight management goals associated with this particular BMI category. Beyond its ability to generate tailored
meal plans, NutriVision also boasts an interactive chatbot function that significantly enhances user engagement and
customer support. Users have the flexibility to engage with the chatbot by asking detailed questions about nutrition,
health-related topics, or specific meal recommendations provided by the system. This interactive feature ensures that
users can receive immediate guidance and clarifications, transforming NutriVision into a dynamic resource for ongoing
nutritional education and personalized meal planning.
Moreover, NutriVision’s commitment to fostering optimal health through individualized, data-driven nutri-
tion guidance is vividly demonstrated by its holistic approach, which integrates BMI-based dietary recommendations
with its interactive chatbot feature. The system emerges as a highly effective tool for assisting users in achieving and
maintaining a balanced and nutritious diet, thanks to its ability to adapt recommendations based on a comprehensive
understanding of the user’s BMI, dietary preferences, and previous nutritional intake. This makes NutriVision not only
a practical solution for personalized meal planning but also a valuable educational resource that empowers users to
make informed decisions about their nutrition and overall health.
B)
Comparison with existing research
While the system in [1] can identify meals and estimate calories, it does not provide personalized advice or
comprehensive calorie estimations that are customized for each user. In a similar way, [2] provides identification tools
along with nutritional information on a variety of cuisines, although it does not offer tailored dietary suggestions. The
literature study reveals a clear gap in current systems: some concentrate on food recognition, but they frequently fail to
automate quantity estimation, requiring food amounts to be manually entered.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
NutriVision, on the other hand, stands out as a unique remedy that successfully resolves these shortcomings. It
outperforms systems such as [1] and [2] in that it incorporates extensive functionalities that cover meal recognition and
calorie estimation in addition to offering tailored advice and accurate nutritional assessments. NutriVision stands out
from its predecessors by providing a comprehensive and personalized nutritional assessment. It markets itself as a
comprehensive and cutting-edge solution for consumers looking for precise and tailored dietary guidance.
Compared with [1] and [2] object identification techniques, NutriVision uses Faster R-CNN, a region-based
convolutional neural network (CNN) adaption that is well-known for its higher efficacy in picture classification and
localization tasks. When compared to conventional CNN techniques, this sophisticated adaption of CNN significantly
enhances image processing speed and efficiency. In numerous crucial areas related to nutritional calculation and user
guiding, faster R-CNN as used in NutriVision displays distinct advantages over traditional CNN algorithms. It is
particularly good at differentiating between things that look alike, which is important when giving proper dietary rec-
ommendations for foods that might look alike. This feature is especially helpful in guaranteeing accurate dietary advice.
Table 3: Comparison of Object Detection Algorithms
Metric
Faster R-CNN
CNN
Mask R-CNN
YOLO
Accuracy
90%
84%
87%
82%
Precision
81%
77%
79%
75%
Recall
79%
73%
76%
70%
Additionally, NutriVision makes use of Faster R-CNN’s ability to identify several food items on a single
plate—a crucial component of a thorough nutritional analysis. Faster R-CNN improves accuracy and dependability in
nutritional analysis applications by precisely identifying and differentiating minute visual differences between different
food items thanks to its fine-grained detection capabilities. The performance metrics for the NutriVision dataset shown
in Table 3 illustrate how Faster R-CNN’s stronger localization ability translates into higher precision and accuracy in
food detection.
Our experimental results show that, over a wide variety of assessment parameters, Faster R-CNN performs
consistently better than YOLO, Mask R-CNN, and conventional CNN models. The Evaluation Metrics graphs for
several object detection algorithms given in Fig. 18 clearly demonstrate this higher performance.
The Training Time vs.
Epochs graph (Fig.
18a) demonstrates that Faster R-CNN exhibits greater accuracy
and convergence while requiring a longer training period. This graph illustrates how Faster R-CNN outperforms the
other models in terms of increasing accuracy over time. Faster R-CNN yields the highest values in terms of F1 score,
indicating a better trade-off between recall and precision. Fig. 18b shows the F1-Score vs. Epochs graph, which reveals
that Faster R-CNN consistently achieves a higher F1 score.
Furthermore, the graphs displaying Recall vs.
Epochs (Fig.
18c) and Precision vs.
Epochs (Fig.
18d) sup-
port the superiority of Faster R-CNN. These graphs highlight the efficacy of Faster R-CNN in reliably detecting objects
by showing that it consistently achieves the highest recall and precision values. These thorough results show that Faster
R-CNN is the most trustworthy and efficient method for achieving overall detection accuracy. It is hence the best option
for tasks involving object detection.
The most distinctive characteristic of NutriVision that other modern platforms do not offer is the degree of
customisation offered. Personalized dietary suggestions are made by NutriVision using a thorough examination of the
user’s past and BMI. The system can provide the user with tailored recommendations that promote long-term health
goals while also satisfying their immediate nutritional demands by incorporating historical food decisions, health
conditions, and BMI data.
NutriVision is different from other systems because of its advanced user feedback and customization fea-
tures, which allow for a highly customized and dynamic user experience. Users have the ability to directly comment on
the meal recommendations they get, regardless of whether they are based on dietary requirements, taste preferences, or
particular nutritional objectives. Through this feedback system, NutriVision is able to continuously improve and modify
its recommendations so that they more closely suit the unique requirements and preferences of each user. Through
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
this iterative process, the system’s recommendations are continuously refined, leading to a more precise and fulfilling
nutritional plan.
(a) Training Time vs Epochs Graph
(b) F1-Score vs Epochs Graph
(c) Recall vs Epochs Graph
(d) Precision vs Epochs Graph
Figure 18: Evaluation metrics graphs of different object detection algorithms
VI
Conclusion and Future Work
When it comes to our health, food is paramount. Studies [3], [4] show that preventing diseases largely depends
on maintaining a healthy diet, making it crucial to monitor not just calorie intake but also other nutrients. This
paper proposes the NutriVision system for food detection, nutritional value estimation, and personalized diet advice.
NutriVision uses advanced algorithms like Faster R-CNN for accurate meal identification, enhancing food detection
and nutritional analysis reliability. The system automates nutritional value calculation from food images, improving
accuracy by determining food volume and converting it to weight. However, it has a limitation in accounting for
partially consumed meals. Personalized recommendations are based on the user’s health profile, including BMI and
dietary preferences, ensuring tailored suggestions. The user-friendly interface allows users to input their unique ID and
receive customized dietary advice, including recipe recommendations and links to instructional videos, improving the
overall user experience. The system offers real-time alerts for excessive sugar or carbohydrate intake, helping users
make informed decisions, and continuously learns from user feedback to refine its recommendations.
In our forthcoming endeavors, we aim to introduce a food quantification system that does not use any refer-
ence, emphasizing the need to improve precision and Intersection over Union (IoU) metrics. This system will undergo
expansion to accommodate a broader array of food items, enhancing its versatility and applicability. Additionally, each
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
cuisine category within the system is slated to receive updates. Future proposals encompass implementing cutting-edge
precision techniques, leveraging the latest in machine learning and computer vision advancements, designing a
user-friendly interface, enabling real-time updates for adaptability, and integrating the system with dietary guidelines to
offer users insightful nutritional information for health-conscious choices.
References
[1] L. Rachakonda, S. P. Mohanty, and E. Kougianos, “iLog: An Intelligent Device for Automatic Food Intake Monitoring and
Stress Detection in the IoMT”, IEEE Transactions on Consumer Electronics (TCE), Vol. 66, No. 2, May 2020, pp. 115–124.
[2] A. Mitra, S. Goel, S. P. Mohanty, E. Kougianos, and L. Rachakonda, “iLog 2.0: A Novel Method for Food Nutritional Value
Automatic Quantification in Smart Healthcare”, in Proceedings of the IEEE International Symposium on Smart Electronic
Systems (iSES), 2022, pp. 683–688
[3] H. Cena and P. C. Calder, “Defining a healthy diet: evidence for the role of contemporary dietary patterns in health and disease,”
Nutrients, vol. 12, no. 2, p. 334, 2020.
[4] D. R. Wahl, K. Villinger, L. M. Konig, K. Ziesemer, H. T. Schupp, and B. Renner, “Healthy food choices are happy food
choices: Evidence from a real life sample using smartphone based assessments,” Scientific Reports, vol. 7, no. 1, pp. 1–8, 2017.
[5] P. Pouladzadeh, P. Kuhad, S. V. B. Peddi, A. Yassine, and S. Shir- mohammadi, “Mobile cloud based food calorie measurement,”
in Proc. of IEEE International Conference on Multimedia and Expo Workshops (ICMEW), 2014, pp. 1–6.
[6] Tahir, Hassam & Khan, Muhammad Shahbaz & Tariq, Muhammad Owais. (2021). Performance Analysis and Comparison of
Faster R-CNN, Mask R-CNN and ResNet50 for the Detection and Counting of Vehicles. 587-59.
[7] Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., & Dollár,
P. (2015). Microsoft COCO: Common objects in context. arXiv. https://arxiv.org/abs/1405.0312 last accessed on September
20,2023
[8] C. Liu, Y. Cao, Y. Luo, G. Chen, V. Vokkarane, M. Yunsheng, S. Chen, and P. Hou, “A New Deep Learning-Based Food
Recognition System for Dietary Assessment on An Edge Computing Service Infrastructure,” IEEE Transactions on Services
Computing, vol. 11, no. 2, pp. 249–261, 2018.
[9] R. Yunus, O. Arif, H. Afzal, M. F. Amjad, H. Abbas, H. N. Bokhari, S. T. Haider, N. Zafar, and R. Nawaz, “A Framework to
Estimate the Nutritional Value of Food in Real Time Using Deep Learning Techniques,” IEEE Access, vol. 7, pp. 2643–2652,
2019.
[10] M.-L.Chiang,C.-A.Wu,J.-K.Feng,C.-Y.Fang,andS.-W.Chen,“Food calorie and nutrition analysis system based on mask r-cnn,”
in Proc. of IEEE 5th International Conference on Computer and Communications (ICCC), 2019, pp. 1721–1728.
[11] Z. Shen, A. Shehzad, S. Chen, H. Sun, and J. Liu, “Machine learning based approach on food recognition and nutrition
estimation,” Procedia Computer Science, vol. 174, pp. 448–453, 2020.
[12] “Data.Gov:Food-a-pedia,”https://catalog.data.gov/dataset/food-a-pedia, accessed on September 22, 2023.
[13] Meteren, R.V. (2000). Using Content-Based Filtering for Recommendation.
[14] “Food Boundary Box Detection Dataset,” https://github.com/stressGC/Food-Boundary-Box-Detection-Dataset, accessed on
September 22, 2023.
[15] Tsung-Yi Lin, Maire, M., Belongie, S. J., Bourdev, L. D., Girshick, R. B., Hays, J., Zitnick, C. L. (2014). Microsoft COCO:
Common Objects in Context. CoRR, abs/1405.0312. Retrieved from http://arxiv.org/abs/1405.0312
[16] Girschick, Ross. (2015). Fast r-cnn. 10.1109/ICCV.2015.169
[17] Lokanath, M & Kumar, K & Keerthi, E. (2017). Accurate object classification and detection by faster-RCNN. IOP Conference
Series: Materials Science and Engineering. 263. 052028. 10.1088/1757-899X/263/5/052028.
[18] M. M. Anthimopoulos, L. Gianola, L. Scarnato, P. Diem, and S. G. Mougiakakou, “A Food Recognition System for Diabetic
Patients Based on an Optimized Bag-of-Features Model,” IEEE Journal of Biomedical and Health Informatics, vol. 18, no. 4,
pp. 1261–1271, 2014.
[19] J. Chen, B. Zhu, C.-W. Ngo, T.-S. Chua, and Y.-G. Jiang, “A Study of Multi-Task and Region-Wise Deep Learning for Food
Ingredient Recognition,” IEEE Transactions on Image Processing, vol. 30, pp. 1514–1526, 2021.
[20] H. Jiang, J. Starkman, M. Liu, and M.-C. Huang, “Food Nutrition Visualization on Google Glass: Design Tradeoff and Field
Evaluation,” IEEE Consumer Electronics Magazine, vol. 7, no. 3, pp. 21–31, 2018.
[21] K. J. Pfisterer, R. Amelard, B. Syrnyk, and A. Wong, “Towards Computer Vision Powered Color-Nutrient Assessment of
Pure’ed Food,” in Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019,
pp. 490–492.
[22] C.Liu,Y.Liang,Y.Xue,X.Qian,andJ.Fu,“FoodandIngredientJoint Learning for Fine-Grained Recognition,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 31, no. 6, pp. 2480–2493, 2021.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
[23] G. Hussain, B. Ali Saleh Al-rimy, S. Hussain, A. M. Albarrak, S. N. Qasem, and Z. Ali, “Smart piezoelectric-based wearable
system for calorie intake estimation using machine learning,” Applied Sciences, vol. 12, no. 12, p. 6135, 2022.
[24] R. D. Kumar, E. G. Julie, Y. H. Robinson, S. Vimal, and S. Seo, “Recognition of food type and calorie estimation using neural
network,” The Journal of Supercomputing, vol. 77, no. 8, pp. 8172–8193, 2021.
[25] T. Miyazaki, G. C. de Silva, and K. Aizawa, “Image-based calorie con- tent estimation for dietary assessment,” in Proc. of
IEEE International Symposium on Multimedia, 2011, pp. 363–368.
[26] V. B. Kasyap and N. Jayapandian, “Food calorie estimation using convolutional neural network,” in Proc. of 3rd International
Conference on Signal Processing and Communication (ICPSC), 2021, pp. 666–670.
[27] T. Ege and K. Yanai, “Image-based food calorie estimation using knowledge on food categories, ingredients and cooking
directions,” in Proceedings of the on Thematic Workshops of ACM Multimedia, 2017, p. 367–375.
[28] T. Ege and K. Yanai, “Estimating food calories for multiple-dish food photos,” in Proc. of 4th IAPR Asian Conference on
Pattern Recognition (ACPR), 2017, pp. 646–651.
[29] A. Harrison, S. Sullivan, K. Tchanturia, and J. Treasure, “Emotional Functioning in Eating Disorders: Attentional Bias,
Emotion Recognition and Emotion Regulation,” Psych. Med., vol. 40, no. 11, pp. 1887–1897, 2010.
[30] O. Beijbom, N. Joshi, D. Morris, S. Saponas, and S. Khullar, “Menu- Match: Restaurant-Specific Food Logging from Images,”
in Proc. IEEE Winter Conf. on App. of Comp. Visn., 2015, pp. 844–851.
[31] H. Jiang, J. Starkman, M. Liu, and M. Huang, “Food Nutrition Visual- ization on Google Glass: Design Tradeoff and Field
Evaluation,” IEEE Consum. Electron. Mag., vol. 7, no. 3, pp. 21–31, May 2018.
[32] J. Taichi and K. Yanai, “A food image recognition system with Multiple Kernel Learning,” in Proc. 16th IEEE ICIP, 2009, pp.
285–288.
[33] “Roboflow: Give your software the power to see objects in images and video.” https://roboflow.com accessed on September
22,2023
[34] P. Pouladzadeh, S. Shirmohammadi, A. Bakirov, A. Bulut, and A. Yas- sine, “Cloud-based SVM for food categorization,”
Multimedia Tools and App., vol. 74, no. 14, pp. 5243–5260, Jul 2015.
[35] L. Rachakonda, A. Kothari, S. P. Mohanty, E. Kougianos, and M. Gana- pathiraju, “Stress-Log: An IoT-based Smart System to
Monitor Stress- Eating,” in Proc. IEEE ICCE, 2019, pp. 1–6.
[36] Schafer, Ben & J, Ben & Frankowski, Dan & Dan, & Herlocker, & Jon, & Shilad, & Sen, Shilad. (2007). Collaborative Filtering
Recommender Systems.
[37] Khurana, D., Koli, A., Khatter, K. et al. Natural language processing: state of the art, current trends and challenges. Multimed
Tools Appl 82, 3713–3744 (2023).
[38] Bi, Y., Lv, M., Song, C., Xu, W., Guan, N., & Yi, W. (2019). AutoDietary: A wearable acoustic sensor system for food intake
recognition in daily life. IEEE Transactions on Mobile Computing, 15(9), 2230–2243.
[39] Jin, Qing, & Yang, Yaping. (2020). Fine-grained food recognition using a convolutional neural network-based method. IEEE
Access, 8, 136945-136954.
[40] Jeyaraj, A., Ajay, M., George, R., & Parthiban, P. (2021). Mask-RCNN based portion size estimation and caloric intake tracking.
IEEE Access, 9, 38244–38253.
[41] Kong, Fanyu & Tan, Jindong. (2012). DietCam: Automatic Dietary Assessment with Mobile Camera Phones. Pervasive and
Mobile Computing, 8, 147-163. 10.1016/j.pmcj.2011.07.003.
[42] Lee, Changmin, & Jin, Sung. (2019). Multimodal dietary intake estimation using visual and acoustic data. Proceedings of the
IEEE Conference on Multimedia and Expo (ICME), 788–793.
[43] Liu, Yang, Cheng, Wei, & Fu, Chen. (2020). Deep learning-based nutrition estimation from food images. Journal of Food
Engineering, 45(3), 109-121.
[44] Perez, Alexander, Thompson, John, & Walker, Mark. (2018). Graph-based segmentation and caloric estimation for mixed food
items. Computer Vision and Image Understanding, 169, 25–39.
[45] Thomaz, Edison, Pering, Tony, & Oliver, Nuria. (2020). A wearable solution for real-time dietary intake recognition. ACM
Transactions on Sensor Networks, 11(4), 78–87.
[46] Vasiloglou, Marios, Christoph, Samuel, & Staub, Andreas. (2019). The role of artificial intelligence in food diary and dietary
monitoring. Journal of Nutrition and Health, 14(3), 189–199.
[47] Wu, Ping, Qiao, Zhen, & Zhang, Lei. (2020). Food ingredient recognition using deep learning models. Computers in Biology
and Medicine, 128, 104096.
[48] Yang, Yu, Zhang, Ling, & Cheng, Xiaoping. (2021). Real-time food recognition and macronutrient estimation using cloud-based
CNN models. IEEE Transactions on Neural Networks, 25(6), 2195-2207.
[49] Zhang, Yong, Wu, Kai, & Zhou, Fan. (2020). Smartphone-based deep learning models for automated food detection. Journal of
Health Informatics, 12(2), 209-215.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
[50] Zhou, Qin, Wang, Xuan, & Chen, Jie. (2021). A CNN approach for portion size and ingredient estimation in food recognition.
Pattern Recognition Letters, 143, 112–119.
[51] Zhu, Yi, Xie, Ying, & Huang, Jie. (2019). Real-time dietary assessment system using CNN and edge computing. Future
Generation Computer Systems, 100, 103–111.
[52] Ming Huang, Xiaohua Zhao, & Xinyu Lin. (2023). AI-powered diet monitoring and personalized nutrition recommendation.
Nutrition & AI, 14(2), 123-134.
[53] Xiaomei Zhang, Wei Li, & Min Zhao. (2022). Nutritional health monitoring using wearable devices and AI technologies.
Journal of Food Science and Technology, 58(6), 4321–4335.
[54] Haoyu Cheng, Qian Liu, & Yang Wang. (2021). Nutritional tracking with image-based deep learning methods: A comprehensive
review. Journal of Food Engineering, 56(7), 143–157.
[55] Yijun Lin, Qing Zhang, & Changming Li. (2022). Advances in nutritional analysis through deep learning. IEEE Transactions
on Computational Biology and Bioinformatics, 19(5), 876–884.
[56] Li Mei, Shiyu Wang, & Lijun He. (2023). Dietary intake monitoring using deep learning models: From calories to macronutri-
ents. Journal of Health Informatics Research, 35(4), 297–308.
[57] Jun Liu, Weilin Zhao, & Ming Li. (2023). Personalized nutritional recommendations using AI-driven systems. Journal of
Artificial Intelligence in Healthcare, 16(3), 245–260.
Madhumita Veeramreddy is a Bachelor of Technology student at SRM University, Amaravati
(SRMAP). She specializes in Artificial Intelligence and Machine Learning. Her research
interests include developing advanced algorithms for image recognition and classification,
optimizing machine learning models for real-time applications, and exploring the intersection
of AI with healthcare and nutrition.
Ashok Kumar Pradhan is currently working as an Associate Professor in the Department
of Computer Science & Engineering, School of Engineering and Applied Science at SRM
University, Amaravati, AP. He earned his M.Tech degree in Computer Science and Engineering
from the National Institute of Technology (NIT), Rourkela in 2010, and completed his Ph.D.
at NIT Durgapur in 2015. His research interests span across several cutting-edge domains
including Optical Communication and Networks, the Internet of Things (IoT), Blockchain
Technology, Cyber Security & Privacy, Machine Learning (ML) & Deep Learning (DL), Cloud
Computing, Edge Computing, Fog Computing, and Computer Algorithms. He has published
over 35 research papers in reputed peer-reviewed journals and conferences, edited two books,
and contributed four book chapters published by leading academic publishers. He has also
been granted one patent and successfully supervised one PhD scholar to completion. In 2019,
he was awarded the prestigious SERB grant (TAR/2019/000286). In addition to his research
contributions, he serves as a reviewer for renowned journals and transactions published by
Springer, Elsevier, and IEEE, helping to ensure the quality and impact of research in his fields
of expertise.
Swetha Ghanta received her Bachelor of Technology (B. Tech.) degree and Master of Tech-
nology (M. Tech.) degree in Computer Science and Engineering from RVR & JC College of
Engineering, Guntur, India. She is currently pursuing her PhD degree in the Department of
Computer Science and Engineering at SRM University, Amaravati, India. Her research interests
include deep learning, enhanced privacy, and security approaches. She is also working on
medical image analysis using Federated Learning.
NutriVision: A System for Automatic Diet Management in Smart Healthcare
A PREPRINT
Laavanya Rachakonda (M’21) is an Assistant Professor in the Department of Computer Sci-
ence in the College of Science and Engineering at the University of North Carolina Wilmington,
Wilmington, NC. She earned her Bachelor of Technology (B. Tech) in Electronics and Commu-
nication from Jawaharlal Nehru Technological University (JNTU), Hyderabad, India, Master
of Sciences (M.S) in Computer Engineering, and Doctor of Philosophy (Ph.D.) in Computer
Science and Engineering from University of North Texas. During her graduate studies, she
was part of the Smart Electronics Systems Laboratory (SESL) research group at Computer
Science and Engineering at the University of North Texas, Denton, TX. Her research interests
include smart healthcare applications using artificial intelligence, deep learning approaches, and
application-specific architectures for consumer electronic systems based on the IoT. She has
3 peer-reviewed journals published, 13 peer-reviewed conference publications, 2 filed patents,
and 1 patent disclosure. She has delivered 15 talks (online and offline) at various IEEE-hosted
conferences. She has won 20 honors and awards and has monitored 6 undergraduate and TAMS students. Her biography,
research, education, and outreach activities are available at www.laavanyarachakonda.com
Saraju P Mohanty (Senior Member, IEEE) received the bachelor’s degree (Honors) in electrical
engineering from the Orissa University of Agriculture and Technology, Bhubaneswar, in 1995,
the master’s degree in Systems Science and Automation from the Indian Institute of Science,
Bengaluru, in 1999, and the Ph.D. degree in Computer Science and Engineering from the
University of South Florida, Tampa, in 2003. He is a Professor with the University of North
Texas. His research is in “Smart Electronic Systems” which has been funded by National
Science Foundations (NSF), Semiconductor Research Corporation (SRC), U.S. Air Force,
IUSSTF, and Mission Innovation. He has authored 550 research articles, 5 books, and 10
granted and pending patents. His Google Scholar h-index is 58 and i10-index is 269 with 15,000
citations. He is regarded as a visionary researcher on Smart Cities technology in which his
research deals with security and energy aware, and AI/ML-integrated smart components. He
introduced the Secure Digital Camera (SDC) in 2004 with built-in security features designed
using Hardware Assisted Security (HAS) or Security by Design (SbD) principle. He is widely
credited as the designer for the first digital watermarking chip in 2004 and first the low-power digital watermarking chip
in 2006. He is a recipient of 19 best paper awards, Fulbright Specialist Award in 2021, IEEE Consumer Electronics
Society Outstanding Service Award in 2020, the IEEE-CS-TCVLSI Distinguished Leadership Award in 2018, and the
PROSE Award for Best Textbook in Physical Sciences and Mathematics category in 2016. He has delivered 30 keynotes
and served on 15 panels at various International Conferences. He has been serving on the editorial board of several
peer-reviewed international transactions/journals, including IEEE Transactions on Big Data (TBD), IEEE Transactions
on Computer-Aided Design of Integrated Circuits and Systems (TCAD), IEEE Transactions on Consumer Electronics
(TCE), and ACM Journal on Emerging Technologies in Computing Systems (JETC). He has been the Editor-in-Chief
(EiC) of the IEEE Consumer Electronics Magazine (MCE) during 2016-2021. He served as the Chair of Technical
Committee on Very Large Scale Integration (TCVLSI), IEEE Computer Society (IEEE-CS) during 2014-2018 and
on the Board of Governors of the IEEE Consumer Electronics Society during 2019-2021. He serves on the steering,
organizing, and program committees of several international conferences. He is the steering committee chair/vice-chair
for the IEEE International Symposium on Smart Electronic Systems (IEEE-iSES), the IEEE-CS Symposium on VLSI
(ISVLSI), and the OITS International Conference on Information Technology (OCIT). He has supervised 3 post-doctoral
researchers, 17 Ph.D. dissertations, 28 M.S. theses, and 28 undergraduate projects.
