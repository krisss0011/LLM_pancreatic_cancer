Deep Learning Approach for Enhancing Oral
Squamous Cell Carcinoma with LIME Explainable
AI Technique
Samiha Islam
Department of Electrical and Computer Engineering
North South University
Dhaka-1229, Bangladesh
samiha.islam2@northsouth.edu
Muhammad Zawad Mahmud
Department of Electrical and Computer Engineering
North South University
Dhaka-1229, Bangladesh
zawad.mahmud1@northsouth.edu
Shahran Rahman Alve
Department of Electrical and Computer Engineering
North South University
Dhaka-1229, Bangladesh
shahran.alve@northsouth.edu
Md. Mejbah Ullah Chowdhury
Department of Electrical and Computer Engineering
North South University
Dhaka-1229, Bangladesh
mejbah.chowdhury@northsouth.edu
Abstract—The goal of the present study is to analyze an appli-
cation of deep learning models in order to augment the diagnostic
performance of oral squamous cell carcinoma (OSCC) with a
longitudinal cohort study using the Histopathological Imaging
Database for oral cancer analysis. The dataset consisted of 5192
images (2435 Normal and 2511 OSCC), which were allocated
between training, testing, and validation sets with an estimated
ratio repartition of about 52% for the OSCC group, and still, our
performance measure was validated on a combination set that
contains almost equal number (50:50) of sample in this use case
as entire database have been divided into half using stratified
splitting technique based again near binary proportion but total
distribution was around even. We selected four deep-learning
architectures for evaluation in the present study: ResNet101,
DenseNet121, VGG16, and EfficientnetB3. EfficientNetB3 was
found to be the best, with an accuracy of 98.33% and F1 score
(0.9844), and it took remarkably less computing power in com-
parison with other models. The subsequent one was DenseNet121,
with 90.24% accuracy and an F1 score of 90.45%. Moreover, we
employed the Local Interpretable Model-agnostic Explanations
(LIME) method to clarify why EfficientNetB3 made certain
decisions with its predictions — to improve the explainability
and trustworthiness of results. This work provides evidence for
the possible superior diagnosis in OSCC activated from the
EfficientNetB3 model with the explanation of AI techniques
such as LIME and paves an important groundwork to build
on towards clinical usage.
Index Terms—OSCC, deep learning, EfficientNetB3, LIME,
XAI.
I. INTRODUCTION
Oral squamous cell carcinoma (OSCC) is a malignant tumor
originating from the oral cavity, oropharynx, and hypopharynx.
It is the most frequently diagnosed form of oral cancer,
being over 90% of all cases. Oral squamous cell carcinoma
(OSCC) is a disease that affects various regions of the mouth,
including lips, tongue, floor of the mouth, and hard palate.
Importantly, early detection greatly enhances the prognosis and
chances of survival. In the first place, early lesions in OSCC
sometimes remain asymptomatic, representing a late diagnosis
at an advanced stage and, consequently, difficult therapy with
a poor prognosis.
OSCC exerts a major burden in Asia, having a high fre-
quency of exposure to risk factors (tobacco use, betel quid
chewing, genetic assets). These factors result in an increase
in oral cancers experienced by Asian countries than Western
populations. For example, countries like Bangladesh, India,
Pakistan, and Sri Lanka frequently report a large burden of
OSCC, making them priority health problems with which to
deal. The disease burden advances not only health outcomes
but also imposes high costs of care and loss of productiv-
ity, highlighting the requirement for efficient and early-stage
recognition programs.
Recent progress with deep learning, a subfield of AI, seems
to be promising for object detection and classification in
OSCC. Deep learning models, and those that use transfer-
earning in particular, take advantage of pre-trained neural net-
works to provide high accuracy with limited data compared to
more traditional machine-learning frameworks. This is partic-
ularly advantageous in medical imaging that typically does not
have large annotated datasets. The transfer learning models are
built on top of VGG/Inception/ResNet, and they can generalize
the well-disguised discriminative image features from millions
of oral histopathological slides that a human expert’s naked eye
may not be able to detect, hence empowering clinicians with
an invincible weapon for early treatment plan intervention in
OSCC cases. The early detection landscape of oral cancers
may be revolutionized with the integration and appropriate
application of AI into diagnostic pathways, leading to possible
advances in better management, improved survival rates, etc.
arXiv:2411.14184v1  [eess.IV]  21 Nov 2024
The main contributions of this study are:
• Our
main
contribution
is
finetuning
a
pre-trained
EffiecientNetB3 model for oral squamous cell carcinoma
classification. This model outplayed all existing accuracy
models for this data set to this point.
• The second contribution is to apply LIME XAI to identify
the working of the EfficientNetB3 model.
II. RELATED WORK
In the last few years, a slew of research has been carried out
with deep learning (DL) to advance early detection diagnosis
and prognosis decoding in oral cancer and other cancers as
well. Here, we perform an extensive review of the related lit-
erature on OSCC detection and using deep learning techniques
in medical images. The objective is to provide an in-depth
overview of the literature on how deep learning has advanced
diagnosis modalities for OSCC and place it within the broader
landscape involving conventional methods.
Redie et al. [1] investigated Oral cancer detection us-
ing a transfer learning-based framework from histopathology
images. The dataset contained 5,192 images in total after
augmentation with two classes. The VGG19 model performed
the highest accuracy of 96.26%.
Alanazi et al. [2] discovered intelligent deep learning en-
abled oral squamous cell carcinoma detection and classifica-
tion using biomedical images. The dataset was a public dataset
from Kaggle containing 131 images with two classes. A novel
IDL-OSCDC model was discovered in this study, and it gave
an accuracy of 95%.
Das et al. [3] investigated the automated classification of
cells into multiple classes in epithelial tissue of oral squamous
cell carcinoma using transfer learning and CNN. The dataset
contained 156 slide images of two classes. The traditional
CNN model outperformed the transfer learning models with
97.5% accuracy.
Warin et al. [4] studied AI-based analysis of oral lesions
using novel deep convolutional neural networks for early
detection of oral cancer. The dataset contains of 980 oral
photographic images with three classes having 365, 315, and
300 images, respectively. The DenseNet-169 achieved the
highest f1 score of 0.98.
Panigrahi et al. [5] investigated capsule network-based
analysis of histopathological images of oral squamous cell
carcinoma. Their dataset had 150 number of samples. The
best model of this work was CapsNet with 97.35% accuracy.
Mira et al. [6] investigated early diagnosis of oral cancer
using image processing and artificial intelligence. The dataset
contains a total of 455 samples. A novel deep learning model
was introduced here, and the model got an accuracy of 84.3%.
Su et al. [7] studied current insights into oral cancer
diagnostics. The dataset contains a total of 455 samples. Their
best model was HRNet-W18, which achieved 84.3% accuracy.
Welikala et al. [8] investigated automated detection and
classification of oral lesions using deep learning for early
detection of oral cancer. The dataset contains a total of 2155
images. Their best model was ResNet-101, with an 87.03% f1
score.
Jubair et al. [9] studied a novel lightweight deep convo-
lutional neural network for early detection of oral cancer.
There were a total of 716 images containing two classes.
EfficientNet-B0 was the best model with 85% accuracy.
Ahmad et al. [10] investigated multi-method analysis of
histopathological images for early diagnosis of oral squamous
cell carcinoma using deep learning and hybrid techniques. The
dataset was the one that we used in this study. It had 5192
images after augmentation, among which 48% was normal and
52% infected. Their hybrid model SVM with feature extraction
of DesnseNet201 achieved the highest accuracy of 97%.
Palaskar et al. [11] studied transfer learning for oral cancer
detection using microscopic images. The dataset had a total
of 1224 images divided into two sets. In the first set, 89
images were normal epithelium, and 439 were OSCC in 100x
magnification. In the second set, 201 were normal, and 495
were OSCC. The best model of this study was InceptionV3,
with 91.13% accuracy and 0.88 f1 score.
III. METHODOLOGY
A. Dataset
This project got data mostly from the public dataset
“Histopathological imaging database for oral cancer analy-
sis” [12]. The dataset was accessed from kaggle [13], where
images are divided into training, testing, and validation sets
accompanied by two labels: Normal and OSCC (Oral Squa-
mous Cell Carcinoma). The total number of images used in the
training data is 2435 Normal and 2511 OSCC. The testing data
contains 31 Normal images and 95 OSCC. The dataset consists
of 5192 images in total, with around 52% being OSCC and
the remaining Normal (about 48%). The data was split into a
70:30 ratio for training and testing, respectively.
B. Data Preprocessing
The resultant images display diversification in their size,
level of zoom, angle, lighting condition, and orientation.
All images were preprocessed to standardize the dataset for
model education. Specifically, they were cropped manually
in 224 × 224 and converted to JPG. This preprocessing
ensures consistency in the input data format across all images,
facilitating compatibility with the chosen deep-learning model
architecture. After the preprocessing of the images, they are
used for the image augmentation process, such as zooming,
right shifting, flipping, etc.
C. Training Methodology
This subsection explains the models used in this study in
detail.
1) ResNet101: ResNet101 is a convolutional neural net-
work architecture, and it is famous for its depth (how many
layers), which makes it really good at image recognition. At
101 layers deep, it outranked the previous winner by using skip
connections to solve the vanishing gradients problem. This
enables a smooth flow of gradients known as “unfettered gra-
dient propagation,” which in turn facilitates training very deep
networks with eventual optimal convergence and classification
accuracy. Residual units are the individual convolutional layers
applied within each ResNet101 block. As a further efficiency
measure, it uses the bottleneck design of deeper layers without
sacrificing performance. ResNet101 models are a common
pre-trained model for image classification or other computer
vision-related tasks. ResNet101 really pushed deep learning in
computer vision through its depth, skip connections and some
creative design.
2) VGG-16: VGG-16 is an example of a convolutional
neural network that was designed to suit image classification
by the Visual Geometry Group (VGG) at the University of
Oxford. VGG-16 has 13 convolutional layers and three fully
connected layers, a total of 16 weight layers. It accepts an
RGB image of 224x224 size and is processed first through
convolutional layers followed by a max pooling layer to learn
details in images at lower dimensions. Each block contains a
few 3x3 convolution layers with ReLU activation to add non-
linearity and also increases the number of filters for higher-
order patterns. Dropout is used in fully connected layers with
4096 units and ReLU activation to prevent overfitting. The
final layer generates the class probabilities using softmax
activation. VGG-16 is a very effective model in image clas-
sification and has become well-used for research in computer
vision.
3) DenseNet121:
DenseNet121
is
a
121-layer
CNN
(Densely connected Neural Network) in the family of
DenseNets. Eyenet has dense connections between two lay-
ers; each unit in a layer is connected to all other units of
its succeeding layer (all-to-all connectivity). This makes the
network feed-forward fully connected. These are re-used via
the propagations and update of gradients, leading to better
performance in terms of convergence. The DenseBlock from
the dense blocks, which are in each of these dense blocks, has
multiple convolutional layers, batch normalization, and ReLu
activation functions. The spatial dimension of input and pa-
rameters is reduced by dense blocks through transition layers.
DenseNet121 mitigates the vanishing gradient problem very
well and performs well in tasks such as image classification,
object detection, and so on. It has been widely used in research
and practical applications thanks to its small size, deployment
efficiency (it is pre-trained on ImageNet), and the fact that it
makes good use of parameters.
4) EfficientNetB3: EfficientNet family member Efficient-
NetB3 is one such family of convolutional neural networks
that have been widely publicized for their efficiency and
performance in image recognition. It was created by Google
AI researchers to offer a balanced solution between model
size, computational resources, and performance for resource-
constrained environments. EfficientNetB3 goes through com-
pound scaling of its depth, width, and resolution to perform
better. It consists of several blocks with inverted bottleneck
structures, which take advantage of depth-wise separable
convolutions and squeeze-and-excitation modules to improve
the representational capacity effectively. EfficientNetB3, pre-
trained on datasets like ImageNet, is a brilliant transformer
for image classification and object detection. Since the design
is quite compact and there are fewer parameters, it is a
perfect fit for research as well as practical applications in deep
learning/computer vision.
D. LIME
Any black-box classifier may be interpreted by LIME (Local
Interpretable Model-Agnostic Explanations), as it is model-
agnostic. One of the most popular and straightforward XAI
methods is LIME. The statement “LIME is explainable for
any supervised deep learning models in this world” relates to
this model agnosticism. A test sample and prediction model
are fed into the LIME. It starts with sampling in order to
obtain a surrogate dataset. It generates 5000 pseudo-samples
and normalizes the project feature vector by default. Through
this method, the predictive model is able to attain the target
variable for these 5000 samples. Additionally, each row in
the surrogate dataset is weighted according to how much the
newly produced samples match the original sample [14].
E. Testing Methodology
This subsection explains how the models were tested for this
study. The equations of the testing metrics are given below:
Precision =
TP
TP + FP
(1)
Recall =
TP
TP + FN
(2)
F1 −score = 2 × Precision × Recall
Precision + Recall
(3)
Accuracy =
TP + TN
TP + TN + FP + FN
(4)
F. Work Flow Diagram
Fig. 1 represents the workflow diagram of this study.
IV. RESULT ANALYSIS
To classify oral cancer images in our study, we used
four different CNN models. Those models are: ResNet101,
VGG16, DenseNet121 and EfficientNetB3. The models eval-
uated various performance metrics, i.e., accuracy, precision,
recall, f1-score, confusion matrix, etc. The performance of
the best two models is mostly displayed in the upcoming
subsections.
A. EfficientNetB3
Fig. 2 displays accuracy curves of the EfficentNetB3 model
for OSCC classification on training and validation datasets.
The training accuracy (red) increases quickly and plateaus
close to 100%, suggesting that the model is successfully learn-
ing from the training data. The validation accuracy (green) also
increases but begins to plateau around 98%, demonstrating that
it generalizes well on unseen data while only slightly diverging
Fig. 1. Workflow diagram of this study
Fig. 2. Training and validation accuracy curve of EfficientNetB3
from training, which may suggest the early onset of overfitting
after epoch 50 - where we previously stated is our “best” point.
The EfficientNetB3 models of this study all reached the
inflection point even before the 20 epoch number for both
the training and validation loss curve, as shown in Fig 3. As
we can see below, the training loss (red) and validation loss
(green) drop off steeply in the early part of 100 epochs before
leveling out — indicating that our model seems to have learned
how to minimize error on both train and valid data. The point
marked with the best epoch at 74 is where one should stop,
beyond which the model may starts over-fitting.
Fig. 4 visualizes the confusion matrix of the EfficientNetB3
model. It is seen that the model classified 356 images correctly
as normal, labeled as 0. 410 images were classified correctly as
infected, labeled as 1. Only 13 images were classified wrongly.
It shows the model’s robustness in classifying the disease.
Fig. 3. Training and validation loss curve of EfficientNetB3
Fig. 4. Confusion Matrix of EfficientNetB3
B. DenseNet121
Fig. 5 indicates the training and validation accuracy curves
reported for the DenseNet121 model. The blue line for the
training accuracy initially starts out low but quickly rises
within just a few epochs to stabilize at around 96% and
remains fairly consistent over all of the remaining epochs.
However, as visible by the validation accuracy in red, it climbs
sharply well but fluctuates and trends lower after peaking
around epoch 5.
In Fig. 6, results of training and testing loss curves are
shown for the DenseNet121 model. The training loss (blue)
and validation loss (red) both decrease rapidly at first, which
means that the model learns very quickly during the early
stages of training. Following the first few epochs, both loss
curves eventually converge and become very similar to each
other — they keep these values stable and low in later runs.
The confusion matrix of the DenseNet121 model was
displayed in Fig. 7. This model didn’t perform as well as
Fig. 5. Training and validation accuracy curve of DenseNet121
Fig. 6. Train and validation loss curve of DenseNet121
the earlier one. It classified 343 images correctly as normal,
labeled as 0. 360 images were classified correctly as infected,
labeled as 1. A total of 76 images were classified wrongly. It
shows the model didn’t perform as well as the earlier one for
classifying this disease.
Fig. 7. Confusion Matrix of DenseNet121
C. Model Evaluation
Table I evaluates the applied models in terms of training
accuracy, validation accuracy, and validation loss. It is ap-
parent from the table that the EfficientNetB3 model worked
better than all other applied models in this dataset, with
97.59% validation accuracy and validation loss of only 0.1117.
The DenseNet121 also performed well with 91% validation
accuracy and 0.3511 validation loss.
TABLE I
RESULT EVALUATION OF MODELS
Models
Training Accuracy
Validation Accuracy
Validation Loss
EfficientNetB3
0.9964
0.9759
0.1117
DenseNet121
0.9600
0.9100
0.3511
ResNet101
0.9343
0.9058
0.2535
VGG16
0.8647
0.8314
0.4515
Table II represents the performance metrics of the models
and also compares those. It is evident here also that Effi-
cientNetB3 outperformed all other models with an excellent
accuracy of 98.33%. The model had a precision of 0.9903,
a recall of 0.9782, and an f1 score of 0.9844. It only used
12M parameters to achieve these metrics. DenseNet121, with
fewer parameters, i.e., 8M, came out second by achieving an
accuracy of 90.45%.
TABLE II
PERFORMANCE METRICS COMPARISON
Models
Parameters
Precision
Recall
F1-Score
Accuracy
EfficientNetB3
12M
0.9903
0.9782
0.9844
0.9833
DenseNet121
8M
0.9207
0.8889
0.9045
0.9024
ResNet101
44.5M
0.9410
0.8947
0.9173
0.8931
VGG1616
138M
0.9036
0.7895
0.8427
0.8177
D. Result Comparison
Table III shows the comparison of the applied models with
the existing ones by others. It is observed that EfficientNetB3
outplayed all existing models with an accuracy of 98.33%.
TABLE III
RESULT COMPARISON
Study
Model
Accuracy (%)
F1 score (%)
This study
EfficientNetB3
98.33
98.44
[4]
DenseNet-169
N/A
98.00
[3]
CNN
97.50
N/A
[5]
CapsNet
97.35
N/A
[1]
VGG19
96.26
N/A
N.B: N/A represents Not Available
E. Explainable AI
In this study, EfficientNetB3 came out to be the best model
among the applied ones. For that reason, we decided to apply
LIME XAI in this model. We have randomly selected two
images, among which the most image of Fig. 8 is an image
with OSCC, and the most image of Fig. 9 is an image without
OSCC. The segmentation masks for the various areas that
the model learns from each supplied picture are displayed in
the center row. More crucial sections are shown by the black
area in the mask, indicating that this leads to a low rating.
Higher-intensity areas in the mask denote areas that have a
bigger impact on the model’s prediction. The figures illustrate
how the rightmost images on both sides draw attention to the
information and places that our model was lucky enough to
attract. This suggests that in order to differentiate between
photos with and without OSCC infection, texture, and color
in these areas are crucial.
Fig. 8. LIME XAI visualization for the EfficientNetB3 model on an infected
image
Fig. 9.
LIME XAI visualization for the EfficientNetB3 model on a non-
infected image
V. CONCLUSION AND FUTURE WORK
This study demonstrates the considerable promise of deep
learning methods in histopathological image analysis and
holds importance for OSCC diagnosis. Although all tested
models performed well, EfficientNetB3 stood out with a
98.33% accuracy score and an F1 score of 98.44%. It is a
computationally efficient model, which makes it an appealing
one for real-time medical diagnostics. DenseNet121 did get
the right answer a step-down, which also indicates that deep-
learning models are performing well with their decision-
making and can handle complications in medical domain
image data. Secondly, we also incorporated Local Interpretable
Model-agnostic Explanations (LIME) along with Efficient-
NetB3 to explain the model. One of the important points
that touches on explainable AI (XAI) techniques, which are
indispensable for boosting trust and reliability in an application
intended to simulate medical diagnostics based solely on
image data is the ability of human experts — doctors with
experience at making predictions from radiological images
to understand what makes a model predict. These findings
imply the potential of deep learning integrated with XAI in
improving OSCC diagnostic accuracy and efficiency, which
could enable earlier, more robust detection. The next steps
should involve improving these models in terms of perfor-
mance and application to other cancer types, along with
images from Bangladeshi hospitals. Also, different explainable
AI techniques will be used to understand the model decision-
making more precisely.
REFERENCES
[1] D. K. Redie, S. Bilgaiyan, and S. Sagnika, “Oral cancer detection using
transfer learning-based framework from histopathology images,” Journal
of Electronic Imaging, vol. 32, no. 5, pp. 053 004–053 004, 2023.
[2] A. A. Alanazi, M. M. Khayyat, M. M. Khayyat, B. M. Elamin Elnaim,
and S. Abdel-Khalek, “Intelligent deep learning enabled oral squamous
cell carcinoma detection and classification using biomedical images,”
Computational Intelligence and Neuroscience, vol. 2022, no. 1, p.
7643967, 2022.
[3] N. Das, E. Hussain, and L. B. Mahanta, “Automated classification of
cells into multiple classes in epithelial tissue of oral squamous cell
carcinoma using transfer learning and convolutional neural network,”
Neural Networks, vol. 128, pp. 47–60, 2020.
[4] K. Warin, W. Limprasert, S. Suebnukarn, S. Jinaporntham, P. Jantana,
and S. Vicharueang, “AI-based analysis of oral lesions using novel deep
convolutional neural networks for early detection of oral cancer,” Plos
one, vol. 17, no. 8, p. e0273508, 2022.
[5] S. Panigrahi, J. Das, and T. Swarnkar, “Capsule network based analysis
of histopathological images of oral squamous cell carcinoma,” Journal
of King Saud University-Computer and Information Sciences, vol. 34,
no. 7, pp. 4546–4553, 2022.
[6] E. S. Mira, A. M. S. Sapri, R. F. Aljehanı, B. S. Jambı, T. Bashir,
E.-S. M. El-Kenawy, M. Saber et al., “Early diagnosis of oral cancer
using image processing and artificial intelligence,” Fusion: Practice and
Applications, vol. 14, no. 1, pp. 293–308, 2024.
[7] Y.-F. Su, Y.-J. Chen, F.-T. Tsai, W.-C. Li, M.-L. Hsu, D.-H. Wang, and
C.-C. Yang, “Current insights into oral cancer diagnostics,” Diagnostics,
vol. 11, no. 7, p. 1287, 2021.
[8] R. A. Welikala, P. Remagnino, J. H. Lim, C. S. Chan, S. Rajendran,
T. G. Kallarakkal, R. B. Zain, R. D. Jayasinghe, J. Rimal, A. R. Kerr
et al., “Automated detection and classification of oral lesions using deep
learning for early detection of oral cancer,” Ieee Access, vol. 8, pp.
132 677–132 693, 2020.
[9] F. Jubair, O. Al-karadsheh, D. Malamos, S. Al Mahdi, Y. Saad, and
Y. Hassona, “A novel lightweight deep convolutional neural network
for early detection of oral cancer,” Oral Diseases, vol. 28, no. 4, pp.
1123–1130, 2022.
[10] M. Ahmad, M. A. Irfan, U. Sadique, I. u. Haq, A. Jan, M. I. Khattak,
Y. Y. Ghadi, and H. Aljuaid, “Multi-method analysis of histopathological
image for early diagnosis of oral squamous cell carcinoma using deep
learning and hybrid techniques,” Cancers, vol. 15, no. 21, p. 5247, 2023.
[11] R. Palaskar, R. Vyas, V. Khedekar, S. Palaskar, and P. Sahu, “Transfer
learning for oral cancer detection using microscopic images,” arXiv
preprint arXiv:2011.11610, 2020.
[12] T. Y. Rahman, L. B. Mahanta, A. K. Das, and J. D. Sarma, “Histopatho-
logical imaging database for oral cancer analysis,” Data in brief, vol. 29,
p. 105114, 2020.
[13] Ashenafi Fasil Kebede, “Histopathologic oral cancer detection us-
ing cnns,” https://www.kaggle.com/datasets/ashenafifasilkebede/dataset/
data, 2021, accessed: [9-August-2024].
[14] F. H. Athina, S. A. Sara, Q. S. Sarwar, N. Tabassum, M. T. J. Era, F. B.
Ashraf, and M. I. Hossain, “Multi-classification Network for Detecting
Skin Diseases using Deep Learning and XAI,” in 2022 International
Conference on Innovation and Intelligence for Informatics, Computing,
and Technologies (3ICT).
IEEE, 2022, pp. 648–655.
