Detection-Guided Deep Learning-Based Model with Spatial
Regularization for Lung Nodule Segmentation
Jiasen Zhanga,b, Mingrui Yangb,c, Weihong Guoa, Brian A. Xavierd, Michael Bolend and
Xiaojuan Lib,c,e,∗
aDepartment of Mathematics, Applied Mathematics, and Statistics, Case Western Reserve University, 2145 Adelbert Rd, Cleveland, OH, 44106, United
States
bProgram for Advanced Musculoskeletal Imaging (PAMI), Cleveland Clinic, 9620 Carnegie Ave N Bldg, Cleveland, 44106, OH, United States
cDepartment of Biomedical Engineering, Lerner Research Institute, Cleveland Clinic, 9620 Carnegie Ave N Bldg, Cleveland, 44106, OH, United States
dImaging Institute, Cleveland Clinic, 9500 Euclid Ave, Cleveland, 44195, OH, United States
eDepartment of Biomedical Engineering, Case Western Reserve University, 10900 Euclid Ave, Cleveland, OH, 44106, United States
A R T I C L E I N F O
Keywords:
lung nodule segmentation
multitask model
feature combination
spatial regularization
deep learning
transfer learning
A B S T R A C T
Lung cancer ranks as one of the leading causes of cancer diagnosis and is the foremost cause of cancer-
related mortality worldwide. The early detection of lung nodules plays a pivotal role in improving
outcomes for patients, as it enables timely and effective treatment interventions. The segmentation of
lung nodules plays a critical role in aiding physicians in distinguishing between malignant and benign
lesions. However, this task remains challenging due to the substantial variation in the shapes and sizes
of lung nodules, and their frequent proximity to lung tissues, which complicates clear delineation.
In this study, we introduce a novel model for segmenting lung nodules in computed tomography
(CT) images, leveraging a deep learning framework that integrates segmentation and classification
processes. This model is distinguished by its use of feature combination blocks, which facilitate
the sharing of information between the segmentation and classification components. Additionally,
we employ the classification outcomes as priors to refine the size estimation of the predicted
nodules, integrating these with a spatial regularization technique to enhance precision. Furthermore,
recognizing the challenges posed by limited training datasets, we have developed an optimal transfer
learning strategy that freezes certain layers to further improve performance. The results show that
our proposed model can capture the target nodules more accurately compared to other commonly
used models. By applying transfer learning, the performance can be further improved, achieving a
sensitivity score of 0.885 and a Dice score of 0.814.
1. Introduction
Lung cancer has been one of the most deadly forms of
cancer throughout the 20th century. In 2023, it accounted
for the second-highest number of new cancer cases and
remained the leading cause of cancer-related deaths Siegel,
Miller, Wagle and Jemal (2023). Early-stage lung cancer
often presents with no clear symptoms, making lung nodules
critical markers for identifying the disease. Consequently,
the early detection of lung nodules is paramount for the
prompt diagnosis and treatment of lung cancer, significantly
enhancing the chances of patient survival. Computed to-
mography (CT) imaging plays a pivotal role in the early
detection of lung tumors, providing radiologists with the
ability to identify and examine suspicious lesions at their
most treatable stage. Typically, CT scans are initially re-
viewed by radiologists to check for the presence of nodules,
which are then categorized as benign or malignant based
on their size and shape, or in some instances based upon
appearance over time. This process, however, can be time-
consuming and labor-intensive, often involving the review
of hundreds or even thousands of image slices per scan,
and may lead to varying interpretations among different
radiologists. Image segmentation techniques offer a solution
by precisely delineating nodules from the surrounding lung
∗Corresponding authors email addresses: lix6@ccf.org
tissue, offering valuable data on their size, shape, and growth
rate. Therefore, the development of accurate and automated
lung nodule segmentation methods is crucial. These tech-
nologies not only assist radiologists in the identification
and classification of nodules but also significantly reduce
their workload, and have the potential to increase efficiency,
leading to improved diagnostic accuracy and patient care.
In recent decades, various medical image segmenta-
tion methods have been developed, enhancing the precision
of medical diagnoses. Thresholding techniques, one of the
earliest methods, segregate an image into two parts using
a predefined intensity threshold Otsu (1980). A more ad-
vanced approach has evolved to allow segmentation into
multiple sub-images, each with its tailored thresholds for
more nuanced analysis Wellner (1993). Morphology meth-
ods Zana and Klein (2001) leverage predefined shapes, scan-
ning images to identify areas that closely match these cri-
teria for segmentation. Variational techniques, on the other
hand, minimize specific energy functions to ensure the seg-
mented boundaries align accurately with the image’s tar-
get structures, exemplified by the Mumford–Shah model,
which segments images into uniform intensity regions Mum-
ford and Shah (1989). Further sophistication is seen in
methods employing level set functions, where boundaries
are represented mathematically, and energy functions are
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 1 of 10
arXiv:2410.20154v1  [cs.CV]  26 Oct 2024
Detection-Guided Deep Learning-Based Model with Spatial Regularization for Lung Nodule Segmentation
crafted to minimize within-region heterogeneity. The Chan-
Vese method is a special case of Mumford-Shah model that
uses level sets to segment images that do not necessar-
ily have clear edges Chan and Vese (2001, 1999). Graph
cut methods represent an image as a weighted undirected
graph and employ algorithms to enhance sub-graph similar-
ity while distinguishing between different sub-graphs Yi and
Moon (2012); Irshad, Veillard, Roux and Racoceanu (2013).
Recent advancements also include unsupervised machine
learning techniques like K-means Dhanachandra, Manglem
and Chanu (2015), KNN (k-Nearest Neighbor) Rangel, Fer-
nández, Murillo, Ortega and Arreguín (2016), and prob-
abilistic models Cuadra, Cammoun, Butz, Cuisenaire and
Thiran (2005), further broadening the toolkit available for
medical image segmentation.
With the advancement of computer technology, deep
learning models, particularly convolutional neural networks
(CNNs) Krizhevsky, Sutskever and Hinton (2012) have sur-
passed traditional methods in efficacy across numerous com-
puter vision tasks. Traditional techniques often depend on
manual feature selection and prior knowledge, whereas deep
learning models autonomously learn to extract relevant fea-
tures at various levels directly from data. Among these, U-
Net Ronneberger, Fischer and Brox (2015) has become a
benchmark model in medical image segmentation for its
efficiency and simplicity. This model utilizes an encoder-
decoder framework with added skip connections, allowing
for the detailed capture and preservation of spatial infor-
mation, a crucial aspect in accurately delineating medical
images. The encoder reduces feature dimensions to cap-
ture hierarchical information, while the decoder restores
these features to their original resolution, ensuring precise
segmentation. The skip connections help preserve spatial
information and aid in the precise localization of structures,
which is crucial in medical image segmentation tasks. The
introduction of U-Net variants, such as ResUNet Zhang, Liu
and Wang (2018) and ResUNet++ Jha, Smedsrud, Riegler,
Johansen, De Lange, Halvorsen and Johansen (2019), fur-
ther enhances segmentation accuracy. Concurrently, seman-
tic segmentation models like DeepLab Chen, Papandreou,
Kokkinos, Murphy and Yuille (2014, 2017a); Chen, Papan-
dreou, Schroff and Adam (2017b); Chen, Zhu, Papandreou,
Schroff and Adam (2018) have set new standards on public
datasets for semantic segmentation like PASCAL VOC 2012
Everingham, Van Gool, Williams, Winn and Zisserman
(2012) by incorporating advanced features such as Xception
Chollet (2017) and atrous spatial pyramid pooling (ASPP)
Chen et al. (2017a), showing strong potential for applica-
tion in medical images. These developments underscore the
potential of deep learning models to revolutionize medical
image analysis, offering significant improvements in diag-
nostics and treatment planning.
While numerous lung nodule segmentation methods
have been introduced, the task remains daunting. A primary
challenge is the limited size of clinical datasets for lung
nodule segmentation, as nodules are infrequent compared
to the prevalence of normal lung tissue in imaging studies.
Additionally, lung nodules exhibit a wide variety in shape,
size, and attenuation, complicating their classification and
segmentation. Generally, lung nodules can be classified
into several types: isolated nodules, juxtapleural nodules,
cavitary nodules, ground-glass opacity (GGO) nodules, and
calcific nodules Liu, Cao, Song, Ma, Xu, Jin, Jin and Hung
(2019). As depicted in Figure 1, juxtapleural nodules are
connected to the lung pleura and chest wall and exhibit
similar attenuation to it. Cavitary nodules pose challenges
for accurate segmentation due to variations in attenuation in
setting of central lucency. Calcific and GGO nodules display
high and low attenuation values, respectively. Moreover, the
presence of numerous small nodules, which can fall into
any of these categories, adds another layer of complexity to
achieving accurate segmentation. Given the predominance
of small size of nodules within clinical datasets relative to the
size of CT scans, our approach concentrates on segmenting
small regions of interest (ROI) around these nodules. Focus-
ing on ROIs, identified either by radiologists or automated
systems, allows for the prioritization of suspect nodules for
in-depth analysis and clinical decision-making. It is usually
integrated with Computer-aided detection (CAD) systems,
which allows for streamlined workflows, and enables more
precise delineation of nodules and more efficient utilization
of computational resources in automated nodule detection
and analysis. Such targeted efficiency is vital in clinical en-
vironments where swift diagnostic and treatment decision-
making are paramount for radiologists.
In the field of deep learning, several strategies have
been developed to overcome segmentation challenges and
address the scarcity of training data. Recent innovations
include applying transfer learning from extensive datasets
Liu, Dong, Wang, Cui, Fan, Ma and Chen (2021); Riaz,
Khan, Abdullah, Khan and Islam (2023), multitask learning
to leverage shared features across tasks like classification
and segmentation Wu, Gao, Mei, Xu, Fan, Zhang and Cheng
(2021), and employing data augmentation, cross-validation,
and regularization to mitigate overfitting.
This paper presents a multitask deep learning model
that unites segmentation and classification tasks, leverag-
ing shared information through feature combination blocks.
The model undergoes pre-training on the large-scale pub-
lic LUNA16 dataset before adapting to a smaller, private
clinical dataset through an optimal transfer learning strategy
and enhancement by spatial regularization. The key contri-
butions of this paper include:
1. We propose a multitask model that enhances segmen-
tation performance through integrated classification
features.
2. We introduce a feature combination block for efficient,
flexible feature utilization without information loss,
avoiding unnecessary upsampling and downsampling.
3. We apply soft threshold dynamic (STD) regulariza-
tion for refined segmentation predictions, incorpo-
rating classification outputs as prior information for
enhanced segmentation accuracy.
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 2 of 10
Detection-Guided Deep Learning-Based Model with Spatial Regularization for Lung Nodule Segmentation
Figure 1: Examples of nodules from LUNA16. From left to right: isolated nodule, juxtapleural nodule, calcific nodule, ground-glass
opacity (GGO) nodule and cavitary nodule.
4. We adopt an optimal transfer learning strategy facili-
tating effective use of pre-training data for segmenta-
tion on a smaller clinical dataset.
5. Our results demonstrate superiority of our model over
other classic medical image segmentation models,
supported by experimental evidence of improved per-
formance through feature combination, spatial regu-
larization, and transfer learning.
The remaining part of this paper is organized as fol-
lows: Section 2 discusses related literature studies; Section
3 explains the proposed methodology; Section 4 presents
our experimental results, with discussions provided for each
experiment; and finally, Section 5 presents the conclusion of
the study.
2. Related works
2.1. Feature combination in multitask model
Multitask learning is a technique in which a model is
trained to perform multiple tasks simultaneously. By lever-
aging information from related tasks, multitask learning can
improve performance and generalization ability, which is es-
pecially helpful when training data is limited. Additionally,
multitask learning can serve as a form of regularization by
introducing additional constraints on the model.
When training a multitask model, it is essential to
combine the features of different tasks to allow them to
share information; otherwise, the tasks would operate in-
dependently. The feature combination, or information fu-
sion scheme, usually depends on the architecture of deep
neural networks. Conventional multitask models with a
single channel Tjon, Moh and Moh (2021); Shen, Gou,
Wang and Wang (2019) typically share parameters across
different tasks in the hidden layers. The final outputs include
predictions for different tasks, which may be generated in
the middle or final layers of the model. However, tasks
in computer vision, such as segmentation, detection, and
classification, often emphasize different types of features.
Sharing too many parameters could limit the model’s per-
formance across tasks. To address this issue, some models
He, Lian, Zhang, Zhang, Cao, Nie, Gao, Zhang and Shen
(2021) split the single channel into double channels for
two tasks or use separate architectures Wu et al. (2021).
Misra et al. Misra, Shrivastava, Gupta and Hebert (2016)
evaluated the performance of two-task architectures split
at different layers and concluded that the optimal multitask
architecture depends on the individual tasks. It is essential
to design feature combination blocks for split architecture so
that the multiple tasks can share information and improve
each other, otherwise the different tasks will just work
separately. Pure concatenation and linear combination Misra
et al. (2016) were naturally used and shown positive effects.
A more general way is to apply more hidden layers in
feature combination blocks. The model in Wu et al. (2021)
upsampled the smaller feature so that the features have
the same size and can be concatenated together. However,
like downsampling, upsampling could also cause loss of
information because it involves replicating or interpolating
existing information to fill in the gaps.
2.2. Soft Threshold Dynamic (STD) regularization
To combine the deep learning framework with spatial
regularization, Liu et al. Liu, Wang and Tai (2022); Jia, Liu
and Tai (2021); Liu, Tai and Luo (2020) provided variational
explanations for some widely used activation functions in
deep learning, including softmax, ReLU, and sigmoid. For
instance, the sigmoid function can be expressed as: 𝑆𝑖𝑔(𝑢) =
1
1+exp(−𝑢) and it can be represented as the argument that
solves the following convex optimization problem when the
parameter 𝜀is equal to 1:
𝑆𝑖𝑔(𝑢) = arg min
0≤𝑥≤1
−⟨𝑢, 𝑥⟩+ 𝜀⟨𝑥, ln 𝑥⟩+ 𝜀⟨(1 −𝑥), ln(1 −𝑥)⟩
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 3 of 10
Detection-Guided Deep Learning-Based Model with Spatial Regularization for Lung Nodule Segmentation
(1)
where 𝑢is the feature map input, and ⟨𝑎, 𝑏⟩means inner
product of 𝑎and 𝑏. The optimal solution 𝑥will be equal
to 𝑆𝑖𝑔(𝑢∕𝜀), which can be proved by simply computing
the derivative of the energy function. One can easily add
regularization terms used in image processing and com-
puter vision into it to achieve smoothness. In Liu et al.
(2020), soft threshold dynamics (STD) Esedo¯g Lu and Otto
(2015) regularization was adopted to control the length of
the boundary so the edges have certain smoothness instead
of being bumpy:
𝑆𝑖𝑔𝑆𝑇𝐷(𝑢) = arg min
0≤𝑥≤1
−⟨𝑢, 𝑥⟩+ 𝜀⟨𝑥, ln 𝑥⟩
+ 𝜀⟨(1 −𝑥), ln(1 −𝑥)⟩
+ 𝜆⟨𝑥, 𝑘𝜎∗(1 −𝑥)⟩
(2)
where ∗means convolution, 𝑘represents a discrete Gaussian
kernel with standard deviation 𝜎and 𝜆is a parameter. The
STD regularization term ⟨𝑥, 𝑘𝜎∗(1 −𝑥)⟩has been proved
to approximate the boundary length Zhang and Guo (2023)
and has similar effect as total variation (TV) regularization.
It involves convolution and inner product operation, which
are cheaper to compute. Compared to Total Variation (TV)
regularization, which is non-smooth, STD regularization is
more efficient in deep learning due to its smooth nature,
making it compatible with neural networks and easier to
solve. Furthermore, the new term (last term) is concave so
that problem (2) can be solved with an efficient and iterative
algorithms. Given the input 𝑢which is the output of the last
hidden layer, the initial value 𝑥0 can be set as 𝑥0 = 𝑆𝑖𝑔(𝑢).
Then for each t-th step, the value of the next step can be
computed as:
𝑥𝑡+1 = 𝑆𝑖𝑔
(
𝑥𝑡−𝜆𝑘𝜎∗(1 −2𝑥𝑡)
𝜀
)
(3)
It is important to note that the network with STD regu-
larization differs from post-processing methods like Condi-
tional Random Fields (CRF) because it is integrated into the
back-propagation process and influences gradient updating.
In practice, parameters such as 𝜀, 𝜆, and 𝜎can be made
learnable, allowing them to be automatically tuned by the
neural network.
3. Proposed methodology
3.1. Model architecture
The proposed multitask model aims to leverage the in-
formation of classification to improve the performance of
segmentation. The architecture is illustrated in Figure 2, and
the size of the output of each block is also shown. The
orange blocks correspond to a ResNet-50 binary classifier
whose final activation function is Sigmoid. In our model,
the classifier learns to output a score between zero and one,
representing the probability of the existence of a nodule.
Because the classification task is relatively simpler than
segmentation, we choose ResNet-50 to guarantee efficiency
without losing accuracy. The blue blocks correspond to a
ResU-Net based segmentation model. The original ResU-
Net model Zhang et al. (2018) consists of an encoder (blocks
S1 to S5) and a decoder (blocks S6 to S9). In the encoder
part, the size of the features gets smaller gradually and the
number of channels gets larger to capture some high-level
features. This means the encoder part works similarly to
ResNet-50, and therefore it is appropriate to combine the
features of ResNet-50 and the U-Net encoder. The feature
combination blocks are represented by green arrows in Fig-
ure 2, which will be introduced later. Finally, after getting
the output of the classification, we embed it into the sigmoid
layer before the segmentation output. Different from feature
combination, we expect the classification output to directly
affect the segmentation output.
In the model, we modified the ResU-Net to achieve better
performance. Originally, each ResU-Net block includes two
BRC blocks (a batch normalization layer, a ReLU, and a
convolution layer). The first convolution layer has a stride
of 2 to reduce the size of features. We replaced it with a
depth-wise separable convolutional layer used in Xception
modules Chollet (2017) to reduce computational cost and
improve performance. This decomposition involves splitting
the standard 3 × 3 convolutional layer into a 3 × 3 channel-
wise spatial convolution followed by a 1 × 1 point-wise
convolution. Additionally, inspired by ResUNet++ Jha et al.
(2019), we added an atrous spatial pyramid pooling (ASPP)
Chen et al. (2017a) module respectively at the end of the en-
coder and decoder (S5 and S9), which can also significantly
improve performance. ASPP typically consists of four atrous
convolutions with different dilation rates to help increase
the receptive field and capture multi-scale information. It is
suggested that the dilation rates should not have common
factor relationships like 2, 4, 8, etc. Wang, Chen, Yuan, Liu,
Huang, Hou and Cottrell (2018), so we chose dilation rates
of 1, 5, 10, and 15.
3.2. Feature combination block
Since we expect the multitask model to leverage the
features of the classification model in order to improve the
performance of the segmentation model, it is necessary to
design an appropriate scheme to merge the classification and
segmentation features. In our method, we only combine the
features with the same size so that there is no up-sampling
and down-sampling. Therefore, the combination strategy is
flexible and depends on the specific model architecture. The
combined blocks are indicated in Figure 3, in which the final
layers are used as the combined feature. Instead of concate-
nating two features directly, we first let the classification
feature go through a 1×1 convolutional layer (the number of
output channels is equal to that of the input channel) so that
it is transformed into a segmentation feature. Then the two
features are concatenated together. We apply another 1 × 1
convolutional layer so that the number of channels is reduced
to the number of segmentation channels. Because the num-
ber of channels of the input features remains unchanged,
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 4 of 10
Detection-Guided Deep Learning-Based Model with Spatial Regularization for Lung Nodule Segmentation
Figure 2: The architecture of the proposed multitask model. The orange blocks correspond to ResNet-50 for classification. The
blue blocks correspond to an ResU-Net based network for segmentation. An input of size 128 × 128 is used for demonstration.
the second 1 × 1 convolutional layer will learn to fuse the
concatenated features together. Finally, the combined feature
is fed into a 3 × 3 depth-wise separable convolutional layer
with the same number of input and output channels, which
is the number of segmentation channels.
Figure 3: Structure of our combination block.
3.3. Embedded STD regularization
Apart from feature combination, we utilize the result of
classification to improve segmentation. We incorporate it
as a prior combined with STD regularization, and the new
optimization problem can be expressed as in Equation 4.
Suppose the classification output is denoted as 𝑐which is
between zero and one, representing the probability that the
patch contains nodule(s). When 𝑐= 0 there is no nodule
in the patch and when 𝑐= 1 there is at least one nodule.
We add a new term (1 −𝑐)⟨𝐼, 𝑥⟩to the STD regularization.
Note ⟨𝐼, 𝑥⟩measures the size/volume of the nodule. When
𝑐is close to 0, the classification indicates that there is
low probability of having nodules and the additional term
controls the size of the nodules. On the other hand, when
𝑐is close to or equal to one, the classifier is confident
that there is something resembling a nodule. In this case,
1 −𝑐is small, and the new prior has almost no effect.
Thus, the new term has the effect of reducing the size of the
nodule in the prediction if the classification indicates a high
chance of noting having nodules (𝑐being close to or equal
to 0). This is very helpful in reducing false positives and
avoiding identifying other tissues as nodules. It effectively
combines classification output into segmentation to help
improve segmentation results. In the neural network, the 𝑐
in Equation 4 is excluded from backpropagation to avoid
affecting the classification result.
𝑆𝑖𝑔𝑝𝑟𝑜𝑝(𝑢) = arg min
0≤𝑥≤1
−⟨𝑢, 𝑥⟩+ 𝜀⟨𝑥, ln 𝑥⟩+
𝜀⟨(1 −𝑥), ln(1 −𝑥)⟩
+ 𝜆1⟨𝑥, 𝑘𝜎∗(1 −𝑥)⟩
+ 𝜆2(1 −𝑐)⟨𝐼, 𝑥⟩
(4)
Both 𝜆1 and 𝜎control the effect of smoothness. When
we use STD regularization in our models, we set 𝜆1 to be
equal to one and tune 𝜆2 and 𝜎automatically. Since the new
term (1−𝑐)⟨𝐼, 𝑥⟩and its derivative are constant, the solution
of Equation 4 is efficient and similar to that of Equation 2.
The new iterative scheme is shown as in Equation 5. The
initial value could be set as 𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝑢), and 𝑥will converge
quickly within ten steps.
𝑥𝑡+1 = 𝑆𝑖𝑔
(
𝑥𝑡−𝜆1𝑘𝜎∗(1 −2𝑥𝑡) −𝜆2(1 −𝑐)
𝜀
)
(5)
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 5 of 10
Detection-Guided Deep Learning-Based Model with Spatial Regularization for Lung Nodule Segmentation
4. Experiments and results
4.1. Dataset
Two kinds of datasets are used in our experimental
evaluation: one for pre-training, and the other one for transfer
training and performance evaluation. Because annotated
clinical lung nodule dataset is typically limited in size,
we use the Lung Nodule Analysis 2016 (LUNA16) dataset
Setio, Traverso, De Bel, Berens, Van Den Bogaard, Cerello,
Chen, Dou, Fantacci, Geurts et al. (2017) for pre-training.
This dataset is a subset of the LIDC-IDRI dataset Armato III,
McLennan, Bidaut, McNitt-Gray, Meyer, Reeves, Zhao,
Aberle, Henschke, Hoffman et al. (2011), containing 1018
CT scans annotated by four radiologists. In LUNA16, scans
thicker than 3mm and those with inconsistent slice spacing
or missing slices were removed, resulting in a total of 888
scans Setio et al. (2017). For each scan, multiple annotations
were merged so that their positions and diameters were
averaged. Our method focuses on segmenting lung nodules
within a predefined region of interest (ROI) containing
lung nodules. In total, we obtained 2523 nodules. For each
nodule, we found the location of the centroid and then crop a
3D region centered around the centroid so that each cropped
region contains only one nodule. The first two dimensions of
the 3D region is 128 by 128 and the third dimension depends
on the lesion size. Note the proposed method is based on
two-dimensional segmentation and some 2D slices without
nodule are remained for pre-training.
We evaluated the performance of the proposed method
on a small lung nodule dataset from Cleveland Clinic (CCF).
It is collected from 7 patients and annotated by one radiolo-
gist. Similarly, 3D regions (128 by 128 along the first two di-
mensions) were cropped around each lesion without overlap.
In total, the dataset from 7 patients consists of 56 lesions, 45
of which were randomly selected and used for training and 5-
fold cross-validation. The remaining 11 lesions were used as
a testing set for performance evaluation. In the CCF dataset,
each CT scan typically contains hundreds of slices, while
lung nodules only appear in about ten of the slices. Since
the proposed model is 2D-based and has a classification
component, most slices without nodules are excluded from
the dataset but we do keep some slices that have no nodules
to train the classifier. Some empty slices that are adjacent to
the lung nodule are retained so that the ratio of slices with
and without nodules is balanced. This resulted in a total of
814 slices remaining after the clean-up.
The image intensity of the LUNA16 dataset ranged from
0 to 255, while the intensity of the CCF dataset ranged from
0 to 1000. To make the pre-trained model compatible for
transfer learning, both datasets were normalized according
to the equation below so that their intensities are in the range
of 0 to 1. Additionally, we applied random flips in the data
augmentation process.
𝐼𝑛𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑒=
𝐼−𝐼𝑚𝑖𝑛
𝐼𝑚𝑎𝑥−𝐼𝑚𝑖𝑛
(6)
4.2. Implementation details
First, we pre-trained the model with the LUNA16 dataset
for 200 epochs with a batch size of 10. The initial learning
rate was set to 0.001 without decay. Then, we trained the
model with the CCF dataset for 50 epochs with a batch size
of 10, which was sufficient considering its small size. The
initial learning rate was 0.001 with a decay of 0.75 for every
five epochs. We used Adam as the optimizer with a weight
decay of 1e-8. The loss function for segmentation was a
combination of binary cross-entropy (BCE) and Dice loss.
The binary classification also used BCE loss. During model
training, the overall loss was calculated as follows:
𝐿= 𝐿𝐷𝑖𝑐𝑒+ 𝜆1𝐿𝑠𝑒𝑔
𝐵𝐶𝐸+ 𝜆2𝐿𝑐𝑙𝑎𝑠𝑠
𝐵𝐶𝐸
(7)
In our experiments both 𝜆1 and 𝜆2 were set to be one.
To utilize the information learned from LUNA16, we
conducted a series of experiments to determine the most
effective transfer learning strategy. Our conclusion was that
the best results are achieved when the model is fully fine-
tuned, meaning no layers are frozen. Additionally, STD
regularization was not applied during pre-training.
4.3. Evaluation metrics
To ensure a comprehensive evaluation of the models’
performance, we employ several commonly used metrics:
precision (positive predictive value or PPV), sensitivity (or
recall), Dice score (also called F1 score), and IoU (intersec-
tion over union). In the case of binary segmentation, they are
defined as follows:
Precision =
TP
TP + FP
(8)
Sensitivity =
TP
TP + FN
(9)
Dice = 2 ∗Sen ∗Pre
Sen + Pre
=
2TP
2TP + FN + FP
(10)
IoU =
TP
TP + FN + FP
(11)
where TP represents true positive, i.e., the number of truly
classified pixels whose true values are positive. Similarly,
TN, FP, and FN are true negative, false positive, and false
negative. Suppose the truth value of lung nodule area is
positive and that of non-infected is negative, precision mea-
sures the accuracy rate of nodules in terms of prediction.
Sensitivity measures the accuracy rate of nodules in terms of
ground truth. Dice score is a combination of precision and
sensitivity, measuring the similarity between the predicted
lung infections and the ground truth. IoU is another metric
measuring the similarity of two finite sets, defined as their
intersection divided by their union. In some cases IoU is also
related to volumetric overlap error (VOE) by IoU = 1−VOE.
Another widely used evaluation metric in medical image
segmentation is Hausdorff distance (HD), measuring the
maximum distance from points in one contour to the closest
point in the other contour. It provides a straightforward
metric for comparison between the segmented region and
the ground truth. Suppose the contours of the prediction and
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 6 of 10
Detection-Guided Deep Learning-Based Model with Spatial Regularization for Lung Nodule Segmentation
Table 1
Comparative study with CCF dataset.
Model
Pre
Sen
Dice
IoU
HD(mm)
ASSD(mm)
UNet Ronneberger et al. (2015)
0.832
0.821
0.753
0.688
4.591
0.341
ResUNet Zhang et al. (2018)
0.821
0.855
0.779
0.708
3.594
0.290
ResUnet++ Jha et al. (2019)
0.844
0.845
0.777
0.712
3.784
0.303
DeepLabv3+ Chen et al. (2018)
0.802
0.831
0.748
0.658
3.318
0.265
Proposed Model
0.870
0.827
0.801
0.735
2.929
0.211
ground truth domains are 𝑋and 𝑌, their Hausdorff distance
is defined as
HD = max{sup
𝑥∈𝑋
𝑑(𝑥, 𝑌), sup
𝑦∈𝑌
𝑑(𝑋, 𝑦)}
(12)
where 𝑑(𝑥, 𝑌) is defined as inf𝑦∈𝑌𝑑(𝑥, 𝑦) and 𝑑(𝑥, 𝑦) is
computed with Euclidean distance in our experiments.
To comprehensively evaluate the results, we also use av-
erage symmetric surface distance (ASSD), which quantifies
the average distance between the surfaces of two segmented
objects. Compared to Hausdorff distance, it measures how
well the boundary of the segmented object and the ground
truth align with each other on average. It is defined as
ASSD =
∑
𝑥∈𝑋𝑑(𝑥, 𝑌) + ∑
𝑦∈𝑌𝑑(𝑋, 𝑦)
|𝑋| + |𝑌|
(13)
where |𝑋| means the length of the contour 𝑋.
4.4. Comparative study
To evaluate the performance of our proposed model,
we first compared our model without transfer learning with
other commonly used segmentation models. The average
performance of each model on the CCF dataset is shown
in Table 1. We observe that our model outperforms other
models in terms of precision, Dice score, IoU and aver-
age symmetric surface distance. While the proposed model
achieves a relatively low sensitivity, our precision value is
significantly higher than others and therefore gets the best
overall performance.
We also visualized the segmentation results for com-
parison. We demonstrate four examples of the CCF dataset
from different patients in Figure 4 with the prediction of
the compared models and corresponding dice scores of
them. The first example is a typical juxtapleural nodule with
some heterogeneous attenuation within it. Although all the
five models successfully identified it, UNet, ResUNet and
ResUNet++ just captured the parts with relatively higher
intensity and resulted in lower Dice scores. In the second
example, UNet and ResUNet failed to identify the tiny
nodule, while our model segmented the nodule accurately.
In the third and fourth examples, our model captured the
shapes of the nodules accurately resulting in the best Dice
score among the models.
4.5. Experiments for transfer learning strategies
During pre-training, the neural network model learns
useful feature representations from the large datasets. There-
fore, the parameters of some layers are usually frozen during
the training process to prevent them from being updated
based on the gradients computed from the loss function. By
freezing some layers, the learned features are preserved and
not overwritten during the fine-tuning process. Additionally,
freezing layers can reduce computational cost and the risk of
overfitting since there are fewer parameters to update. The
choice of which layers to freeze depends on factors such as
the similarity between the small and large datasets. In an
encoder-decoder based model, the early convolutional layers
in encoder block are typically responsible for extracting
low-level features like edges and textures, which are often
generalizable across different datasets. These layers can be
frozen if the pre-trained weights are from a dataset similar
to the target training data. The final layers in encoder block
can capture high level and task-specific features, and the de-
coder layers are responsible for upsampling and combining
features from different resolutions.
To ensure the best result using transfer learning, we
conducted a set of experiments to determine the optimal
transfer learning strategy, as shown in Table 2. We froze
S1 (the first segmentation encoder block), S5 (the last seg-
mentation encoder block), S9 (the last segmentation decoder
block), and C1 (the first classification block). Since both
the pre-training and the fine-tuning involve binary classifi-
cation, we also tested the case when freezing FC (the fully
connected layer of the classification part). The results in
Table 2 indicate that fully fine-tuning without freezing any
layers outperforms all the other strategies, demonstrating
that there are essential differences between LUNA16 and
the CCF dataset. However, the ablation study in the next
section shows that the model still benefits from pre-training
on LUNA16.
4.6. Ablation study
We conducted an ablation study to assess the effect of
each component in our model, as presented in Table 3. In the
first row, we implemented the proposed backbone without
feature combination blocks and STD regularization. This
means the segmentation part works independently without
being combined with classification. The results show a Dice
score of 0.755, Hausdorff distance of 3.509 mm, and average
symmetric surface distance of 0.276 mm. In the second row,
we implemented the proposed model without STD regular-
ization and observed better performance in all metrics ex-
cept average symmetric surface distance. This highlights the
improvement brought by the feature combination blocks. In
the third row, we applied STD regularization into our model
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 7 of 10
Detection-Guided Deep Learning-Based Model with Spatial Regularization for Lung Nodule Segmentation
Figure 4: Visual comparison of segmentation results for different methods, including four examples in the CCF dataset. The dice
score of each result is shown on the upper left of each prediction.
Table 2
Experiments testing different TL strategies
Model
Pre
Sen
Dice
IoU
HD(mm)
ASSD(mm)
W/o freezing
0.828
0.885
0.814
0.744
3.188
0.280
Freeze S1
0.804
0.889
0.798
0.728
3.611
0.310
Freeze S5
0.817
0.888
0.810
0.741
3.608
0.296
Freeze S9
0.879
0.709
0.708
0.627
4.644
0.401
Freeze C1
0.824
0.879
0.802
0.734
3.394
0.287
Freeze FC
0.815
0.884
0.799
0.728
3.226
0.295
but setting the value of 𝜆2 in Equation 4 as zero to show the
effect of the third term in Equation 4. Comparing the second
and third rows, it is evident that STD regularization leads to
a noticeable improvement in precision and Dice score. In the
fourth row, we implemented the model to show the effect of
the fourth term in Equation 4. Compared with the second the
third rows, all the metrics except sensitivity are significantly
improved. Finally, we applied the transfer learning strategy
in our model, achieving a Dice score of 0.814 and IoU of
0.744. Although transfer learning leads to slightly worse
HD and ASDD, it again shows the difference between the
two datasets and that the model still benefits from transfer
learning.
5. Conclusions
In this paper, we present a multitask deep learning model
designed and implemented for segmenting small clinical
lung nodule datasets. Our model integrates segmentation
and classification components through feature combina-
tion blocks to facilitate information sharing between the
two tasks. We incorporate spatial regularization adaptively
guided by the classification output in the segmentation
process. Additionally, we employ transfer learning by pre-
training the model on LUNA16, a large public dataset. Our
experimental results demonstrate that the proposed model
surpasses other classic models in the clinical dataset. We
conduct a series of experiments by freezing different hidden
layers during fine-tuning to determine the optimal transfer
learning strategy. Furthermore, our ablation study showcases
the enhancements provided by various components within
our model.
While our model demonstrates promising results, there
remains ample room for improvement, particularly when
applied to small clinical datasets. Our model, especially
the classification component, is designed to be simple and
efficient for such datasets. However, we recognize that there
is considerable flexibility in altering both the classification
and segmentation components. Additionally, the distribution
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 8 of 10
Detection-Guided Deep Learning-Based Model with Spatial Regularization for Lung Nodule Segmentation
Table 3
Ablation studies.
Model
Pre
Sen
Dice
IoU
HD(mm)
ASSD(mm)
Backbone w/o combination
0.820
0.848
0.755
0.693
3.509
0.276
Backbone w/o STD
0.823
0.849
0.764
0.704
4.020
0.285
Backbone + STD with 𝜆2 = 0
0.846
0.847
0.787
0.720
3.927
0.287
Backbone + STD
0.870
0.827
0.801
0.735
2.929
0.211
Backbone + TL + STD
0.828
0.885
0.814
0.744
3.188
0.280
of feature combination blocks may need to be modified
to further enhance the model’s performance on specific
datasets.
Credit author statement
Jiasen Zhang: implementation, conceptualization, method-
ology, evaluation, writing. Mingrui Yang: methodology,
validation, supervision, review & editing. Weihong Guo:
methodology, conceptualization, supervision, review & edit-
ing. Brian A. Xavier: clinical data, annotation, conceptual-
ization. Michael Bolen: clinical data, annotation, conceptu-
alization. Xiaojuan Li: investigation, supervision, review &
editing.
Conflict of interest
The authors declare no conflict of interest.
Acknowledgements
This work was supported by the Cleveland Clinic Re-
search Co-Laboratories Award.
References
Armato III, S.G., McLennan, G., Bidaut, L., McNitt-Gray, M.F., Meyer,
C.R., Reeves, A.P., Zhao, B., Aberle, D.R., Henschke, C.I., Hoffman,
E.A., et al., 2011. The lung image database consortium (lidc) and image
database resource initiative (idri): a completed reference database of
lung nodules on ct scans. Medical physics 38, 915–931.
Chan, T., Vese, L., 1999.
An active contour model without edges, in:
International conference on scale-space theories in computer vision,
Springer. pp. 141–151.
Chan, T.F., Vese, L.A., 2001.
Active contours without edges.
IEEE
Transactions on image processing 10, 266–277.
Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2014.
Semantic image segmentation with deep convolutional nets and fully
connected crfs. arXiv preprint arXiv:1412.7062 .
Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2017a.
Deeplab: Semantic image segmentation with deep convolutional nets,
atrous convolution, and fully connected crfs.
IEEE transactions on
pattern analysis and machine intelligence 40, 834–848.
Chen, L.C., Papandreou, G., Schroff, F., Adam, H., 2017b.
Rethinking
atrous convolution for semantic image segmentation.
arXiv preprint
arXiv:1706.05587 .
Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., 2018. Encoder-
decoder with atrous separable convolution for semantic image segmen-
tation, in: Proceedings of the European conference on computer vision
(ECCV), pp. 801–818.
Chollet, F., 2017.
Xception: Deep learning with depthwise separable
convolutions, in: Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1251–1258.
Cuadra, M.B., Cammoun, L., Butz, T., Cuisenaire, O., Thiran, J.P., 2005.
Comparison and validation of tissue modelization and statistical classi-
fication methods in t1-weighted mr brain images. IEEE transactions on
medical imaging 24, 1548–1565.
Dhanachandra, N., Manglem, K., Chanu, Y.J., 2015. Image segmentation
using k-means clustering algorithm and subtractive clustering algorithm.
Procedia Computer Science 54, 764–771.
Esedo¯g Lu, S., Otto, F., 2015.
Threshold dynamics for networks with
arbitrary surface tensions. Communications on pure and applied math-
ematics 68, 808–864.
Everingham, M., Van Gool, L., Williams, C., Winn, J., Zisserman,
A., 2012.
The pascal visual object classes challenge 2012
(voc2012) results. 2012 http://www. pascal-network. org/challenges, in:
VOC/voc2012/workshop/index. html.
He, K., Lian, C., Zhang, B., Zhang, X., Cao, X., Nie, D., Gao, Y., Zhang, J.,
Shen, D., 2021. Hf-unet: learning hierarchically inter-task relevance in
multi-task u-net for accurate prostate segmentation in ct images. IEEE
transactions on medical imaging 40, 2118–2128.
Irshad, H., Veillard, A., Roux, L., Racoceanu, D., 2013. Methods for nuclei
detection, segmentation, and classification in digital histopathology: a
review—current status and future potential. IEEE reviews in biomedical
engineering 7, 97–114.
Jha, D., Smedsrud, P.H., Riegler, M.A., Johansen, D., De Lange, T.,
Halvorsen, P., Johansen, H.D., 2019. Resunet++: An advanced archi-
tecture for medical image segmentation, in: 2019 IEEE international
symposium on multimedia (ISM), IEEE. pp. 225–2255.
Jia, F., Liu, J., Tai, X.C., 2021. A regularized convolutional neural network
for semantic image segmentation. Analysis and Applications 19, 147–
165.
Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classification
with deep convolutional neural networks. Advances in neural informa-
tion processing systems 25.
Liu, H., Cao, H., Song, E., Ma, G., Xu, X., Jin, R., Jin, Y., Hung, C.C., 2019.
A cascaded dual-pathway residual network for lung nodule segmentation
in ct images. Physica Medica 63, 112–121.
Liu, J., Dong, B., Wang, S., Cui, H., Fan, D.P., Ma, J., Chen, G., 2021.
Covid-19 lung infection segmentation with a novel two-stage cross-
domain transfer learning framework.
Medical image analysis 74,
102205.
Liu, J., Tai, X.C., Luo, S., 2020.
Convex shape prior for deep neural
convolution network based eye fundus images segmentation.
arXiv
preprint arXiv:2005.07476 .
Liu, J., Wang, X., Tai, X.C., 2022. Deep convolutional neural networks
with spatial regularization, volume and star-shape priors for image
segmentation. Journal of Mathematical Imaging and Vision 64, 625–
645.
Misra, I., Shrivastava, A., Gupta, A., Hebert, M., 2016.
Cross-stitch
networks for multi-task learning, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 3994–4003.
Mumford, D.B., Shah, J., 1989.
Optimal approximations by piecewise
smooth functions and associated variational problems. Communications
on pure and applied mathematics .
Otsu, N., 1980.
An automatic threshold selection method based on
discrimination and least squares criteria. Trans. Inst. Electr. Comm. Eng.
Jpn., D 63, 349–356.
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 9 of 10
Detection-Guided Deep Learning-Based Model with Spatial Regularization for Lung Nodule Segmentation
Rangel, B.M.S., Fernández, M.A.A., Murillo, J.C., Ortega, J.C.P., Arreguín,
J.M.R., 2016. Knn-based image segmentation for grapevine potassium
deficiency diagnosis, in: 2016 International conference on electronics,
communications and computers (CONIELECOMP), IEEE. pp. 48–53.
Riaz, Z., Khan, B., Abdullah, S., Khan, S., Islam, M.S., 2023.
Lung
tumor image segmentation from computer tomography images using
mobilenetv2 and transfer learning. Bioengineering 10, 981.
Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks
for biomedical image segmentation, in: Medical image computing and
computer-assisted intervention–MICCAI 2015: 18th international con-
ference, Munich, Germany, October 5-9, 2015, proceedings, part III 18,
Springer. pp. 234–241.
Setio, A.A.A., Traverso, A., De Bel, T., Berens, M.S., Van Den Bogaard, C.,
Cerello, P., Chen, H., Dou, Q., Fantacci, M.E., Geurts, B., et al., 2017.
Validation, comparison, and combination of algorithms for automatic
detection of pulmonary nodules in computed tomography images: the
luna16 challenge. Medical image analysis 42, 1–13.
Shen, T., Gou, C., Wang, J., Wang, F.Y., 2019. Simultaneous segmentation
and classification of mass region from mammograms using a mixed-
supervision guided deep model.
IEEE Signal Processing Letters 27,
196–200.
Siegel, R.L., Miller, K.D., Wagle, N.S., Jemal, A., 2023. Cancer statistics,
2023. Ca Cancer J Clin 73, 17–48.
Tjon, E., Moh, M., Moh, T.S., 2021. Eff-ynet: A dual task network for
deepfake detection and segmentation, in: 2021 15th International Con-
ference on Ubiquitous Information Management and Communication
(IMCOM), IEEE. pp. 1–8.
Wang, P., Chen, P., Yuan, Y., Liu, D., Huang, Z., Hou, X., Cottrell, G.,
2018. Understanding convolution for semantic segmentation, in: 2018
IEEE winter conference on applications of computer vision (WACV),
Ieee. pp. 1451–1460.
Wellner, P.D., 1993.
Adaptive thresholding for the digitaldesk.
Xerox,
EPC1993-110 404.
Wu, Y.H., Gao, S.H., Mei, J., Xu, J., Fan, D.P., Zhang, R.G., Cheng,
M.M., 2021. Jcs: An explainable covid-19 diagnosis system by joint
classification and segmentation. IEEE Transactions on Image Processing
30, 3113–3126.
Yi, F., Moon, I., 2012.
Image segmentation: A survey of graph-cut
methods, in: 2012 international conference on systems and informatics
(ICSAI2012), IEEE. pp. 1936–1941.
Zana, F., Klein, J.C., 2001.
Segmentation of vessel-like patterns using
mathematical morphology and curvature evaluation. IEEE transactions
on image processing 10, 1010–1019.
Zhang, J., Guo, W., 2023. A new regularization for deep learning-based
segmentation of images with fine structures and low contrast. Sensors
23, 1887.
Zhang, Z., Liu, Q., Wang, Y., 2018. Road extraction by deep residual u-net.
IEEE Geoscience and Remote Sensing Letters 15, 749–753.
Jiasen Zhang et al.: Preprint submitted to Elsevier
Page 10 of 10
