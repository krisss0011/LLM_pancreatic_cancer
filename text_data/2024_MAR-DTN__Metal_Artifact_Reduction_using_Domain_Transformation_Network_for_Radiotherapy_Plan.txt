MAR-DTN: Metal Artifact Reduction using
Domain Transformation Network for
Radiotherapy Planning
Belén Serrano-Antón1,2,3[0000−0001−9328−9525], Mubashara
Rehman4,5[0009−0007−2935−0409]⋆, Niki Martinel4[0000−0002−6962−8643], Michele
Avanzo5[0000−0003−1711−4242], Riccardo Spizzo5[0000−0001−7772−0960], Giuseppe
Fanetti5[0000−0002−9677−3176], Alberto P. Muñuzuri2,3[0000−0002−0579−9347], and
Christian Micheloni4[0000−0003−4503−7483]
1 FlowReserve Labs S.L., 15782 Santiago de Compostela, Spain
2 CITMAga, 15782 Santiago de Compostela, Spain
3 Group of Nonlinear Physics, University of Santiago de Compostela, 15782 Santiago
de Compostela, Spain
4 Machine Learning and Perception Lab, Università degli Studi di Udine, 33100
Udine (UD) Italy
5 Centro di Riferimento Oncologico di Aviano IRCCS, 33081 Aviano (PN) Italy
Abstract. For the planning of radiotherapy treatments for head and
neck cancers, Computed Tomography (CT) scans of the patients are
typically employed. However, in patients with head and neck cancer,
the quality of standard CT scans generated using kilo-Voltage (kVCT)
tube potentials is severely degraded by streak artifacts occurring in the
presence of metallic implants such as dental fillings. Some radiotherapy
devices offer the possibility of acquiring Mega-Voltage CT (MVCT) for
daily patient setup verification, due to the higher energy of X-rays used,
MVCT scans are almost entirely free from artifacts making them more
suitable for radiotherapy treatment planning.
In this study, we leverage the advantages of kVCT scans with those of
MVCT scans (artifact-free). We propose a deep learning-based approach
capable of generating artifact-free MVCT images from acquired kVCT
images. The outcome offers the benefits of artifact-free MVCT images
with enhanced soft tissue contrast, harnessing valuable information ob-
tained through kVCT technology for precise therapy calibration. Our
proposed method employs UNet-inspired model, and is compared with
adversarial learning and transformer networks. This first and unique ap-
proach achieves remarkable success, with PSNR of 30.02dB across the
entire patient volume and 27.47dB in artifact-affected regions exclusively.
It is worth noting that the PSNR calculation excludes the background,
concentrating solely on the region of interest.
Keywords: kilo-Voltage-CT (kVCT)· Mega-Voltage-CT (MVCT)· Metal
artifact reduction (MAR)· Artificial intelligence (AI).
⋆co-first author of the manuscript
arXiv:2409.15155v1  [eess.IV]  23 Sep 2024
Accepted in 27th International Conference on Pattern Recognition (ICPR)
1
Introduction
Since their introduction in the 1970s, advanced medical imaging techniques,
particularly high-resolution Computed Tomography (CT), have been crucial for
computer-assisted diagnosis [5]. However, when patients with metal implants
undergo imaging, such as dental fillings or hip prostheses, severe beam attenua-
tion occurs, resulting in discernible streaks that compromise image fidelity and
hampering clinical assessment [1].
Recent advancements in deep learning have shown promise in mitigating
metal artifacts through supervised learning methodologies. However, obtaining
ground truth images without artifacts is challenging. [9] tackles this issue by
generating datasets with and without metal artifacts, enabling the development
of numerous algorithms for Metal Artifact Reduction (MAR). Other approaches
encompass a variety of image-to-image deep learning models, including deep
residual architectures [6] and interpretable convolutional dictionary networks
[24]. Numerous other methodologies utilize sinogram-to-sinogram deep learn-
ing models [20,30] or dual-domain deep learning models using both image and
sinogram data [15,12,28]. These models can be further extended by incorporat-
ing state-of-the-art interpolation-based algorithm Normalized MAR corrected
data as an extra input [4,14]. A combination of multiple supervised deep learn-
ing methods can be effective in reducing metal artifacts from complex cases of
cardiac CT images [17]. Approach [25] uses pix2pix [7] for MAR, it introduces
band-wise normalization method, which splits a CT image into three channels ac-
cording to the intensity value and considerably improves the performance of the
cGAN. CNN-based approach [29] is introduced to predict an artifact-suppressed
prior image. Extending these concepts, [15] introduced DuDoNet, a dual-domain
learning technique combining sinogram enhancement and image domain recon-
struction. Improved version of DuDoNet [15], restores sinogram consistency and
simultaneously enhance CT images by incorporating metal segmentation in both
domains. In more recent work, [18] introduced an alternative dual-domain ap-
proach, emphasizing deep sinogram completion for improved MAR performance.
Mega-Voltage Computed Tomography (MVCT) is used for verification of pa-
tient positioning immediately before the radiotherapy treatment. It is less prone
to streak artifacts from metallic implants because it uses high-energy beams pro-
duced by a radiotherapy linear accelerator, which are less attenuated by metal
than conventional diagnostic X-rays. The main drawback of MVCT is that it
is available only in some specialized radiotherapy machines [5]. [16] proposes to
reduce metal artifacts in kVCT by using MVCT images as prior images. The
iterative method proposed in [19] segments tissue regions in Megavoltage cone-
beam CT images and the metal region in kVCT images for template creation.
Forward projection of the templates generates sinograms. Artifact images are
reconstructed from the sinograms. Finally, corrected images are obtained by sub-
tracting artifact images from original kVCT images. [21] utilizes the sinogram
of kVCT and MVCT along with the corresponding metal trace to ultimately
produce artifact-free kVCT images. Methodology proposed in [10], employing
convolutional neural networks to obtain artifact-free kVCT images, by utilizing
Accepted in 27th International Conference on Pattern Recognition (ICPR)
two networks where the first generates synthetic artifact-free kVCT images from
MVCT, which are then used to train the second network. The second network
takes kVCT images with artifacts as input and produces artifact-free kVCT im-
ages as output.
Raw data
Aligned &
Pre-processed
Dataset
Alignment
INPUT
kVCT
Artifact-contaminated
Features Fusion
FEATURE ENCODER
FEATURE DECODER
OUTPUT
Ground Truth
MAR Domain Transformation Network
(a)
(b)
(c)
Fig. 1: (a) Abstract overview of proposed Domain Transformation Network. (b)
Sagittal view of the body with distinct delineations of the head, neck, and body
regions(blue). (c) kVCT (top) and MVCT (bottom) axial artifact slices after
normalization and masking.
Different techniques have been utilized to mitigate metal artifacts in kVCT
scans, yet they predominantly operate within the same domain. In contrast,
our innovative approach involves transforming the CT domain from kVCT to
MVCT, as MVCT is inherently less artifact-sensitive, henceforth preferred for
its robustness in clinical applications. We hereby propose Metal Artifact Re-
duction using Domain Transformation Network (MAR-DTN) to address metal
artifacts in oncological imaging. Our approach generates MVCT using a model
Accepted in 27th International Conference on Pattern Recognition (ICPR)
that employs a UNet architecture with skip connections, tailored for MAR from
kVCT images, to systematically mitigate artifacts during the transformation pro-
cess from kVCT to MVCT. Leveraging its encoder-decoder structure and spatial
awareness, it effectively processes 512x512 pixel images to produce MVCT out-
put (see Fig. 1a). This network is trained by employing 3858 kVCT slices of head
and neck region (Fig. 1b).
It has achieved exceptional results, a noticeable point is that Peak Signal-
to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) in all
tables concern the regions of interest only, not the background like the other
methodologies. This approach aims to improve CT quality, interpretability, and
analysis of medical images by transforming artifact-contaminated kVCT into
artifact-free MVCT. This novel method enables radiation oncologists to gain
insights into MVCT from kVCT alone, potentially avoiding repeated imaging and
its implications for patients’ health. In addition, our study entails a comparative
evaluation of MAR-DTN’s performance in relation to three current state-of-the-
art methods, based on adversarial learning and transformers.
2
Methods
2.1
Dataset Collection and Processing
Due to the lack of available aligned kVCT and MVCT datasets, a new dataset
consisting of 5469 images from 52 patients from the National Cancer Institute
(CRO) IRCCS 6. For each patient, we acquired kVCT and MVCT images; the
kVCT images obtained have matrix size of 512 × 512, on the axial plane with a
pixel size of 1.074 mm × 1.074 mm, and slice thickness of 2 mm, furthermore,
the MVCT images obtained have a matrix size of 512 × 512, on the axial plane
with a pixel spacing of 0.754 mm × 0.754 mm with the slice thickness of 2 mm
or 4 mm.
Patients underwent intensity-modulated radiotherapy for oropharyngeal or
nasopharyngeal cancer. Non-contrast-enhanced CT imaging was performed us-
ing a 32-slice scanner (Toshiba Aquilion LB, Toshiba Medical Systems Europe,
Zoetermeer, the Netherlands) with parameters set at 120 kVp, 2-5 mm slice
thickness, and 1.07-1.17 mm pixel size. Additionally, patients underwent scan-
ning with helical tomotherapy (Hi-Art II Tomotherapy System, Tomotherapy
Inc., Madison, Wisconsin, USA), utilizing a radiotherapy 6MV linear accelera-
tor capable of acquiring MVCT images for daily patient setup verification. The
imaging beam, produced by the same LINAC as the therapeutic beam, had a
nominal energy of 3.5 MV, with slice thickness ranging from 2-5 mm and a pixel
size of 0.75 mm.
The slices of each modality volume are manually categorized into three re-
gions: head, neck, and body (see Fig. 1b). The head region comprises from the
beginning of the cranial cavity to the chin, while the neck region spans from
6 Centro di Riferimento Oncologico di Aviano IRCCS, Via F.Gallini 2, Aviano (PN),
33081, Italy
Accepted in 27th International Conference on Pattern Recognition (ICPR)
the chin to the shoulders. The remaining (body region) slices are not considered
since we care about removing artifacts caused by metal implants in the teeth
area. To separate the artifact-corrupted slices from artifact-free slices, we de-
fine artifacts in kVCT images as values exceeding 2000 Hounsfield Units (HU),
while for MVCT images, the artifact threshold is set at 1000 HU. These thresh-
olds were determined through visual inspection and following recommendations
from [10,16].
Table 1: Number of patients and slices (images) in the acquired dataset.The head
and neck region include the artifact slices since we work with artifacts caused
by metallic dental implants.
Set
Number of patients
Number of slices of the
head and neck regions
Number of slices
with artifacts
Train
36
3858
560
Validation
10
1031
153
Test
6
580
96
For the training and subsequent evaluation of the proposed model, two datasets
are constructed; the first is DAll, the dataset comprises CT slices up to the neck
region (including slices with and without artifacts), and the second dataset is
DArt, which contains only artifact-contaminated CT slices. Out of the total num-
ber of slices in the dataset, 14.78% exhibit artifacts, hence belong to DArt. Both
datasets are further sub-divided into three distinct datasets, as specifically, 70%
of the patients are used for training (DT r
All and DT r
Art), 20% for validation (DV al
All
and DV al
Art ), and the remaining 10% for testing (DT s
All and DT s
Art) (see Table 1).
2.2
kVCT-MVCT Alignment and Preprocessing
The primary goal is to create a dataset with aligned kVCT and MVCT images.
Despite originating from the same patient and reference system (with the same
origin point), both image volumes (kVCT and MVCT) were not pixel-aligned
leading to increased challenges (i.e., such as the need to address alignment and
artifact reduction simultaneously). To achieve this, image alignment was per-
formed using the Elastix module of 3D Slicer, open source software (version
5.6.1) [3,11].
The aligned kVCT and MVCT volumes undergo normalization to the range
[−1, 1]. This process involves setting the lower threshold at −1000 for air and
upper thresholds at 2000 for kVCT artifacts and 1000 for MVCT artifacts. Ad-
ditionally, utilizing the segmentation provided by clinicians (depicted as green
segmentation in Fig. 1b), the image background is standardized to the value of
−1. The result of such a preprocessing on two sample kVCT and MVCT slices
is shown in Fig.1c.
Accepted in 27th International Conference on Pattern Recognition (ICPR)
2.3
Proposed Methodology
The objective is to project images acquired in the kVCT domain onto the MVCT
domain while removing/reducing the artifacts induced by metallic implants.
In what follows, with m denoting the kVCT (k) or MVCT (M) modality, we
let Xr
m ∈Rd×d be a raw (r) slice with d = 512 denoting the image resolution.
The volume containing the n slices of a patient is Vr
m ∈Rd×d×n. The original
images undergo an alignment process (see Section 2.2), resulting in two new
volumes, Va
k, Va
M = alignment(Vr
k, Vr
M), which are aligned pixel by pixel.
After this process, all the slices in a volume are preprocessed (see Section
2.2) to obtain Xp
m = preprocess(Xa
m), ∀Xa
m ∈Va
m that collectively define the
dataset for the experiments. The summary diagram is shown in Fig.2.
The input to our model is a preprocessed kVCT image, Xp
k, while the ground
truth is the corresponding preprocessed MVCT image, Xp
M. The output of the
model is the domain transferred kVCT to MVCT slice, denoted as ˆXM ∈Rd×d.
Raw data
Data alignment
Preprocessing
Fig. 2: Steps followed for dataset generation. We start with raw and unaligned
kVCT and MVCT volumes –slices (lines in the cube) do not correspond. Then,
volumes are pixel-aligned and so the slices correspond (Section 2.2). Finally,
corresponding slices in kVCT and MVCT volumes are normalized and masked
(Section 2.2).
Network architectures We propose a Metal Artifact Reduction using Domain
Transformation Network (MAR-DTN), which closely aligns with the architec-
tural principles of the UNet framework [22]. The UNet architecture has been
widely used in previous works for pixel-to-pixel image tasks. In medical imag-
ing, specifically, it has demonstrated excellent results in segmentation, denois-
ing, and MAR [22,23,25]. The detailed architectural explanation of our proposed
model, named MAR-DTN, can be found in the supplementary material (Section
1: Explanation of Proposed Model Architecture).
Our investigation involves a comparative analysis of the performance of
MAR-DTN against three contemporary state-of-the-art methods. The first one
is a Conditional Generative Adversarial Network (cGAN), named pix2pix [7].
Accepted in 27th International Conference on Pattern Recognition (ICPR)
Additionally, a modification of this network is included, replacing the original
generator with MAR-DTN (referred to as custom-pix2pix).
In addition, a network leveraging transformers is implemented due to their
demonstrated high performance in addressing pixel-to-pixel image tasks. The
SwinIR architecture [13] is structured into three key components: shallow fea-
ture extraction, deep feature extraction, and high-quality image reconstruction.
Notably, the deep feature extraction module integrates numerous residual Swin
Transformer blocks (RSTB), each incorporating multiple Swin Transformer lay-
ers alongside a residual connection.
Finally, an architecture initially designed for medical image segmentation is
included. This is the INet architecture, a network that does not perform down-
sampling. It simply enlarges receptive fields by increasing the kernel sizes of
convolutional layers in steps (e.g., from 3 × 3 to 7 × 7 and then 15 × 15). In our
case, the final activation is not performed in order to obtain a network capable
of generating images. We used this architecture for image generation because
INet maintains spatial information by fixing the sizes of feature maps and fuses
multilevel semantics by concatenating feature maps of all preceding layers. This
allows INet to enhance optimization capabilities.
Loss functions In addressing artifact reduction with neural networks, various
loss functions such as L1, FFL (Focal Frequency Loss), MSE (Mean Squared Er-
ror), SSIM (Structural Similarity Index), and MS-SSIM (Multi-Scale Structural
Similarity Index) offer distinct advantages.
– The weighted Lw
1
loss function is defined by: Lw
1
= ∥ˆXM −Xp
M∥1 · w,
where w ∈Rd×d is the pixel weight. This loss emphasizes the absolute
differences between predicted and ground truth values and penalizes outliers,
contributing to robust artifact reduction.
– FFL [8] is defined by: Lβ,α
FFL =
1
d·d
Pd−1
u=0
Pd−1
v=0 z(u, v)|F ˆ
XM(u, v)−FXp
M(u, v)|2·
β, where z(u, v) = |F ˆ
XM(u, v) −FXp
M(u, v)|α, β ∈R is the weight of spatial
frequency, α ∈R is the scaling factor, and F(u, v) is the spatial frequency
value at the spectrum coordinate (u, v). This loss focuses on high-frequency
artifacts, helps in preserving image details while suppressing artifacts, thus
enhancing perceptual quality.
– MSE is defined by ∥ˆXMij −XMij∥2
2 which measures the average squared
distance, and provides simplicity and ease of interpretation.
– SSIM [26], evaluates luminance, contrast and structure, ensuring preserva-
tion of perceptual features, making them suitable for maintaining image
fidelity during artifact reduction tasks.
– MS-SSIM divides images into multiple scales and computes SSIM for each
scale separately. Then, it averages these SSIM values to get a single value
representing structural similarity. This method offers a more comprehensive
evaluation, considering structural similarity across different resolutions.
Implementation details and evaluation metrics All networks have the
same input and output shape, 512 × 512, corresponding to the size of Xp
k and
Accepted in 27th International Conference on Pattern Recognition (ICPR)
Xp
M. Models were optimized using Adam with learning rate and weight decay set
to 0.001 and 5e−4, respectively. The batch size was set to 4 for all networks except
SwinIR for which we used 2 samples (due to computational memory issues). We
trained for 20 epochs with early stopping with a patience of 5 epochs. Data
augmentation [2] includes horizontal flip with a probability of 0.5 and shift,
scale, and rotate with a probability of 0.8 (shift_limit= 0.0625, scale_limit=
0.1, rotate_limit= 5). This introduces variability into our dataset by applying
transformation probabilities to alter the dataset in each epoch, thus aiding in
the mitigation of data limitation.
Models were trained on an Intel Xeon Server with 188GB of RAM and an
Nvidia A100 GPU. We evaluated our methodology using PSNR and SSIM met-
rics.
3
Experimental Results
3.1
Loss function analysis
The impact of different loss functions, whether used individually or in combina-
tion, is analyzed in this study. We excluded the INet network from our evaluation
because its performance, as detailed in Section 3.1, is significantly lower com-
pared to the other architectures. Including INet could skew the comparative
analysis and potentially introduce biases, thus detracting from a fair assessment
of the loss functions’ effects on more competitive networks.
First, we explore the impact of using an L1 loss function with weights (Lw
1 ) on
images containing artifacts. Weight assignment is based on body segmentation
provided by clinicians (see Fig. 1b), where w[i, j] is set to 0.1 outside the body
segment and varies within the set {1, 25, 50, 100} inside the segment for slices
with artifacts. Slices without artifacts maintain a weight of 1 throughout the
body segment. Since the only variable is the weight within the body segment,
we simplify the notation in the following sections and denote this value as w.
Therefore, L100
1
indicates a weight of 100 within the body segment for slices with
artifacts.
Additionally, the parameters β and α of the Lβ,α
FFL are discretely varied in the
set of values {0.5, 1, 1.5}. This variation allows for exploring different weightings
and contributions of both parameters in the neural network’s learning process,
particularly in handling images with artifacts.
Lw
1 Analysis Fig. 3a and Fig. 3b show the PSNR and SSIM values obtained by
the networks of the study after training with Lw
1 loss when w ∈{1, 25, 50, 100}
using DAll.
The first thing to note is the limited variability of results obtained when
modifying the parameter w. In terms of PSNR, the results do not vary by more
than 3dB, while for SSIM, the results demonstrate a variance of no more than
10%.
Accepted in 27th International Conference on Pattern Recognition (ICPR)
In the case of MAR-DTN, a positive trend in the artifact set is observed when
w > 25 increases. Conversely, with SwinIR, better results are achieved when no
supplementary weight is allocated to the artifact class. Moreover, when w > 1
parameters have similar results.
For pix2pix, no significant difference in results is observed, and the same
holds true for custom-pix2pix when w > 1. However, increasing the value of w
for the artifact class does lead to an improvement in the artifact set results.
Regarding the results in the DAll, represented by dots in Fig. 3a and Fig.
3b, we observe slightly inferior results when giving more weight to the DArt set.
However, we are not concerned as the focus is on the artifact region. Therefore,
for the remaining experiments, we will use w = 100, as it yields the best result
for MAR-DTN and similar values to the state-of-the-art L1 for the rest of the
networks.
(a)
(b)
Fig. 3: PSNR (a) and SSIM (b) values evaluated on the DAll. The dots represent
the mean value of all slices in the dataset, while the bars represent the mean
value of slices with artifacts. Values obtained using the four considered networks
(MAR-DTN, pix2pix, custom-pix2pix and SwinIR) trained on the DAll with the
Lw
1 loss function only.
Lβ,α
FFL Analysis β and α were varied within the set {0.5, 1, 1.5}. The average
value of the metrics evaluated on the DT s
All set can be seen in Fig. 4a and Fig.
4b.
As with the prior study, we observe some variability in the results with less
than 2dB in PSNR and 10% in SSIM.
However, it is observed that increasing the value of alpha decreases the met-
rics. For the minimum α value, α = 0.5, the best PSNR result is for β = 1,
with a PSNR value of 27.81dB. On the other hand, the mean SSIM value when
alpha = 0.5 and β = 1 is 0.64, very close to the best value, which is 0.65. Taking
this into account, we conclude that the best combination of values is β = 1 and
α = 0.5.
Accepted in 27th International Conference on Pattern Recognition (ICPR)
(a)
(b)
Fig. 4: Heatmaps with the mean values of PSNR (a) and SSIM (b) evaluated on
the test dataset after training the networks using the Lβ,α
FFL loss function with
various combinations of the parameters α and β (x and y-axis, respectively).
Each cell represents the mean of 8 values, the first 4 corresponding to the pa-
rameter value evaluated on DT s
Art, and the last 4 corresponding to the parameter
value evaluated on the DT s
All, for each neural network in the study, MAR-DTN,
pix2pix, custom-pix2pix, and SwinIR, respectively.
Loss function comparison Table 2 compares the results obtained considering
different loss functions combinations and datasets.
The L100
1
loss function achieves the best results on DArt for MAR-DTN, with
a PSNR of 27.17dB, and it is the second best result for pix2pix, with a PSNR
of 26.31dB. However, for both custom-pix2pix and SwinIR, the performance is
reduced by almost 2dB.
Also the L1,0.5
FFL loss function has been tested alone. It yields less accurate
results than L100
1
, decreasing the PSNR value by up to 2dB for pix2pix. However,
custom-pix2pix maintains a PSNR value of 26.15dB, competitive with the rest
of the loss functions. Regarding the rest of the loss functions, the improvement
in using LMS−SSIM instead of LSSIM stands out, especially notable in custom-
pix2pix. The most complex loss function (L100
1
+LMS−SSIM +L1,0.5
FFL) introduces
noise during training and fails to surpass the metric value achieved by simpler
functions. Nonetheless, SwinIR achieves the best result, with a PSNR of 26.42dB.
An example of the reconstruction of a slice with artifacts can be seen in Fig. 5.
However, the results in the DAll improve when other loss functions are added
to L100
1
. In the case of MAR-DTN and custom-pix2pix, the best combination is
L100
1
+ LSSIM, reaching a PSNR value of 30.02dB. For pix2pix and SwinIR, the
best combination is L100
1
+ LMSE with a PSNR of 28.92 and 29.39dB, respec-
tively.
In general, the metric values for slices with artifacts are lower when trained on
DAll. This is consistent with having an unbalanced dataset and means that the
loss functions are not entirely capable of addressing the issue of class imbalance.
On the other hand, the results obtained with INet can be found in the last
column of Table 2. The highest PSNR achieved is 12.67dB for the artifact set,
Accepted in 27th International Conference on Pattern Recognition (ICPR)
Table 2: Comparative analysis for different networks and loss function combina-
tions, indicated with a check mark which sum of loss functions have been used
for training. For the Pix2Pix networks, it indicates the loss function of the gener-
ator. The dataset column indicates the dataset with which the network has been
trained and evaluated; where dataset is DAll then model is trained on DT r
All and
tested on DT s
All, and in case of DArt then model is trained on DT r
Art, and tested on
DT s
Art. Finally, the remaining columns show the PSNR and SSIM values obtained
for the test sets. Where the dataset is the DAll, we report both on the perfor-
mance obtained on artifact slices from within the DT s
Art, and the mean of PSNR
and SSIM on whole dataset DT s
All (in parentheses). Underlined values indicate the
highest performance for each network with certain loss function combinations,
while highlighted values indicate the highest overall performing model across all
configurations.
Loss combination
Dataset
MAR-DTN
pix2pix [7]
custom-pix2pix
SwinIR [13]
INet [27]
L100
1
LSSIM
LMS-SSIM
LMSE
L1,0.5
FFL
PSNR
SSIM
PSNR
SSIM
PSNR
SSIM
PSNR
SSIM
PSNR
SSIM
✓
DArt
27.17
0.69
26.31
0.64
25.24
0.68
25.46
0.61
11.61
0.04
DAll
26.99
(28.97)
0.71
(0.73)
26.36
(28.7)
0.63
(0.69)
26.61
(29.08)
0.67
(0.7)
25.29
(28.79)
0.59
(0.66)
12.02
(12.03)
0.04
(0.04)
✓
✓
DArt
27.11
0.69
26.21
0.63
26.98
0.71
26.16
0.62
10.94
0.08
DAll
27.09
( 30.02 )
0.69
( 0.73 )
26.39
(28.58)
0.64
(0.68)
27.13
(29.85)
0.68
(0.70)
24.90
(28.97)
0.68
(0.68)
12.01
(12.27)
0.03
(0.03)
✓
✓
DArt
27.46
0.69
26.32
0.65
27.06
0.67
26.26
0.63
11.96
0.01
DAll
27.08
(29.97)
0.69
(0.73)
26.37
(28.69)
0.58
(0.68)
27.04
(29.35)
0.64
(0.70)
26.25
(29.39)
0.63
(0.67)
12.67
(12.95)
0.05
(0.04)
✓
✓
DArt
26.94
0.68
26.25
0.64
26.55
0.64
26.18
0.63
11.70
0.02
DAll
27.11
(29.89)
0.69
(0.72)
26.42
(28.92)
0.68
(0.7)
26.98
(29.22)
0.64
(0.68)
25.24
(29.00)
0.60
(0.66)
11.23
(10.96)
0.01
(0.01)
✓
DArt
26.35
0.64
24.03
0.51
26.15
0.61
24.58
0.59
9.05
0.08
DAll
26.52
(29.51)
0.66
(0.70)
25.85
(28.88)
0.56
(0.61)
26.02
(29.04)
0.60
(0.69)
25.39
(29.33)
0.61
(0.69)
10.03
(10.01)
0.02
(0.01)
✓
✓
DArt
27.06
0.69
25.66
0.63
26.66
0.60
25.40
0.61
11.95
0.03
DAll
26.99
(29.85)
0.69
(0.72)
25.99
(28.56)
0.59
(0.65)
26.48
(29.06)
0.62
(0.69)
25.55
(29.18)
0.60
(0.68)
11.54
(11.86)
0.08
(0.07)
✓
✓
✓
DArt
27.08
0.69
25.66
0.63
26.18
0.56
26.42
0.64
9.58
0.05
DAll
25.65
(28.65)
0.63
(0.69)
26.41
(28.74)
0.68
(0.64)
27.08
(28.04)
0.64
(0.68)
25.45
(28.79)
0.61
(0.66)
12.53
(12.21)
0.06
(0.05)
highlighting the architecture’s inability to compete with the other architectures
in the study. For the SSIM, we observe a similar behavior, with a maximum value
of 8%. In Fig. 5, the results obtained by INet can be seen. The artifacts not only
have not been reduced, but they also acquire higher contrast, along with the
rest of the image. However, new artifacts appear, which blur and deform other
structures; for example, noticeable in the gird where streaks obtained with a
combination of loss functions L100
1
+ LMS−SSIM + L1,0.5
FFL.
3.2
State-of-the-Art Comparison
The comparison between MAR-DTN and state-of-the-art networks (pix2pix,
custom-pix2pix, and SwingIR using L1
1 as loss function) shows MAR-DTN achieves
the best result for the DArt, with a PSNR of 26.99dB and an SSIM of 0.69 points.
However, custom-pix2pix and MAR-DTN achieve the best overall result for the
Accepted in 27th International Conference on Pattern Recognition (ICPR)
DAll, with a PSNR of 29.88dB and an SSIM of 0.73 points for pix2pix. SwinIR
exhibits a decrease of up to 0.76dB for PSNR and 0.07 points for SSIM across
the DAll, with a larger decrease observed within the DArt, reaching a difference
of 1.7dB compared to MAR-DTN. Table 3 presents a comparison between the
performance time and complexity of the networks.
Table 3: Comparison of trainable parameters, number of multiplications and
additions (MACs), training time computed for the DAll in 1 epoch and patient
reconstruction time (in this case 170 slices) for state-of-the-art methods under
study.
Network
Parameters
(M)
MACs
(G)
Training time
(s)
Patient reconstruction
time (s)
MAR-DTN
1.882
116.686
65.32
3.56
pix2pix
54.413
77.99
80.02
3.75
custom-pix2pix
4.646
123.277
67.42
4.25
SwinIR
1.614
425.034
2, 774.76
47.27
INet
2.96
896.31
807.38
5.31
3.3
Clinical evaluation
Initial feedback from clinicians indicates that the quality of the MVCT images
generated through our proposed method is highly regarded. Clinicians have noted
that synthetic MVCT images exhibit excellent contrast for both soft tissues
and bones, which is essential for accurate diagnosis and treatment planning in
clinical practice. These qualitative observations suggest promising outcomes in
terms of image quality and clinical utility, laying a strong foundation for further
quantitative evaluation and validation studies in the future.
4
Discussion and Conclusion
In this study, we compared our proposed domain transformation methodology
with some state-of-the-art methods, where kVCT images serve as input and
MVCT images as output. Our results demonstrate that a lightweight model like
MAR-DTN can effectively reduce artifacts with the appropriate combination
of loss functions, even with a reasonable dataset size. The performance of the
models is evaluated on two datasets: DArt, which contains only images with
artifacts, and DAll includes both artifact-affected and non-affected images.
Numerous combinations of loss functions were tested, though only a select
few are presented in Table 2 due to space constraints. Consequently, a deliberate
Accepted in 27th International Conference on Pattern Recognition (ICPR)
kVCT
MVCT
MAR-DTN
26.07 / 0.65
25.75 / 0.65
26.42 / 0.66
25.83 / 0.63
23.83 / 0.55
25.70 / 0.65
25.66 / 0.64
24.45 / 0.55
20.16 / 0.47
21.78 / 0.47
24.25 / 0.54
21.78 / 0.75
20.56 / 0.52
24.21 / 0.56
23.67 / 0.52
20.98 / 0.47
24.47 / 0.67
20.62 / 0.51
20.18 / 0.49
20.67 / 0.48
21.83 / 0.51
20.48 / 0.48
20.69 / 0.54
20.06 / 0.45
19.89 / 0.45
25.05 / 0.47
25.64 / 0.49
22.07 / 0.63
PSNR / SSIM
14.11 / 0.39
15.83 / 0.45
15.46 / 0.45
14.06 / 0.41
9.68 / 0.24
14.01 / 0.33
15.14 / 0.35
custom-pix2pix
pix2pix
swinIR
INet
Fig. 5: Reconstruction of a slice with artifacts by the different models and loss
functions. First row shows preprocessed kVCT and MVCT images (ground
truth). First column indicates the loss function, and the following ones indi-
cate the model used. Networks have been trained on the DArt.
choice was made to include those combinations yielding more promising results
within the allocated space.
As we compare the performance of models trained on DAll, MAR-DTN shows
the best performance in several cases, especially with the combination of L100
1
+
LSSIM when tested on DT s
All, achieving the highest PSNR of 30.02 dB and a high
SSIM of 0.73 on over all patient volume, in addition to that when tested on DT s
Art
still achieves competitive results. Model pix2pix and custom-pix2pix show similar
performance, with custom-pix2pix slightly outperforming pix2pix in most cases.
custom-pix2pix performs best on DAll with the loss combination of L100
1
+LSSIM
and pix2pix show fair performance using L100
1
+ LMSE. SwinIR exhibits decent
performance but is generally outperformed by MAR-DTN, particularly in terms
of PSNR. However, it shows competitive SSIM values. Model INet performs the
worst among all models, with significantly lower PSNR (max 12.67 dB) and
SSIM (max 0.08) values, highlighting its inability to effectively reduce artifacts
or maintain structural similarity.
Accepted in 27th International Conference on Pattern Recognition (ICPR)
Furthermore, as we compare the performance of models trained on DArt,
MAR-DTN achieves the highest performance on this dataset with the combina-
tion of L100
1
+ LMS−SSIM, achieving a PSNR of 27.46 dB and an SSIM of 0.69
when tested on DT s
Art. Overall, MAR-DTN performs better than all other models
across various loss combinations, particularly on the DAll dataset. Model pix2pix
and custom-pix2pix show similar PSNR and SSIM values, typically around 26-27
dB for PSNR and 0.64-0.68 for SSIM, depending on the loss function combina-
tion used. Model custom-pix2pix slightly outperforms pix2pix in most of the
combinations. Model SwinIR performs reasonably well, achieving PSNR values
around 25-26 dB and SSIM values around 0.64-0.67, depending on the loss func-
tion combination but it is outperformed by MAR-DTN in most combinations.
INet shows the poorest performance on DArt. We can conclude that it is not
capable of eliminating artifacts using the loss functions in this study, falling far
behind its competitors. What is achieved, however, is an increase in contrast
between different bone and muscle structures. Nevertheless, it also introduces
new artifacts, which hinder the correct evaluation of the images. It is important
to note that INet’s initial goal is image segmentation, not image generation.
Additionally, INet performs better with low-resolution images, making it less
appropriate for our dataset.
Despite achieving satisfactory results, it is worth noting that our study’s
considered dataset is relatively small. Nevertheless, our approach demonstrates
significant potential, as evidenced by MAR-DTN’s robust performance metrics
achieved across various network architectures and loss functions. To further en-
hance the impact of our findings, we plan to incorporate systematic qualitative
evaluations by clinical staff. Additionally, we aim to expand our dataset to in-
clude more types of artifacts across different body regions. These steps will
provide deeper insights and potentially lead to even more improved results, re-
inforcing the efficacy and applicability of our methodology in broader contexts.
Moreover, our future work aims to develop a generalized model for the entire
body. This extension will significantly broaden the applicability and robustness
of our approach, paving the way for more comprehensive and versatile artifact
management in medical imaging.
Acknowledgements This work is supported by the Italian Ministry of Health
(Ricerca Corrente). APM and BSA acknowledge financial support by the Spanish
Ministerio de Economía y Competitividad and European Regional Development
Fund, MCIN/AEI/ 10.13039/501100011033 and by “ERDF A way of making Eu-
rope”. Xunta de Galicia funded research under Research Grant No. 2021-PG036
and the Spanish Ministerio de Ciencia e Innovación MCIN/AEI/10.13039/501100011033
through the Industrial Doctorates Grant.
References
1. Boas, F.E., Fleischmann, D.: Ct artifacts: causes and reduction techniques. Imaging
in Medicine 4(2), 229–240 (04 2012), copyright - © 2012 Future Medicine Ltd
Accepted in 27th International Conference on Pattern Recognition (ICPR)
2. Buslaev, A., Iglovikov, V.I., Khvedchenya, E., Parinov, A., Druzhinin, M., Kalinin,
A.A.: Albumentations: Fast and flexible image augmentations. Information 11(2),
125 (Feb 2020)
3. Fedorov, A., Beichel, R., Kalpathy-Cramer, J., Finet, J., Fillion-Robin, J.C., Pu-
jol, S., Bauer, C., Jennings, D., Fennessy, F., Sonka, M., et al.: 3d slicer as an
image computing platform for the quantitative imaging network. Magnetic reso-
nance imaging 30(9), 1323–1341 (2012)
4. Gjesteby, L., Shan, H., Yang, Q., Xi, Y., Jin, Y., Giantsoudi, D., Paganetti, H.,
De Man, B., Wang, G.: A dual-stream deep convolutional network for reducing
metal streak artifacts in ct images. Physics in Medicine & Biology 64(23), 235003
(2019)
5. Hounsfield, G.N.: Computerized transverse axial scanning (tomography): Part 1.
Description of system. British Journal of Radiology 46(552), 1016–1022 (01 2014)
6. Huang, X., Wang, J., Tang, F., Zhong, T., Zhang, Y.: Metal artifact reduction
on cervical ct images by deep residual learning. Biomedical engineering online 17,
1–15 (2018)
7. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with con-
ditional adversarial networks. In: 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). pp. 5967–5976 (2017)
8. Jiang, L., Dai, B., Wu, W., Loy, C.C.: Focal frequency loss for image reconstruc-
tion and synthesis. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 13919–13929 (2021)
9. Kaposi, P., Youn, T., Tóth, A., Frank, V., Shariati, S., Szendrői, A., Magyar, P.,
Bérczi, V.: Orthopaedic metallic artefact reduction algorithm facilitates ct evalu-
ation of the urinary tract after hip prosthesis. Clinical Radiology 75(1), 78.e17–
78.e24 (2020)
10. Kim, H., Yoo, S.K., Kim, D.W., Lee, H., Hong, C.S., Han, M.C., Kim, J.S.: Metal
artifact reduction in kv ct images throughout two-step sequential deep convolu-
tional neural networks by combining multi-modal imaging (martian). Scientific
Reports 12(1), 20823 (2022)
11. Klein, S., Staring, M., Murphy, K., Viergever, M.A., Pluim, J.P.: Elastix: a tool-
box for intensity-based medical image registration. IEEE transactions on medical
imaging 29(1), 196–205 (2009)
12. Lee, D., Park, C., Lim, Y., Cho, H.: A metal artifact reduction method using a fully
convolutional network in the sinogram and image domains for dental computed
tomography. J. Digital Imaging 33(2), 538–546 (2020)
13. Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., Timofte, R.: Swinir: Image
restoration using swin transformer. In: Proceedings of the IEEE/CVF international
conference on computer vision. pp. 1833–1844 (2021)
14. Liang, K., Zhang, L., Yang, H., Yang, Y., Chen, Z., Xing, Y.: Metal artifact reduc-
tion for practical dental computed tomography by improving interpolation-based
reconstruction with deep learning. Medical Physics 46(12), e823–e834 (2019)
15. Lin, W.A., Liao, H., Peng, C., Sun, X., Zhang, J., Luo, J., Chellappa, R., Zhou,
S.K.: Dudonet: Dual domain network for ct metal artifact reduction. In: 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
pp. 10504–10513 (2019)
16. Liugang, G., Hongfei, S., Xinye, N., Mingming, F., Zheng, C., Tao, L.: Metal ar-
tifact reduction through mvcbct and kvct in radiotherapy. Scientific reports 6(1),
37608 (2016)
Accepted in 27th International Conference on Pattern Recognition (ICPR)
17. Lossau (née Elss), T., Nickisch, H., Wissel, T., Morlock, M., Grass, M.: Learning
metal artifact reduction in cardiac ct images with moving pacemakers. Medical
Image Analysis 61, 101655 (2020)
18. Lyu, Y., Lin, W.A., Liao, H., Lu, J., Zhou, S.K.: Encoding metal mask projection
for metal artifact reduction in computed tomography. In: Medical Image Comput-
ing and Computer Assisted Intervention – MICCAI 2020: 23rd International Con-
ference, Lima, Peru, October 4–8, 2020, Proceedings, Part II. p. 147–157. Springer-
Verlag, Berlin, Heidelberg (2020)
19. Ni, X., Shi, Z., Song, X., Tang, T., Li, S., Hou, Z., Zhang, W., Wang, W.F.,
Chen, F., Li, J., et al.: Metal artifacts reduction in kv-ct images with polymetallic
dentures and complex metals based on mv-cbct images in radiotherapy. Scientific
Reports 13(1), 8970 (2023)
20. Park, H.S., Lee, S.M., Kim, H.P., Seo, J.K., Chung, Y.E.: Ct sinogram-consistency
learning for metal-induced beam hardening correction. Medical Physics 45(12),
5376–5384 (2018)
21. Paudel, M.R., Mackenzie, M., Fallone, B.G., Rathee, S.: Clinical evaluation of nor-
malized metal artifact reduction in kvct using mvct prior images (mvct-nmar) for
radiation therapy treatment planning. International Journal of Radiation Oncol-
ogy* Biology* Physics 89(3), 682–689 (2014)
22. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-
ical image segmentation. In: Medical image computing and computer-assisted
intervention–MICCAI 2015: 18th international conference, Munich, Germany, Oc-
tober 5-9, 2015, proceedings, part III 18. pp. 234–241. Springer (2015)
23. Serrano-Antón, B., Otero-Cacho, A., López-Otero, D., Díaz-Fernández, B., Bastos-
Fernández, M., Pérez-Muñuzuri, V., González-Juanatey, J.R., Muñuzuri, A.P.:
Coronary artery segmentation based on transfer learning and unet architecture
on computed tomography coronary angiography images. IEEE Access (2023)
24. Wang, H., Li, Y., He, N., Ma, K., Meng, D., Zheng, Y.: Dicdnet: deep interpretable
convolutional dictionary network for metal artifact reduction in ct images. IEEE
Transactions on Medical Imaging 41(4), 869–880 (2021)
25. Wang, J., Zhao, Y., Noble, J.H., Dawant, B.M.: Conditional generative adversarial
networks for metal artifact reduction in ct images of the ear. In: Medical Image
Computing and Computer Assisted Intervention–MICCAI 2018: 21st International
Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I. pp. 3–11.
Springer (2018)
26. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. IEEE transactions on image processing
13(4), 600–612 (2004)
27. Weng, W., Zhu, X.: Inet: convolutional networks for biomedical image segmenta-
tion. Ieee Access 9, 16591–16603 (2021)
28. Yu, L., Zhang, Z., Li, X., Ren, H., Zhao, W., Xing, L.: Metal artifact reduction
in 2d ct images with self-supervised cross-domain learning. Physics in Medicine &
Biology 66(17), 175003 (2021)
29. Zhang, Y., Yu, H.: Convolutional neural network based metal artifact reduction
in x-ray computed tomography. IEEE Transactions on Medical Imaging 37(6),
1370–1381 (2018)
30. Zhu, L., Han, Y., Xi, X., Li, L., Yan, B.: Completion of metal-damaged traces based
on deep learning in sinogram domain for metal artifacts reduction in ct images.
Sensors 21(24) (2021)
