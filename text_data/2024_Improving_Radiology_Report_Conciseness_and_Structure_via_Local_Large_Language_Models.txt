 
Improving Radiology Report Conciseness and Structure via Local Large Language 
Models 
Iryna Hartsock, PhD1, Cyrillo Araujo, MD2, Les Folio, DO, MPH, ACHIP, CIIP2, and 
Ghulam Rasool, PhD1 
1Department of Machine Learning,  
2Deparmtent of Diagnostic Imaging and Interventional Radiology 
Moffitt Cancer Center and Research Institute, Tampa, FL, USA 
Corresponding author: ghulam.rasool@moffitt.org 
 
This study demonstrates that open-source large language models (LLMs) run locally 
behind institutional firewalls can improve the conciseness and structure of radiology 
reports by eliminating redundancies and organizing findings by organs, thus enhancing 
clarity and accessibility for referring physicians. 
 
Keywords: radiology reports, large language models, conciseness, structure 
 
Abbreviations: Large Language Model (LLM), Conciseness Percentage (CP), Application 
Programming Interface (API), Artificial Intelligence (AI) 
 
Key Points 
â–ª 
Locally-run LLMs behind institutional firewalls improve the conciseness and structure of 
radiology reports while keeping data secure. 
â–ª 
Condensing the radiology report before applying specific formatting instructions is an 
effective prompting strategy for creating concise, well-structured radiology reports. 
 
Abstract  
Purpose: We aim to improve radiology reporting by enhancing report conciseness and 
structuring (also known as templating) the findings according to organs, enabling physicians to 
locate relevant information quickly.  
Materials and Methods: We employ LLMs (e.g., Mixtral, Mistral, and Llama) to create concise, 
well-structured reports, primarily focusing on the Mixtral model due to its superior adherence to 
a specific output formatting requirement compared to other models. We run these LLMs locally 
behind our institutionâ€™s firewall, ensuring the safety and privacy of data. In addition, we utilize 
the LangChain framework and five different prompt approaches to enforce a specific structure in 
radiology reporting and remove excessive words and phrases to enhance the conciseness of 
reports. We introduce a new metric, the Conciseness Percentage (CP) score, to evaluate report 
conciseness. Our dataset comprises 814 radiology reports from seven board-certified body 
radiologists from our cancer center. 
Results: Our study evaluated various prompting approaches to condensing and structure 
radiology reports. Our results demonstrate that first prompting the LLM to make the report 
concise and then structuring it according to specific instructions given in the prompt is the best 
approach for creating concise, well-structured reports. We assessed all prompting methods 
based on how they handled formatting errors, reduced report length, and followed formatting 
instructions. 
Conclusion: We show that open-source and locally run LLMs can improve the conciseness and 
structure of radiology reports according to specific instructions.  
 
 
 
 
 
Introduction  
A significant challenge in radiology reporting is that reports tend to be overly verbose and poorly 
structured, making it difficult for referring physicians to discern crucial findings and potentially 
overlooking important information (1,2). Implementing structured (templated) reporting methods 
provides a viable solution, enabling physicians to access relevant information efficiently (3â€“5). 
Imaging findings can be structured in various ways, such as organizing them from head-to-toe, 
prioritizing from the most critical to the least important, and/or itemizing them by specific organs 
(6,7). Removing redundancies, unnecessary words, and phrases from a radiology report without 
compromising its meaning further enhances interpretation efficiency (8,9). Ultimately, well-
structured and concise radiology reporting is not just about documentation; it is crucial for 
delivering high-quality healthcare (2,3,10).  
In recent years, numerous studies have explored using LLMs to improve the readability of 
radiology reports (11â€“14). LLMs are powerful artificial intelligence (AI) models capable of 
analyzing and generating human-like natural language text (15â€“19). For instance, Jeblick et al. 
used ChatGPT to simplify radiology reports for a child's understanding (11). While 15 
radiologists generally found the simplified reports to be factually accurate and not harmful, some 
noted errors and overlooked details. Adams et al. employed GPT-4 to transform 170 free-text 
CT and MRI reports into structured formats by selecting the best templates from a predefined 
list, achieving successful conversions for all reports (12). Additionally, Mallio et al. demonstrated 
the efficacy of ChatGPT-3.5 Turbo and GPT-4 models in reducing the verbosity of radiology 
reports (13). However, a key limitation of these three studies is the use of application 
programming interfaces (APIs) or LLMs provided as a service on the Internet, which often 
involves sharing data with third parties or using synthetically-generated radiology reports (11â€“
13). Using external LLMs raises concerns regarding data privacy, security, adherence to 
regulations such as the Health Insurance Portability and Accountability Act (HIPAA) (20,21), and 
growing usage costs.  
In our IRB-approved study, we run state-of-the-art LLMs such as Mixtral (15), Mistral (16), and 
Llama 3 (18) locally on a Windows Desktop computer with a standard GPU. This helps us keep 
patient data within our healthcare institutionâ€™s secure infrastructure, reducing the risk of 
unauthorized access. Our findings show that these LLMs not only improve radiology reports by 
making them concise and well-structured but also enhance data security and standardization in 
reporting practices. 
 
Materials and Methods  
Radiology Reports 
In this retrospective IRB-approved quality improvement study, we sample 814 radiology reports 
collected from seven board-certified body radiologists at our Cancer Center. The reports were 
generated from CT exams of the chest, abdomen, and pelvis and were written in the years 2023 
and 2024. Each report comprises two sections: "Findings", which describes medical 
observations in these regions, and "Impressions", which summarizes key findings. The length of 
the reports varies from 182 to 981 words, with a mean of 372 words and a median of 344 words. 
While most reports are structured by organs, they lack a consistent formatting style due to 
differing approaches among radiologists. Before analysis, all reports were de-identified to 
ensure patient confidentiality.  
 
Large Language Models (LLMs) 
The study employs the Mixtral 8x7B LLM, which utilizes a sparse mixture-of-experts (SMoE) 
architecture with a total of 56B parameters (i.e., eight experts, each with approximately 7B 
parameters) (15). For efficiency, the study employed pre-existing 6Q quantized weights of 
Mixtral. In the Mixtral architecture, each layer incorporates eight distinct feedforward expert 
blocks. During inference, based on the specific characteristics of each token, two out of these 
eight experts are selected to process the input, and thus, only 14B parameters are active during 
inference. Mixtral benefits from a substantially large context window of 32,768 tokens and was 
pre-trained on multilingual data extracted from the open web (15). Mixtral was selected over 
other LLMs, including Mistral 7B (16) and Llama 3 8B (18), due to its superior adherence to a 
specific output formatting requirement compared to other models. All LLMs were run locally 
behind a secure firewall using the Ollama framework to ensure data security and privacy (22). 
The temperature of the LLMs was set to zero to reduce randomness and ensure more 
consistent outputs. 
 
Prompt Engineering (In-Context Learning) 
We employed the LangChain library to programmatically prompt LLMs (23). The LLMs used in 
our experiments were downloaded from the Ollama store and were not fine-tuned or updated in 
any manner. Each report was processed with one or two calls to the LLM to enforce adherence 
to the structure depicted in Figure 1. Our study explored five distinct prompt approaches: 
â–ª 
Structure: This approach used only a single prompt to structure reports in a predefined 
format (see Supplementary Figure 1).  
â–ª 
Structure >> Conciseness: The LLM was initially prompted to structure the reports in a 
specified format. Subsequently, a second prompt instructed the model to refine the 
structured output further for conciseness. Thus, each report processing consisted of two 
calls to the LLM (see Supplementary Figure 2).   
â–ª 
Conciseness >> Structure: The LLM was first prompted to generate concise reports and 
then instructed to organize the concise information into a predefined structure. In this 
case, each report processing consisted of two calls to the LLM (see Figure 2). 
â–ª 
Structure + Conciseness: The LLM was prompted to structure reports in a predefined 
format while also emphasizing the need for conciseness. Both instructions were included 
in a single prompt (see Supplementary Figure 3).  
â–ª 
Structure + Conciseness (F, I): Given two prompts, the LLM was provided the entire 
report each time, with one prompt focusing on structuring the "Findings" section and the 
other the "Impressions" section, with both prompts emphasizing conciseness (see 
Supplementary Figure 4).   
Occasionally, the LLM may fail to format a report according to the required structure, causing an 
abrupt termination of the Python program. To address this, we configured our code to use the 
'OutputFixingParser,' which gives the LLM a second chance to correct formatting issues. This 
parser resubmits the mis-formatted output and the original instructions, asking the LLM to fix the 
errors. Our code can be found in the GitHub repository: GitHub. 
 
Concise Percentage (CP) Score 
We introduce a Concise Percentage (CP) score to evaluate the conciseness of the radiology 
reports by measuring the percentage of "meaningful" words in the report. Assuming that all 
words in the LLM-generated report are meaningful, the CP score is computed as follows: 
ð¶ð‘ƒ ð‘†ð‘ð‘œð‘Ÿð‘’= ð‘‡ð‘œð‘¡ð‘Žð‘™ ð‘›ð‘¢ð‘šð‘ð‘’ð‘Ÿ ð‘œð‘“ ð‘¤ð‘œð‘Ÿð‘‘ð‘  ð‘–ð‘›  ð¿ð¿ð‘€ ð‘”ð‘’ð‘›ð‘’ð‘Ÿð‘Žð‘¡ð‘’ð‘‘ ð‘…ð‘’ð‘ð‘œð‘Ÿð‘¡
ð‘‡ð‘œð‘¡ð‘Žð‘™ ð‘›ð‘¢ð‘šð‘ð‘’ð‘Ÿ ð‘œð‘“ ð‘¤ð‘œð‘Ÿð‘‘ð‘  ð‘–ð‘›  ð‘‚ð‘Ÿð‘–ð‘”ð‘–ð‘›ð‘Žð‘™ ð‘…ð‘’ð‘ð‘œð‘Ÿð‘¡
 Ã—  100%. 
A CP score near 100% indicates that the original report was already relatively concise, requiring 
the LLM to remove a small number of words. Conversely, a lower CP score suggests that the 
original report was less concise and required significant condensation by the LLM. A CP score 
exceeding 100% is also possible, as the LLM can be verbose and add content, potentially 
lengthening rather than condensing the report. While the CP score is valuable for evaluating the 
LLM effectiveness in reducing unnecessary content and improving report conciseness, it does 
not fully reflect the clinical utility or relevance of a radiology report's content. Also, we may get a 
different CP score for the same report due to the different prompts or models used for 
processing the report. 
 
Results  
We processed radiology reports with different LLM prompting strategies, assessing their 
effectiveness in handling formatting errors, streamlining the unstructured radiology reports, and 
managing different radiologistsâ€™ report writing styles. 
 
Formatting Errors of LLMs 
We observed two types of formatting errors in the radiology reports processed by LLMs. The 
first type of error occurs when the LLM fails to structure a report according to the format shown 
in Figure 1 after two attempts. In the second type of error, the LLM generates a list of individual 
letters from impressions rather than the full impressions. This results in an excessively long list 
of impressions and CP scores exceeding 100%. We excluded such reports from further 
analysis. Also, a report might have formatting issues with one prompting method but not others. 
In such cases, we excluded the report only for the problematic prompting approaches. 
 
In this study, we focus on the Mixtral LLM, as it has the lowest rate of formatting errors 
compared to Mistral 7B (v0.2) and Llama 3 8B. Llama 3 8B struggled significantly with following 
the formatting instructions. For instance, under the "Conciseness >> Structure" prompting 
approach, Llama 3 8B caused formatting errors in 726 (89.2%) reports. The Mistral 7B 
performed more similarly to Mixtral in adhering to formatting instructions.  Under the 
"Conciseness >> Structure" prompting approach, Mistral 7B produced formatting errors in 107 
(13.1%) reports, while Mixtral had formatting errors only in 88 (10.8%) reports.  
 
 
 Table 1 shows the number of reports streamlined by Mixtral without formatting errors for all 
radiologists and prompting strategies. Most of the formatting errors were of the first type. 
Interestingly, the "Structure + Conciseness (F, I)" strategy did not cause any errors of the 
second type, and only 24 reports from other approaches had such errors. The "Structure + 
Conciseness (F, I)" approach also had the lowest number of formatting errors, with only 23 
(2.8%) reports being formatted incorrectly, as it used separate prompts for the "Findings" and 
"Impressions" sections. Thus, the formatting error could only occur if both prompts failed after 
the second attempt. In contrast, the "Structure >> Conciseness" approach resulted in the 
highest number of formatting errors, with 181 (22.3%) reports affected. This strategy chains two 
Radiologists 
# 
Reports 
Structure 
Structure >> 
Conciseness 
Conciseness  
>> Structure 
Structure + 
Conciseness 
Structure + 
Conciseness 
(F, I) 
# Reports 
w/o 
formatting 
errors 
# Reports 
w/o 
formatting 
errors 
# Reports 
w/o  
formatting 
errors 
# Reports 
w/o 
formatting 
errors 
# Reports 
w/o 
formatting 
errors 
Radiologist 1 
111 
103 
85 
82 
99 
104 
Radiologist 2 
151 
148 
122 
138 
140 
148 
Radiologist 3 
126 
118 
102 
119 
114 
123 
Radiologist 4 
96 
83 
73 
88 
92 
93 
Radiologist 5 
192 
183 
143 
191 
175 
186 
Radiologist 6 
80 
71 
59 
55 
67 
80 
Radiologist 7 
58 
50 
49 
53 
52 
57 
Total 
814 
756 
(92.9%) 
633 
(77.8%) 
726 
(89.2%) 
739 
(90.8%) 
791 
(97.2%) 
prompts, each enforcing the LLM to follow formatting instructions, so an error from any prompt 
in the chain impacts the entire approach. Other prompting approaches encountered formatting 
errors in 58 to 88 reports (7-11%). 
 
Streamlining an Unstructured Radiology Report: A Case Study 
We applied the Mixtral LLM to the radiology report with unstructured findings (see Figure 3 A) 
to assess its ability to structure the report while enhancing its conciseness. The resulting 
structured and concise reports generated using five different prompting strategies are shown in 
Figure 3 B-F. All CP scores of the LLM-streamlined reports range from 49.7% to 62.4%. The 
"Structure" approach, which only prompts the LLM to follow formatting instructions without 
emphasizing conciseness, achieved the highest CP score and reduced the report length by 
37.6%.  The lowest CP scores had the "Conciseness >> Structure" and "Structure + 
Conciseness (F, I)" methods, indicating they made the reports the most concise.  
 
We observed significant variation in how Mixtral handled the â€œImpressionsâ€ section. With the 
"Conciseness >> Structure" and "Structure + Conciseness (F, I)" approaches, Mixtral extracted 
and condensed content directly from the "Impressions" section as instructed. In contrast, with 
the other three strategies, Mixtral relied more on information from the "Findings" section to 
generate "Impressions", deviating from the intended prompt instructions.  
 
While Mixtral processed the "Findings" section similarly across prompting approaches, we 
noticed several discrepancies. We instructed Mixtral to extract findings for an organ, specifying 
that "Unremarkable" should be used for normal findings and "None" for the absence of findings 
(see Figure 2). As shown in Figure 3 B, E-F, Mixtral incorrectly indicated no findings for the 
hepatobiliary system and adrenals, possibly due to failure to identify certain findings or 
misclassifying them as "None". 
 
Additionally, we noticed that Mixtral can attribute the same clinical findings to multiple organs 
simultaneously. For example, in Figure 3 E, Mixtral added "general osteopenia with moderate 
degenerative changes in the thoracic and lumbar spine" as a medical finding of other abdominal 
and pelvic organs as well as bones and soft tissues. Sometimes, the LLM can place findings 
under one organ when they are more relevant to another. For instance, in Figure 3 E, the 
finding "Slightly prominent mediastinal lymph nodes likely related to heart failure" was listed 
under other chest findings instead of mediastinum.  
 
Evaluation of Conciseness and Prompting Approaches Across Radiologists 
For each participating radiologist, we computed the CP scores for their sampled reports using 
five prompting approaches. These scores are represented using boxplots in Figure 4, 
illustrating their distribution, quartiles, and outliers. The figure also reveals consistent trends in 
concise report writing among radiologists, with Radiologist 1 producing the least concise reports 
and Radiologist 7 the most concise, regardless of the prompting method. Figure 5 compares 
the average word count of original reports and their successfully LLM-processed versions 
(where CP â‰¤ 100%) using the "Conciseness >> Structure" strategy across all radiologists. 
Similarly, it also compares the total word count for a sample of reports, illustrating Mixtral's 
effectiveness in reducing verbosity. 
 
 
 Table 2 summarizes the number of reports with CP scores exceeding 100% for each prompting 
approach and radiologist. The largest number of such reports, namely 36 (4.4%) reports, was 
recorded for the "Structure + Conciseness (F, I)" approach. Other methods had fewer 
occurrences, with no instances in the "Conciseness >> Structure" approach.  In cases when CP 
> 100%, Mixtral often deviated from the prompting instructions, especially in the "Impressions" 
section, which listed most findings as impressions (see "Impressions" sections in Figure 3 B-D 
for examples). 
 
Radiologists 
# 
Reports 
Structure 
Structure >> 
Conciseness 
Conciseness  
>> Structure 
Structure + 
Conciseness 
Structure + 
Conciseness 
(F, I) 
# Reports 
w/  
CP > 100% 
# Reports 
w/  
CP > 100% 
# Reports 
w/  
CP > 100% 
# Reports 
w/  
CP > 100% 
# Reports 
w/ 
CP > 100% 
Radiologist 1 
111 
1 
0 
0 
0 
2 
Radiologist 2 
151 
0 
0 
0 
0 
1 
Radiologist 3 
126 
3 
0 
0 
0 
10 
Radiologist 4 
96 
1 
0 
0 
0 
11 
Radiologist 5 
192 
7 
0 
0 
2 
5 
Radiologist 6 
80 
3 
2 
0 
4 
6 
Radiologist 7 
58 
1 
0 
0 
1 
1 
Total 
814 
16 
(2.0%) 
         2 
(0.2%) 
0 
(0.0%) 
7 
(0.9%) 
36 
(4.4%) 
We also observed that Mixtral can overlook clinical findings in one or more organs, especially 
when those findings are unremarkable, marking them as "None". This occurred in three out of 
ten random reports per radiologist using the "Conciseness >> Structure" approach and in six out 
of ten reports with every other prompting strategy.  
 
Discussion  
In this study, we demonstrated that LLMs, locally run behind institutional firewalls on a Windows 
Desktop with GeForce RTX 3060 GPU with 12 Gigabytes of VRAM, can effectively enhance the 
conciseness and structure of radiology reports. When we refer to "structured reports", we mean 
reports that are "templated" by organs, which differs from the ACR RADS definition that includes 
detailed categorization and management recommendations, potentially incorporating CDE 
macros (modules). In our initial experiments, we utilized several state-of-the-art open-source 
LLMs like Mixtral (15), Mistral (16), and Llama 3 (18). However, we focused our efforts on 
Mixtral due to its comparatively lower rate of formatting errors. We evaluated five different 
prompting methods using Mixtral across 814 radiology reports of the chest, abdomen and pelvis 
written by seven body radiologists, assessing the Mixtralâ€™s ability to reduce report length, 
minimize formatting issues, and follow prompt instructions. To our knowledge, this study is 
among the first to explore the application of LLMs in private, resource-constrained environments 
for processing radiology reports, underscoring the potential of locally deployed models in clinical 
settings. 
 
Among the various prompting strategies tested, the "Conciseness >> Structure" approach 
proved to be the most effective. This method prioritized conciseness first, followed by 
addressing formatting issues, and excelled in keeping the report length within the original limits 
while adhering closely to the prompting instructions. We believe this strategy is superior 
because condensing the radiology reports first simplifies the subsequent formatting task, 
reducing the risk of omitting relevant details during the formatting phase. Notably, the "Structure 
+ Conciseness (F, I)" approach, which prompted the LLM separately for the "Findings" and 
"Impressions" sections, yielded the fewest formatting errors among all methods. This suggests 
that focused prompting for specific sections can improve accuracy in structure without 
compromising content. 
 
We proposed using the CP score to evaluate the conciseness of radiology reports before and 
after processing by LLMs. By measuring the percentage of "meaningful" words, the CP score 
highlights opportunities to reduce report length while preserving essential content. However, 
caution is warranted when using this metric, as not all words contribute equally to the clinical 
relevance of a report. Certain words or phrases, though they increase the word count, may be 
vital for clinical interpretation, and removing them in the pursuit of conciseness could 
compromise the reportâ€™s accuracy. The CP score can facilitate peer benchmarking, motivating 
radiologists to improve conciseness while maintaining both clinical relevance and informational 
integrity. Incorporating the CP score into training programs could further support radiology 
residents in tracking their progress in producing concise reports. Nevertheless, the CP score 
should not solely drive decisions about content reduction, as the clinical significance of specific 
details may outweigh the benefits of brevity and conciseness. 
 
We observed several limitations that may be associated with the use of locally-run LLMs. One of 
the major challenges was the occurrence of formatting errors, which appeared in 23 to 181 
reports across the five prompting strategies. These errors may be less frequent in API-based 
LLMs, which are often more robust due to extensive pre-training and broader infrastructure. 
Additionally, despite our efforts to ensure that Mixtral followed prompt instructions accurately, it 
occasionally missed key medical findings, particularly for organs such as the hepatobiliary 
system, pancreas, spleen, and adrenals, incorrectly marking them as "None." This issue was 
least frequent with the "Conciseness >> Structure" approach. However, it is important to note 
that this limitation is not unique to smaller, locally-run LLMs; larger API-based models can also 
struggle to follow prompts accurately under certain conditions. Another notable issue was that 
61 LLM-processed reports ended up longer than the original versions, contrary to the objective 
of producing concise outputs. On rare occasions, Mixtral even assigned medical findings to 
multiple organs simultaneously or to less relevant organs, further highlighting the need for 
ongoing refinement in prompting strategies and model training/fine-tuning. However, we 
intentionally limited our current study to prompt-only without any model fine-tuning.  
 
In summary, our study demonstrates Mixtral's capability to streamline and structure radiology 
reports, with the "Conciseness >> Structure" prompting approach proving particularly effective. 
Despite these strengths, challenges such as formatting errors and the omission of medical 
findings for certain organs persist. Future work will focus on addressing these limitations 
through prompt refinement, expanding the LLMâ€™s vocabulary to include missing medical 
terminology, and incorporating feedback from participating radiologists. Additionally, model fine-
tuning will be explored to improve performance in clinical contexts. These enhancements will 
contribute to more precise and clinically relevant report generation, ultimately advancing the 
practical application of LLMs in medical environments. 
 
References 
1.  
Wallis A, McCoubrie P. The radiology report â€” Are we getting the message across? Clin 
Radiol. 2011;66:1015â€“1022. doi: 10.1016/j.crad.2011.05.013. 
2.  
Franconeri A, Fang J, Carney B, Justaniah A, Miller L, Hur H-C, King LP, Alammari R, 
Faintuch S, Mortele KJ, et al. Structured vs narrative reporting of pelvic MRI for fibroids: 
clarity and impact on treatment planning. Eur Radiol. 2018;28:3009â€“3017. doi: 
10.1007/s00330-017-5161-9. 
3.  
Jorg T, Heckmann JC, Mildenberger P, Hahn F, DÃ¼ber C, Mildenberger P, Kloeckner R, 
Jungmann F. Structured reporting of CT scans of patients with trauma leads to faster, 
more detailed diagnoses: An experimental study. Eur J Radiol. 2021;144:109954. doi: 
10.1016/j.ejrad.2021.109954. 
4.  
Kim SH, Sobez LM, Spiro JE, Curta A, Ceelen F, Kampmann E, Goepfert M, Bodensohn 
R, Meinel FG, Sommer WH, et al. Structured reporting has the potential to reduce 
reporting times of dual-energy x-ray absorptiometry exams. BMC Musculoskelet Disord. 
2020;21:248. doi: 10.1186/s12891-020-03200-w. 
5.  
Schwartz LH, Panicek DM, Berk AR, Li Y, Hricak H. Improving Communication of 
Diagnostic Radiology Findings through Structured Reporting. Radiology. 2011;260:174â€“
181. doi: 10.1148/radiol.11101913. 
6.  
Nobel JM, Kok EM, Robben SGF. Redefining the structure of structured reporting in 
radiology. Insights Imaging. 2020;11:10. doi: 10.1186/s13244-019-0831-6. 
7.  
Sistrom CL, Langlotz CP. A framework for improving radiology reporting. Journal of the 
American College of Radiology. 2005;2:159â€“167. doi: 10.1016/j.jacr.2004.06.015. 
8.  
Hartung MP, Bickle IC, Gaillard F, Kanne JP. How to Create a Great Radiology Report. 
RadioGraphics. 2020;40:1658â€“1670. doi: 10.1148/rg.2020200020. 
9.  
Webster Riggs. Making a case for concise narrative radiology reports. Appl Radiol. 
2015;44:4â€“6. 
10.  
Schoeppe F, Sommer WH, NÃ¶renberg D, Verbeek M, Bogner C, Westphalen CB, 
Dreyling M, Rummeny EJ, Fingerle AA. Structured reporting adds clinical value in 
primary CT staging of diffuse large B-cell lymphoma. Eur Radiol. 2018;28:3702â€“3709. 
doi: 10.1007/s00330-018-5340-3. 
11.  
Jeblick K, Schachtner B, Dexl J, Mittermeier A, StÃ¼ber AT, Topalis J, Weber T, Wesp P, 
Sabel BO, Ricke J, et al. ChatGPT makes medicine easy to swallow: an exploratory case 
study on simplified radiology reports. Eur Radiol. 2023;34:2817â€“2825. doi: 
10.1007/s00330-023-10213-1. 
12.  
Adams LC, Truhn D, Busch F, Kader A, Niehues SM, Makowski MR, Bressem KK. 
Leveraging GPT-4 for Post Hoc Transformation of Free-text Radiology Reports into 
Structured Reporting: A Multilingual Feasibility Study. Radiology. 2023;307. doi: 
10.1148/radiol.230725. 
13.  
Mallio CA, Bernetti C, Sertorio AC, Zobel BB. ChatGPT in radiology structured 
reporting: analysis of ChatGPT-3.5 Turbo and GPT-4 in reducing word count and 
recalling findings. Quant Imaging Med Surg. 2024;14:2096â€“2102. doi: 10.21037/qims-23-
1300. 
14.  
Butler JJ, Puleo J, Harrington MC, Dahmen J, Rosenbaum AJ, Kerkhoffs GMMJ, 
Kennedy JG. From technical to understandable: Artificial Intelligence Large Language 
Models improve the readability of knee radiology reports. Knee Surgery, Sports 
Traumatology, Arthroscopy. 2024;32:1077â€“1086. doi: 10.1002/ksa.12133. 
15.  
Jiang AQ, Sablayrolles A, Roux A, Mensch A, Savary B, Bamford C, Chaplot DS, Casas 
D de las, Hanna EB, Bressand F, et al. Mixtral of Experts. 2024; 
16.  
Jiang AQ, Sablayrolles A, Mensch A, Bamford C, Chaplot DS, Casas D de las, Bressand 
F, Lengyel G, Lample G, Saulnier L, et al. Mistral 7B. 2023; 
17.  
Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, Bashlykov N, Batra S, 
Bhargava P, Bhosale S, et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. 
2023; 
18.  
Dubey A, Jauhri A, Pandey A, Kadian A, Al-Dahle A, Letman A, Mathur A, Schelten A, 
Yang A, Fan A, et al. The Llama 3 Herd of Models. 2024; 
19.  
OpenAI, Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, Almeida D, 
Altenschmidt J, Altman S, et al. GPT-4 Technical Report. 2023; 
20.  
Rosenbloom ST, Smith JRL, Bowen R, Burns J, Riplinger L, Payne TH. Updating HIPAA 
for the electronic medical record era. Journal of the American Medical Informatics 
Association. 2019;26:1115â€“1119. doi: 10.1093/jamia/ocz090. 
21.  
Hjerppe K, Ruohonen J, Leppanen V. The General Data Protection Regulation: 
Requirements, Architectures, and Constraints. 2019 IEEE 27th International Requirements 
Engineering Conference (RE). IEEE; 2019. p. 265â€“275. 
22.  
Ollama [Internet]. [cited 2024 Oct 13]. Available from: https://ollama.com. 
23.  
LangChain [Internet]. [cited 2024 Oct 13]. Available from: https://www.langchain.com. 
  
 
 
Figure 1. The structure of a radiology report, with organs and sub-organs ordered from head to 
toe, makes it intuitive and predictable for readers to quickly locate the necessary information. All 
five prompts proposed in this study use this structure as a template for the LLMs to follow when 
processing radiology reports. Thus, regardless of which radiologist writes the initial report, the 
LLM should generate a radiology report that adheres to this structured format. 
 
Figure 2. The prompts and formatting instructions used in the "Conciseness >> Structure" 
prompting approach involve two steps to process the given radiology report. In the first step 
(prompt 1), the LLM is instructed to make the input radiology report more concise. In the second 
step (prompt 2), the LLM receives further instructions to structure the concise report according 
to specific formatting guidelines, as shown in Figure 1. This two-step approach attempts to 
ensure that the output report is both concise and well-structured. 
 
 
 
 
Figure 3. The performance of the Mixtral LLM on making the unstructured radiology report 
concise and well-structured under various prompting strategies. (A) Unstructured radiology 
report alongside its corresponding LLM-processed versions using the following prompting 
approaches: (B) "Structure", (C) "Structure + Conciseness", (D) "Structure >> Conciseness", (E) 
"Conciseness >> Structure", and (F) "Structure + Conciseness (F, I)". The word count for each 
report is provided in parentheses. The CP scores of all LLM-processed reports are also 
indicated, with reports E and F having the lowest CP scores, making them the most concise.  
 
 
 
 
 
 
 
Figure 4. Boxplots of CP scores of seven participating radiologists for five different prompting 
approaches using the Mixtral LLM. Each box represents the interquartile range (IQR), with the 
top and bottom corresponding to the 75th and 25th percentiles, respectively. A line inside the 
box marks the median (50th percentile). The tips of the top and bottom whiskers correspond to 
the highest and lowest values, excluding outliers. The hollow circles indicate outliers. The trend 
in concise report writing among radiologists across all prompting strategies is overall consistent. 
The trend in concise report writing among radiologists is overall consistent across all prompting 
strategies. Reports with CP scores exceeding 100%, where the LLM-processed report is longer 
than the original report, fall above the gray dashed line, with none observed in the "Conciseness 
>> Structure" prompting approach (see  
 Table 2 for the exact numbers of such reports). 
 
 
 
Radiologists 
# 
Reports 
Structure 
Structure >> 
Conciseness 
Conciseness  
>> Structure 
Structure + 
Conciseness 
Structure + 
Conciseness 
(F, I) 
# Reports 
w/  
CP > 100% 
# Reports 
w/  
CP > 100% 
# Reports 
w/  
CP > 100% 
# Reports 
w/  
CP > 100% 
# Reports 
w/ 
CP > 100% 
Radiologist 1 
111 
1 
0 
0 
0 
2 
Radiologist 2 
151 
0 
0 
0 
0 
1 
Radiologist 3 
126 
3 
0 
0 
0 
10 
Radiologist 4 
96 
1 
0 
0 
0 
11 
Radiologist 5 
192 
7 
0 
0 
2 
5 
Radiologist 6 
80 
3 
2 
0 
4 
6 
Radiologist 7 
58 
1 
0 
0 
1 
1 
Total 
814 
16 
(2.0%) 
         2 
(0.2%) 
0 
(0.0%) 
7 
(0.9%) 
36 
(4.4%) 
 
 
 
Figure 5. The upper left plot shows the average word count of the original reports and their 
successfully condensed versions generated by the Mixtral LLM (i.e., reports with CP scores not 
exceeding 100%). The remaining plots provide a side-by-side comparison of total word counts 
for a sample of original and LLM-processed reports. Five radiology reports were randomly 
selected for each radiologist, with CP scores between the 25th and 75th percentiles, indicating a 
mid-range level of conciseness. All reports were processed using the "Conciseness >> 
Structure" approach. This analysis highlights the effectiveness of the Mixtral LLM in streamlining 
content by reducing unnecessary words and phrases. 
 
 
 
 
 
 
 
 Table 1. The total number of radiology reports processed without formatting errors by the 
Mixtral LLM across five prompting approaches and seven body radiologists. The last row 
displays the total number of radiology reports without formatting errors for each approach and 
its percentage of the overall reports. The "Structure+Conciseness (F,I)" prompting strategy has 
the least number of formatting errors, meaning it adheres to the formatting instructions better 
than the other approaches. 
 
 
 
Radiologists 
# 
Reports 
Structure 
Structure >> 
Conciseness 
Conciseness  
>> Structure 
Structure + 
Conciseness 
Structure + 
Conciseness 
(F, I) 
# Reports 
w/o 
formatting 
errors 
# Reports 
w/o 
formatting 
errors 
# Reports 
w/o  
formatting 
errors 
# Reports 
w/o 
formatting 
errors 
# Reports 
w/o 
formatting 
errors 
Radiologist 1 
111 
103 
85 
82 
99 
104 
Radiologist 2 
151 
148 
122 
138 
140 
148 
Radiologist 3 
126 
118 
102 
119 
114 
123 
Radiologist 4 
96 
83 
73 
88 
92 
93 
Radiologist 5 
192 
183 
143 
191 
175 
186 
Radiologist 6 
80 
71 
59 
55 
67 
80 
Radiologist 7 
58 
50 
49 
53 
52 
57 
Total 
814 
756 
(92.9%) 
633 
(77.8%) 
726 
(89.2%) 
739 
(90.8%) 
791 
(97.2%) 
 
 Table 2. The total number of radiology reports processed by the Mixtral LLM with a CP score 
greater than 100%, indicating that the LLM-processed report is longer than the original report, is 
summarized across five prompting approaches and seven body radiologists. The last row 
displays the total number of radiology reports with a CP score greater than 100% for each 
approach and its percentage of the overall reports. Only a small number of radiology reports 
have a CP score over 100%, with no such cases in the "Conciseness >> Structure" prompting 
strategy.  
 
Radiologists 
# 
Reports 
Structure 
Structure >> 
Conciseness 
Conciseness  
>> Structure 
Structure + 
Conciseness 
Structure + 
Conciseness 
(F, I) 
# Reports 
w/  
CP > 100% 
# Reports 
w/  
CP > 100% 
# Reports 
w/  
CP > 100% 
# Reports 
w/  
CP > 100% 
# Reports 
w/ 
CP > 100% 
Radiologist 1 
111 
1 
0 
0 
0 
2 
Radiologist 2 
151 
0 
0 
0 
0 
1 
Radiologist 3 
126 
3 
0 
0 
0 
10 
Radiologist 4 
96 
1 
0 
0 
0 
11 
Radiologist 5 
192 
7 
0 
0 
2 
5 
Radiologist 6 
80 
3 
2 
0 
4 
6 
Radiologist 7 
58 
1 
0 
0 
1 
1 
Total 
814 
16 
(2.0%) 
         2 
(0.2%) 
0 
(0.0%) 
7 
(0.9%) 
36 
(4.4%) 
Supplementary Material 
 
Supplementary Figure 1. The prompt and formatting instructions used in the "Structure" 
prompting approach. The LLM is only prompted to structure the given radiology report according 
to specific formatting guidelines, as shown in Figure 1.  
 
 
Supplementary Figure 2. The prompts and formatting instructions used in the "Structure >> 
Conciseness" prompting approach involve two steps to process the given radiology report. In 
the first step (prompt 1), the LLM is instructed to structure the radiology report according to 
specific formatting guidelines, as shown in Figure 1. In the second step (prompt 2), the LLM 
receives further instructions to make the structured radiology report more concise while 
maintaining the structure. This two-step approach attempts to ensure that the output report is 
both concise and well-structured. 
 
 
 
 
 
 
Supplementary Figure 3. The prompt and formatting instructions used in the "Structure + 
Conciseness" prompting approach. The LLM is prompted to structure the given radiology report 
according to specific formatting guidelines, as shown in Figure 1, while also emphasizing the 
need for conciseness. 
 
 
Supplementary Figure 4. The prompts and formatting instructions in the "Structure + 
Conciseness (F, I)" approach involved giving the LLM two prompts, each with the entire 
radiology report as input. One prompt instructed the LLM to structure and make concise only the 
"Findings" section, while the other focused solely on the "Impressions" section. As a result, the 
LLM-processed report is expected to have the structure shown in Figure 1. 
