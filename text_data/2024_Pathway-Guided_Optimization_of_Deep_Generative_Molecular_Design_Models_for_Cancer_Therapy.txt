Pathway-Guided Optimization of Deep Generative
Molecular Design Models for Cancer Therapy
Alif Bin Abdul Qayyum1∗
Susan D. Mertins2∗
Amanda K. Paulson3∗
Nathan M. Urban4
Byung-Jun Yoon1,4
1Department of Electrical and Computer Engineering, Texas A&M University,
College Station, TX 77843-3128, USA.
2Cancer Data Science Initiative, Frederick National Laboratory for Cancer Research,
Leidos Biomedical Research,
Frederick, MD 21702, USA.
3Department of Pharmaceutical Chemistry, University of California,
San Francisco, CA 94158, USA.
4 Computational Science Initiative, Brookhaven National Laboratory,
Upton, NY 11973, USA.
Abstract
The data-driven drug design problem can be formulated as an optimization task of
a potentially expensive black-box objective function over a huge high-dimensional
and structured molecular space. The junction tree variational autoencoder (JTVAE)
has been shown to be an efficient generative model that can be used for suggesting
legitimate novel drug-like small molecules with improved properties. While the
performance of the generative molecular design (GMD) scheme strongly depends
on the initial training data, one can improve its sampling efficiency for suggesting
better molecules with enhanced properties by optimizing the latent space. In this
work, we propose how mechanistic models - such as pathway models described by
differential equations - can be used for effective latent space optimization(LSO)
of JTVAEs and other similar models for GMD. To demonstrate the potential of
our proposed approach, we show how a pharmacodynamic model, assessing the
therapeutic efficacy of a drug-like small molecule by predicting how it modulates a
cancer pathway, can be incorporated for effective LSO of data-driven models for
GMD.
1
Introduction
Cancer is the second leading cause of death in United States and a major public health concern across
the globe [1], posing the highest clinical, social, and economic burden among all human diseases
[37, 9, 17].
Traditional chemotherapy has significant toxicity and side effects due to its inability to distinguish
between tumor and normal tissue. This has led to a tremendous shift in cancer research, driven by
interest among researchers in drug-like molecule design with high specificity and low off-target
effects for cancer therapy [5, 65, 61, 58, 64, 28]. Computation-driven generative molecular design
that optimizes multiple drug-like properties at once can be formulated as a mathematical optimization
problem of an objective function over a high-dimensional, structured and complicated input space.
Optimizing drug-like molecule efficacy is a daunting endeavor for two significant reasons. First, it
is difficult to search through millions or even billions of natural and artificial molecules to identify
an optimal candidate for modulating the function of a protein that drives the specific disease in
∗These authors contributed equally to this work.
Preprint. Under review.
arXiv:2411.03460v1  [cs.LG]  5 Nov 2024
consideration, similar to the problem of finding a needle in a haystack [25]. Second, even if a
hit is discovered, establishing a precise model to understand the connection between the protein’s
modulation and the effect on the disease in question poses a formidable challenge [56].
Since the advancement in deep learning, researchers have pursued novel deep generative models
to computationally generate new ideas. Some of the most used methods among deep generative
models are variational autoencoders (VAE) [32, 10, 14], generative adversarial networks (GAN)
[21, 57, 11, 3], diffusion models [24, 43] and consistency models [52]. Due to the presence of a
continuous low-dimensional latent space, a VAE is most suitable in conditional data generative tasks
which optimize an objective function over an input space with the main goal of generating new data
points [51]. Many promising results have surfaced in such optimization problems, such as conditional
image generation [42, 54], text generation [44], molecular and materials design [46, 16], and neural
architecture search [15].
Even after the success of deep generative models in various research fields, optimization of a deep
generative model over a specific and hard to evaluate objective function such as cancer therapeutic
capability of drug-like molecules on scarce structured input data, e.g. molecular structures, is still
an area of open research [62]. Recently, latent space optimization (LSO) has shown very promising
results in optimization tasks on high dimensional structured input data [22]. Additionally, in order
to effectively optimize the latent space for molecular property enhancement, it is crucial to have a
dependable evaluation function. A rule-based model eliminates the requirement for a labeled dataset,
offering a novel approach to directly optimizing the generative model to produce molecular structures
with superior cancer therapeutic scores.
We introduce an innovative approach for optimizing deep molecular generative models through
incorporation of a rule-based, pathway-guided mechanistic model to evaluate the therapeutic efficacy
of generated molecules. To the best of our knowledge, this work represents the pioneering effort to
integrate a mechanistic model as an evaluative component within the optimization framework of deep
molecular generative models. The outline of this paper is as follows: we discuss some backgrounds
of our work in Sec, 2, explain our proposed methodology in Sec. 3, demonstrate the observed results
in Sec. 4 and discuss some limitations of our proposal in Sec. 5.
2
Background
The inherent complexity of graph-like molecular structures, with their arbitrary connectivity, poses
a challenging obstacle when attempting to arrange their construction into a sequential process.
Furthermore, the discrete nature of decision-making involved in establishing the order for incremental
graph construction renders such processes non-differentiable.
Several different deep generative molecular design models have been proposed for generation of valid
molecular structures. GraphVAE proposes an approach for generating a probabilistic fully-connected
graph of a predefined maximum size directly at once, sidestepping the hurdles involved in incremental
construction of molecular graphs [49]. Generative models like GraphVAE frequently encounter
the issue of producing invalid data points when working with structured data, such as drug-like
small molecules. Grammar variational autoencoder generates molecules by representing them as
parse trees from a context-free grammar and ensures that the generated molecular structures are
syntactically valid [33]. In order to maintain semantic validity when generating molecules, such
as adhering to the SMILES language’s requirement that generated rings must be closed, syntax-
directed variational autoencoders enforce constraints on the output space so that the model not only
generates syntactically valid molecules, but also the molecular structures are semantically reasonable
[12]. Structure generating variational autoencoders leverage Bayesian optimization to fine-tune a
low-dimensional continuous latent space [35]. GANs have also been used for molecule generation.
MolGAN combines GAN with reinforcement learning to generate molecule structures with specific
desired chemical properties [13]. GraphGAN proposes an approach for learning the molecular graph
connectivity distribution among atoms in a molecule [55]. Denoising diffusion probabilistic models
have recently gained popularity for molecule generation task [31, 60, 26, 59].
A junction tree variational autoencoder (JTVAE) has been proposed to address the problem of
molecular optimization by directly encoding and decoding molecular graphs instead of generating
linear SMILES strings which can contribute to uncertainty and error[30, 22]. The autoencoder trains
a deep neural network, consisting of an encoder-decoder pair, on hundreds of thousands of existing
chemical structures. The encoder maps molecular structures into low-dimensional latent space and the
decoder converts latent space vectors back into molecular structures, along with a predictor network
2
predicting the property of molecular structures from the low-dimensional latent representations. To
allow generation of valid chemical structures, the JTVAE initiates the process by generating a scaffold
in a tree-like structure, which encompasses chemical substructures. It subsequently merges these
substructures into a complete molecule using a graph message passing network, thereby enabling the
gradual expansion of molecules while ensuring their chemical integrity is preserved at each stage of
development.
The optimization of a generative molecular design model, such as JTVAE, involves fine-tuning the
model in a manner that encourages it to generate molecular structures that align with the desired
optimal properties. In this process, we adjust the model’s parameters, training procedures, or other
relevant aspects to influence its output. The ultimate aim is to guide the generative model towards
producing molecules that exhibit the specific properties we desire, which serves as our primary
optimization objective. Latent space optimization (LSO) has shown promising results in optimization
tasks on high dimensional structured input data, e.g. molecular structures [22]. LSO consists of
two stages of computation. First, a latent space based deep generative model, such as VAE, is
trained to encode tensors, representing structured input data, into a low-dimensional continuous
space. This effectively converts the optimization problem of high dimensional structured input data
into an optimization problem on low-dimensional continuous data. Finally, the objective function
is fine-tuned to guide the decoder model in mapping the latent space to data points that exhibit
optimized objective function values. LSO has found applications in diverse domains, including
automatic machine learning [41, 63, 35], conditional image generation [36, 42], and enhancing model
interpretability [4].
Sample efficient periodic weighted retraining combines the idea of latent space optimization with a
JTVAE model [53]. This approach proposes weighting of the data distribution and periodic retraining
of the generative model with Bayesian optimization on the latent space to generate molecules with
specific desired properties. At each periodic retraining iteration, a set of sampled molecules from
the latent space with desired properties is appended to the existing training dataset to persuade the
generative model to map the latent space more into a desired direction to optimize an objective
function. A similar periodic weighted retraining of generative model uses the idea of pareto-front
optimality for multi-objective optimization [2].
In order to effectively optimize the latent space for molecular property enhancement, it is crucial to
have a dependable evaluation function. A rule-based model eliminates the requirement for a labeled
dataset, offering a novel approach to directly optimizing the generative model to produce molecular
structures with superior cancer therapeutic scores, in contrast to an indirect approach of optimizing a
basic protein inhibition constant (IC50). In this work, we incorporate periodic retraining as a strategy
to improve the navigation through a lower-dimensional representation of the chemical space. This
enhancement enables us to efficiently identify a subset of the chemical space that contains candidates
with optimal molecular properties. The utilization of LSO serves to reduce the number of candidate
molecules that must be virtually screened before reaching an optimal subspace. We demonstrate
that LSO can flexibly optimize toward quantitative, QSAR-predicted properties like single protein
inhibitory constants or more complex outcomes like therapeutic score as predicted by a cell-level
mechanistic pathway model. In particular, we investigate the use of mechanistic models, such as
rule-based pathway-guided mechanistic model described by differential equations, as a powerful
approach for optimizing the latent space in the context of generative molecular design for cancer
therapy. This is especially valuable in scenarios where data is limited, and traditional data-driven
models may not be as effective.
Pharmacodynamic models capture biochemical pathways that underscore cellular function and offer
a means to describe drug-like small molecule downstream activity. In particular, ordinary differential
equations, solved either deterministically or stochastically, are parameterized with measured values
such reaction rates and species concentrations and lead to a kinetic solution [7]. These mechanistic
or pathway models have been utilized in the context of comprehending cancer carcinogenesis and
possess the capacity to delineate a targeted therapeutic objective, such as inducing programmed cell
death, also known as apoptosis. Other investigators have provided useful mechanistic models that
predict drug timing [45] and combinations [47], both important challenges in predictive oncology. As
an example, we use the DNA damage response pathway and specifically focus on PARP1 inhibition.
poly(ADP-ribose) polymerase 1 (PARP1) is a key protein that initiates the DNA damage response
to single stranded breaks (SSBs) and other DNA lesions [34]. PARP1 binds to DNA damage and
PARylates histones and other protein targets, which in turn recruit downstream DNA repair enzymes.
Proliferating tumor cells are under replicative stress and often have increased DNA damage, making
PARP a promising target for cancer therapies. PARP inhibition induces synthetic lethality in BRCA
3
mutant tumors and other tumors that exhibit defects in the double strand break (DSB) repair pathway
[18]. When SSBs are not properly repaired, they can progress to DSBs which can’t be repaired by
the mutant BRCA proteins, leading to eventual cell death. Several clinical PARP inhibitors are on
the market that perform well for BRCA-mutant tumors, but they suffer from dose-limiting toxicities
and therefore do not perform well as single agents [27]. Design of more potent inhibitors with less
off-target effects and additional favorable ADME and medicinal chemistry properties represents an
excellent use-case for multi-objective optimization.
The primary objective of this research is to develop an optimization approach that harnesses the
generative capabilities of JTVAE to generate novel, synthesizable molecular structures and to utilize
a rule-based pathway-guided mechanistic model as the evaluation function for the cancer therapeutic
efficacy property to be optimized.
3
Methods
Let, X be the molecular structure input space and let f : X 7→R be the objective function,
the cancer therapeutic efficacy evaluation function of the molecular structure. X is high dimen-
sional and structured, and f(x) is a black-box, hard to evaluate function and not many points,
x ∈X with objective function value f(x) is available in hand.
Let, Z be the latent space
and D(z), z ∈Z be the the generative model that transforms the latent space into molecular
structures. The effective and efficient latent space based generative molecular design with an
optimization objective problem seeks to optimize f over X by training D, in a manner that D
transforms Z into O ∈X where O consists of molecular structures with optimal f(o), o ∈O.
Train with
Uniform Weights
JTVAE Model
Encoder
Decoder
Drug-Like Molecule
Dataset
GMD Optimization through Iterative Weighted Retraining
Property Evaluation Model
Molecular Structure
PARP1 Inhibition
Model
Rule Based
Mechanistic
Model
IC50
Therapeutic
Score
JTVAE Model
Retrain with
Rank Based
Weights
JTVAE Model
Sampled Molecules
from Latent Space
Therapeutic Score from
Molecular Structures
Using Pathway Model
Drug-Like Molecule
Dataset
Sample Optimized Molecules with
Bayesian Optimization
Retrain with
Rank Based
Weights
Figure 1: Overview of methodology
The initial phase of the proposed
method involves employing a JT-
VAE model, consisting an en-
coder E : X 7→Z and a de-
coder D : Z 7→X, that has
been trained with equal weights
assigned to all molecules in the
training set D. Subsequently, we
refine the model using a rank-
based weighted retraining tech-
nique. In contrast to uniformly
weighting all molecules, we al-
locate distinct weights to each
molecule based on the specific
property value we aim to en-
hance. To evaluate the cancer
therapeutic score, we use a com-
bination of data-driven and mech-
anistic model, consisting a machine learning based PARP1 inhibition model for IC50 evaluation and a
rule-based pathway-guided mechanistic model for therapeutic score evaluation, as an approximation
to f. We sample optimal points from the latent space of the JTVAE following Bayesian optimization
process, which are used for subsequent refinement of the JTVAE model. The whole methodology can
be best described by Fig. 1.
3.1
Unweighted JTVAE training
JTVAE represents an expansion of the latent space-driven deep generative model, known as a varia-
tional autoencoder, designed specifically for molecular graphs. JTVAE approaches the interpretation
of molecules by considering them as assemblies of valid chemical components, thus avoiding the
more intricate process of constructing molecules atom by atom. It accomplishes automated molecule
design by following a two-step procedure. Firstly, it generates tree-like molecular substructures,
breaking down the molecule into its constituent parts. Finally, it combines these substructures using
a message passing graph neural network. This network allows for the integration of information
between the substructures, ultimately resulting in the creation of molecules tailored to specific chemi-
cal properties [30]. A molecule is represented by two complimentary representations, the original
molecular graph and its associated junction tree. The whole encoder E consists of two distinct
4
encoders q(zT |T) and q(zG|G) for encoded representations of the junction tree and graph structure
accordingly. The latent representation {zT , zG} is then decoded back to the molecular graph by first
decoding the tree structure from its latent representation using the tree decoder, p(T|zT ), then by
decoding to the molecular graph from the decoded tree and latent graph using the graph decoder
p(G|zG, T). Here, the tree decoder p(T|zT ) and the graph decoder p(G|zG, T) consists the overall
decoder D. During the optimization process, the loss function value for all molecules are aggregated
by assigning weights wi∈{1,2,...,|D|} on each molecule in the training data D. Within the unweighted
training scheme, every molecule is treated with the same level of importance or significance during
the training process. The weights for all molecules in the training data are equal, hence we refer it as
unweighted JTVAE training. This indicates that no distinctions or preferences are made among the
molecules regarding their individual contributions to the learning process. We utilize a JTVAE model
that has previously undergone training, without any specific weighting scheme, to generate novel
molecules, primarily focusing on different chemical properties such as logP or QED. In practical
terms, this means that all molecules within the training set were assigned uniform weights, without
any differentiation based on their individual attributes or characteristics.
3.2
Periodic weighted JTVAE training
Our approach involves a synergy of techniques, namely data weighting and periodic retraining, aimed
at elevating the generative model from a passive role as a decoder to an active contributor in the
optimization process. This transformation is orchestrated to fine-tune the latent space, making it
increasingly proficient at generating molecules with high property values over time.
3.2.1
Data weighting
The quest for designing a generative model that is most conducive to optimization within the latent
space remains an ongoing research endeavor. One effective strategy is to focus exclusively on the
subset containing data points characterized by high property values. This approach is beneficial
because it concentrates the modeling efforts on the most promising and desirable data instances,
which can lead to more accurate and effective results. But with limited or scarce data in hand,
it becomes advantageous to train the model using a probabilistic distribution that exhibits a high
likelihood for data points with superior property scores while assigning a notably lower likelihood
to those with lower property scores. This distribution essentially emphasizes the importance of
high-scoring data points during the training process, enabling the model to make the most out of the
limited data available and potentially enhance its performance on the desired property optimization
task. We apply the later approach by assigning an explicit weight wi to each data point, such that
P
i wi = 1. We follow the rank-based weight function proposed in [53]. The weighting approach
can be described by the following equations:
rankf,D(x) = |{xi : f(xi) > f(x), xi ∈D}|
(1)
w(x; D, k) ∝
1
10−kN + rankf,D(x)
(2)
This weighting function assigns a weight roughly proportional to the reciprocal (zero-based) rank of
each data point. Also, a high k value assigns more weights on the high value points compared to the
low value points.
3.2.2
Periodic retraining
To optimize the latent space of the JTVAE, periodically retraining the generative model is a simple
but effective solution. In order to ensure the latent space remains effectively optimized [48] and to
prevent the occurrence of catastrophic forgetting [38], we employ a systematic approach of periodic
retraining for the JTVAE model. This process involves several key steps, among which are weighted
retraining, finding optimum latent samples from the latent space, appending those optimal molecules
to the training set and iterative retrain of the JTVAE model. We periodically retrain the JTVAE model
following the weighting scheme discussed in Sec. 3.2.1, find optimum samples from the latent space,
decode those latent samples back to molecular space, append those optimum samples to the training
set, and then continue this iterative retraining process for a number of times. Rather than subjecting
the JTVAE model to a complete retraining cycle using the entire updated training set, we adopt a
more selective approach during each retraining iteration. We carefully choose only a portion of the
5
training set for retraining the JTVAE model at each iteration. Also to maximize the optimization of
the latent space, we ensure that the optimal molecules sampled during one retraining iteration are
incorporated into subsequent iterations of retraining. In other words, the valuable insights gained
from the selected optimum molecules in one iteration continue to influence and enhance the model
to understand the latent space in subsequent iterations. By implementing this approach, we strike
a balance between updating the model’s knowledge with new data and preserving the knowledge
gained from optimal molecules, resulting in a more effective and efficient optimization of the latent
space over multiple iterations. To find the optimum latent samples, we follow Bayesian optimization.
Bayesian optimization sustains a probabilistic model of the target function in order to select new
points for assessment, guided by the modeled distribution of the objective values at points that have
not yet been observed [19, 8, 50]. Our objective function is a rule-based pathway guided mechanistic
model to predict the therapeutic score of a molecule. Our evaluation function consists of two parts: a
machine learning model to predict the IC50 value from the molecular graph, and a rule based pathway
model to predict therapeutic score from the predicted IC50 value.
3.3
Therapeutic Score Calculation
The calculation of the therapeutic score is performed on two steps. First, the pIC50 value of the
generated molecule is calculated using a machine learning based PARP1 inhibition model, which is
then used to predict the IC50 values of new compounds. Then from the IC50 values we calculate
the therapeutic score using an rule-based pathway-guided mechanistic model. We use three different
versions of the rule-based pathway-guided mechanistic model.
• Physiologically viable pathway model.
• Modified pathway model.
• Physiologically impractical pathway model.
1
2
3
4
5
6
7
8
9
10
11
pIC50
2500
5000
7500
10000
12500
15000
Therapeutic Score
Viable
Modified
Impractical
0
2000
4000
6000
8000
10000
Samples in Training Set
pIC50 Distribution of Training Data
Figure 2: Relationship between therapeutic score
and pIC50 according to the pathway guided mech-
anistic models.
The relationship between the therapeutic score
and pIC50 value according to each version of
the pathway model is shown in Fig. 2. The
relation between therapeutic score and pIC50
is approximately linear in the lower range of
pIC50, reaches to a maximum therapeutic score
at around pIC50=8 for physiologically viable
pathway model, at around pIC50=6 for modi-
fied pathway model and at around pIC50=3 for
physiologically impractical pathway model, and
then stays at a fixed therapeutic score for suc-
cessive pIC50 values. The three versions of the
pathway models primarily differ in the position
of the peak point on the relationship curve between therapeutic score and pIC50.
3.3.1
PARP1 Inhibition Model
PARP1 inhibition data was gathered from public and private databases including ChEMBL30 [39],
GoStar [20], and HiTS [23]. A basic curation pipeline was applied that includes compound standard-
ization, relegation of all applicable values to pIC50’s (negative log of the IC50 in molar), discarding
outliers and averaging repeated measurements. Special attention was paid to assay descriptions
for each measurement to ensure data comparability across different sources. The final dataset con-
sisted of 9411 compounds. The data was partitioned into training, validation and test sets using an
80/10/10 split. A comprehensive hyperparameter search was conducted for split type (scaffold or
fingerprint), model type (RF, XGboosted RF, FCNN or graph convolutional NN), and feature set
(ECFP4 fingerprints, MOE, Mordred or RDKit descriptors) in addition to parameters for each model
type. The best performing model was selected based on the validation set performance of R-squared
metrics. It is a graph convolutional NN fingerprint split model. Model performance is listed in Table
1. The data curation, hyperparameter search, and model evaluation was conducted using the ATOM
Modeling Pipeline version 1.4.2 [40].
6
Table 1: Performance of the PARP1 inhibition model
R-squared
MAE
RMSE
Training Set
0.363
0.756
0.937
Validation Set
0.419
0.757
0.982
Test Set
0.158
1.045
1.298
3.3.2
Therapeutic Score Calculation Using Rule-based Pathway-Guided Mechanistic Model
The pathway model utilized in this project was adjusted for PARP1 binding to DNA double strand
breaks but derived from a previously published report defining cell death (via apoptosis) by two
separate mediators, the transcription factor, p53, or a membrane associated kinase, AKT [6].
Figure 3: Extended contact map of pathway model.
There are 14 molecules arranged for their location in the
nucleus (white background) and the cytoplasm (shaded
background). Nineteen reaction rules are depicted by
the lines with and without arrows which further define
binding and/or catalytic reactions.
In the updated model used here, the role
of AKT is eliminated and only p53 ac-
tion is considered through competition with
PARP1 in the presence of DNA double
stranded breaks. In the absence of an in-
hibitor, PARP1 functions to preserve cell
viability in the presence of DNA double
strand breaks. However, if PARP1 is bound
to an inhibitor of sufficient strength, its
function may become diminished, allowing
p53 to trigger cell death via caspase acti-
vation. This caspase activated cell death
is the therapeutic score. To reiterate, po-
tent molecules with low IC50s in the model
bind strongly to PARP1, leading to ex-
pected cell viability or death. And further,
molecular abundance of activated caspase
above 5000 was considered the trigger for
apoptosis. The three versions of the path-
way models are different according to their
peaks. Fig. 3 presents an extended con-
tact map of the reaction network included
in the published pathway model that con-
forms to guidelines accommodating rule-
based models. Our methods utilized the
reactions found in the white box and caspase activation (shaded box).
4
Results & Discussion
Rather than embarking on the training of a JTVAE model from scratch, we employ a pretrained
JTVAE model that has been previously trained in an unweighted fashion on the ZINC250K dataset
[29]. The initial training dataset consists of 228497 small drug-like molecular structures. During
the periodic weighted retraining and latent space optimization, the very first retraining iteration
commences by utilizing the complete initial training dataset. In the subsequent retraining iterations,
only a subset comprising 10% of the original training datapoints is employed as the training set. This
subset is complemented by the inclusion of optimized molecules that have been sampled through
latent space Bayesian optimization. The optimized molecules that are sampled during one retraining
iteration are employed to train the JTVAE model for all successive retraining iterations. We retrain
for 10 iterations with a retraining frequency of 50, meaning at each retraining iteration we sample 50
optimal points and append to the training dataset and carry on the retraining iterations. During every
retraining iteration, we utilize Bayesian optimization to obtain a set of optimal data points sampled
from the latent space. We employ a combination of 2000 molecules with optimal scores and 8000
molecules randomly selected from the training dataset to fit a Gaussian process model. It took about
12 hours to optimize the model through retraining for 10 iterations on a workstation with a single
NVIDIA RTX3090 GPU.
In this study, our primary emphasis is directed towards evaluating the impact of weighted retraining
7
5000
10000
5000
10000
Therapeutic Score
Train
0
1
2
3
4
5
6
7
8
9
10
Retraining Iteration
12000
12500
13000
(a)
(b)
(c)
k=4
k=5
k=6
Figure 4: Distribution of therapeutic scores of generated molecules in consecutive retraining iterations
for physiologically (a)viable, (b)modified, and (c)impractical pathway model.
Train
0
1
2
3
4
5
6
7
8
9
10
Retraining Iteration
2
4
6
8
pIC50
k=4
k=5
k=6
Figure 5: Distribution of pIC50 of generated molecules in consecutive retraining iterations.
on the distribution of therapeutic score. In each retraining iteration, we take a random sampling of
molecules from the latent space. This deliberate choice is motivated by our objective, which is to
closely monitor and analyze the transformations occurring within the latent space as a result of the
weighted retraining process. Following the completion of each retraining iteration, we proceed to
randomly sample 5000 molecules from the latent space. For the sake of fair evaluation, we use the
same 5000 randomly sampled latent space points at each retraining iteration to decode into molecular
structures.
To understand the effect of different weighting scales, we run the retraining experiments for different
k values. In this work, we observe the retraining effect for three different values of k = 4, 5 and 6.
Fig. 4 shows the sample distributions of the generated molecules at consecutive retraining iterations
for different k values for all the pathway models.
Fig. 4 shows that k = 6 pushes the model most to generate molecules with high therapeutic scores
(typically ≥12000) in consecutive retraining iterations for all the pathway models. The largest num-
ber of generated molecules with high therapeutic scores is observed in the physiologically impractical
pathway model, followed by the modified pathway model, and the least in the physiologically viable
pathway model. This phenomenon can be explained from the relation between therapeutic score
and pIC50 for different pathway models, and the data distribution of the training set as shown in
Fig. 2. Intuitively, the greater the number of molecules with optimal therapeutic scores in the
training set, the easier the optimization task becomes. For the physiologically impractical pathway
model, the training set contains the largest number of molecules with optimal therapeutic scores,
followed by the modified pathway model, and the least in the physiologically viable pathway model.
Differences in training data distributions for different pathway models align the observations with our
intuition. Additionally, the optimization process causes the greatest shift in sample distribution for
the physiologically viable pathway model compared to the modified and physiologically impractical
pathway models, due to the greater potential for distributional shift of the generated molecules in the
viable pathway model.
Figure 5 illustrates the pIC50 distribution of the generated molecules when pIC50 is the optimization
objective. As with the therapeutic score optimization, increasing k drives the model to predominantly
generate molecules with higher pIC50 values.
8
0
1
2
3
4
5
6
7
8
9
10
Retraining Iteration
3000
4000
5000
Physiologically Viable Pathway Model
4
5
6
0
1
2
3
4
5
6
7
8
9
10
Retraining Iteration
Modified Pathway Model
4
5
6
0
1
2
3
4
5
6
7
8
9
10
Retraining Iteration
Physiologically Impractical Pathway Model
4
5
6
Figure 6: Number of generated unique molecules.
5
Limitations of Proposed Approach
There are three main aspects of our proposed methodology: periodic weighted retraining of JTVAE
model, PARP inhibition model and rule-based mechanistic model. We discuss the limitations of these
three different aspects of our proposed methodology.
5.1
Limitations of Periodic Weighted Retraining of JTVAE
Fig. 6 shows the number of generated unique molecules at each retraining iteration for different k
values and for all three pathway models. At each retraining iterations, the plot shows the number of
generated unique molecules among the randomly generated 5,000 molecules from the latent space.
With higher k values, the number of uniquely generated molecules gets lower. From all the results
shown, we see that k = 6 experiments produce molecules with highest property values, but at the
expense of reducing the latent space’s capability of generating unique molecules. This means that
weighting too much on high property molecules at the retraining phase may lead to the generation
of high property molecules, but at the same time it converts the latent space in such a way that
the generative model looses its capability of generating unique molecules. The decrease in unique
molecules with each consecutive retraining iteration is the steepest for the physiologically viable
pathway model. This reflects the fact that the training set contains the fewest molecules with optimal
therapeutic scores for the physiologically viable pathway model.
5.2
Limitations of PARP1 Inhibition Model
The PARP1 inhibition model suffers from two main limitations. First, the majority of molecules
with PARP1 inhibition data in the training set have medium to low potency values. The highest
potency molecules are not structurally diverse and centered around a few select scaffolds like those
of published clinical inhibitors. When optimizing toward high potency inhibitors of PARP1, the
model may be biased toward these few scaffolds. To combat this, a fingerprint split model was used.
Fingerprint splitting uses the Tanimoto similarity metric between Morgan fingerprints calculated
from each molecule to ensure that the train, validation, and test sets are as structurally dissimilar
as possible. During training, this should encourage the model to generalize its predictions beyond
memorizing which scaffolds are the most potent. However, the test set performance metrics (Table 1)
show that the model does not necessarily generalize well to the new, structurally distinct molecules in
the test set. These limitations can be especially detrimental for generative models where the goal is to
propose novel and potent leads. If the model is biased or cannot generalize to new scaffolds well, it
may steer the generative process toward the wrong area of chemical space.
5.3
Limitations of Rule-based Pathway-guided Mechanistic Model
There are two limitations of the pathway model utilized in this study. First, the pathway model
focused on a small number of biochemical reactions in interest of limiting computational time. In
particular, competition for DNA double strand breaks occurs between PARP1, the initiator DNA
repair processes and p53, a known tumor suppressor, that is able to induce cell death in the advent of
no repair. In fact, multiple repair processes exist as well as reaction networks leading to apoptosis.
Thus, the absence of important details may alter systems dynamics such as timing of those events
leading to different molecular abundances of activated caspases and calculated therapeutic scores.
In particular, the pathway model used does not model the expected ultrasensitive activated caspase
response found for other pathway models and living cells. Future studies may improve the usefulness
9
of LOS if the pathway models better reflect biology. Another limitation of the pathway model,
known for all mechanistic models, is parameter uncertainty. While every effort was made to utilized
measured values reported in the literature, the pathway model only reflects one single cell, perhaps
one of many heterogeneous ones in a tumor. Altering parameters or accounting for uncertainty in
later computations may overcome this.
6
Conclusions
We present an optimization of a generative model combining deep generative models for molecular
design with a rule-based, pathway-guided mechanistic model serving as the evaluation function for
the generated molecular structures. This combined strategy aims to optimize the generation process
of drug-like molecular structures. The results we’ve observed indicate that periodically retraining the
generative model with adjusted weights gradually refines the model’s parameters, resulting in the
production of more optimal molecular structures. Additionally, our mechanistic model, guided by
pathways, serves as the optimization objective function, negating the requirement for a dataset of
molecular structures with known cancer therapeutic efficacy.
References
[1] Cancer. https://www.who.int/news-room/fact-sheets/detail/cancer. [Accessed
21-09-2023].
[2] A. N. M. N. Abeer, N. Urban, M. R. Weil, F. J. Alexander, and B.-J. Yoon. Multi-objective
latent space optimization of generative molecular design models, 2022.
[3] A. Aggarwal, M. Mittal, and G. Battineni. Generative adversarial network: An overview of
theory and applications. International Journal of Information Management Data Insights, 1(1):
100004, 2021.
[4] J. Antoran, U. Bhatt, T. Adel, A. Weller, and J. M. Hernández-Lobato. Getting a {clue}:
A method for explaining uncertainty estimates. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=XSLF1XFq5h.
[5] P. L. Bedard, D. M. Hyman, M. S. Davids, and L. L. Siu. Small molecules, big impact: 20 years
of targeted therapy in oncology. The Lancet, 395(10229):1078–1088, Mar. 2020. doi: 10.1016/
s0140-6736(20)30164-1. URL https://doi.org/10.1016/s0140-6736(20)30164-1.
[6] M. N. Bogdał, B. Hat, M. Kocha´nczyk, and T. Lipniacki. Levels of pro-apoptotic regulator bad
and anti-apoptotic regulator bcl-xl determine the type of the apoptotic logic gate. BMC systems
biology, 7(1):1–17, 2013.
[7] S. Braakman, P. Pathmanathan, and H. Moore. Evaluation framework for systems models. CPT:
pharmacometrics & systems pharmacology, 11(3):264–289, 2022.
[8] E. Brochu, V. M. Cora, and N. de Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning,
2010.
[9] S. Chen, Z. Cao, K. Prettner, M. Kuhn, J. Yang, L. Jiao, Z. Wang, W. Li, P. Geldsetzer,
T. Bärnighausen, D. E. Bloom, and C. Wang. Estimates and Projections of the Global Economic
Cost of 29 Cancers in 204 Countries and Territories From 2020 to 2050. JAMA Oncology, 9
(4):465–472, 04 2023. ISSN 2374-2437. doi: 10.1001/jamaoncol.2022.7826. URL https:
//doi.org/10.1001/jamaoncol.2022.7826.
[10] L. Cinelli, M. Marins, E. da Silva, and S. Netto. Variational Methods for Machine Learning with
Applications to Deep Networks. Springer International Publishing, 2021. ISBN 9783030706791.
URL https://books.google.com/books?id=N5EtEAAAQBAJ.
[11] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and A. A. Bharath. Gen-
erative adversarial networks: An overview. IEEE signal processing magazine, 35(1):53–65,
2018.
10
[12] H. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song. Syntax-directed variational autoencoder for
structured data. In 6th International Conference on Learning Representations, ICLR 2018, Van-
couver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018. URL https://openreview.net/forum?id=SyqShMZRb.
[13] N. De Cao and T. Kipf. MolGAN: An implicit generative model for small molecular graphs.
ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative Models,
2018.
[14] C. Doersch. Tutorial on variational autoencoders, 2016.
[15] T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture search: A survey. J. Mach. Learn.
Res., 20(1):1997–2017, jan 2019. ISSN 1532-4435.
[16] D. C. Elton, Z. Boukouvalas, M. D. Fuge, and P. W. Chung. Deep learning for molecular
design—a review of the state of the art. Molecular Systems Design & Engineering, 4(4):828–
849, 2019. doi: 10.1039/C9ME00039A. URL http://dx.doi.org/10.1039/C9ME00039A.
[17] B. M. Essue, N. Iragorri, N. Fitzgerald, and C. de Oliveira. The psychosocial cost burden of
cancer: A systematic literature review. Psycho-Oncology, 29(11):1746–1760, Sept. 2020. doi:
10.1002/pon.5516. URL https://doi.org/10.1002/pon.5516.
[18] H. Farmer, N. McCabe, C. J. Lord, A. N. J. Tutt, D. A. Johnson, T. B. Richardson, M. Santarosa,
K. J. Dillon, I. Hickson, C. Knights, N. M. B. Martin, S. P. Jackson, G. C. M. Smith, and
A. Ashworth. Targeting the DNA repair defect in BRCA mutant cells as a therapeutic strategy.
Nature, 434(7035):917–921, Apr. 2005. doi: 10.1038/nature03445. URL https://doi.org/
10.1038/nature03445.
[19] P. I. Frazier. A tutorial on bayesian optimization, 2018.
[20] Global Online Structure Activity Relationship Database. https://www.gostardb.com/.
Accessed: 06/20/2022.
[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence,
and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27.
Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/
file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.
[22] R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling,
D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Au-
tomatic chemical design using a data-driven continuous representation of molecules. ACS
Central Science, 4(2):268–276, 2018.
doi: 10.1021/acscentsci.7b00572.
URL https:
//doi.org/10.1021/acscentsci.7b00572. PMID: 29532027.
[23] Hi-Throughput Screening Database. https://www.HiTS.ucsf.edu. Accessed: 06/20/2022.
[24] J. Ho,
A. Jain,
and P. Abbeel.
Denoising diffusion probabilistic models.
In
H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in
Neural Information Processing Systems, volume 33, pages 6840–6851. Curran Associates,
Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.
[25] S. Hoelder, P. A. Clarke, and P. Workman. Discovery of small molecule cancer drugs: Successes,
challenges and opportunities. Molecular Oncology, 6(2):155–176, 2012. ISSN 1574-7891. doi:
https://doi.org/10.1016/j.molonc.2012.02.004. URL https://www.sciencedirect.com/
science/article/pii/S1574789112000166. Personalized cancer medicine.
[26] E. Hoogeboom, V. G. Satorras, C. Vignac, and M. Welling. Equivariant diffusion for molecule
generation in 3D. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato,
editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of
Proceedings of Machine Learning Research, pages 8867–8887. PMLR, 17–23 Jul 2022. URL
https://proceedings.mlr.press/v162/hoogeboom22a.html.
11
[27] G. Illuzzi, A. D. Staniszewska, S. J. Gill, A. Pike, L. McWilliams, S. E. Critchlow, A. Cronin,
S. Fawell, G. Hawthorne, K. Jamal, J. Johannes, E. Leonard, R. Macdonald, G. Maglennon,
J. Nikkilä, M. J. O'Connor, A. Smith, H. Southgate, J. Wilson, J. Yates, S. Cosulich, and E. Leo.
Preclinical characterization of AZD5305, a next-generation, highly selective PARP1 inhibitor
and trapper. Clinical Cancer Research, 28(21):4724–4736, Aug. 2022. doi: 10.1158/1078-0432.
ccr-22-0301. URL https://doi.org/10.1158/1078-0432.ccr-22-0301.
[28] K. Imai and A. Takaoka. Comparing antibody and small-molecule therapies for cancer. Nature
Reviews Cancer, 6(9):714–727, 2006.
[29] J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G. Coleman. Zinc: A free
tool to discover chemistry for biology. Journal of Chemical Information and Modeling, 52(7):
1757–1768, 2012. doi: 10.1021/ci3001277. URL https://doi.org/10.1021/ci3001277.
PMID: 22587354.
[30] W. Jin, R. Barzilay, and T. Jaakkola. Junction tree variational autoencoder for molecular graph
generation. In International conference on machine learning, pages 2323–2332. PMLR, 2018.
[31] B. Jing, G. Corso, R. Barzilay, and T. S. Jaakkola. Torsional diffusion for molecular conformer
generation. In ICLR2022 Machine Learning for Drug Discovery, 2022. URL https://
openreview.net/forum?id=D9IxPlXPJJS.
[32] D. P. Kingma and M. Welling.
Auto-Encoding Variational Bayes.
In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
[33] M. J. Kusner, B. Paige, and J. M. Hernández-Lobato. Grammar variational autoencoder. In
D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine
Learning, volume 70 of Proceedings of Machine Learning Research, pages 1945–1954. PMLR,
06–11 Aug 2017. URL https://proceedings.mlr.press/v70/kusner17a.html.
[34] C. J. Lord and A. Ashworth. PARP inhibitors: Synthetic lethality in the clinic. Science, 355
(6330):1152–1158, Mar. 2017. doi: 10.1126/science.aam7344. URL https://doi.org/10.
1126/science.aam7344.
[35] X. Lu, J. Gonzalez, Z. Dai, and N. Lawrence. Structured variationally auto-encoded optimization.
In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, volume 80 of Proceedings of Machine Learning Research, pages 3267–3275. PMLR,
10–15 Jul 2018. URL https://proceedings.mlr.press/v80/lu18c.html.
[36] R. Luo, F. Tian, T. Qin, E. Chen, and T.-Y. Liu.
Neural architecture optimization.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems, volume 31. Curran
Associates, Inc., 2018.
URL https://proceedings.neurips.cc/paper/2018/file/
933670f1ac8ba969f32989c312faba75-Paper.pdf.
[37] C. Mattiuzzi and G. Lippi. Current cancer epidemiology. Journal of Epidemiology and Global
Health, 9(4):217, 2019. doi: 10.2991/jegh.k.191008.001. URL https://doi.org/10.2991/
jegh.k.191008.001.
[38] M. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem.
In G. H. Bower, editor, Psychology of Learning and Moti-
vation, volume 24, pages 109–165. Academic Press, 1989.
doi: https://doi.org/10.1016/
S0079-7421(08)60536-8. URL https://www.sciencedirect.com/science/article/
pii/S0079742108605368.
[39] K. M. Mendez, S. N. Reinke, and D. I. Broadhurst. A comparative evaluation of the generalised
predictive ability of eight machine learning algorithms across ten clinical metabolomics data sets
for binary classification. Metabolomics, 15(12), Nov. 2019. doi: 10.1007/s11306-019-1612-4.
URL https://doi.org/10.1007/s11306-019-1612-4.
12
[40] A. J. Minnich, K. McLoughlin, M. Tse, J. Deng, A. Weber, N. Murad, B. D. Madej, B. Ram-
sundar, T. Rush, S. Calad-Thomson, J. Brase, and J. E. Allen. AMPL: A data-driven modeling
pipeline for drug discovery. Journal of Chemical Information and Modeling, 60(4):1955–1968,
Apr. 2020. doi: 10.1021/acs.jcim.9b01053. URL https://doi.org/10.1021/acs.jcim.
9b01053.
[41] A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune. Synthesizing the preferred
inputs for neurons in neural networks via deep generator networks. In D. Lee, M. Sugiyama,
U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Sys-
tems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/
paper_files/paper/2016/file/5d79099fcdf499f12b79770834c0164a-Paper.pdf.
[42] A. Nguyen, J. Clune, Y. Bengio, A. Dosovitskiy, and J. Yosinski. Plug & play generative
networks: Conditional iterative generation of images in latent space. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pages 4467–4477, 2017.
[43] A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In Interna-
tional Conference on Machine Learning, pages 8162–8171. PMLR, 2021.
[44] D. W. Otter, J. R. Medina, and J. K. Kalita. A survey of the usages of deep learning for natural
language processing. IEEE Transactions on Neural Networks and Learning Systems, 32(2):
604–624, 2021. doi: 10.1109/TNNLS.2020.2979670.
[45] O. S. Rukhlenko, F. Khorsand, A. Krstic, J. Rozanc, L. G. Alexopoulos, N. Rauch, K. E.
Erickson, W. S. Hlavacek, R. G. Posner, S. Gómez-Coca, et al.
Dissecting raf inhibitor
resistance by structure-based modeling reveals ways to overcome oncogenic ras signaling. Cell
systems, 7(2):161–179, 2018.
[46] B. Sanchez-Lengeling and A. Aspuru-Guzik. Inverse molecular design using machine learning:
Generative models for matter engineering. Science, 361(6400):360–365, 2018.
[47] R. Schmucker, G. Farina, J. Faeder, F. Fröhlich, A. S. Saglam, and T. Sandholm. Combination
treatment optimization using a pan-cancer pathway model. PLoS computational biology, 17
(12):e1009689, 2021.
[48] O. Sener and S. Savarese. Active learning for convolutional neural networks: A core-set
approach. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=H1aIuk-RW.
[49] M. Simonovsky and N. Komodakis. Graphvae: Towards generation of small graphs using
variational autoencoders. In International conference on artificial neural networks, pages
412–422. Springer, 2018.
[50] J. Snoek, H. Larochelle, and R. P. Adams.
Practical bayesian optimization of ma-
chine learning algorithms.
In F. Pereira, C. Burges, L. Bottou, and K. Weinberger, edi-
tors, Advances in Neural Information Processing Systems, volume 25. Curran Associates,
Inc., 2012. URL https://proceedings.neurips.cc/paper_files/paper/2012/file/
05311655a15b75fab86956663e1819cd-Paper.pdf.
[51] K. Sohn, H. Lee, and X. Yan. Learning structured output representation using deep condi-
tional generative models. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates,
Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/
8d55a249e6baa5c06772297520da2051-Paper.pdf.
[52] Y. Song, P. Dhariwal, M. Chen, and I. Sutskever. Consistency models, 2023.
[53] A. Tripp, E. Daxberger, and J. M. Hernández-Lobato. Sample-efficient optimization in the
latent space of deep generative models via weighted retraining. In H. Larochelle, M. Ranzato,
R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
volume 33, pages 11259–11272. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper/2020/file/81e3225c6ad49623167a4309eb4b2e75-Paper.pdf.
13
[54] A. van den Oord, N. Kalchbrenner, L. Espeholt, k. kavukcuoglu, O. Vinyals, and A. Graves.
Conditional image generation with pixelcnn decoders. In D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-
ume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/
2016/file/b1301141feffabac455e1f90a7de2054-Paper.pdf.
[55] H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, W. Li, X. Xie, and M. Guo. Learning
graph representation with generative adversarial nets. IEEE Transactions on Knowledge and
Data Engineering, 33(8):3090–3103, 2021. doi: 10.1109/TKDE.2019.2961882.
[56] J. Wang, J. Luttrell, N. Zhang, S. Khan, N. Shi, M. X. Wang, J.-Q. Kang, Z. Wang, and
D. Xu. Exploring human diseases and biological mechanisms by protein structure prediction
and modeling. In Advances in Experimental Medicine and Biology, pages 39–61. Springer
Singapore, 2016. doi: 10.1007/978-981-10-1503-8_3. URL https://doi.org/10.1007/
978-981-10-1503-8_3.
[57] K. Wang, C. Gou, Y. Duan, Y. Lin, X. Zheng, and F.-Y. Wang. Generative adversarial networks:
introduction and outlook. IEEE/CAA Journal of Automatica Sinica, 4(4):588–598, 2017.
[58] D. Wen, M. Danquah, A. K. Chaudhary, and R. I. Mahato. Small molecules targeting microrna
for cancer therapy: Promises and obstacles. Journal of Controlled Release, 219:237–247,
2015. ISSN 0168-3659. doi: https://doi.org/10.1016/j.jconrel.2015.08.011. URL https://
www.sciencedirect.com/science/article/pii/S0168365915300572. Drug Delivery
Research in North America – Part I.
[59] L. Wu, C. Gong, X. Liu, M. Ye, and Q. Liu. Diffusion-based molecule generation with informa-
tive prior bridges. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
editors, Advances in Neural Information Processing Systems, volume 35, pages 36533–36545.
Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/
paper/2022/file/eccc6e11878857e87ec7dd109eaa9eeb-Paper-Conference.pdf.
[60] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang. Geodiff: A geometric diffusion model for
molecular conformation generation. In International Conference on Learning Representations,
2022. URL https://openreview.net/forum?id=PzcvxEMzvQC.
[61] J. Zhang, D. Duan, Z.-L. Song, T. Liu, Y. Hou, and J. Fang. Small molecules regulating reactive
oxygen species homeostasis for cancer therapy. Medicinal Research Reviews, 41(1):342–394,
Sept. 2020. doi: 10.1002/med.21734. URL https://doi.org/10.1002/med.21734.
[62] L. Zhang, J. Tan, D. Han, and H. Zhu. From machine learning to deep learning: progress in
machine intelligence for rational drug discovery. Drug Discovery Today, 22(11):1680–1685,
2017. ISSN 1359-6446. doi: https://doi.org/10.1016/j.drudis.2017.08.010. URL https:
//www.sciencedirect.com/science/article/pii/S1359644616304366.
[63] M. Zhang, S. Jiang, Z. Cui, R. Garnett, and Y. Chen. D-vae: A variational autoencoder for
directed acyclic graphs. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran
Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/
2019/file/e205ee2a5de471a70c1fd1b46033a75f-Paper.pdf.
[64] M. Zhao, B. Jiang, and F.-H. Gao. Small molecule inhibitors of STAT3 for cancer therapy. Cur-
rent Medicinal Chemistry, 18(26):4012–4018, Sept. 2011. doi: 10.2174/092986711796957284.
URL https://doi.org/10.2174/092986711796957284.
[65] L. Zhong, Y. Li, L. Xiong, W. Wang, M. Wu, T. Yuan, W. Yang, C. Tian, Z. Miao, T. Wang,
and S. Yang. Small molecules in targeted cancer therapy: advances, challenges, and future
perspectives. Signal Transduction and Targeted Therapy, 6(1):201, May 2021. doi: 10.1038/
s41392-021-00572-w. URL https://doi.org/10.1038/s41392-021-00572-w.
14
