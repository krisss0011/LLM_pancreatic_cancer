Monitoring time to event in registry data using CUSUMs based on
excess hazard models
Jimmy Huy Tran1, Jan Terje Kvaløy2 and Hartwig Kørner3
November 13, 2024
Abstract
An aspect of interest in surveillance of diseases is whether the survival time distribution changes
over time. By following data in health registries over time, this can be monitored, either in real
time or retrospectively. With relevant risk factors registered, these can be taken into account in the
monitoring as well. A challenge in monitoring survival times based on registry data is that data
on cause of death might either be missing or uncertain. To quantify the burden of disease in such
cases, excess hazard methods can be used, where the total hazard is modelled as the population
hazard plus the excess hazard due to the disease.
We propose a CUSUM procedure for monitoring for changes in the survival time distribution
in cases where use of excess hazard models is relevant. The procedure is based on a survival log-
likelihood ratio and extends previously suggested methods for monitoring of time to event to the
excess hazard setting. The procedure takes into account changes in the population risk over time,
as well as changes in the excess hazard which is explained by observed covariates.
Properties,
challenges and an application to cancer registry data will be presented.
Keywords: Excess risk, relative survival, survival time monitoring, risk-adjusted monitoring.
1
Introduction
In health registries, such as cancer registries, patients are routinely registered at diagnosis, and outcome
data like survival times are often added later. Based on such data, an aspect of interest could be to
monitor whether the distribution of the time to an outcome of interest changes over time, for instance
if the survival times of cancer patients change over time while adjusting for known risk factors. Such
monitoring could be of interest both for real-time monitoring of incoming data and for retrospective
analyses to pinpoint when in time important changes took place. A common challenge in monitoring
survival times based on such registry data is that time to death, but not necessarily cause of death, is
registered. To quantify the burden of disease in such cases, excess hazard methods can be used. With
excess hazard models, the total hazard is modelled as the population hazard plus the excess hazard
due to the disease. The population hazard is found from national life tables.
Approaches for monitoring of ordinary time to event data were first proposed by Biswas and
Kalbfleisch (2008), Sego, Reynolds Jr and Woodall (2009), Gandy, Kvaløy, Bottle and Zhou (2010)
and Steiner and Jones (2010). These have since been extended to things like frailty models (Begun,
Kulinskaya and MacGregor, 2019; Keshavarz, Asadzadeh and Niaki, 2021), cure models (Oliveira,
Valen¸ca, Medeiros and Mar¸cula, 2016), illness-death models (Liu, Lai, Wang, Zhu and Liu, 2023) and
queue models (Kuang, Das, Sir and Pasupathy, 2023). Impact of estimation error in time to event data
1Department of Mathematics and Physics, University of Stavanger, Norway.
Corresponding author.
E-mail:
jimmy.tran@uis.no.
2Department of Mathematics and Physics, University of Stavanger, Norway.
3Department of Gastrointestinal Surgery, Stavanger University Hospital Stavanger, Norway and Department of Clin-
ical Medicine, University of Bergen, Bergen, Norway.
1
arXiv:2411.09353v2  [stat.AP]  19 Nov 2024
monitoring was studied by Zhang, Xu, He and Hou (2016) and effectiveness versus periodic evaluations
was studied by Massarweh, Chen, Rosen, Dong, Richardson, Axelrod, Harris, Wilson and Petersen
(2021). Applications have been demonstrated in, for instance, monitoring of perioperative mortality
(Lai, Li, Liu, Tsung, Lai, Wang, Zhang, Zhu and Liu, 2021; Chen, Chidi, Dong, Richardson, Axelrod,
Petersen and Massarweh, 2023).
We propose a CUSUM procedure for monitoring for changes in the time to event distribution when
the use of excess hazard models is relevant. The procedure is based on a survival log-likelihood ratio
and extends the literature discussed above to monitoring of time to event in the excess hazard setting.
The procedure takes into account changes in the population risk over time, as well as changes in the
excess hazard which are explained by observed covariates. Properties, challenges and an application
to cancer registry data will be presented.
The structure of the paper is as follows: Section 2 introduces the setup and notation and presents
the proposed method. In Section 3, numerical simulations and experiments are carried out to demon-
strate the use of the method and how it performs under different scenarios, including studying the
impact of estimation error. An application of the method on a real data set obtained from the Norwe-
gian Cancer Registry is illustrated in Section 4. Finally, Section 5 provides some concluding remarks.
R-code implementing the proposed methods and for running the simulations in Section 3 can be found
at https://github.com/jihut/cusum_relative_survival_simulations.
2
CUSUM chart based on excess risk models
2.1
Setup and Notation
The setting considered is monitoring time to event data under the assumption of an additive hazard
model h(t) = hP (t) + hE(t). Here, hP (·) denotes the population hazard that is assumed to be known
and usually found from population life tables, while hE(·) denotes the excess hazard. We would like
to monitor for changes in hE(·) over calendar time.
Let B1 ≤B2 ≤. . . denote the calendar times at which individuals enter. Also, let Z1, Z2, . . .
denote the corresponding vectors of demographic variables affecting hP (t) and X1, X2, . . . the covariate
vectors affecting hE(t). The demographic variables, often age, gender and birth year, may or may not
be a subset of the covariates. Furthermore, let Ti denote the time to event and Ci the censoring
time for individual i.
We then define the time at risk up to calendar time t for individual i as
Ai(t) = min(Ti, Ci, max(t −Bi, 0)) and the event indicator δi(t) = I(Ti = Ai(t)).
Let h0i(·) = hP (·, Zi) + hE,0(·, Xi) be the in-control hazard rate. The known population hazard
hP (·) might change over calendar time, but this is suppressed in the notation. The excess hazard rate
in the in-control (baseline) situation is assumed to be constant over calendar time. At some calendar
time η, the hazard switches to some out of control hazard rate h1i(·) = hP (·, Zi) + hE,1(·, Xi) and we
would like to quickly detect this change. Finally, let the cumulative hazard be Hji(t) =
R t
0 hji(s)ds for
j = 0, 1.
2.2
The proposed CUSUM procedure
Following Gandy et al. (2010), we define a continuous time CUSUM procedure based on the survival
likelihood ratio. Moustakides (1986) has shown optimality properties for CUSUMs based on likelihood
ratios. The (partial) likelihood based on hji up to calendar time t is (Aalen, Borgan and Gjessing, 2008)
Lj(t) =
Y
i:Bi≤t
hji(Ti)δi(t) exp[−Hji(Ai(t))].
2
The (partial) log-likelihood ratio between the in- and out-of-control likelihood up to calendar time t
thus becomes
R(t) =
X
i:Bi≤t
δi(t) log
h1i(Ti)
h0i(Ti)

−
X
i:Bi≤t
[H1i(Ai(t)) −H0i(Ai(t))] .
R(t) is the continuous time analogue to a cumulative sum. By restarting this sum whenever it drops
to 0, we get the continuous time CUSUM control chart
Ψ(t) = R(t) −min
s≤t R(s)
that signals at a time τ = inf{t : Ψ(t) > c} for some threshold c > 0.
What differs from the procedure in Gandy et al. (2010) is that we here work with an excess risk
model, and this in particular implies that it is more challenging to determine c and that we need to
keep track of the population hazard in the first term of R(t). Usually, the population hazard can
be found in national life tables. Also, in most practical applications, one has to estimate hE,0(·, Xi).
The most common excess hazard models in relative survival are based on the proportional hazard
model, i.e. hE,0(·, Xi) = h0(·) exp(βXi), where h0 corresponds to the baseline excess hazard. For
our purposes, a piecewise constant baseline excess hazard model, estimated by a GLM-approach with
Poisson error structure (Dickman, Sloggett, Hills and Hakulinen, 2004), and a semiparametric excess
hazard model with a smooth nonparametric estimate of h0(·), fitted by an EM-algorithm (Perme,
Henderson and Stare, 2009), will be the relevant models in the upcoming sections.
2.3
Out-of-control alternatives
In this section, we consider different alternative models for the out-of-control hazard h1i(·). As mo-
tivated in the previous sections, the main focus here is to monitor the change in the excess hazard
part related to the disease of interest. Therefore, the standard proportional alternative on the overall
hazard considered in Gandy et al. (2010), which implies monitoring change in the total hazard, is
not suitable for the current purpose. However, based on the same idea, a proportional alternative
directly on the excess hazard, i.e. hE,1(·, Xi) = ρhE,0(·, Xi) for some ρ > 0, can be considered. The
corresponding log-likelihood ratio at a given calendar time t becomes
R(t) =
X
i:Bi≤t
δi(t) log
hP (Ti, Zi) + ρhE,0(Ti, Xi)
hP (Ti, Zi) + hE,0(Ti, Xi)

−(ρ −1)
X
i:Bi≤t
[HE,0i(Ai(t))] .
(1)
A number of simulated scenarios and illustrations with this model was presented in the master thesis
of Tran (2022). A similar, but more enhanced simulation study inspired by real data will be presented
here.
The proportional alternative implies assuming a larger change in absolute value of the hazard for
patients with a higher hazard. However, this might not always be reasonable, and an alternative model
for the change could be an additive model where the hazard changes by the same absolute amount for
all patients. This motivates an additive out-of-control alternative for the excess hazard. Assuming that
we work with non-negative excess hazard, which is usually the case when dealing with cancer patients,
the additive alternative is given as hE,1(·, Xi) = max(0, hE,0(·, Xi)+γ) for some −∞< γ < ∞so that
3
HE,1(t, Xi) =
R t
0 max(0, hE,0(·, Xi) + γ)du and
R(t) =
X
i:Bi≤t
δi(t) log
hP (Ti, Zi) + max(0, hE,0(Ti, Xi) + γ)
hP (Ti, Zi) + hE,0(Ti, Xi)

−
X
i:Bi≤t
  Z Ai(t)
0
max(0, hE,0(u, Xi) + γ)du
!
−HE,0(Ai(t), Xi)
!
.
(2)
The use of the maximum function is to ensure that we do not get a negative excess hazard in the
out-of-control scenario for some individuals when γ < 0. In the case where γ > 0, the maximum
function can be omitted and the log-likelihood ratio simplifies to
R(t) =
X
i:Bi≤t
δi(t) log
hP (Ti, Zi) + hE,0(Ti, Xi) + γ
hP (Ti, Zi) + hE,0(Ti, Xi)

−γ
X
i:Bi≤t
Ai(t).
(3)
The setup can in theory be extended to non-negative excess hazard as well by rather requiring that
the total hazard is non-negative, i.e. h1i(·) = max(hP (·, Zi) + hE,0(·, Xi) + γ, 0). For situations where
h1i(Ti) = 0, the corresponding observations will have no events. The hazard term can thus be omitted
for these. However, we will not pursue this somewhat odd situation any further.
Further, as considered in Gandy et al. (2010) for ordinary hazard models, time-transformation
alternatives can be used to model changes in the hazard. We consider the following linear accelerated
time alternative specified as HE,1(u, Xi) = HE,0(ku, Xi) for some k > 0. This leads to the following
log-likelihood ratio for the linear accelerated time alternative:
R(t) =
X
i:Bi≤t
δi(t) log
hP (Ti, Zi) + khE,0(kTi, Xi)
hP (Ti, Zi) + hE,0(Ti, Xi)

−
X
i:Bi≤t
[HE,0i(kAi(t)) −HE,0i(Ai(t))] .
(4)
This parameterization implies in most cases an easy interpretation of the alternative - a value of k < 1
yields HE,1i(u) = HE,0i(ku) < HE,0i(u), which implies that the survivor function is larger at the
same time point u in the out-of-control setting compared to the in-control scenario, and the other way
around for k > 1. One could also choose the parameterization hE,1(u, Xi) = hE,0(ku, Xi), but the
interpretation is less clear as it will in general depend on the shape of hE,0(u, Xi) as a function of u
and thus will not be considered further here.
A possible challenge with the linear accelerated time alternative occurs if the monitoring system
is used to monitor survival up to a certain time point, for instance 10 year survival, and a k > 1
alternative is of interest. Then, estimation of the hazard function up to 10k years will be required
for calculating hE,0(ku, Xi) and HE,0(ku, Xi). If nonparametric modelling is used, this requires that
survival data up to 10k years must be available for estimating the hazard function. If survival data
beyond 10 years are not available, a parametric hazard model can be used, but this will require
extrapolations beyond the range of the data used to estimate the model.
2.4
Criteria for determining the signal threshold
The signal threshold c needs to be tuned to achieve the desired performance of the CUSUM charts.
Usually, the threshold is chosen so that the charts achieve a certain performance under the in-control
alternative. For example, one could relate c with the in-control average run length defined as ARL =
E(τ | η = ∞), which corresponds to the expected time until the charts cross the threshold c when the
hazard never changes to the out-of-control state. Then, c is chosen such that the in-control ARL is
equal to a desired value. Another common strategy is to choose c such that the probability of a false
signal until a given time point tm follows a desired level α, i.e. phit = P(τ ≤tm | η = ∞) = α.
4
Gandy et al. (2010) explored the setting of choosing c with respect to the expected number of
events until hitting or the probability that the number of events at hitting time point does not exceed
a given amount when in-control, i.e. E(N(τ) | η = ∞) and P(N(τ) ≤Nmax | η = ∞). In the overall
hazard change studied in Gandy et al. (2010), c can be analytically computed using a method based
on a discrete time Markov chain with finite state space when the mentioned quantities are specified.
However, this methodology does not apply for the setting of monitoring the excess hazard as the jump
of the chart is no longer constant across observations. Nevertheless, it is possible to approximate the
current setting to a situation where the Markov chain method is applicable whenever the population
hazard is much smaller compared to the excess hazard. This was examined in the master thesis of Tran
(2022), where the thresholds obtained from the method yield approximately the desired performances
in circumstances where the excess hazard dominates the population hazard.
In the excess hazard scenarios studied in the present paper, it is seemingly not possible to compute
c analytically. In the remaining sections, the thresholds will thus be calculated via different simulation
approaches depending on the setting.
2.5
Simulating the signal threshold
In the following, an overview of the general approach to simulate the threshold c based on a predefined
phit is described.
Algorithm 1 Simulating c based on a chosen phit
1: Specify the start time of the monitoring process, specify tm, phit = α for some α ∈(0, 1) and
the number of simulations N. Assume that the arrivals of observations follow a homogeneous
Poisson process with intensity λa and the interim censoring time C∗
i an exponential distribution
with censoring rate λC∗.
2: for j = 1, ..., N do
3:
Simulate the number of observations n up to the time point tm with the assumed arrival process.
Obtain B1, · · · , Bn.
4:
for i = 1, ..., n do
5:
Simulate Xi and Zi.
6:
Simulate TEi based on hE,0(·, Xi) and TPi based on hP (·, Zi).
7:
Simulate interim censoring time C∗
i
∼Exp(λC∗).
Calculate final censoring time Ci =
min(C∗
i , tm −Bi).
8:
Calculate Ti = min(TEi, TPi, Ci).
9:
end for
10:
Run CUSUM chart and let Wj = max0≤t≤tmΨj(t), where Ψj(t) is the CUSUM chart run for
this specific iteration.
11: end for
12: Calculate c as the α · 100% upper quantile of the values W1, · · · , WN.
We will consider two different scenarios for simulating the covariates Xi and Zi. For the first
simulations performed in Section 3, we assume that the true covariate distribution is known and
therefore simulate Xi and Zi directly from these distributions. In real-life applications, this is usually
not the case. We then opt for a nonparametric bootstrap procedure that estimates the baseline/in-
control distribution in such scenarios. Also, the interim censoring rate is estimated by counting the
number of observations in the baseline period that are censored and the corresponding observed time
is less than the maximum follow-up time.
5
2.6
Monitoring and updating schemes
The proposed method is best suited for detecting changes in the setting that all observations in the
system will experience the out-of-control hazard whenever t > η, e.g. if a new regime for post-operative
treatment is introduced for all patients. However, in certain cases, only the individuals arriving after
the change point η will be affected by the change in the hazard, for instance if a new surgery procedure
is introduced. Our method should still be able to detect the change in this latter scenario, but there
will be more delay in detecting the change.
With the definition Ai(t) = min(Ti, Ci, max(t −Bi, 0)), we assume that information on the status
of a patient is available continuously, from arrival and onwards. This is realistic if cases are registered
to the database more or less in real time and information about censoring or events is swiftly added.
For instance, this will be the case when the method is used for retrospective analysis, as in our example
coming later. Another scenario could be that information about each case becomes available only after
censoring or an event. For this scenario, Gandy et al. (2010) suggested to redefine Ai(t) such that the
observation contributes to the likelihood ratio only after an event or censoring has occurred:
Ai(t) = min(Ti, Ci)I(min(Ti, Ci) + Bi ≤t).
It is clear that the chart will not have a continuous drift, but only jumps in value whenever an
observation has experienced an event or censoring. The same result can be applied here, but for the
purpose of the real data set considered in the later section, this version will not be considered in the
remainder of the text.
Another situation that could appear is when the information is only available at certain time
points. An example is when a patient is diagnosed in the middle of a year, but the observation is only
included in the database at the end of the year. A possible solution is to again redefine Ai(t) so that
the observation is included in the system periodically, e.g. at the start of every new year so that
Ai(t) = min(Ti, Ci, t −Bi)I(⌈Bi⌉≤t),
where ⌈·⌉is the ceiling function.
Similarly to the above, we can extend the first situation where the information of an observation is
only available periodically after the observation experiences an event or has been censored, e.g. when
a patient either dies or drops out of a study in the middle of a year and the database is updated with
the given observation at the start of the new year. Then, Ai(t) can be defined as follows:
Ai(t) = min(Ti, Ci)I(⌈min(Ti, Ci) + Bi⌉≤t).
Another scenario is when the entire database is updated in regular intervals, adding all information
accumulating since the last update. Then, an option would be to run the CUSUM retrospectively by
running the CUSUM for the last period once the data for that period become available. This would
of course lead to a certain delay in signalling versus when running the chart in real-time.
In some cases, it is of interest to just monitor for survival up to a certain time after diagnosis
tD, for instance tD = 5 years survival in cancer patients. A benefit with such monitoring is that
we only need baseline data over a time period long enough to be able to estimate hE,0(·) up to tD.
Furthermore, with this set-up, there is no restriction as to how long we can run the monitoring as we
will only need to evaluate hE,0(t) for t ≤tD. Thus, it is not required to have an estimate of hE,0(t)
for t ≥tD. Estimation of the in-control model is further discussed in the next subsection.
6
2.7
Estimating the in-control model
In practical applications, the true in-control excess hazard will be unknown and has to be estimated
from baseline data. To run the monitoring procedure, we need to be able to evaluate hE,0(t) and
HE,0(t) for any survival time t that could be observed during the monitoring. In practice, this means
that if nonparametric estimates of hE,0(t) and HE,0(t) are used, baseline data for at least as long period
as the maximum monitoring period would be required to estimate these functions, if the monitoring
is run without any upper limit on the time to event of interest. If a parametric estimate is used, one
could in principle use the estimated model beyond the event horizon observed in the baseline data.
When estimating the in-control excess hazard, the estimation error will propagate to the achieved
in-control performance. The impact of this, as well as the impact of model misspecifications, will be
illustrated in Section 3.5.
3
Simulation study
In this section, properties of the suggested procedures are studied by simulations.
3.1
Simulation set-up
For the following, we will use a proportional excess hazard model, i.e. hE,0(·, Xi) = h0(·) exp(βXi).
Inspired by data from the Norwegian Cancer Registry, Table 5 in Appendix A presents the covariates
that we will consider in the simulations, with corresponding parameter values.
The interim censoring time is set to be exponentially distributed with the rate parameter equal to
0.000275. The monitoring is run for 10 years (in one case 5 years) and is using the population hazard
of Norway from the beginning of 2010 to the end of 2019. The baseline excess hazard is chosen to be
a piecewise constant function of the form h0(t) = exp [P
k χkIk(t)], where Ik(t) = 1 whenever t lies in
the k-th band of the 10-year follow-up interval. Here, we partition the follow-up interval into yearly
bands during the first five years before defining one single band for t ∈[5, 10]. Inspired by the cancer
data, we will use χ = (−1.4, −1.6, −1.8, −2.0, −2.1, −3.0).
In most simulations, we assume that both the distributions of the covariates and the parameter
vectors β and χ are known. Thus, the charts are calculated using the true parameters, and hence the
true functional form of h0. However, the impact of estimation error will be studied in Section 3.5.
Arrival of patients is simulated as a homogeneous Poisson process with arrival rate λa. To determine
the threshold c, we use the false signal probability during the 10 years of monitoring as criteria. The
threshold is then computed by simulating 1000 or 10 000 CUSUMs from the in-control model, and we
tune the threshold to achieve the desired false alarm signal probability, which we specify to either 0.05
or 0.01.
After the signal threshold c is determined, the signal probabilities for various out-of-control situa-
tions are simulated by simulating 1000 or 10 000 CUSUMs.
We study three different scenarios for the shift from in-control to out-of-control: i) All patients are
in the out-of-control situation from the beginning (η = 0), ii) All patients change to the out-of-control
hazard at some time point η years (either η = 2.5 or η = 5), iii) Only patients arriving after some
time point η∗have the out-of-control hazard (either η∗= 2.5 or η∗= 5).
3.2
Proportional alternative
This subsection considers different scenarios with the proportional alternative represented by (1). The
charts are computed using the correct value of ρ that determines the various out-of-control situations.
7
The first simulation setup examines the scenario of four different values of ρ in a situation with a
moderate yearly arrival rate of λa = 250.
The results from the described simulation setup are given in Table 1. One can observe that in the
situation when the excess hazard is in the out-of-control state from the start, almost all simulations
yield a signal during the 10-year monitoring period for the largest shifts, i.e. ρ = 0.80 and ρ = 1.20.
When the change is less pronounced like ρ = 0.90 and ρ = 1.10, the signal ratio is reduced, and of
course, lower for the 0.01 false alarm probability versus 0.05. If the shift happens in the monitoring
period, it is natural that the signal ratio is further decreased as there is less time for the charts to
signal. Finally, the situation where only the individuals arriving after 5 years will experience the
out-of-control state yields the smallest signal ratio. As expected, the proposed CUSUM charts have
less power to detect these shifts.
Signal Ratio
Scenario
η = 0
η = 5
η∗= 5
ρ
P(τ ≤10 | η = ∞)
0.80
0.01 (c = 6.41)
95.34%
78.39%
42.97%
0.05 (c = 5.02)
98.50%
89.85%
64.17%
0.90
0.01 (c = 4.37)
43.57%
21.49%
9.21%
0.05 (c = 3.41)
65.28%
42.61%
24.44%
1.10
0.01 (c = 4.32)
36.09%
17.39%
8.02%
0.05 (c = 3.36)
59.78%
37.78%
21.40%
1.20
0.01 (c = 6.12)
91.20%
67.78%
32.45%
0.05 (c = 4.80)
96.75%
83.95%
54.09%
Table 1: The table reports proportions of cases when the CUSUM signals in a setting where individuals
arrive over calendar time according to a Poisson process with rate λa = 250, and the out-of-control
model is h1i(·) = hP (·, Zi) + ρhE,0(·, Xi) with the latter having a piecewise constant baseline excess
hazard. Different shifts in terms of ρ and different shift points η are considered. For the shift after
5 years of monitoring, both the setting that the shift affects all individuals (η = 5) and the setting
when the shift only affects new individuals (η∗= 5) are considered. 10 000 simulations are performed
for each combination of ρ and scenario of shift, and for finding thresholds c.
Next, a situation with a much larger arrival rate, similar to what is observed in the cancer registry
data used in Section 4, is considered by letting λa = 3700.
To balance out the increased power
due to the larger amount of arrived individuals, we just run the CUSUM for 5 years and examine
less prominent changes in the excess hazard by investigating values of ρ that differ less from unity
compared to the previous situation. From Table 2, the power of the monitoring system is relatively
high if the excess hazard changes by 10% for both scenarios of η. On the other hand, even with the
much larger amount of arrivals, just above half of the charts will signal when the false alarm signal
probability is 0.05 and the excess hazard changes by 5%. In order to detect shifts quickly in these
scenarios, more observations are needed to increase the power.
3.3
Additive alternative
In this subsection, we perform a similar experiment as in the preceding for the additive alternative
given by (2). By letting λa = 3700, we examine three values of γ: -0.002, 0.002 and 0.005. It turns
out that combining the general simulation setup described earlier with γ = −0.002 yields individual
8
Signal Ratio
Scenario
η = 0
η = 2.5
η∗= 2.5
ρ
P(τ ≤10 | η = ∞)
0.90
0.01 (c = 8.00)
97.3%
84.9%
26.0%
0.05 (c = 6.23)
99.2%
94.0%
46.0%
0.95
0.01 (c = 6.06)
37.2%
16.4%
3.0%
0.05 (c = 4.96)
54.8%
33.3%
8.5%
1.05
0.01 (c = 6.08)
33.3%
13.8%
3.3%
0.05 (c = 4.69)
57.6%
37.1%
10.1%
1.10
0.01 (c = 7.63)
96.0%
84.6%
22.2%
0.05 (c = 6.06)
98.5%
92.5%
42.0%
Table 2: The table reports proportions of cases when the CUSUM signals in a setting where individuals
arrive over calendar time according to a Poisson process with rate λa = 3700, and the out-of-control
model is h1i(·) = hP (·, Zi) + ρhE,0(·, Xi) with the latter having a piecewise constant baseline excess
hazard. The CUSUM is run for 5 years. Different shifts in terms of ρ and different shift points η
are considered.
For the shift after 2.5 years of monitoring, both the setting that the shift affects
all individuals (η = 2.5) and the setting when the shift only affects new individuals (η∗= 2.5) are
considered. 1000 simulations are performed for each combination of ρ and scenario of shift, and for
finding thresholds c.
Signal Ratio
Scenario
η = 0
η = 5
η∗= 5
γ
P(τ ≤10 | η = ∞)
-0.002
0.01 (c = 4.93)
61.8%
49.3%
5.0%
0.05 (c = 3.78)
79.3%
70.7%
17.6%
0.002
0.01 (c = 4.78)
60.6%
46.9%
5.6%
0.05 (c = 3.83)
76.6%
64.7%
13.5%
0.005
0.01 (c = 6.81)
100.0%
100.0%
22.5%
0.05 (c = 5.50)
100.0%
100.0%
40.0%
Table 3: The table reports proportions of cases when the CUSUM signals in a setting where individuals
arrive over calendar time according to a Poisson process with rate λa = 3700, and the out-of-control
model is h1i(·) = hP (·, Zi)+max(0, hE,0(·, Xi)+γ) with the latter having a piecewise constant baseline
excess hazard. Different shifts in terms of γ and different shift points η are considered. For the shift
after 5 years of monitoring, both the setting that the shift affects all individuals (η = 5) and the setting
when the shift only affects new individuals (η∗= 5) are considered. 1000 simulations are performed
for each combination of γ and scenario of shift, and for finding thresholds c.
9
out-of-control excess hazards that are non-negative so that (3) can also be used for faster calculations.
For moderate shifts like 0.002 in absolute value, we can see from Table 3 that the power of the charts
is moderate when using the threshold giving a 5% probability of false signal. On the other hand,
increasing the effect of change to 0.005 leads to signals for all simulated charts for the cases where all
individuals are affected by the shift. We also notice that the γ = −0.002 and γ = 0.002 cases achieve
roughly the same power. As before, the situation where only new arrivals are affected by the change
has the smallest signal ratios for all values of γ.
3.4
Linear accelerated time alternative
Signal Ratio
Scenario
η = 0
η = 5
η∗= 5
k
P(τ ≤10 | η = ∞)
0.90
0.01 (c = 7.83)
100.0%
100.0%
44.7%
0.05 (c = 6.50)
100.0%
100.0%
62.8%
0.95
0.01 (c = 7.29)
99.9%
99.7%
10.6%
0.05 (c = 5.89)
99.9%
100.0%
25.4%
1.05
0.01 (c = 7.18)
99.9%
99.6%
14.3%
0.05 (c = 5.97)
100.0%
99.9%
26.2%
1.10
0.01 (c = 8.49)
100.0%
100.0%
35.3%
0.05 (c = 6.47)
100.0%
100.0%
64.2%
Table 4: The table reports proportions of cases when the CUSUM signals in a setting where individuals
arrive over calendar time according to a Poisson process with rate λa = 3700, and the out-of-control
model is h1i(·) = hP (·, Zi) + khE,0(k·, Xi) with the latter having a piecewise constant baseline excess
hazard. Different shifts in terms of γ and different shift points η are considered. For the shift after
5 years of monitoring, both the setting that the shift affects all individuals (η = 5) and the setting
when the shift only affects new individuals (η∗= 5) are considered. 1000 simulations are performed
for each combination of γ and scenario of shift, and for finding thresholds c.
For the last setup in this section, we investigate the performance of the linear accelerated time
alternative (4) with four different values of k. When k > 1, we extend the upper limit of the last band
of h0(t) = exp [P
k χkIk(t)] covering the time interval between 5 and 10 years to 10k years. Table 4
shows that almost all charts signal for the situations when η = 0 and η = 5 across all four values of k.
For η∗= 5, we can still observe that the procedure struggles more to capture these changes.
3.5
Estimation error and model misspecification
In the preceding simulation studies, the true covariate parameters and functional form of the baseline
excess hazard are used when computing the CUSUM charts.
In practice, these are all unknown
quantities and need to be estimated as discussed in Section 2.7. To illustrate the effect of estimation
error in achieving e.g. the specified in-control false probability or average run length, we perform the
simulation setup presented in Algorithm 2.
Furthermore, we will also consider some model misspecification aspects. One would presume that
the observed in-control false signal probability will be further off from the nominal value if the specified
form of the baseline is not correct, e.g. when using a piecewise constant baseline excess hazard model
10
Algorithm 2 Simulation setup to illustrate effect of estimation error and model misspecification
1: We consider a specific underlying proportional excess hazard model hE,0(·, Xi) = h0(·) exp(βXi)
with h0 being a parametric baseline excess hazard function.
2: From the true model, we simulate a data set to be used for estimating the in-control model.
3: Based on the simulated data set, we estimate an in-control model (which could be an estimated
”true” model, e.g. a model with a piecewise constant baseline if the true excess hazard is on this
form, or an estimated ”wrong model”, e.g. with a smooth baseline if the true excess hazard is
piecewise constant.)
4: We treat the estimated model as a true model and simulate the threshold c to obtain a certain
rejection probability under this assumed true model by simulating observations from the in-control
model. Two possibilities regarding the covariate distribution will be considered here: In the first
approach, the true covariate distribution is assumed to be known, the second uses a nonparametric
bootstrap procedure on the simulated data set from step 2.
5: We use the estimated model from step 3 with the threshold from step 4 and run the CUSUM
many times on data simulated from the true model specified in step 1. The achieved rejection
probability is calculated using these runs.
6: Steps 2 to 5 are repeated many times in order to characterize the distribution of the in-control
false signal probability obtained under the estimated model.
when the true excess hazard is a continuous function. Consequently, we will consider two different
types of baseline excess hazard in this subsection: A piecewise constant baseline and a flexible smooth
nonparametric baseline hazard. Both methods can be found in the R package relsurv via the rsadd-
function, see e.g. Perme and Pavlic (2018). A note regarding the smoothing procedure to obtain
continuous nonparametric estimates of the baseline excess hazard is given in Appendix B.
In addition, we also investigate for each combination of estimation procedure and true excess hazard
if the results obtained from bootstrapping the covariate distributions from the data set obtained in
step 2 of Algorithm 2 differ from simulating directly from the true covariate distributions. We will only
perform the simulation using the proportional alternative, but the idea is the same for the remaining
out-of-control situations.
3.5.1
Piecewise constant baseline
In this part of the simulation, we consider the same piecewise constant baseline excess hazard setup
presented previously in this section. The arrival rate is set to λa = 3750, the proportionality constant
relating the out-of-control and in-control scenario is ρ = 0.90. We let the desired in-control false
probability during the first five years of monitoring be equal to P(τ ≤5 | η = ∞) = 0.05. Steps 2
to 5 are performed 1000 times to obtain the distribution of in-control false probability under a given
combination of estimation method and covariate simulation.
For each iteration, 1000 simulations
are also done in order to obtain the threshold value c. Figure 1 shows the results when the model
estimation is performed using the piecewise baseline excess hazard model. This resembles the situation
where the estimated baseline excess hazard is correctly specified. We can observe that both the median
and mean in-control signal ratio, regardless if the covariate distributions are simulated using bootstrap
or the true distributions, only differ slightly from the desired nominal value of 5% due to the effect of
the estimation error of β and χ.
On the other hand, when examining the results of the same procedure using the smooth baseline
excess hazard estimate, we see that the median and mean in-control signal ratio, independent of the
scenario considered in step 4, are further away from the nominal value. Not only do we have the
11
0
5
10
15
0.0
0.1
0.2
0.3
0.4
Signal ratio
Density
(a) True covariate distribution used in step 4. Mean
signal ratio is 5.73%, median signal ratio is 4.30%.
0
5
10
15
0.0
0.1
0.2
0.3
0.4
Signal ratio
Density
(b) Bootstrapping used to estimate the covariate dis-
tribution. Mean signal ratio is 5.65%, median signal
ratio is 4.30%.
Figure 1: Histograms of the achieved rejection probabilities under in-control scenario with the true
baseline excess hazard following a piecewise constant function. The model estimation is done using
the model with piecewise constant baseline in step 3. Here, the blue vertical line represents the median
and the red line corresponds to the mean signal ratio.
estimation error in the covariate parameters that affects the results, the incorrect specification of the
form of the baseline excess hazard when using the smooth baseline estimate leads to larger deviation
from the desired false signal ratio. Therefore, if the true baseline excess hazard is indeed a piecewise
constant function, approximating it with a continuous and smooth function might cause undesired
behaviour in the monitoring procedure.
3.5.2
Weibull baseline
In theory, the previous true form of baseline excess hazard favors the model with piecewise constant
baseline as it is difficult for the estimated smoothed nonparametric baseline excess hazard to capture
the stepwise behaviour. We now illustrate the opposite scenario by considering a Weibull baseline as
the true form, where
h0(t) = abta−1.
Inspired by the data from the Norwegian Cancer Registry, we let a = 0.65 and b = 0.25. Otherwise, the
other quantities remain the same as in the piecewise constant baseline scenario. Results are reported
in Figure 3.
Compared to the situation with a piecewise constant baseline as the true form, the
differences between the 5% nominal value and the mean and median signal ratio are now increased as
expected due to the misspecification of the baseline excess hazard in the model estimation.
Figure 4 reports the results obtained when using the smooth nonparametric baseline excess hazard
estimate. The obtained in-control signal ratio is slightly closer towards the desired value of 5%, but
the difference in absolute value is still larger compared to the piecewise constant model from above.
We suspect that this is a consequence of the choice of the smoothing parameters in the experiment.
This example shows a potential issue with the nonparametric baseline excess hazard estimate (Perme
et al., 2009) - the smoothing parameters will have a large impact on how the charts will perform
compared to the required in-control performance. An ideal situation is to obtain the full continuous
12
0
5
10
0.0
0.2
0.4
Signal ratio
Density
(a) True covariate distribution used in step 4. Mean
signal ratio is 10.23%, median signal ratio is 8.20%.
0.0
2.5
5.0
7.5
10.0
12.5
0.0
0.2
0.4
Signal ratio
Density
(b) Bootstrapping used to estimate the covariate dis-
tribution. Mean signal ratio is 10.10%, median signal
ratio is 8.10%.
Figure 2: Histograms of the achieved rejection probabilities under in-control scenario with the true
baseline excess hazard following a piecewise constant function. The model estimation is done using
a smooth nonparametric baseline excess hazard in step 3. Here, the blue vertical line represents the
median and the red line corresponds to the mean signal ratio.
0
10
20
0.0
0.1
0.2
0.3
Signal ratio
Density
(a) True covariate distribution used in step 4. Mean
signal ratio is 4.14%, median signal ratio is 3.00%.
0
10
20
0.0
0.1
0.2
0.3
Signal ratio
Density
(b) Bootstrapping used to estimate the covariate dis-
tribution. Mean signal ratio is 4.09%, median signal
ratio is 2.90%.
Figure 3: Histograms of the achieved rejection probabilities under in-control scenario with the true
baseline excess hazard following the hazard function of the Weibull distribution. The model estimation
is done using the model with piecewise constant baseline in step 3. Here, the blue vertical line represents
the median and the red line corresponds to the mean signal ratio.
13
0
3
6
9
12
0.0
0.1
0.2
0.3
0.4
0.5
Signal ratio
Density
(a) True covariate distribution used in step 4. Mean
signal ratio is 8.55%, median signal ratio is 6.80%.
0.0
2.5
5.0
7.5
10.0
12.5
0.0
0.1
0.2
0.3
0.4
0.5
Signal ratio
Density
(b) Bootstrapping used to estimate the covariate dis-
tribution. Mean signal ratio is 8.24%, median signal
ratio is 6.50%.
Figure 4: Histograms of the achieved rejection probabilities under in-control scenario with the true
baseline excess hazard following the hazard function of the Weibull distribution. The model estimation
is done using a smooth nonparametric baseline excess hazard. Here, the blue vertical line represents
the median and the red line corresponds to the mean signal ratio.
estimate of h0 at the last E-step of the EM-algorithm when fitting the Cox-type excess hazard model
to avoid another smoothing operation on top of the already smoothed estimates. However, this is not
possible, and one could argue that this combination might give rise to oversmoothing. Nonetheless,
this simulation setup illustrates the challenges of a nonparametric baseline in the model estimation
when using the proposed monitoring scheme.
4
Application to colorectal cancer data monitoring
In this section, we apply the proposed methods to a subset of a data set of 171 087 patients diagnosed
with colorectal cancer in Norway between the start of 1953 and the end of 2020. We will here only
consider data from 1970 and onwards. We will also focus on patients that have received major surgery,
have a cancer stage grading of 1-3 and with specified and known position of the tumor, i.e. patients
treated with a curative intention. With reference to Table 5, this means that the relevant subset is the
patients in surgery group 0, with SEER stadium equal to localised or regional and with ICD indicator
equal to 0, 1 or 2. Furthermore, a recent focus in studies of colorectal cancer has been regarding
early-onset colorectal cancer, the occurrence of the disease among younger patients (see e.g REACCT
Collaborative, Zaborowski, Abdile, Adamina, Aigner et al. (2021) for an overview). Inspired by this
fact, we divide the data into patients up to 50 years and patients above 50 years of age. This leaves
us with 5321 patients in the younger age group and 86 360 patients in the older age group.
In this example, we explore monitoring in bands of 10 years, i.e. we choose a baseline period of
10 years and use this period to monitor for the next 10 years. More specifically, we define the first
baseline period to be between 1970-1980. The patient cohort diagnosed in this period is used to fit
an excess hazard model that will represent hE,0(·, Xi) in the CUSUM chart. The covariate vector Xi
here contains gender, ICD indicator, morphology type and SEER stadium.
We will here only consider a piecewise constant baseline excess hazard model. With these results
as the in-control period, we run the CUSUM charts for the next 10 years, i.e. the time period between
14
1980-1990. Subsequently, this period is then used as the baseline period for the new monitoring period
of 1990-2000, and this is done until we monitor the period between 2010 and 2020.
4.1
Proportional alternative
First, we consider the proportional alternative. For each combination of age group and monitoring
period, the chart is computed using four different values of ρ: 0.80, 0.90, 0.95 and 1.05, i.e. three
scenarios corresponding to decreased and one to increased excess hazard. Figure 5 shows the calculated
charts for different combinations of monitoring time period and ρ in the two age groups. The dashed
lines are the thresholds obtained by simulations using the bootstrap procedure mentioned in Section
3.5. It is clear from both plots that the charts fluctuate and do not signal for any combinations of age
group and time period when considering the increased burden of disease scenario of ρ = 1.05.
All charts signal for all the ρ < 1 scenarios, indicating that improvement versus the previous decade
is observed for all the periods considered. For the elders, a striking observation is that in the 1990s,
no improvement was seen during the first five years. An explanation for this could be that possible
advancements in diagnosis and treatment of colon cancer during the 1980s were not reflected in changes
in outcomes during the first half of the 1990s. On the other hand, the chart signals fastest in the period
2010-2020. This could indicate early advances in this period, but the faster signal could also possibly
be partly explained by more patients arriving during this time interval. For the younger, there seems
to be an indication of a better treatment advance in the period 2000-2010 versus the previous decade
than in the other periods. The generally later signals for the younger could be explained by much
fewer patients in this group.
4.2
Additive alternative
Next, we investigate the setting of an additive alternative. As in the previous setting, we also here
use three shift parameters that correspond to decreasing and one corresponding to increasing excess
hazard, i.e. the values of γ considered are -0.020, -0.010, -0.005 and 0.005. The results are depicted in
Figure 6. We observe that none of the charts yield a signal for the γ = 0.005 case that implies a worse
outcome over time in this additive setting, although for the younger patients, the charts are actually
close to a signal in the periods 1980-1990 and 1990-2000.
For the remaining values of γ, all the charts related to the old age group signal with an increasing
trend. The behaviors of the charts across all the values of γ are very similar to the proportional
alternative, with the latest two monitoring periods appearing to have the largest improvement.
Looking at the charts obtained from the younger age group, no signals are now obtained in the
first two monitoring periods for γ < 1 unlike the conclusions from the proportional alternative. The
values of the charts for the periods 2000-2010 and 2010-2020 when γ = −0.010 and γ = −0.005 are
also much closer. In addition, for the largest shift γ = −0.020, the chart for the period 2010-2020
starts decreasing again after signalling at around 5 years. Therefore, the evidence of a larger reduction
of −0.020 in the excess hazard is much weaker for this period compared to the evidence for smaller
changes like −0.010 and −0.005. This shows the effect of setting a value of the change parameter to
a larger change than the observed.
4.3
Linear accelerated time alternative
Finally, we try a linear accelerated time alternative with the following four values of the acceleration
parameter k: 0.80, 0.90, 0.95 and 1.05.
Therefore, this corresponds again to three improvement
alternatives and one worsening situation. For the older age group, we see a similar behaviour as in the
previous models, although with a later signal in the case with the smallest improvement of k = 0.95.
15
ρ = 0.80
ρ = 0.90
ρ = 0.95
ρ = 1.05
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0 0.0
2.5
5.0
7.5
10.0
0
1
2
3
0
20
40
60
80
0
50
100
150
0
100
200
300
t (in years)
Ψ(t)
Monitor period
Monitor period: 1980−1990
Monitor period: 1990−2000
Monitor period: 2000−2010
Monitor period: 2010−2020
(a) Age > 50
ρ = 0.80
ρ = 0.90
ρ = 0.95
ρ = 1.05
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0 0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0
0.0
0.5
1.0
1.5
0
2
4
6
8
0
5
10
15
0
10
20
30
t (in years)
Ψ(t)
Monitor period
Monitor period: 1980−1990
Monitor period: 1990−2000
Monitor period: 2000−2010
Monitor period: 2010−2020
(b) Age ≤50
Figure 5: CUSUM charts using the proportional alternative for each age group monitoring 10-year
survival in four different time periods with the estimated output from the piecewise constant baseline
excess hazard model fitted on the preceding 10-year time period as the in-control. Four different values
of ρ are explored here.
16
γ = −0.020
γ = −0.010
γ = −0.005
γ = 0.005
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0 0.0
2.5
5.0
7.5
10.0
0
1
2
3
4
5
0
25
50
75
100
125
0
50
100
150
200
0
100
200
300
t (in years)
Ψ(t)
Monitor period
Monitor period: 1980−1990
Monitor period: 1990−2000
Monitor period: 2000−2010
Monitor period: 2010−2020
(a) Age > 50
γ = −0.020
γ = −0.010
γ = −0.005
γ = 0.005
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0 0.0
2.5
5.0
7.5
10.0
0
1
2
3
0.0
2.5
5.0
7.5
10.0
12.5
0
5
10
15
20
0
5
10
15
20
t (in years)
Ψ(t)
Monitor period
Monitor period: 1980−1990
Monitor period: 1990−2000
Monitor period: 2000−2010
Monitor period: 2010−2020
(b) Age ≤50
Figure 6: CUSUM charts using the additive alternative for each age group monitoring 10-year survival
in four different time periods with the estimated output from the piecewise constant baseline excess
hazard model fitted on the preceding 10-year time period as the in-control. Four different values of γ
are explored here.
17
k = 0.80
k = 0.90
k = 0.95
k = 1.05
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0 0.0
2.5
5.0
7.5
10.0
0
1
2
3
4
5
0
20
40
0
30
60
90
120
0
50
100
150
200
t (in years)
Ψ(t)
Monitor period
Monitor period: 1980−1990
Monitor period: 1990−2000
Monitor period: 2000−2010
Monitor period: 2010−2020
(a) Age > 50
k = 0.80
k = 0.90
k = 0.95
k = 1.05
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0
0.0
2.5
5.0
7.5
10.0 0.0
2.5
5.0
7.5
10.0
0
1
2
3
0.0
2.5
5.0
7.5
0
5
10
15
0
10
20
t (in years)
Ψ(t)
Monitor period
Monitor period: 1980−1990
Monitor period: 1990−2000
Monitor period: 2000−2010
Monitor period: 2010−2020
(b) Age ≤50
Figure 7: CUSUM charts using the linear accelerated time alternative for each age group monitoring
10-year survival in four different time periods with the estimated output from the piecewise constant
baseline excess hazard model fitted on the preceding 10-year time period as the in-control.
Four
different values of k are explored here.
18
The most interesting observation from the results regarding the younger age group is the charts
corresponding to the periods 1990-2000 and 2010-2020. With the additive alternative, the charts mon-
itoring towards improvement for 1990-2000 did not signal at all. However, with the linear accelerated
time alternative, all charts for this period signal for all values of k < 1, as was also the case for the
corresponding charts with the proportional model. Further, the charts for 2010-2020 have a much later
response than what was observed for the additive and proportional alternative. Also, when k = 0.95,
the chart does not even signal. This is another example of how different alternatives can potentially
lead to different conclusions.
5
Summary
We have presented a CUSUM-based method for monitoring changes in excess hazard for situations
where the population hazard is known. This complements the literature on monitoring based on time
to event models. The proposed method can be used either for real-time monitoring or for retrospective
analyses. The method can be adapted to various data updating schemes.
We have considered proportional, additive and linear accelerated time change models and studied
properties by simulations and in an application to cancer registry data. In particular, we have also
considered the impact of estimation error and some forms of model misspecifications. Simulations
indicate that with a decent amount of baseline data, the impact of estimation error is not critical.
However, model misspecifications might be a more severe issue. Applications to data illustrate that
careful considerations need to be made when specifying the type of changes in the excess hazard to
monitor against.
Disclaimer
The study has used data from the Cancer Registry of Norway. The interpretation and reporting of
these data are the sole responsibility of the authors, and no endorsement by the Cancer Registry of
Norway is intended nor should be inferred.
19
A
Summary table of the covariates in the simulation setup of Sec-
tion 3
Table 5: Overview of the covariates used in the simulations, with the corresponding parameter value
used and proportion of observations simulated to be in each category for the categorical covariates.
Age is simulated from a normal distribution with mean 75 and standard deviation 10, truncated at 50
and 105. See Tran (2022) for details about the medical interpretations of the covariates.
Variable
β
Proportion
Gender = Male
·
50.00%
Gender = Female
0.005
50.00%
ICD indicator = 0
·
27.00%
ICD indicator = 1
0.500
44.00%
ICD indicator = 2
0.200
28.00%
ICD indicator = 3
0.300
1.00%
Morphology type = Adenocarcinoma
·
90.00%
Morphology type = Mucinous carcinoma
−0.050
10.00%
SEER stadium = Distant
·
20.00%
SEER stadium = Localised
−3.000
20.00%
SEER stadium = Regional
−1.750
55.00%
SEER stadium = Unknown
−1.000
5.00%
Surgery group = 0
·
82.75%
Surgery group = 1
1.500
17.15%
Surgery group = 2
2.500
0.10%
B
Smoothing procedure for the semiparametric excess hazard model
in the CUSUM method
An important note regarding the semiparametric excess hazard model is that the user can specify a
smoothing parameter bwin. Originally, the estimation of the model is done using the EM-algorithm
by Dempster, Laird and Rubin (1977). Perme et al. (2009) proposed to apply kernel smoothing to
the nonparametric baseline excess hazard estimates at each E-step of the algorithm. The argument
bwin therefore controls the bandwidth in the smoothing process, with larger values implying larger
bandwidth, and therefore smoother estimates. In the end, only the estimates of h0 at the times to
event are returned. However, in order to apply the proposed CUSUM procedures, h0(t) is required
at all possible values of t ∈[0, 10] for continuous monitoring if the main interest is the 10-year excess
survival. To get a continuous estimate of h0(t), we must again apply some sort of manual smoothing
on top of the returned estimates at the times to event. For this purpose, smoothing splines (see e.g.
James, Witten, Hastie and Tibshirani (2013)) are used with the baseline excess hazard estimates as
the response variable and the times to event as the sole predictor. Therefore, there are two extra
sources that can impact the upcoming results: The smoothing parameter bwin at each E-step of the
EM-algorithm when fitting the model and the effective degrees of freedom from the smoothing splines
procedure. To get somewhat sensible results, we have decided to set bwin and the effective degrees of
20
freedom to 25. It is therefore important to note that a different combination of these two parameters
will lead to a different result than what we have obtained in Section 3.5.
References
Aalen, O. O., Borgan, Ø. and Gjessing, H. K. (2008). Survival and Event History Analysis: A Process
Point of View., New York: Springer.
Begun, A., Kulinskaya, E. and MacGregor, A. J. (2019). Risk-adjusted CUSUM control charts for
shared frailty survival models with application to hip replacement outcomes: a study using the
NJR dataset, BMC Medical Research Methodology 19.
Biswas, P. and Kalbfleisch, J. D. (2008). A risk-adjusted CUSUM in continuous time based on the
Cox model, Statistics in Medicine 27(17): 3382–3406.
Chen, V. W., Chidi, A. P., Dong, Y., Richardson, P. A., Axelrod, D. A., Petersen, L. A. and Massarweh,
N. N. (2023).
Risk-Adjusted Cumulative Sum for Early Detection of Hospitals With Excess
Perioperative Mortality, JAMA Surgery 158(11): 1176–1183.
Dempster, A. P., Laird, N. M. and Rubin, D. B. (1977). Maximum likelihood from incomplete data via
the EM algorithm, Journal of the Royal Statistical Society: Series B (Methodological) 39(1): 1–22.
Dickman, P. W., Sloggett, A., Hills, M. and Hakulinen, T. (2004). Regression models for relative
survival, Statistics in Medicine 23(1): 51–64.
Gandy, A., Kvaløy, J., Bottle, A. and Zhou, F. (2010). Risk-adjusted monitoring of time to event,
Biometrika 97(2): 375–388.
James, G., Witten, D., Hastie, T. and Tibshirani, R. (2013). An Introduction to Statistical Learning,
Vol. 112, Springer.
Keshavarz, M., Asadzadeh, S. and Niaki, S. T. A. (2021). Risk-adjusted frailty-based CUSUM con-
trol chart for phase I monitoring of patients’ lifetime, Journal of Statistical Computation and
Simulation 91(2): 334–352.
Kuang, Y., Das, D., Sir, M. and Pasupathy, K. (2023). Likelihood ratio-based CUSUM charts for real-
time monitoring the quality of service in a network of queues, IISE Transactions on Healthcare
Systems Engineering 13(4): 344–354.
Lai, X., Li, X., Liu, L., Tsung, F., Lai, P. B., Wang, J., Zhang, X., Zhu, X. and Liu, J. (2021). A
risk-adjusted approach to monitoring surgery for survival outcomes based on a weighted score
test, Computers & Industrial Engineering 160: 107568.
Liu, R., Lai, X., Wang, J., Zhu, X. and Liu, Y. (2023). SCR-CUSUM: An illness-death semi-Markov
model-based risk-adjusted CUSUM for semi-competing risk data monitoring, Computers & In-
dustrial Engineering 184: 109530.
Massarweh, N. N., Chen, V. W., Rosen, T., Dong, Y., Richardson, P. A., Axelrod, D. A., Harris, A. H.,
Wilson, M. A. and Petersen, L. A. (2021). Comparative Effectiveness of Risk-adjusted Cumulative
Sum and Periodic Evaluation for Monitoring Hospital Perioperative Mortality, Medical Care
59(7): 639–645.
21
Moustakides, G. (1986). Optimal stopping times for detecting changes in distributions, Annals of
Statistics 14(4): 1379–1387.
Oliveira, J. W., Valen¸ca, D. M., Medeiros, P. G. and Mar¸cula, M. (2016). Risk-adjusted monitoring
of time to event in the presence of long-term survivors, Biometrical Journal 58(6): 1485–1505.
Perme, M. P., Henderson, R. and Stare, J. (2009). An approach to estimation in relative survival
regression, Biostatistics 10(1): 136–146.
Perme, M. P. and Pavlic, K. (2018). Nonparametric Relative Survival Analysis with the R package
relsurv, Journal of Statistical Software 87: 1–27.
REACCT Collaborative, Zaborowski, A. M., Abdile, A., Adamina, M., Aigner, F. et al. (2021).
Characteristics of Early-Onset vs Late-Onset Colorectal Cancer:
A Review, JAMA surgery
156(9): 865–874.
Sego, L. H., Reynolds Jr, M. R. and Woodall, W. H. (2009). Risk-adjusted monitoring of survival
times, Statistics in Medicine 28(9): 1386–1401.
Steiner, S. H. and Jones, M. (2010). Risk-adjusted survival time monitoring with an updating expo-
nentially weighted moving average (EWMA) control chart, Statistics in Medicine 29(4): 444–454.
Tran, J. H. (2022). Relative Survival Methods. Theory, Applications and Extensions to Monitoring,
Master’s thesis, Department of Mathematics and Physics, University of Stavanger.
Zhang, M., Xu, Y., He, Z. and Hou, X. (2016). The Effect of Estimation Error on Risk-Adjusted
Survival Time CUSUM Chart Performance, Quality and Reliability Engineering International
32(4): 1445–1452.
22
