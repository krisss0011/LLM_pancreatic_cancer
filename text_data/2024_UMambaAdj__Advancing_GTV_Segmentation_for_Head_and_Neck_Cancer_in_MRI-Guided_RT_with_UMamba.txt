UMambaAdj: Advancing GTV Segmentation for
Head and Neck Cancer in MRI-Guided RT with
UMamba and nnU-Net ResEnc Planner
Jintao Ren ‚ãÜ1,2[0000‚àí0002‚àí1558‚àí7196], Kim Hochreuter1,2[0009‚àí0000‚àí7347‚àí1688],
Jesper Folsted Kallehauge1,2[0000‚àí0003‚àí3705‚àí5390], and Stine Sofia
Korreman1,2,3[0000‚àí0002‚àí3523‚àí382X]
1 Aarhus University, Department of Clinical Medicine, Nordre Palle Juul-Jensens
Blvd. 11, 8200 Aarhus, Denmark
2 Aarhus University Hospital, Danish Centre for Particle Therapy, Palle Juul-Jensens
Blvd. 25, 8200 Aarhus, Denmark
3 Aarhus University, Department of Oncology, Palle Juul-Jensens Blvd. 35, 8200
Aarhus, Denmark
jintaoren@clin.au.dk
Abstract. Magnetic Resonance Imaging (MRI) plays a crucial role in
MRI-guided adaptive radiotherapy for head and neck cancer (HNC)
due to its superior soft-tissue contrast. However, accurately segmenting
the gross tumor volume (GTV), which includes both the primary tu-
mor (GTVp) and lymph nodes (GTVn), remains challenging. Recently,
two deep learning segmentation innovations have shown great promise:
UMamba, which effectively captures long-range dependencies, and the
nnU-Net Residual Encoder (ResEnc), which enhances feature extrac-
tion through multistage residual blocks. In this study, we integrate these
strengths into a novel approach, termed ‚ÄôUMambaAdj‚Äô. Our proposed
method was evaluated on the HNTS-MRG 2024 challenge test set using
pre-RT T2-weighted MRI images, achieving an aggregated Dice Sim-
ilarity Coefficient (DSCagg) of 0.751 for GTVp and 0.842 for GTVn,
with a mean DSCaggof 0.796. This approach demonstrates potential for
more precise tumor delineation in MRI-guided adaptive radiotherapy, ul-
timately improving treatment outcomes for HNC patients. Team: DCPT-
Stine‚Äôs group.
Keywords: Deep learning ¬∑ Mamba ¬∑ Tumor segmentation ¬∑ Head and
Neck Cancer ¬∑ MRI
1
Introduction
Magnetic Resonance Imaging (MRI) plays a pivotal role in radiotherapy (RT),
particularly in MRI-guided adaptive radiotherapy, due to its superior soft-tissue
contrast compared to other imaging modalities like computed tomography (CT).
‚ãÜCorresponding author: jintaoren@clin.au.dk
arXiv:2410.12940v1  [cs.CV]  16 Oct 2024
2
J. Ren et al.
This soft-tissue contrast enables more accurate delineation, which is especially
crucial in head and neck cancer (HNC), where the intricate anatomy and proxim-
ity of vital structures, such as the salivary glands, optic nerves, and spinal cord
[1,4], make precise tumor targeting critical. MRI‚Äôs ability to differentiate be-
tween tumor tissues and surrounding normal tissues enhances radiation delivery
accuracy, reducing the risk of collateral damage to critical structures [3,24,23].
Despite these advantages, accurate delineation of HNC tumors, including
both the primary tumor volume (GTVp) and involved nodal metastasis (GTVn),
remains challenging. The heterogeneous and diffuse nature of HNC tumors often
makes obtaining clear margins difficult [28,17]. Additionally, MRI‚Äôs lower spa-
tial resolution in the third dimension (through-slice direction) compared to the
in-plane resolution can complicate tumor delineation, leading to variability in in-
terpretation among clinicians and resulting in significant inter-observer variation
(IOV).
Given these challenges, there is a growing need for automated, accurate seg-
mentation methods to enhance the consistency and precision of tumor delin-
eation in MRI-guided adaptive radiotherapy. Deep learning-based medical image
segmentation has emerged as a promising solution, often building on the classic
U-Net architecture, known for its symmetrical encoder-decoder design and skip
connections [27]. It plays a crucial role in medical image analysis by identifying
and delineating structures such as organs, lesions, tumors, and tissues across
various 2D and 3D imaging modalities, including CT, PET, and MRI, thereby
aiding in diagnosis, treatment planning, and prognoses.
Recently, the leading deep learning models for segmentation have shifted
between convolutional neural networks (CNNs) and transformer-based architec-
tures. CNNs excel at capturing translational invariances and local features but of-
ten face challenges with long-range dependencies [21]. In contrast, Vision Trans-
formers (ViTs) [8] effectively capture global context by treating the image as a
sequence of patches. However, their self-attention mechanism incurs a quadratic
computational cost relative to the number of patches [9], and Transformers tend
to be prone to overfitting, especially when working with limited datasets [19,13].
Leveraging the complementary strengths of both architectures, many studies
have explored hybrid models that integrate ViTs with CNNs, resulting in ar-
chitectures such as nnFormer [35], TransUNet [5], UNETR [12], SwinUNETR
[11], and UNETR++ [29]. These hybrid models have also gained popularity in
HNC GTV segmentation. For instance, Hung Chu et al. [6] demonstrated that
the SwinUNETR achieved an average Dice similarity coefficient (DSC) of 0.626
on CT/PET data, while a cross-modal Swin transformer achieved a mean DSC
of 0.769 for GTVp using CT/PET modalities [18]. Despite these advancements,
the U-Net architecture continues to be a foundational design in all segmentation
models.
The field is now advancing with structured state-space models (SSMs), such
as Mamba [10,9,7], which offer improved segmentation performance by efficiently
modeling long-range dependencies and scaling effectively with sequence length
[34,13]. Mamba‚Äôs ability to capture complex anatomical relationships makes it
UMambaAdj for Head and Neck Cancer Tumor Segmentation
3
well-suited for segmenting GTVp and GTVn in HNC, as their locations are often
closely correlated with each other. However, the optimal model configuration de-
pends on task-specific factors such as the foreground-to-background ratio, image
resolution, and tumor size variability, as each architecture‚Äôs effectiveness varies
with different imaging challenges and anatomical complexities.
Recently, two innovative approaches, UMamba [22] and the new nnU-Net
Residual Encoder (ResEnc) planner [15], have gained significant attention in
medical image segmentation. The default UMamba encoder incorporates a Mamba
layer after each CNN block, which can be computationally expensive, especially
at the first level where image features have a large resolution. This design, which
includes both a residual encoder and decoder, can be cumbersome to train and
provides only limited accuracy improvements in its default configuration [15].
In contrast, the nnU-Net ResEnc enhances feature extraction through multiple
blocks of residual CNN encoding and employs only a single CNN layer in the
decoder, offering a more efficient solution.
This study focuses on addressing the first task of the HNTS-MRG 2024 chal-
lenge, which aims to segment both GTVp and GTVn using pre-RT T2-weighted
MRI images. We aim to improve gross tumor volume (GTV) segmentation in T2-
weighted MRI for head and neck cancer by integrating the strengths of UMamba
and nnU-Net ResEnc. We refer to this integrated approach as UMambaAdj in
this study.
Our contributions are as follows:
‚Äì We optimize UMamba by removing the Mamba layer in the first stage and
the residual blocks in the decoder, significantly enhancing computational
efficiency while preserving its ability to capture long-range dependencies in
deeper stages.
‚Äì We combine UMamba‚Äôs long-range dependency modeling with nnU-Net Res-
Enc‚Äôs enhanced residual encoding to improve the accuracy of GTV delin-
eation in the complex anatomy of head and neck cancer.
2
Material and Methods
2.1
Data
The dataset used in this study was provided by the organizers of the HNTS-
MRG 2024 challenge task 1, consisting of 150 HNC patients, primarily with
oropharyngeal cancer (OPC). Each patient had T2-weighted MRI sequences of
the head and neck region, acquired at University of Texas MD Anderson Can-
cer Center [30]. The images included pre-RT scans taken 1-3 weeks before the
start of radiotherapy. For all cases, GTV for the primary tumor (GTVp) and
involved lymph nodes (GTVn) were independently segmented by 3 to 4 expert
physician observers based on the MRI images. The ground truth segmentation
was then generated using the Simultaneous Truth And Performance Level Esti-
mation (STAPLE) algorithm.
4
J. Ren et al.
2.2
Network Architecture
The proposed network architecture is based on a combination of a 3D ResEnc
U-Net and Mamba blocks. The CNN part of network architecture was designed
according to the new nnU-Net Residual encoder planner (M). The U-Net consists
of 6 stages, each with varying features per stage (32, 64, 128, 256, 320, 320). The
network uses 3D convolutional layers with kernel sizes mostly set to (3, 3, 3),
except for the first stage where it is (1, 3, 3). The strides vary across stages to
enable down-sampling at different levels, with a stride set of (1, 2, 2) between the
first and second stages, and (2, 2, 2) for the remaining stages. Each stage contains
a different number of residual CNN blocks with counts of (1, 3, 4, 6, 6, 6) in the
encoder, and a Mamba layer is appended after each residual CNN block except
the first stage. A skip connection with concatenation was connecting the Mamba
layer and the decoder blocks, while each decoder block consists of only one 3D
CNN block. Instance normalization and the Leaky ReLU activation function
are used. Additionally, deep supervision is applied at the top four levels of the
network outputs. The overall structure of the network can be seen in Figure 1a.
Residual 
CNN √ó 1
Residual 
CNN √ó 3
Residual 
CNN √ó 4
Residual 
CNN √ó 6
Residual 
CNN √ó 6
Residual 
CNN √ó 6
Mamba
Mamba
Mamba
Mamba
Mamba
CNN 
block
CNN 
block
CNN 
block
CNN 
block
CNN 
block
H√óW√óD√ó32
ùêª
2 √ó
ùëä
2 √ó
ùê∑
2 √ó64
ùêª
4 √ó
ùëä
4 √ó
ùê∑
4 √ó128
ùêª
8 √ó
ùëä
8 √ó
ùê∑
8 √ó256
ùêª
8 √ó
ùëä
8 √ó
ùê∑
8 √ó320
ùêª
16 √ó
ùëä
16 √ó
ùê∑
16 √ó320
Deep Supervision
Input
Output
ùë†ùë°ùëéùëîùëí 2
ùë†ùë°ùëéùëîùëí 1
ùë†ùë°ùëéùëîùëí 3
ùë†ùë°ùëéùëîùëí 4
ùë†ùë°ùëéùëîùëí 5
ùë†ùë°ùëéùëîùëí 6
Mamba layer
Flatten
(B, C, H, W, D)
Layer 
Norm
(B, L, C)
Linear
Linear
SSM
Linear
Reshape
(B, C, H, W, D)
SiLU
SiLU
a
b
Strided Convolution
Convolution Transposed
Mamba
Mamba Layer
Skip Connection
(B, L, C)
1D CNN
Fig. 1. (a) Overview of the proposed UMamba adjustment (UMambaAdj) network
architecture. (b) Details of the Mamba layer.
2.3
Mamba layer
The Mamba layer, adapted from the UMamba design for capturing long-range
dependencies, processes input image feature maps of shape (B, C, H, W, D),
where B is the batch size, C the channel, and H, W, D the spatial dimensions.
These feature maps are first reshaped and transposed into a flattened representa-
tion (B, L, C), where L = H √óW √óD, treating all spatial locations as individual
UMambaAdj for Head and Neck Cancer Tumor Segmentation
5
patch tokens. The reshaped features are then normalized using Layer Normal-
ization. Following normalization, the features undergo two parallel branches. In
the first branch, the features are expanded to (B, 2L, C) through a linear trans-
formation, followed by a 1D convolutional layer and a SiLU activation function,
ultimately passing through the SSM layer. In the second branch, a similar ex-
pansion process is performed via a linear transformation and a SiLU activation
function, but without the convolutional or SSM steps. The outputs from the two
branches are then combined using element-wise (Hadamard) multiplication. The
resulting features are projected back to the original token dimension, reshaped,
and transposed to restore the original input shape of (B, C, H, W, D), maintain-
ing the spatial structure of the image feature maps for further processing. The
detailed flow of a Mamba layer can be seen in Figure 1b.
2.4
Training Parameters
Training was conducted with a batch size of 4, using a patch size of (48, 192,
192) and Z-score normalization for data preprocessing. The median image size in
voxels was (123, 512, 511), with spacing set at (1.199, 0.5, 0.5). Resampling spline
interpolation functions were employed to adjust both image and segmentation
data, using an interpolation order of 3 for images and an order of 1 (linear) for
masks. The training was performed using the SGD optimizer with a PolyLR
scheduler (exponent = 0.9), starting with a learning rate of 0.01. The adoption
of the Mamba layer often led to gradient vanishing or explosion during training,
especially when using mixed-precision (fp16) with automatic casting. To address
this, normalized gradients were clipped with a value of 1.
The 150 patients were randomly divided into 5 folds, with each fold compris-
ing 120 patients for training and 30 for validation. Each model was trained for a
maximum of 1,000 epochs, and the final models from the last epoch were saved
for prediction. For the final challenge submission, predictions on the test set were
generated using an ensemble of all models trained across the five folds. In line
with reproducibility and verification guidelines [14], all source code, predicted
masks, training logs and trained weights have been made publicly available on
GitHub‚Ä†.
2.5
Evaluation
The aggregated Dice Similarity Coefficient (DSCagg) [2] was used as the pri-
mary evaluation metric in accordance with the guidelines of the HNTS-MRG
2024 challenge. Additionally, we employed the mean 95th percentile Hausdorff
Distance (HD95), and the mean surface distance (MSD) as supplementary met-
rics to further evaluate the segmentation performance for both GTVp and GTVn.
Hausdorff Distance (HD) was used for case study.
To evaluate the performance of the proposed method, we compared both
the segmentation accuracy and training epoch times of the default nnU-Net,
‚Ä† https://github.com/Aarhus-RadOnc-AI/UMambaAdj
6
J. Ren et al.
nnU-Net Residual Encoder (ResEnc), UMamba Encoder (UMambaEnc), and
the proposed UMambaAdj using the DSCagg metric. Since the Mamba block
involves multiple tensor shape manipulations that are not fully represented by
FLOPs, we measured the stable epoch time after the first epoch as a direct
indicator of model efficiency. All compared groups were trained with a batch
size of 4.
2.6
System environment
The experiments were conducted on a system equipped with dual AMD Ryzen
Threadripper 3990X 64-core processors (128 threads) and 256GB of system mem-
ory. An NVIDIA RTX A6000 GPU with 48GB VRAM was used for training.
The software environment included Python 3.12.4, PyTorch 2.4.0, CUDA 12.6
and nnU-Net 2.5.1. Distance metrics were calculated using SimpleITK 2.4.0.
3
Results
3.1
Cross-Validation performance
Table 1. GTVp performance on 5-Fold cross-validation
Metric
Fold 0 Fold 1 Fold 2 Fold 3 Fold 4 Average
DSCagg
0.804
0.742
0.804
0.774
0.776
0.779
HD95 [mm]
5.7
8.1
6.8
6.5
10.4
7.5
MSD [mm]
2.6
2.8
1.9
1.9
4.2
2.68
Table 2. GTVn performance on 5-Fold cross-validation
Metric
Fold 0 Fold 1 Fold 2 Fold 3 Fold 4 Average
DSCagg
0.874
0.849
0.751
0.875
0.885
0.847
HD95 [mm]
15.2
15.4
24.5
21.2
17.6
18.78
MSD [mm]
3.1
2.6
4.2
3.6
3.3
3.36
The performance metrics for GTVp and GTVn across 5-fold cross-validation
are summarized in Table 1 and Table 2. For GTVp, DSCagg ranged from 0.742
to 0.804 across different folds, with an average of 0.779. The HD95 varied from
5.7 mm to 10.4 mm, yielding an average of 7.5 mm. MSD ranged between 1.9
mm and 4.2 mm, with an average of 2.68 mm. For GTVn, DSCagg ranged from
0.751 to 0.885, with an average of 0.847. The HD95 showed a wider range from
15.2 mm to 24.5 mm, resulting in an average of 18.78 mm. The MSD values
spanned from 2.6 mm to 4.2 mm, with an average of 3.36 mm.
UMambaAdj for Head and Neck Cancer Tumor Segmentation
7
Table 3. Performance comparison between nn-UNet default, ResEnc, UMambaEnc,
and the proposed UMambaAdj.
GTVp
GTVn
Methods
DSCagg HD95 MSD DSCagg HD95 MSD
nnUNet default
0.78
14.5
3.7
0.859
28.3
4.4
nnUNet ResEnc
0.803
10.5
5.5
0.872
15.8
3.1
UMambaEnc
0.794
8.7
3.5
0.880 10.5
2.2
UMambaAdj
0.804
5.7
2.6
0.874
15.2
3.1
Bold numbers indicate the best performance for each metric.
T2w
Ground Truth
nnUNet default
nnUNet ResEnc
UMambaEnc
UMambaAdj
GTVp DSC: 0.277
GTVn DSC: 0.815
GTVp HD: 23.9 mm
GTVn HD: 75.2 mm
3D Rendering
GTVp DSC: 0.584
GTVn DSC: 0.882
GTVp HD: 14.9 mm
GTVn HD: 3.2 mm
GTVp HD: 14.6 mm
GTVn HD: 3.2 mm
GTVp DSC: 0.703
GTVn DSC: 0.870
GTVp DSC: 0.340
GTVn DSC: 0.862
GTVp HD: 23.6 mm
GTVn HD: 3.5 mm
(a)
T2w
Ground Truth
nnUNet default
nnUNet ResEnc
UMambaEnc
UMambaAdj
GTVp DSC: 0.0 (vol 1.45cc)
GTVn DSC: 0.903
GTVp HD: -
GTVn HD: 60.1 mm
3D Rendering
GTVp DSC: 0.0 (vol0.27cc)
GTVn DSC: 0.909
GTVp HD: -
GTVn HD: 59.6 mm
GTVp HD: -
GTVn HD: 60.0 mm
GTVp DSC: -
GTVn DSC: 0.898
GTVp DSC 0.0 (vol 0.03cc)
GTVn DSC: 0.920
GTVp HD: -
GTVn HD: 10.5 mm 
(b)
Fig. 2.
Two patients (a and b) were selected for illustration. For each patient, the
first row (left to right) displays the original T2-weighted MRI image, the ground truth
overlaid on the image, and the segmentation results from all compared methods over-
laid on the image. The second row shows the 3D renderings of the delineations and
segmentations. Red represents the GTVp, and Green represents the GTVn. Segmenta-
tion metrics, including DSC and HD, are shown on the rendering subfigures. A yellow
arrow in patient (b) indicates a nearly invisible small false-positive GTVp segmentation
predicted by UMambaEnc.
3.2
Comparision of nnU-Net, ResEnc, UmambaEnc and
UmambaAdj
Table 3 summarizes the performance of various models for GTVp and GTVn seg-
mentation. For GTVp, the proposed UMambaAdj achieved the highest DSCagg
8
J. Ren et al.
(0.804) and the best results in terms of HD95 (5.7 mm) and MSD (2.6 mm).
In the case of GTVn, UMambaEnc achieved the highest DSCagg (0.880) and
outperformed others with the lowest HD95 (10.5 mm) and MSD (2.2 mm).
Two cases were selected for illustration in Figure 2. For patient (a), all meth-
ods except UMambaAdj predicted a significantly smaller GTVp (DSC range
0.277‚Äì0.584), with the lower part missing, whereas UMambaAdj achieved a
higher DSC of 0.703, despite all methods failing to capture the upper part.
Additionally, the default nnU-Net model incorrectly identified a lymph node as
GTVn, resulting in an HD of 75.2 mm, compared to 3.2‚Äì3.5 mm for the other
methods. For patient (b), all methods except UMambaAdj made false positive
predictions of GTVp in the same location. Moreover, all methods except UMam-
baEnc incorrectly predicted a lymph node as positive bilaterally, leading to an
HD of 60 mm.
The training epoch time for the nnU-Net default model was 116 seconds,
while the nnU-Net ResEnc took 127 seconds. The UMambaEnc required 400
seconds, and UMambaAdj took 199 seconds. Mixed-precision (fp16) autocast
was enabled for all layers, except the Mamba layer, to ensure training stability.
3.3
Final test score
We submitted our trained UMambaAdj models in a Docker container to the
HNTS-MRG 2024 challenge on the grand challenge platform. Predictions were
made using an ensemble of all five models trained across the 5-folds. Our model
was evaluated on the test set using pre-RT T2-weighted MRI images, achieving
an DSCagg of 0.751 for GTVp and 0.842 for GTVn, resulting in an overall mean
DSCagg of 0.796.
4
Discussion
In this study, we developed a customized network that integrates features from
both UMamba and the nnU-Net Residual Encoder for T2-weighted MRI head
and neck tumor segmentation. The aim was to combine the feature extraction
strength of the residual encoder with the long-range dependency capabilities of
Mamba blocks. Compared to the original UMambaEnc, the proposed UMam-
baAdj demonstrated comparable segmentation accuracy with reduced training
and inference time, and outperformed UMambaEnc for GTVp. It also achieved
significantly better HD95 and MSD while maintaining similar DSCagg compared
to nnU-Net ResEnc. All recent methods outperformed the default nnU-Net
model across all metrics, confirming the complementary strengths of UMamba
and nnU-Net ResEnc in the proposed approach.
The cross-validation and final test results revealed a notable performance
gap between the segmentation accuracy of the primary tumor (GTVp) and the
nodal disease (GTVn). Although the DSCagg for GTVn was substantially higher
than for GTVp, the HD95 and MSD metrics were significantly larger for GTVp.
UMambaAdj for Head and Neck Cancer Tumor Segmentation
9
This discrepancy suggests that the model struggled more with accurately delin-
eating the nodal boundaries or even on detecting the nodes, often due to falsely
predicted lymph nodes. These false predictions greatly influenced the distance
based metrics.
Our experiments demonstrated that models incorporating the UMamba block
achieved significant improvements in distance-based metrics (HD95 and MSD),
underscoring the value of long-range dependencies provided by the Mamba. This
capability is crucial for capturing the intricate structures of HNC tumors, where
understanding dependencies between primary tumors and metastatic lymph
nodes is vital. Notably, the proposed UMambaAdj model, which excludes the
Mamba block from the first stage, matched the ResEnc model‚Äôs DSCagg per-
formance while achieving HD95 and MSD metrics similar to UMambaEnc. This
suggests UMambaAdj effectively balances volumetric overlap and boundary de-
lineation, although GTVn results indicate a need for the Mamba layer in the
first stage.
The evaluation results show that while HD95 and MSD metrics vary signif-
icantly among the methods, DSCagg remains relatively consistent. This differ-
ence is due to the metrics‚Äô sensitivities: DSCagg, being a global overlap measure,
is less affected by minor boundary discrepancies or isolated false predictions.
In contrast, HD95 is highly sensitive to boundary inaccuracies, making it more
responsive to small over-segmentation, under-segmentation, or isolated false pre-
dictions. This sensitivity makes these metrics more reflective of clinically rele-
vant errors, where even small false positives or negatives would be unacceptable.
This observation underscores the importance of employing multiple evaluation
metrics for a comprehensive assessment of segmentation performance. Although
DSCagg might be suitable as a single ranking metric in public challenges due
to its straightforward interpretation, relying solely on it can be misleading. The
best DSCagg method does not always correspond to the best overall segmenta-
tion performance, particularly in accurately capturing the boundaries or avoiding
false predictions‚Äîa critical factor in real-world clinical applications.
Despite the promising performance of the proposed UMambaAdj model, its
effectiveness must be validated on external datasets. Our current validation was
limited to a single fold from a single institutional dataset, and thus, further
testing on public datasets or other private datasets is essential to confirm the
generalizability and robustness of our approach. This need for broader valida-
tion is especially relevant given the rapid emergence of various Mamba-based
segmentation models since Mamba‚Äôs initial publication.
Recent adaptations, such as the ‚ÄúSwin‚Äù feature with Mamba [20], the tri-
oriented vision Mamba approach [33], and the Visual Mamba U-Net [31], have
shown that integrating Mamba with existing CNN blocks can lead to notable
segmentation accuracy, all claiming achieved state-of-the-art (SOTA). However,
some of these models still require significant modifications to match the 3D seg-
mentation performance of nnU-Net. Our adjustments based on UMamba indicate
that Mamba holds particular promise for HNC tumor segmentation, especially
for GTVn.
10
J. Ren et al.
Nevertheless, even with these advances, head and neck cancer tumor seg-
mentation remains a challenging task that is far from being a "solved" problem.
Fully automatic segmentation methods often face limitations that necessitate hu-
man intervention to ensure accurate treatment planning. In our study, despite
achieving decent DSCagg scores, even SOTA models struggled with accurately
identifying tumor locations, highlighting the persistent difficulties in this do-
main. The complex anatomy of the head and neck region, coupled with the
challenge of distinguishing tumors in T2-weighted images (as evidenced in the
GTVp cases from Figure 2), reinforces these challenges. Therefore, incorporat-
ing complementary information such as FDG-PET imaging [16,26], biopsy data,
patient reports [25], or human interaction [32] may be crucial for improving the
accuracy and reliability of HNC GTV segmentation.
5
Conclusion
In conclusion, our customized UMambaAdj model successfully combines the
strengths of long-range dependencies from UMamba blocks with the feature en-
coding capabilities of the nnU-Net Residual encoder, offering a balanced solution
for GTV segmentation in HNC. The model showed promise in achieving accu-
rate segmentations with a more efficient architecture, demonstrating compara-
ble or improved performance over existing methods. However, further validation
on diverse datasets and incorporating complementary tumor information with
human-in-the-loop strategies will be necessary to advance the application of au-
tomatic segmentation in clinical practice for MRI guided adaptive RT.
References
1. Ahmed, M., Schmidt, M., Sohaib, A., Kong, C., Burke, K., Richardson, C., Usher,
M., Brennan, S., Riddell, A., Davies, M., et al.: The value of magnetic resonance
imaging in target volume delineation of base of tongue tumours‚Äìa study using
flexible surface coils. Radiotherapy and Oncology 94(2), 161‚Äì167 (2010)
2. Andrearczyk, V., Oreiller, V., Boughdad, S., Rest, C.C.L., Elhalawani, H., Jreige,
M., Prior, J.O., Valli√®res, M., Visvikis, D., Hatt, M., et al.: Overview of the heck-
tor challenge at miccai 2021: automatic head and neck tumor segmentation and
outcome prediction in pet/ct images. In: 3D head and neck tumor segmentation in
PET/CT challenge, pp. 1‚Äì37. Springer (2021)
3. Benitez, C.M., Chuong, M.D., K√ºnzel, L.A., Thorwarth, D.: Mri-guided adaptive
radiation therapy. In: Seminars in Radiation Oncology. vol. 34, pp. 84‚Äì91. Elsevier
(2024)
4. Brouwer, C.L., Steenbakkers, R.J., Bourhis, J., Budach, W., Grau, C., Gr√©goire,
V., Van Herk, M., Lee, A., Maingon, P., Nutting, C., et al.: Ct-based delineation of
organs at risk in the head and neck region: Dahanca, eortc, gortec, hknpcsg, ncic
ctg, ncri, nrg oncology and trog consensus guidelines. Radiotherapy and Oncology
117(1), 83‚Äì90 (2015)
5. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou,
Y.: Transunet: Transformers make strong encoders for medical image segmentation.
arXiv preprint arXiv:2102.04306 (2021)
UMambaAdj for Head and Neck Cancer Tumor Segmentation
11
6. Chu, H., De la O Ar√©valo, L.R., Tang, W., Ma, B., Li, Y., De Biase, A., Both, S.,
Langendijk, J.A., van Ooijen, P., Sijtsema, N.M., et al.: Swin unetr for tumor and
lymph node segmentation using 3d pet/ct imaging: A transfer learning approach.
In: 3D Head and Neck Tumor Segmentation in PET/CT Challenge, pp. 114‚Äì120.
Springer (2022)
7. Dao, T., Gu, A.: Transformers are ssms: Generalized models and efficient algo-
rithms through structured state space duality. arXiv preprint arXiv:2405.21060
(2024)
8. Dosovitskiy, A.: An image is worth 16x16 words: Transformers for image recogni-
tion at scale. arXiv preprint arXiv:2010.11929 (2020)
9. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state
spaces. arXiv preprint arXiv:2312.00752 (2023)
10. Gu, A., Goel, K., R√©, C.: Efficiently modeling long sequences with structured state
spaces. arXiv preprint arXiv:2111.00396 (2021)
11. Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H.R., Xu, D.: Swin unetr:
Swin transformers for semantic segmentation of brain tumors in mri images. In:
International MICCAI brainlesion workshop. pp. 272‚Äì284. Springer (2021)
12. Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B.,
Roth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmentation. In:
Proceedings of the IEEE/CVF winter conference on applications of computer vi-
sion. pp. 574‚Äì584 (2022)
13. Heidari, M., Kolahi, S.G., Karimijafarbigloo, S., Azad, B., Bozorgpour, A., Hatami,
S., Azad, R., Diba, A., Bagci, U., Merhof, D., et al.: Computation-efficient era:
A comprehensive survey of state space models in medical image analysis. arXiv
preprint arXiv:2406.03430 (2024)
14. Hurkmans, C., Bibault, J.E., Brock, K.K., van Elmpt, W., Feng, M., Fuller, C.D.,
Jereczek-Fossa, B.A., Korreman, S., Landry, G., Madesta, F., et al.: A joint estro
and aapm guideline for development, clinical validation and reporting of artificial
intelligence models in radiation therapy. Radiotherapy and Oncology 197, 110345
(2024)
15. Isensee, F., Wald, T., Ulrich, C., Baumgartner, M., Roy, S., Maier-Hein, K., Jaeger,
P.F.: nnu-net revisited: A call for rigorous validation in 3d medical image segmen-
tation. arXiv preprint arXiv:2404.09556 (2024)
16. Jensen, K., Al-Farra, G., Dejanovic, D., Eriksen, J.G., Loft, A., Hansen, C.R.,
Pameijer, F.A., Zukauskaite, R., Grau, C.: Imaging for target delineation in head
and neck cancer radiotherapy. In: Seminars in nuclear medicine. vol. 51, pp. 59‚Äì67.
Elsevier (2021)
17. Jensen, K., Friborg, J., Hansen, C.R., Sams√∏e, E., Johansen, J., Andersen, M.,
Smulders, B., Andersen, E., Nielsen, M.S., Eriksen, J.G., et al.: The danish head
and neck cancer group (dahanca) 2020 radiotherapy guidelines. Radiotherapy and
Oncology 151, 149‚Äì151 (2020)
18. Li, G.Y., Chen, J., Jang, S.I., Gong, K., Li, Q.: Swincross: Cross-modal swin trans-
former for head-and-neck tumor segmentation in pet/ct images. Medical physics
51(3), 2096‚Äì2107 (2024)
19. Lin, T., Wang, Y., Liu, X., Qiu, X.: A survey of transformers. AI open 3, 111‚Äì132
(2022)
20. Liu, J., Yang, H., Zhou, H.Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang,
S., Zheng, H., et al.: Swin-umamba: Mamba-based unet with imagenet-based pre-
training. arXiv preprint arXiv:2402.03302 (2024)
12
J. Ren et al.
21. Luo, W., Li, Y., Urtasun, R., Zemel, R.: Understanding the effective receptive field
in deep convolutional neural networks. Advances in neural information processing
systems 29 (2016)
22. Ma, J., Li, F., Wang, B.: U-mamba: Enhancing long-range dependency for biomed-
ical image segmentation. arXiv preprint arXiv:2401.04722 (2024)
23. McDonald, B.A., Dal Bello, R., Fuller, C.D., Balermpas, P.: The use of mr-guided
radiation therapy for head and neck cancer and recommended reporting guidance.
In: Seminars in radiation oncology. vol. 34, pp. 69‚Äì83. Elsevier (2024)
24. Mohamed, A.S., Bahig, H., Aristophanous, M., Blanchard, P., Kamal, M., Ding,
Y., Cardenas, C.E., Brock, K.K., Lai, S.Y., Hutcheson, K.A., et al.: Prospective in
silico study of the feasibility and dosimetric advantages of mri-guided dose adap-
tation for human papillomavirus positive oropharyngeal cancer patients compared
with standard imrt. Clinical and translational radiation oncology 11, 11‚Äì18 (2018)
25. Rajendran, P., Yang, Y., Niedermayr, T.R., Gensheimer, M., Beadle, B., Le, Q.T.,
Xing, L., Dai, X.: Large language model-augmented auto-delineation of treatment
target volume in radiation therapy. arXiv preprint arXiv:2407.07296 (2024)
26. Ren, J., Eriksen, J.G., Nijkamp, J., Korreman, S.S.: Comparing different ct, pet
and mri multi-modality image combinations for deep learning-based head and neck
tumor segmentation. Acta Oncologica 60(11), 1399‚Äì1406 (2021)
27. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-
ical image segmentation. In: Medical image computing and computer-assisted
intervention‚ÄìMICCAI 2015: 18th international conference, Munich, Germany, Oc-
tober 5-9, 2015, proceedings, part III 18. pp. 234‚Äì241. Springer (2015)
28. R√ºhle, A., Nicolay, N.H.: Head and neck cancer. In: Grosu, A.L., Nieder, C.,
Nicolay, N.H. (eds.) Target Volume Definition in Radiation Oncology, pp. 91‚Äì
114. Springer International Publishing, Cham (2023). https://doi.org/10.1007/
978-3-031-45489-9_5, https://doi.org/10.1007/978-3-031-45489-9_5
29. Shaker, A.M., Maaz, M., Rasheed, H., Khan, S., Yang, M.H., Khan, F.S.: Un-
etr++: delving into efficient and accurate 3d medical image segmentation. IEEE
Transactions on Medical Imaging (2024)
30. Wahid, K., Dede, C., Naser, M., Fuller, C.: Training dataset for hntsmrg 2024
challenge. https://doi.org/10.5281/zenodo.11199559 (2024), [Data set]
31. Wang, Z., Zheng, J.Q., Zhang, Y., Cui, G., Li, L.: Mamba-unet: Unet-like pure
visual mamba for medical image segmentation. arXiv preprint arXiv:2402.05079
(2024)
32. Wei, Z., Ren, J., Korreman, S.S., Nijkamp, J.: Towards interactive deep-learning for
tumour segmentation in head and neck cancer radiotherapy. Physics and Imaging
in Radiation Oncology 25, 100408 (2023)
33. Xing, Z., Ye, T., Yang, Y., Liu, G., Zhu, L.: Segmamba: Long-range sequential mod-
eling mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560
(2024)
34. Xu, R., Yang, S., Wang, Y., Cai, Y., Du, B., Chen, H.: Visual mamba: A survey
and new outlooks (2024)
35. Zhou, H.Y., Guo, J., Zhang, Y., Yu, L., Wang, L., Yu, Y.: nnformer: Interleaved
transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201 (2021)
