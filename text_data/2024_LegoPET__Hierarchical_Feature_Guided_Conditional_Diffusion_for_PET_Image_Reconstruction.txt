LegoPET: Hierarchical Feature Guided Conditional Diffusion for PET Image
Reconstruction
Yiran Sun
Rice University
6100 Main St, Houston, TX 77005
ys92@rice.edu
Osama Mawlawi
The University of Texas MD Anderson Cancer Center
1155 Pressler St, Houston, TX 77030
omawlawi@mdanderson.org
Abstract
Positron emission tomography (PET) is widely utilized
for cancer detection due to its ability to visualize func-
tional and biological processes in vivo. PET images are
usually reconstructed from histogrammed raw data (sino-
grams) using traditional iterative techniques (e.g., OSEM,
MLEM). Recently, deep learning (DL) methods have shown
promise by directly mapping raw sinogram data to PET im-
ages. However, DL approaches that are regression-based
or GAN-based often produce overly smoothed images or in-
troduce various artifacts respectively. Image-conditioned
diffusion probabilistic models (cDPMs) are another class
of likelihood-based DL techniques capable of generating
highly realistic and controllable images.
While cDPMs
have notable strengths, they still face challenges such as
maintain correspondence and consistency between input
and output images when they are from different domains
(e.g., sinogram vs. image domain) as well as slow con-
vergence rates. To address these limitations, we introduce
LegoPET, a hierarchicaL feature guided conditional dif-
fusion model for high-perceptual quality PET image re-
construction from sinograms.
We conducted several ex-
periments demonstrating that LegoPET not only improves
the performance of cDPMs but also surpasses recent DL-
based PET image reconstruction techniques in terms of vi-
sual quality and pixel-level PSNR/SSIM metrics. Our code
is available at https://github.com/yransun/LegoPET.
1. Introduction
Positron emission tomography (PET) is a functional
imaging technique that visualizes various biochemical and
physiological processes across different tissues. During the
image acquisition process, high-energy photons generated
by positron annihilation are detected and counted by de-
tectors arranged in rings within a PET scanner. Usually,
the measured counts from various angles form projections,
which are then stacked into sinograms. These sinograms
serve as the basis for reconstructing the final PET image,
enabling the visualization of functional processes within the
body [2].
Conventional PET image reconstruction techniques,
such
as
Ordered
Subsets
Expectation
Maximization
(OSEM) and Maximum Likelihood Expectation Maximiza-
tion (MLEM), often encounter challenges like data-model
mismatch, data inconsistency, and overfitting, which may
introduce artifacts and noise into the reconstructed images
[4].
Recently, deep learning (DL) methods that directly
transform raw PET sinograms into images have gained at-
tention due to their ability to learn complex, non-linear
physical processes from data. However, regression-based
DL approaches are typically supervised with mean square
error (MSE) or mean absolute error (MAE), leading to re-
sultant images with overly smooth textures and suboptimal
perceptual quality [11,15]. Generative models, such as gen-
erative adversarial networks (GANs) [3], represent an ad-
vanced class of DL techniques designed to capture inher-
ent data distributions and generate realistic images. Never-
theless, the underlying competitive nature between genera-
tor and discriminator segments of GAN models often leads
to issues such as non-convergence and mode collapse dur-
ing training [17]. Image-conditioned diffusion probabilistic
models (cDPMs) [7, 8, 16] have lately demonstrated com-
petitive performance, which exhibits better convergence
behavior and generates more realistic images.
However,
cDPMs also face several key challenges: (1) insufficient
correspondence and consistency between conditioning in-
puts and outputs, especially when they originate from dif-
ferent domains (e.g., sinogram vs. image domains), (2) dif-
ficulty in capturing high frequency details, and (3) low train-
ing efficiency [5,19].
To address above limitations, we propose LegoPET, a
hierarchicaL feature guided conditional diffusion model
for high-perceptual quality PET image reconstruction from
sinograms.
Specifically, building on previous work [15,
18, 20], we first train a convolutional U-Net on sinogram-
arXiv:2411.16629v1  [eess.IV]  25 Nov 2024
PET pairs as a plug-and-play prior (PnPNet), and then effi-
ciently fine-tune a standard cDPM by using structured mul-
tilevel features from the learned PnPNet as biases. Addi-
tionally, we use the classifier-free training strategy to bal-
ance mode coverage and sample fidelity [8]. Experimen-
tal results demonstrate that LegoPET not only improves
the performance of cDPMs but also surpasses recent DL-
based PET image reconstruction techniques in terms of vi-
sual quality and pixel-level PSNR/SSIM metrics.
2. Method
Let s denote input sinogram image, where s ∈R1×H×W
is a single-channel 2D image with resolution H × W. We
denote the reference PET scan by x0 ∈R1×H×W . Our gen-
eral objective is to approximate and sample from the condi-
tional distribution p(x0|s). We develop LegoPET to address
this task, containing two components (see Fig. 1): a learned
plug-and-play prior network (right) that generates hierarchi-
cal feature maps, and a standard 2D sinogram-conditioned
diffusion model (left) that reconstructs final clean PET im-
age. We describe details of LegoPET in the following sec-
tions.
2.1. Plug-and-Play Prior Network (PnPNet)
We utilize a conditional convolutional U-Net, which is
the same architecture as commonly-used U-Net backbone in
diffusion models, consisting of ResNet blocks [6] and spa-
tial self-attention blocks. Specifically, PnPNet comprises 4
encoder blocks, 2 middle blocks, and 4 decoder blocks, tak-
ing a single sinogram s as input and getting the correspond-
ing PET image information (see Fig. 1b). In our experi-
ments, input and output have been padded with dimensions
of 1 × 256 × 256. During downsampling stage, the blocks
operate at 4 resolutions: 256 × 256, 128 × 128, 64 × 64,
32 × 32. Note that PnPNet’s architecture is not limited to
this design and may perform similarly well with other ar-
chitectural variations.
PnPNet is trained in an end-to-end manner using spa-
tial MSE-based (LMSE) and Discrete Wavelet Transform-
based (LDW T ) [13] losses for supervision on sinogram-
PET pairs:
LPnPNet = LMSE + λ1 ∗LDW T
(1)
where λ1 controls the importance of DWT loss term (we
set λ1 = 0.1 in our experiments). LDW T mainly focuses
on calculating the difference between ground truth and pre-
diction’s high-frequency spectrum, which ensures that the
hierarchical feature maps from PnPNet are able to addition-
ally capture more high-frequency information (e.g. tissue
edges, anatomical structures, textures).
Then we extract two lists of hierarchical feature maps
from downsampling blocks (bd) and middle blocks (bm) in
Figure 1. Overview of LegoPET. LegoPET includes two parts:
a learned PnPNet for hierarchical feature maps extraction (right),
and a standard 2D sinogram-conditioned DPM for PET image re-
construction (left). Two lists of structured layered feature maps,
bd and bm, from the pre-trained PnPNet are added to the cDPM as
biases.
the latent space of learned PnPNet. We connect the pre-
trained PnPNet to cDPM by incorporating bd and bm as ex-
tra biases to the encoder and middle blocks of cDPM. This
integration allows the cDPM to leverage the low- and high-
frequency information from learned feature representations.
2.2. LegoPET for Sinogram-to-PET
Conditional DPMs (cDPMs) are designed to learn a pa-
rameterized Markov chain that transforms a Gaussian dis-
tribution into a conditional data distribution. In particular,
LegoPET approximates the conditional distribution through
a fixed forward process and a learning-based reverse pro-
cess.
The forward process starts with a clean PET sample
from the input data distribution x0 ∼q(x0) and gradually
adds Gaussian noise according to a variance schedule β1:T ,
where βt ∈(0, 1) for all t ∈[1, T]:
q(xt|xt−1) := N(xt;
p
1 −βtxt−1, βtI)
(2)
where xT is an isotropic Gaussian distribution for large
enough T. We express xt in closed form with respect to
x0 directly, which allows for efficient training. Let αt :=
1 −βt, ¯αt := Qt
s=1 αs. Then we can sample xt at any time
step t using a linear combination of noise ϵ ∼N(0, I) and
x0:
xt = √αtx0 +
√
1 −αtϵ
(3)
The goal of reverse process is to generate a clean
PET image x0 from the noisy vector xT given the con-
ditions C = {s, bd, bm}, i.e.
approximate specific con-
ditional distribution p(x0|C).
We use a joint Markov
chain distribution to model this process:
pθ(x0:T ) :=
p(xT ) QT
t=1 pθ(xt−1|xt, C), where p(xT ) = N(xT ; 0, I).
We learn the transition pθ(xt−1|xt, C) using a neural net-
work µθ(·, ·):
pθ(xt−1|xt, C) := N(xt−1; µθ(xt, t, C), P
θ(xt, t, C))
(4)
where θ represents the learnable parameters of the neural
network. We can further reparameterize µθ(·, ·) by:
µθ(xt, t, C) =
1
√αt

xt −1 −αt
√1 −¯αt
ϵθ(xt, t, C)

(5)
where ϵθ(·, ·) predicts the noise added at each time step.
2.3. Training and Inference
Following previous diffusion model-based work, we use
a time-conditioned convolutional U-Net to perform denois-
ing at each time step of the reverse process. We incorporate
conditions C into the denoising process. During each iter-
ation of training, a batch of random sinogram-PET pairs is
selected, and LegoPET is trained by minimizing the denois-
ing loss:
LLegoPET := Et∼[1,T ],x0,ϵ

||ϵt −ϵθ(xt, t, C)||2
(6)
We use classifier-free diffusion guidance training strat-
egy. The conditioning information C is randomly removed
with probability pdp and replaced with the same shape null
tensors representing the absence of conditioning informa-
tion. This strategy assists LegoPET to effectively capture
both conditional and unconditional distributions, also the
differences between them, thereby generating results that
are more accurately aligned with the conditioning informa-
tion [8].
At inference time, given a sample of Gaussian noise
xT ∼N(0, I), we use new denoising expression ˜ϵθ =
(1 + λ2)ϵθ(xt, t, C) −λ2ϵθ(xt, t) to progressively denoise
xT over T steps to generate a clean PET image x0, where
λ2 controls the relative importance of different terms.
3. Experiments
3.1. Datasets and Preprocessing
We simulated 2D 18F-FDG PET images using the public
20 3D brain phantoms from BrainWeb [1] with the resolu-
tion and matrix size of 2.086 × 2.086 × 2.031 mm3 and
344 × 344 × 127 acquired from a Siemens Biograph mMR.
We used data augmentation technique by rotating each 3D
brain phantom 5 times. For each 3D brain phantom, we
selected 55 non-continuous slices from axial view to gen-
erate high count sinograms which were used to reconstruct
the reference PET images. We used 17 brain samples (4675
slices) for training, 1 brain sample (275 slices) for valida-
tion and 2 brain samples (550 slices) for testing.
3.2. Implementation Details
We implemented all experiments using PyTorch [14] on
NVIDIA A100 GPUs. We trained PnPNet using the Adam
optimizer with a fixed learning rate of 3×10−5, and selected
the model at epoch 119. We set batch size to 4 and trained
500 epochs for each LegoPET model, with T
= 1000
timesteps and pdp = {0, 0.1, 0.2, 0.5}. We chose the fi-
nal checkpoints for all LegoPETs without any specific se-
lection strategies. We used pixel-level metrics for quantita-
tive evaluation, i.e. Peak Signal to Noise Ratio (PSNR) and
Structural Similarity Index Measure (SSIM) [9]. Our code
is available at https://github.com/yransun/LegoPET.
3.3. Baselines
We compared LegoPET against four baselines: Deep-
PET [4], cGAN [12], Pix2Pix [10] and cDPM [8,16]. Deep-
PET is a regression-based method that utilizes a deep con-
volutional encoder-decoder network to map sinograms di-
rectly to PET images. cGAN is a standard conditional GAN
for image reconstruction. Pix2Pix uses a conditional least
squares GAN for image-to-image translation tasks. cDPM
is a standard sinogram-conditioned diffusion model, which
can also be regarded as “LegoPET w/o guidance”. We used
the publicly provided repository configurations for all base-
line models and trained them until full convergence.
3.4. Reconstruction Results
We summarize the average pixel-wise PSNR/SSIM val-
ues across all reconstructed slices and the number of train-
able parameters for each method in Table 1.
LegoPET
demonstrates a 0.59dB improvement in PSNR compared
to cDPM, validating the effectiveness of the proposed hi-
erarchical feature-guided module. Additionally, we observe
a substantial improvement of approximately 5dB/3.7dB in
PSNR when compared with cGAN/Pix2Pix.
Although
DeepPET achieves comparable PSNR values to LegoPET,
the PET images reconstructed by DeepPET are signifi-
cantly less aligned with human visual perception, as shown
in Fig. 2.
This may be due to LegoPET’s reliance on
diffusion-based generative models and stochastic posterior
sampling, which compromises pixel-wise distortion (lead-
ing to lower PSNR) while maintaining fidelity to the target
image [11, 15]. Visual reconstruction results are further il-
lustrated in Fig. 2, with side-by-side comparisons of four
baseline methods.
LegoPET generates the most realistic
reconstruction results among all methods, with the second
lightest color in the squared error maps. cGAN, Pix2Pix,
Input
Reference
DeepPET
cGAN
Pix2Pix
cDPM
LegoPET
28.47dB/0.946
24.92dB/0.898
25.99dB/0.948
27.09dB/0.946
28.06dB/0.958
27.14dB/0.940
23.73dB/0.887
24.86dB/0.940
25.05dB/0.942
26.26dB/0.952
Figure 2. Comparison of LegoPET with Four Baselines on Two Example Reconstructed Slices. The first column shows the input
sinogram images, and the second column shows the reference images reconstructed using OSEM algorithm. The third to sixth columns
correspond to the four baselines (labeled above each image), and the final column shows the reconstructed PET image using proposed
LegoPET method. PSNR/SSIM values are reported below each slice, and squared error maps between each method and the reference
image are also displayed (second and fourth rows). We prove that LegoPET generates PET images with the highest perceptual quality, and
also improves the performance of cDPM through feature guidance.
Category
Method
Metrics
PSNR ↑SSIM ↑MParam.
Regression-based DeepPET [4]
28.30
0.944
11.02
GAN-based
cGAN [12]
22.58
0.864
11.37
Pix2Pix [10]
23.91
0.922
11.37
Likelihood-based cDPM [8]
27.00
0.945
35.71
LegoPET (Ours)
27.59
0.956
35.71
Table 1.
Quantitative Evaluation using PSNR, SSIM, and
Number of Trainable Parameters (MParam). Red and blue col-
ors indicate the best and second-best results respectively.
and cDPM introduce numerous artifacts, while DeepPET
produces blurry reconstructions with unrealistic fine details.
3.5. Ablation Study
To further verify the effectiveness of our proposed
LegoPET, we conducted an ablation study. For a fair com-
parison, we used the same training settings for both cDPM
and LegoPET, including learning rate and the number of
trainable parameters. Classifier-free diffusion guidance [8]
was disabled during the ablation study, i.e. pdp = 0. We
Figure 3.
Effectiveness of Hierarchical Feature Guidance.
We compare the performance of LegoPET and cDPM within
500 epochs in terms of PSNR and SSIM. We regard cDPM as
“LegoPET w/o guidance”, and LegoPET as “cDPM w/ guidance”.
compared their performance every 20 epochs in terms of
PSNR/SSIM over a total of 500 epochs. The results pre-
sented in Fig. 3 reveal several key insights as expected.
First, LegoPET consistently outperforms cDPM in terms of
quantitative metrics, which underscores the benefits of in-
tegrating hierarchical feature representations with rich low-
and high-frequency information from learned PnPNet into
a cDPM. Second, LegoPET illustrates strong performance
even at early epochs, suggesting its potential to improve
training efficiency.
4. Conclusion
We propose LegoPET, which enables high-perceptual
quality PET image reconstruction that preserves geomet-
ric structure and sharp edges from raw sinograms.
The
experimental results show that LegoPET outperforms sev-
eral baseline algorithms in terms of visual inspection and
PSNR/SSIM metrics. The ablation study further validates
that our novel hierarchical feature-guided method has sig-
nificant potential to improve both reconstructed image qual-
ity and training efficiency. Our future work will focus on
extending current model into an efficient 3D version and
validating on patient data.
References
[1] D Louis Collins, Alex P Zijdenbos, Vasken Kollokian,
John G Sled, Noor Jehan Kabani, Colin J Holmes, and
Alan C Evans. Design and construction of a realistic digi-
tal brain phantom. IEEE transactions on medical imaging,
17(3):463–468, 1998. 3
[2] Frederic H Fahey. Data acquisition in pet imaging. Journal
of nuclear medicine technology, 30(2):39–49, 2002. 1
[3] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM, 63(11):139–144, 2020. 1
[4] Ida H¨aggstr¨om, C Ross Schmidtlein, Gabriele Campanella,
and Thomas J Fuchs.
Deeppet: A deep encoder–decoder
network for directly solving the pet image reconstruction in-
verse problem. Medical image analysis, 54:253–262, 2019.
1, 3, 4
[5] Zeyu Han, Yuhan Wang, Luping Zhou, Peng Wang, Binyu
Yan, Jiliu Zhou, Yan Wang, and Dinggang Shen. Contrastive
diffusion model with auxiliary guidance for coarse-to-fine
pet reconstruction.
In International Conference on Medi-
cal Image Computing and Computer-Assisted Intervention,
pages 239–249. Springer, 2023. 1
[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 2
[7] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems, 33:6840–6851, 2020. 1
[8] Jonathan Ho and Tim Salimans.
Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598, 2022. 1, 2, 3, 4
[9] Alain Hore and Djemel Ziou. Image quality metrics: Psnr
vs. ssim. In 2010 20th international conference on pattern
recognition, pages 2366–2369. IEEE, 2010. 3
[10] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1125–1134,
2017. 3, 4
[11] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 4681–4690,
2017. 1, 3
[12] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 3, 4
[13] Gheyath Othman and Diyar Qader Zeebaree.
The appli-
cations of discrete wavelet transform in image processing:
A review.
Journal of Soft Computing and Data Mining,
1(2):31–43, 2020. 2
[14] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in neural information processing systems, 32, 2019.
3
[15] Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido
Gerig, and Peyman Milanfar.
Multiscale structure guided
diffusion for image deblurring.
In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 10721–10733, 2023. 1, 3
[16] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi.
Palette: Image-to-image diffusion models.
In
ACM SIGGRAPH 2022 conference proceedings, pages 1–10,
2022. 1, 3
[17] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. Advances in neural information processing
systems, 29, 2016. 1
[18] Yiran Sun and Osama R Mawlawi. Diffpet: A fine tuned
sinogram-to-pet conditional diffusion model. In AAPM 66th
Annual Meeting & Exhibition. AAPM, 2024. 1
[19] Huijie Zhang, Yifu Lu, Ismail Alkhouri, Saiprasad Ravis-
hankar, Dogyoon Song, and Qing Qu.
Improving train-
ing efficiency of diffusion models via multi-stage framework
and tailored multi-decoder architecture. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7372–7381, 2024. 1
[20] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models.
In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 3836–3847, 2023. 1
