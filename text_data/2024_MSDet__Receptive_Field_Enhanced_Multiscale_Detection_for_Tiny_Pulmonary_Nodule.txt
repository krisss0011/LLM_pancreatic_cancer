MSDet: Receptive Field Enhanced Multiscale
Detection for Tiny Pulmonary Nodule
Guohui Cai, Ying Cai∗, Zeyu Zhang, Daji Ergu, Yuanzhouhan Cao, Binbin Hu, Zhibin Liao, and Yang Zhao
Abstract—Pulmonary nodules are critical indicators for the
early diagnosis of lung cancer, making their detection essential
for timely treatment. However, traditional CT imaging methods
suffered from cumbersome procedures, low detection rates,
and poor localization accuracy. The subtle differences between
pulmonary nodules and surrounding tissues in complex lung
CT images, combined with repeated downsampling in feature
extraction networks, often lead to missed or false detections of
small nodules. Existing methods such as FPN, with its fixed
feature fusion and limited receptive field, struggle to effectively
overcome these issues. To address these challenges, our paper
proposed three key contributions: Firstly, we proposed MSDet, a
multiscale attention and receptive field network for detecting tiny
pulmonary nodules. Secondly, we proposed the extended receptive
domain (ERD) strategy to capture richer contextual information
and reduce false positives caused by nodule occlusion. We also
proposed the position channel attention mechanism (PCAM) to
optimize feature learning and reduce multiscale detection errors,
and designed the tiny object detection block (TODB) to enhance
the detection of tiny nodules. Lastly, we conducted thorough
experiments on the public LUNA16 dataset, achieving state-of-
the-art performance, with an mAP improvement of 8.8% over the
previous state-of-the-art method YOLOv8. These advancements
significantly boosted detection accuracy and reliability, providing
a more effective solution for early lung cancer diagnosis. The code
will be available at https://github.com/CaiGuoHui123/MSDet.
Index Terms—Pulmonary nodule; Computer tomography;
Tiny object detection; Hybrid CNN-Transformer;
I. INTRODUCTION
L
UNG cancer remains the leading malignancy in terms
of both incidence and mortality rates worldwide [1].
Pulmonary nodules are among the earliest clinical manifes-
tations of lung cancer, underscoring the critical importance
of early detection. Currently, computed tomography (CT) is
the primary modality utilized by clinicians for the screening
of pulmonary nodules [2], [3]. Pulmonary nodules, typically
characterized as intrapulmonary spherical lesions, range in
diameter from 3 to 30 mm. The complexity of the lung
environment and the variability of nodule size and morphology
make nodule detection difficult. At the same time, small nod-
ules in lung CT scans are often visually difficult to distinguish
Guohui Cai, Ying Cai (∗corresponding author), Daji Ergu, and Binbin Hu
are with the College of Electronic and Information, Southwest Minzu Univer-
sity, Chengdu, China (e-mail: guohuicai123@163.com; caiying34@yeah.net;
ergudaji@163.com; binhu0821@163.com).
Zeyu Zhang is with the Australian National University, Canberra ACT 2601,
Australia (e-mail: steve.zeyu.zhang@outlook.com).
Yuanzhouhan Cao is with the School of Computer Science and Technology,
Beijing Jiaotong University, Beijing, China (e-mail: yzhcao@bjtu.edu.cn).
Zhibin Liao is with the University of Adelaide, Adelaide, Australia (e-mail:
zhibin.liao@adelaide.edu.au).
Yang Zhao is with La Trobe University, Melbourne, Australia (e-mail:
y.zhao2@latrobe.edu.au).
from surrounding tissues, resulting in a high rate of missed
detection or false positives.
Fig. 1.
Comparative histogram between the state-of-the-art network and
MSDet, MSDet (Ours) achieved the best result of 97.30% in terms of
pulmonary nodule detection accuracy in CT images.
Object detection methods have been widely adapted in
medical imaging to automatically analyze imaging data and
identify suspicious lesion areas. Two-stage detection algo-
rithms [4]–[7] initially detect all potential nodules from CT
scans, which often results in a high number of false positives.
However, these networks are typically complex and time-
consuming, relying on feature maps and prior knowledge (e.g.,
anchor frames), leading to inaccurate localization, particularly
for small or variably shaped nodules. Besides, one-stage detec-
tion algorithms [8]–[14] integrate nodule candidate detection
and false positive reduction into a single model, achieving
higher detection speeds but struggling with small nodules and
occlusions, which can result in missed detections and false
positives.
Meanwhile, YOLOv5, a popular one-stage detection algo-
rithm, offers notable advantages such as adaptive anchor box
computation and improved feature fusion, which enhance its
speed and spatial information processing capabilities. Hence,
developed a one-stage detection method based on YOLOv5
presented to be a promising trend to address these challenges.
However, YOLOv5 still struggles with feature extraction,
especially in complex backgrounds or with small targets.
Transformers [15]–[17] have shown great potential in address-
ing these limitations. Their self-attention mechanism captures
long-range dependencies and global context, making them
arXiv:2409.14028v1  [eess.IV]  21 Sep 2024
effective for small object detection like pulmonary nodules.
Incorporating attention mechanisms can thus improve feature
representation and reduce both missed detections and false
positives.
Inspired by these advancements, we proposed a novel one-
stage detection model, MSDet, specifically designed to ad-
dress the challenges of pulmonary nodule detection, including
high false positive rates and low detection accuracy. Our
contributions include a tiny object detection block (TODB),
which captures finer feature details to improve small nodule
detection, and an extended receptive domain (ERD) strategy
that refines occluded nodule features to reduce false positives.
Additionally, we designed the positional channel attention
mechanism (PCAM) to optimize feature representation and
mitigate high miss and false detection rates in multiscale
nodule detection. As shown in Figure 1, MSDet achieved a
significant 8.8% improvement in mAP compared to the state-
of-the-art nodule detection method YOLOv8.
Hence, this paper proposed three key contributions as fol-
lows:
• We proposed MSDet, a novel one-stage detection model
specifically designed for detecting tiny pulmonary nod-
ules. MSDet integrates multiscale attention and an en-
hanced receptive field, addressing challenges of high false
positive rates and low detection accuracy.
• We proposed the extended receptive domain (ERD) strat-
egy to capture richer contextual information and reduce
false positives caused by nodule occlusion. We also pro-
posed the position channel attention mechanism (PCAM)
to optimize feature learning and reduce multiscale detec-
tion errors, and designed the tiny object detection block
(TODB) to enhance the detection of tiny nodules.
• We conducted thorough experiments on the public
LUNA16 dataset [18], achieving state-of-the-art perfor-
mance, with an mAP improvement of 8.8% over the
previous state-of-the-art method YOLOv8.
II. RELATED WORKS
Dense prediction techniques [19] have been widely adopted
in medical imaging analysis [20]–[23] particularly for tasks
such as semantic segmentation [24]–[27], and object detection
[28], due to their ability to generate pixel-level predictions that
enhance the precision of identifying and localizing anatomical
structures or pathological regions. Detection algorithms for
pulmonary nodules based on deep learning can be broadly
classified into two categories: two-stage detection algorithms
and one-stage detection algorithms.
Two-stage detection algorithms, such as R-CNN, first gen-
erate candidate regions and then classify and localize targets.
Fast R-CNN [5] accelerates detection by sharing convolu-
tional features, while Faster R-CNN [6] improves both speed
and accuracy with Region Proposal Networks (RPNs). For
instance, Xu et al. [29] enhanced the Faster R-CNN model
by adopting multi-scale training strategies, Online Hard Ex-
ample Mining (OHEM), customized anchor sizes and ratios,
deformable convolutions, and path augmentation in Feature
Pyramid Networks (FPNs). This improved model significantly
enhanced small target detection capabilities. Su et al. [30]
proposed a Faster R-CNN-based method for lung nodule de-
tection, achieving over 20% accuracy improvement compared
to traditional algorithms, showing potential clinical value in
lung disease diagnosis. Tong et al. [31] proposed a pulmonary
nodule detection system using Faster R-CNN with ISODATA
[32] for candidate detection and 3D-CNN with Focal Loss for
false positive reduction, achieving strong results on LUNA16.
However, the primary drawback of two-stage detection algo-
rithms is the time-consuming process of generating candidate
regions, which leads to slower detection speeds. Additionally,
the generation of candidate regions often relies on prior
knowledge, potentially resulting in inaccurate localization,
especially when targets vary significantly in shape or size.
These limitations hinder the efficiency and performance of
two-stage detection algorithms in real-time applications.
One-stage detection algorithms mitigate the limitations of
two-stage algorithms by performing object detection directly
on the input image without the need for candidate region
generation, thus significantly accelerating the detection pro-
cess. Representative one-stage detection algorithms include the
YOLO series and SSD. These algorithms achieve both local-
ization and classification in a single pass, making them highly
advantageous in applications requiring real-time performance.
In the context of pulmonary nodule detection, the YOLO series
has been widely adopted due to its rapid detection speed
and high accuracy. For example, Wu et al. [33] proposed a
YOLOv7-based method for pulmonary nodule detection that
incorporates Efficient Omni-Dimensional Convolution (EOD-
Conv) to enhance feature extraction precision, achieving a
mAP of 94.6% on the LUNA16 dataset. Additionally, Zhanlin
Ji et al. [34] improved the nodule detection performance
across different scales by decoupling the feature pyramid into
high-semantic and low-semantic regions to reduce semantic
conflicts during fusion, achieving an accuracy of 92.3% on
the Lung-PETCT-Dx dataset.
However, one-stage detection algorithms face challenges
due to multiple down-sampling steps, which progressively
reduce the feature map size. This reduction causes the size
of small pulmonary nodules on the feature map to become
very small, potentially leading to the loss of fine features
and making small nodules difficult to detect. Moreover, in the
complex pulmonary environment, lung tissues often occlude
nodules, further complicating accurate detection. These chal-
lenges result in missed detections, false positives, and high
false-positive rates for occluded nodules. To address these
issues, this paper proposes a novel one-stage detection network
designed to integrate multiple modules specifically aimed at
efficiently and accurately detecting small pulmonary nodules.
This network achieves precise detection of small nodules and
accurate and efficient detection of occluded and multi-scale
nodules within complex pulmonary environments.
Fig. 2. Overall architecture of the MSDet network for lung nodule detection. The initial convolutional layers, represented as CBS blocks, process the input lung
CT image to extract preliminary features. These features undergo a series of transformations through the ERD modules, which broaden the receptive field to
capture more contextual information. PCAM modules are strategically placed to refine feature representation by focusing on crucial spatial and channel-related
information. Multiple feature maps generated at different stages are then concatenated and further processed through upsampling and additional CBS blocks
to construct refined prediction feature maps.
III. METHODS
A. Overview
We proposed a novel lung nodule detection network MSDet.
The overall architecture of MSDet is shown in Figure 2. Our
method captures richer contextual information and reduces
false positives caused by nodule occlusion through the ex-
tended receptive domain (ERD) strategy. Then, we design a
dedicated position channel attention mechanism (PCAM) to
effectively optimize feature learning and reduce multi-scale
detection errors. Finally, the connection between different
feature maps is captured by the design of the tiny object
detection block (TODB), which reduces the interference of
complex background tissues and aims to improve the detection
accuracy of small nodules of the model. Specifically, given
a lung CT image input, after convolution and feature fusion
operations, multi-scale feature fusion and information transfer
are realized. In the feature fusion stage, the feature map
size is restored by the TODB through upsampling operation,
and it is fused with feature maps of different resolutions
to generate a more refined prediction feature map. Then,
following the concatenation operation, the multiscale feature
maps are connected and input into the ERD module designed
by us, which effectively expands the receptive field of the
feature map, captures more contextual information and details,
and enables the network to integrate information from a
larger area. As shown in Figure 3, the SPP module further
enhances this capability by aggregating features at multiple
scales, improving the robustness of the feature representation
against variations in object size and position. A key aspect of
our design is the PCAM. In the PCAM module, the position
attention module captures long-range contextual information,
while the channel attention module models the interdepen-
dencies between channels, thereby adjusting the attention
weights according to the importance of different positions and
channels. Finally, the model generates output feature maps at
multiple scales to produce the final prediction results.
Fig. 3.
Architecture of the Spatial Pyramid Pooling (SPP) module. The
module utilizes a Convolutional Layer Block (CLB) followed by three parallel
Maxpool layers with varying sizes to capture multi-scale features. These
features are then concatenated and processed by another CLB to enhance
the final feature representation, ensuring robust spatial invariance.
B. Tiny Object Detection Block (TODB)
The vanilla YOLOv5, primarily designed for handling nat-
ural images, shows limited efficacy in detecting pulmonary
nodules in CT images. During network downsampling, the
deepest feature map size is reduced to 20×20 pixels . In
the YOLOv5, multiple downsampling operations progressively
reduce the feature map size, causing small pulmonary nodules
to appear extremely tiny in the feature maps. For example, a
32x downsampling reduces a 20×20 pixel nodule to 1×1 pixel,
making it nearly undetectable. As a result, YOLOv5 performs
poorly on pulmonary nodule detection in CT scans.
To address this challenge, we designed the TODB module
to capture the relationships between different feature maps.
This module aims to minimize interference from complex
surrounding tissues in medical images, thereby enhancing
the recognition accuracy of small nodules. By leveraging
the TODB module, the network can more effectively ex-
tract and identify features of pulmonary nodules, improving
detection accuracy. Figure 4 illustrates the specific structure
of the TODB, which optimizes feature map connections and
information extraction, significantly boosting the detection
performance of small nodules as detailed below.
The network input size is 640×640×3, and after a series
of downsampling operations, an 80×80×128 feature map F1
is generated. To enrich it with contextual information, a
1×1 convolution is applied, altering the channel number for
cross-channel interaction, enhancing nodule detection. This
operation is expressed as:
F ′
1 = σ(W1 ∗F1)
(1)
where σ is the activation function and ∗denotes convolu-
tion. Next, F1 is upsampled to 160 × 160 × 64 and added to
another feature map F2, obtained via a 4x downsampling of
the input. This operation combines multi-resolution features
to improve detection accuracy, expressed as:
F3 = Upsample(F ′
1) + F2
(2)
After further convolutional processing, the final output F4
is generated, which is a 160 × 160 × 18 feature map used for
nodule prediction:
F4 = σ(W2 ∗F3)
(3)
This process divides the input into 160×160 regions for pre-
cise nodule prediction, efficiently integrating multi-resolution
and cross-channel information to enhance detection accuracy.
By concentrating on smaller regions and leveraging multi-scale
features, the model’s robustness is greatly enhanced.
Fig. 4.
TODB Structure. This module integrates multi-resolution features
through upsampling and feature map fusion, allowing the network to capture
small pulmonary nodules more accurately. The structure enhances detection
robustness by combining features from different resolutions.
C. Extended Receptive Domain (ERD)
Decoupling the network’s bottleneck structures can enhance
lung nodule detection but may reduce the receptive field,
limiting the ability to capture global contextual information. To
address this, we propose an extended receptive domain (ERD)
strategy, which expands the receptive field while maintaining
computational efficiency, allowing for improved detection of
nodules across various sizes and scales.
As shown in Figure 6, ERD uses a multi-branch structure
to capture multiscale features. It integrates three parallel 3×3
dilated convolution branches, a 1×1 convolution branch, and
an identity branch. The dilated convolutions use dilation rates
R = (1, 3, 5) to provide varying receptive fields while keeping
the resolution intact by setting the padding to P = (1, 3, 5).
The 1×1 convolution branch captures fine details and enhances
positional information.
Fig. 5.
Illustration of the ERD architecture integrating multiple dilated
convolutions for lung nodule detection. The diagram on the left shows the
Neck with a Cascaded Refinement Scheme, and the right side details the
Series Receptive Field Enhancement Module, employing dilated convolutions
with varying dilation factors to capture multiscale features effectively.
Dilated convolutions introduce fixed gaps between kernel
values, allowing expansion of the receptive field without
increasing the number of parameters. The output of a dilated
convolution with input x(m, n) and kernel weights ω(i, j) can
be expressed as:
y(m, n) =
M
X
i=1
N
X
j=1
x(m + r · i + n + r · j)ω(i, j)
(4)
Fig. 6.
ERD Structure. The ERD consists of multiple parallel convolution
branches with varying dilation rates, designed to capture features at different
spatial scales. This structure enhances the detection of lung nodules by
expanding the receptive field while maintaining computational efficiency.
where r is the dilation rate, and M, N are kernel sizes
(typically 3×3). By varying r, the receptive field expands: r =
1 results in a 3×3 receptive field, r = 2 gives 5×5, and r =
3 gives 7×7, all while keeping the computation similar to a
standard convolution.
As illustrated in Figure 5, the ERD strategy employs a
sophisticated multi-branch structure to effectively capture fea-
tures at multiple scales. This figure visually demonstrates the
arrangement and interaction of various convolution branches
within the ERD, highlighting the specific roles of dilated
and 1×1 convolutions in enhancing the model’s sensitivity to
spatial details and contextual variations across different nodule
sizes.
The equivalent receptive field (RF) for a k × k dilated
convolution and the output feature map resolution (H) are
calculated as:
RF = (r −1)(k −1) + k
(5)
H = (h + 2p −RF)
s
+ 1
(6)
where p is the padding, h is the input feature map resolution,
and s is the stride. This method enables efficient scaling of
the receptive field without increasing computational cost or
altering the spatial resolution of the feature maps.
D. Position Channel Attention Mechanism (PCAM)
Attention mechanisms in CNNs highlight key image fea-
tures while suppressing irrelevant information, improving
model performance. Existing methods primarily rely on first-
order statistics, limiting their ability to capture complex feature
interactions. To address this, we propose PCAM, as shown
in Figure 7, which incorporates higher-order statistics to
improve the representation of complex activation features, and
is applied to the bottleneck of the MSDet model to enhance
focus on key regions in pulmonary nodule images.
PCAM consists of a positional attention module and a
channel attention module. The positional attention module
captures long-range spatial dependencies by encoding con-
textual information into local features. Given a feature map
Q ∈RC×H×W , we generate two feature maps, R and S, and
compute a spatial attention map U ∈RN×N as follows:
uji =
e(Ri·Sj)
PN
i=1 e(Ri·Sj)
(7)
Here, uji represents the correlation between positions i and
j. The resulting attention-weighted feature map is then added
to the original features, scaled by a learned parameter β:
Vj = β
N
X
i=1
(ujiTi) + Qj
(8)
Similarly, the channel attention module models dependen-
cies between feature channels. Given Q, the channel attention
map Z ∈RC×C is computed as:
zji =
e(Qi·Qj)
PC
i=1 e(Qi·Qj)
(9)
The final feature map is computed as:
Vj = γ
C
X
i=1
(zjiQi) + Qj
(10)
Both β and γ are initialized to 0 and learn appropriate
weights during training. By combining positional and channel
attention, PCAM effectively enhances feature discriminability.
Fig. 7. PCAM Structure. The diagram illustrates the PCAM employing a dual-
module approach to capture and integrate complex feature interactions through
spatial and channel attentions. This innovative mechanism leverages higher-
order statistics to enhance activation feature characterization, optimizing the
performance for intricate image analysis challenges.
IV. EXPERIMENTS
A. Dataset and Evaluation Matrices
The experimental data utilized in this study were obtained
from the publicly available LUNA16 lung CT imaging dataset.
The LUNA16 dataset is derived from the LIDC-IDRI dataset,
with slices thicker than 3 mm and nodules smaller than 3
mm removed, encompassing a total of 888 cases. To facilitate
analysis, we converted the three-dimensional data into two-
dimensional slices.
In terms of evaluation metrics, the performance of the
pulmonary nodule detection model is assessed using several
standard measures. Precision quantifies the accuracy of posi-
tively predicted instances, aiming to minimize false positives.
Recall evaluates the model’s ability to identify all relevant
cases [35], which is crucial for thorough detection. The F1-
score merges precision and recall in a single metric, reflecting
the balance between detection accuracy and completeness.
Average Precision (AP), calculated over the Precision-Recall
curve, provides detailed insights into the model’s precision
across various recall levels. Mean Average Precision (mAP)
averages these AP values across different object classes and
sizes, offering a global view of the model’s overall accuracy.
B. Implementation Details
The proposed MSDet model was implemented using the
PyTorch 1.13.1 framework and trained on an NVIDIA GPU.
The batch size was set to 8, and training spanned 200 epochs.
We employed the SGD optimizer with a momentum of 0.937
and an initial learning rate of 0.01. Data augmentation tech-
niques, including random rotations, flips, brightness, contrast,
and color adjustments, along with salt-and-pepper noise, were
applied to enhance model robustness.
Prior to training, the data underwent a comprehensive pre-
processing phase. The CT image grayscale values were first
converted to Hounsfield Units (HU), which reflect the radio-
density of human tissues, with lung tissue typically around
-500 HU. During preprocessing, we retained regions with HU
values between -1000 and 400 while truncating values outside
this range. The raw data were then clipped to a range of [-1200,
600], setting values below -1200 and above 600 to -1200 and
600 respectively, effectively filtering out elements like water
and air. Lung parenchyma segmentation was performed using
erosion to remove granular regions, followed by dilation to
encompass blood vessels and eliminate black noise from non-
transparent rays within the lung regions. The central part of
the CT image was selected for lung mask extraction, focusing
on the largest connected component, which was further dilated
to fully capture the lung region. If the edges remained uneven,
additional erosion was applied to refine them. Finally, the
preprocessed images were normalized to a range of 0 to 255,
completing the data preprocessing pipeline.
After preprocessing, the performance of the models was
evaluated using standard metrics such as mAP0.5 and mAP
at 0.5 to 0.95. MSDet consistently outperformed models like
YOLOv3, YOLOv7, Scaled YOLOv4, and TPH-YOLOv5,
achieving a 60% mAP at 0.5 to 0.95 after 125 epochs,
demonstrating higher precision and recall with fewer training
iterations.
The lung nodule detection workflow is depicted in Figure
8.
Fig. 8.
Illustration of the pulmonary nodule detection process using the
MSDet model. The sequence shows (a) the initial input CT image, (b)
processed two-dimensional slices after image preprocessing, (c) zoomed-in
views highlighting candidate nodules after candidate detection, and (d) the
final detection results with a confidence score.
C. Comparative Studies
We conducted comparative analyses using the LUNA16
lung nodule detection dataset with state-of-the-art methods, as
shown in Table III. The results demonstrate that our proposed
MSDet algorithm significantly outperforms various leading
lung nodule detection models.
Specifically, MSDet achieved a mAP of 97.3%, surpass-
ing several popular models. Notably, MSDet exceeded the
performance of the two-stage model Faster R-CNN by 6.6
percentage points and outperformed the DLDS-CT method
by 16.1 percentage points. Furthermore, compared to re-
cent YOLO models, MSDet demonstrated improvements of
11.5%, 14.51%, 16.02%, and 8.8% over YOLOv5, YOLOv6,
YOLOv7, and YOLOv8, respectively.
In addition, MSDet outperformed EfficientDet-d, Center-
Net, YOLOv5-CASP, and YOLO-MSRF models, with mAP
improvements of 9.9%, 12%, 25.3%, and 2.7%, respectively.
These results indicate the overall superiority of MSDet in
terms of detection accuracy and efficiency. The experimen-
tal findings validate the robustness and effectiveness of our
proposed approach in lung nodule detection.
D. Ablation Studies
The ablation study results in Table I demonstrate that both
TODB and ERD significantly improve the performance of the
Basic Model on the LUNA16 dataset. TODB alone increased
precision from 83.7% to 88.4%, recall from 81.3% to 87.2%,
and mAP0.5 from 84.7% to 89.1%. Similarly, ERD alone
improved precision to 88.5%, recall to 87.6%, and mAP0.5
to 88.9%. When combined with other modules (ERD, TODB,
and PCAM), these metrics reached their highest values: 97.2%
precision, 96.1% recall, and 97.3% mAP0.5. These results
highlight the significant role of TODB and ERD in enhancing
detection precision, recall, and overall performance in lung
nodule detection.
As shown in Table II and Table I, the PCAM (Ours)
module further enhances pulmonary nodule detection. With
a precision of 93.1%, recall of 92.5%, and an F1 score
of 92.8%, PCAM effectively balances precision and recall
while minimizing false positives. It also achieves superior
mAP scores across various IoU thresholds, demonstrating
robust detection performance for nodules of different sizes.
The integration of position-sensitive and channel attention
mechanisms enhances multi-scale detection, as illustrated in
Figure 9. The ablation study confirms PCAM’s impact on
the Basic Model’s performance on the LUNA16 dataset. With
PCAM alone, precision increased from 83.7% to 87.1%, recall
from 81.3% to 88.7%, and mAP0.5 from 84.7% to 88.3%.
When combined with TODB and ERD, these metrics reached
final values of 97.2% precision, 96.1% recall, and 97.3%
mAP0.5, underscoring PCAM’s effectiveness in boosting de-
tection accuracy and overall performance.
E. Discussion
1) Clinical Impact: MSDet’s multiscale detection and at-
tention mechanisms, including ERD and PCAM, address key
challenges in identifying small and occluded pulmonary nod-
ules, which are often missed due to their size and complex
surroundings. By enhancing detection accuracy and reducing
TABLE I
ABLATION STUDY OF MSDET ON THE LUNA16 DATASET, EVALUATING THE IMPACT OF DIFFERENT MODULES (TODB, ERD, AND PCAM) ON
DETECTION PERFORMANCE. THE TABLE PRESENTS THE PRECISION, RECALL, AND MAP0.5 RESULTS FOR VARIOUS CONFIGURATIONS OF THE MSDET
MODEL, DEMONSTRATING SIGNIFICANT IMPROVEMENTS WHEN ALL MODULES ARE COMBINED.
YOLOv5
TODB
ERD
PCAM
Precision (%)
Recall (%)
mAP0.5 (%)
✓
83.7 ± 0.09
81.3 ± 0.07
84.7 ± 0.03
✓
✓
88.4 ± 0.04
87.2 ± 0.05
89.1 ± 0.08
✓
✓
88.5 ± 0.02
87.6 ± 0.06
88.9 ± 0.04
✓
✓
87.1 ± 0.09
88.7 ± 0.05
88.3 ± 0.07
✓
✓
✓
91.6 ± 0.08
90.1 ± 0.06
92.8 ± 0.02
✓
✓
✓
91.4 ± 0.04
91.2 ± 0.03
92.5 ± 0.08
✓
✓
✓
91.2 ± 0.09
91.4 ± 0.05
91.9 ± 0.01
✓
✓
✓
✓
97.2 ± 0.06
96.1 ± 0.05
97.3 ± 0.03
TABLE II
DETECTION PERFORMANCE COMPARISON USING DIFFERENT ATTENTION MODULES ON THE LUNA16 DATASET. THIS TABLE PRESENTS PRECISION,
RECALL, F1 SCORE, AND MAP RESULTS ACROSS VARIOUS IOU THRESHOLDS AND NODULE SIZES, DEMONSTRATING THE SUPERIOR PERFORMANCE OF
THE PCAM (OURS) MODULE COMPARED TO OTHER STATE-OF-THE-ART ATTENTION MECHANISMS.
Methods
Precision(%)
Recall(%)
F1Score(%)
mAP0.5 (%)
mAP0.75 (%)
mAP0.95 (%)
mAPS (%)
mAPM (%)
mAPL (%)
SE [36]
89.5 ± 0.02
90.1 ± 0.04
89.8 ± 0.06
89.7 ± 0.03
79.2 ± 0.08
60.7 ± 0.09
67.4 ± 0.1
74.1 ± 0.02
80.1 ± 0.08
CBAM [37]
91.4 ± 0.1
91.9 ± 0.03
91.6 ± 0.08
91.7 ± 0.03
81.1 ± 0.1
63.2 ± 0.04
68.1 ± 0.07
75.2 ± 0.05
81.4 ± 0.02
SA [38]
89.8 ± 0.06
90.6 ± 0.07
90.2 ± 0.01
90.2 ± 0.09
79.8 ± 0.08
61.1 ± 0.03
67.1 ± 0.03
74.0 ± 0.06
80.5 ± 0.01
NAM [39]
88.6 ± 0.1
89.1 ± 0.06
88.8 ± 0.03
88.4 ± 0.02
79.1 ± 0.08
60.4 ± 0.08
67.3 ± 0.09
74.7 ± 0.04
80.3 ± 0.07
CA [40]
90.3 ± 0.04
89.7 ± 0.07
89.9 ± 0.08
90.1 ± 0.09
79.4 ± 0.03
61.6 ± 0.04
67.6 ± 0.02
75.1 ± 0.05
81.0 ± 0.09
ECA [41]
89.5 ± 0.09
90.2 ± 0.06
89.8 ± 0.06
89.7 ± 0.03
79.6 ± 0.07
60.8 ± 0.02
67.9 ± 0.08
74.1 ± 0.1
80.2 ± 0.1
NLNN [42]
90.4 ± 0.1
90.6 ± 0.03
89.6 ± 0.05
89.2 ± 0.02
79.4 ± 0.07
61.2 ± 0.09
67.2 ± 0.05
74.9 ± 0.04
81.1 ± 0.08
GAT [43]
89.2 ± 0.03
89.7 ± 0.01
89.4 ± 0.05
89.8 ± 0.09
80.5 ± 0.1
60.7 ± 0.07
67.0 ± 0.04
74.6 ± 0.02
80.7 ± 0.08
BAM [44]
89.8 ± 0.07
90.2 ± 0.08
90.2 ± 0.07
90.5 ± 0.04
80.1 ± 0.02
61.4 ± 0.06
67.9 ± 0.1
74.8 ± 0.05
81.0 ± 0.01
MAB [15]
90.9 ± 0.02
91.2 ± 0.05
91.1 ± 0.03
91.3 ± 0.06
79.9 ± 0.09
61.9 ± 0.08
68.1 ± 0.02
75.1 ± 0.03
80.2 ± 0.1
CCA [45]
91.4 ± 0.1
91.8 ± 0.07
91.6 ± 0.02
91.5 ± 0.09
80.2 ± 0.01
62.3 ± 0.03
68.0 ± 0.05
74.4 ± 0.04
80.4 ± 0.06
SOCA [46]
90.7 ± 0.07
90.3 ± 0.06
90.5 ± 0.02
90.3 ± 0.1
80.6 ± 0.03
61.7 ± 0.1
68.2 ± 0.04
75.0 ± 0.08
81.5 ± 0.04
PCAM (Ours)
93.1 ± 0.05
92.5 ± 0.04
92.8 ± 0.07
93.7 ± 0.05
83.7 ± 0.1
65.8 ± 0.03
70.6 ± 0.03
77.1 ± 0.06
83.7 ± 0.07
TABLE III
COMPARISON OF MSDET WITH STATE-OF-THE-ART LUNG NODULE
DETECTION NETWORKS IN TERMS OF MAP. THE TABLE HIGHLIGHTS THE
SUPERIOR PERFORMANCE OF MSDET COMPARED TO WIDELY-USED
MODELS SUCH AS YOLO, FASTER R-CNN, AND LUNGSEEK,
DEMONSTRATING THE ROBUSTNESS OF OUR APPROACH IN ACHIEVING
HIGHER DETECTION ACCURACY.
Model
mAP
YOLOv5 [47]
85.8%
YOLOv6 [48]
82.79%
YOLOv7 [49]
81.28%
YOLOv8 [50]
88.5%
Faster R-CNN [29]
90.7%
Two-Stage CNN [51]
84.4%
LungSeek [52]
91.75%
DLDS-CT [53]
81.2%
EfficientDet-d [54]
87.4%
CenterNet [55]
85.3%
STBi-YOLO [56]
95.9%
YOLOv5-CASP [57]
72.0%
YOLO-MSRF [33]
94.6%
MSDet (Ours)
97.3%
false positives, MSDet assists radiologists in early malignancy
identification, boosting diagnostic confidence and enabling
timely interventions. Its integration into clinical workflows can
streamline lung cancer screening by automating nodule detec-
tion, reducing the radiologists’ workload, and allowing them
to focus on critical cases. MSDet’s lightweight architecture
supports real-time deployment without compromising speed
or accuracy, making it well-suited for high-volume imaging
environments. As screening programs expand, MSDet can
help prioritize urgent cases and ease the burden on healthcare
systems.
2) Contributions to Early Diagnosis and Broader Societal
Impact: MSDet’s precise detection of small pulmonary nod-
ules facilitates earlier lung cancer diagnoses, improving sur-
vival rates and enabling less invasive treatments. By reducing
false positives, it minimizes unnecessary follow-ups, alleviat-
ing patient stress and lowering healthcare costs. This enhanced
accuracy streamlines the diagnostic process, making healthcare
more efficient. Globally, MSDet can have a transformative im-
pact, especially in regions with limited access to radiologists.
By providing high-quality automated screening, it helps bridge
healthcare disparities, ensuring timely and accurate diagnoses
for all patients. Ultimately, MSDet contributes to improved
patient outcomes, reduced lung cancer mortality, and lower
healthcare burdens.
Fig. 9. Comparison chart of mAPS–mAPL–mAP0.5. The y-axis corresponds to
mAP0.5 (mean Average Precision at IoU threshold of 0.5), where higher values
indicate better performance. The x-axis represents the detection performance
of micro-small and relatively large pulmonary nodules. Our method, PCAM,
achieved the best balance between performance and accuracy in multi-scale
pulmonary nodule detection.
F. Visualization
In this section, we present a visual comparison of the detec-
tion results to validate the effectiveness of the proposed MSDet
model for lung nodule detection. As illustrated in Figure 10,
MSDet consistently outperforms YOLOv4, Scaled YOLOv4,
and the Basic Model across various scenarios, including both
single and multiple nodule detections. For instance, MSDet
achieved 94% accuracy in detecting nodules in complex
imaging backgrounds, while YOLOv4 and Scaled YOLOv4
exhibited lower accuracies due to false positives and false
negatives. In multi-nodule cases, MSDet demonstrated supe-
rior precision, accurately identifying all nodules without false
positives or false negatives, outperforming the other models
by 6% to 8%. These results emphasize the robustness and
accuracy of MSDet, especially in handling complex imaging
conditions and challenging nodules, making it highly reliable
for clinical applications.
Lung parenchyma segmentation further enhances detec-
tion accuracy by providing cleaner image backgrounds. This
process involves renormalizing the images, using K-means
clustering to separate lung tissue from the background, and
refining the lung mask through erosion and dilation. As shown
in Figure 11, models such as YOLOv4, Scaled YOLOv4, the
Fig. 10. Visual comparison of lung nodule detection results on the LUNA16
dataset, highlighting MSDet’s superior performance compared with other
models. The figure illustrates MSDet’s effectiveness in reducing false pos-
itives and achieving higher accuracy, showcasing its robustness for clinical
applications.
Basic Model, and MSDet demonstrate improved performance
post-segmentation compared to raw data. MSDet, in particular,
achieved up to 98% accuracy in detecting multiple nodules,
reducing both false positives and false negatives. Overall,
lung parenchyma segmentation significantly boosts detection
precision across all models, highlighting its importance in
complex imaging tasks.
V. CONCLUSION
This paper proposes a novel method, MSDet, for lung nod-
ule detection in CT images, specifically designed to address
multi-scale nodule detection and ensure accurate identification.
To overcome the limitations of traditional algorithms in de-
tecting small nodules, we introduced TODB, which enhances
feature extraction for tiny lung nodules. Additionally, we
designed an ERD strategy to refine occluded nodule features
by constructing an effective receptive field, enabling accurate
detection of occluded nodules. Furthermore, we proposed a
new attention mechanism, PCAM, which adjusts attention
weights based on the importance of different positions and
channels. This optimizes feature representation learning and
improves adaptability to nodules of varying scales, thereby
reducing missed and false detections. In summary, MSDet
presents a novel framework for multi-scale tiny nodule de-
tection. Empirical validation demonstrates that our model not
only detects small nodules effectively in complex backgrounds
but also surpasses existing benchmarks on the public LUNA16
Fig. 11. Visual comparison of lung nodule detection results on the LUNA16
dataset, illustrating the impact of lung parenchyma segmentation on improving
detection accuracy. MSDet shows a notable reduction in false positives and an
accuracy of up to 98% in complex scenarios, highlighting the effectiveness
of segmentation in enhancing the clarity and precision of nodule detection
across different algorithms.
dataset. This study marks a significant advancement in medical
image detection, with the potential to profoundly influence
future medical image processing endeavors.
ACKNOWLEDGMENT
This work was supported by the National Natural Science
Foundation of China (Grant No. 62306027) and Southwest
Minzu University. We would like to express our gratitude to
our collaborators for their valuable contributions and support
throughout this study.
REFERENCES
[1] R. L. Siegel, A. N. Giaquinto, A. Jemal, Cancer statistics, 2024, CA: a
cancer journal for clinicians 74 (1) (2024) 12–49.
[2] E. R. N´u˜nez, S. Zhang, M. E. Glickman, S. X. Qian, J. H. Boudreau,
P. K. Lindenauer, C. G. Slatore, D. R. Miller, T. J. Caverly, R. S.
Wiener, What goes into patient selection for lung cancer screening?
factors associated with clinician judgments of suitability for screening,
American journal of respiratory and critical care medicine 209 (2) (2024)
197–205.
[3] T. L. Leong, A. McWilliams, G. M. Wright, Incidental pulmonary
nodules: An opportunity to complement lung cancer screening, Journal
of Thoracic Oncology 19 (4) (2024) 522–524.
[4] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies for
accurate object detection and semantic segmentation, in: Proceedings of
the IEEE conference on computer vision and pattern recognition, 2014,
pp. 580–587.
[5] R. Girshick, Fast r-cnn, in: Proceedings of the IEEE international
conference on computer vision, 2015, pp. 1440–1448.
[6] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time
object detection with region proposal networks, IEEE transactions on
pattern analysis and machine intelligence 39 (6) (2016) 1137–1149.
[7] K. He, G. Gkioxari, P. Doll´ar, R. Girshick, Mask r-cnn, in: Proceedings
of the IEEE international conference on computer vision, 2017, pp.
2961–2969.
[8] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once:
Unified, real-time object detection, in: Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, 2016, pp. 779–788.
[9] J. Redmon, A. Farhadi, Yolo9000: better, faster, stronger, in: Proceedings
of the IEEE conference on computer vision and pattern recognition,
2017, pp. 7263–7271.
[10] J. Redmon, A. Farhadi, Yolov3: An incremental improvement, arXiv
preprint arXiv:1804.02767 (2018).
[11] A. Bochkovskiy, C.-Y. Wang, H.-Y. M. Liao, Yolov4: Optimal speed and
accuracy of object detection, arXiv preprint arXiv:2004.10934 (2020).
[12] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C.
Berg, Ssd: Single shot multibox detector, in: Computer Vision–ECCV
2016: 14th European Conference, Amsterdam, The Netherlands, October
11–14, 2016, Proceedings, Part I 14, Springer, 2016, pp. 21–37.
[13] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, Q. Tian, Centernet:
Keypoint triplets for object detection, in: Proceedings of the IEEE/CVF
international conference on computer vision, 2019, pp. 6569–6578.
[14] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Doll´ar, Focal loss for dense
object detection, in: Proceedings of the IEEE international conference
on computer vision, 2017, pp. 2980–2988.
[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural
information processing systems 30 (2017).
[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,
An image is worth 16x16 words: Transformers for image recognition at
scale, in: International Conference on Learning Representations, 2020.
[17] Y. Ji, H. Saratchandran, C. Gordon, Z. Zhang, S. Lucey, Sine acti-
vated low-rank matrices for parameter efficient learning, arXiv preprint
arXiv:2403.19243 (2024).
[18] A. A. A. Setio, A. Traverso, T. De Bel, M. S. Berens, C. Van Den Bo-
gaard, P. Cerello, H. Chen, Q. Dou, M. E. Fantacci, B. Geurts, et al.,
Validation, comparison, and combination of algorithms for automatic
detection of pulmonary nodules in computed tomography images: the
luna16 challenge, Medical image analysis 42 (2017) 1–13.
[19] J. Ge, Z. Zhang, M. H. Phan, B. Zhang, A. Liu, Y. Zhao, Esa:
Annotation-efficient active learning for semantic segmentation, arXiv
preprint arXiv:2408.13491 (2024).
[20] B. Wu, Y. Xie, Z. Zhang, M. H. Phan, Q. Chen, L. Chen, Q. Wu, Xlip:
Cross-modal attention masked modelling for medical language-image
pre-training, arXiv preprint arXiv:2407.19546 (2024).
[21] Y. Zhao, Z. Liao, Y. Liu, K. O. Nijhuis, B. Barvelink, J. Prijs, J. Colaris,
M. Wijffels, M. Reijman, Z. Zhang, et al., A landmark-based approach
for instability prediction in distal radius fractures, in: 2024 IEEE
International Symposium on Biomedical Imaging (ISBI), IEEE, 2024,
pp. 1–5.
[22] A. D. Hiwase, C. D. Ovenden, L. M. Kaukas, M. Finnis, Z. Zhang,
S. O’Connor, N. Foo, B. Reddi, A. J. Wells, D. Y. Ellis, Can rotational
thromboelastometry rapidly identify theragnostic targets in isolated
traumatic brain injury?, Emergency Medicine Australasia (2024).
[23] Z. Zhang, X. Qi, M. Chen, G. Li, R. Pham, A. Qassim, E. Berry, Z. Liao,
O. Siggs, R. Mclaughlin, et al., Jointvit: Modeling oxygen saturation
levels with joint supervision on long-tailed octa, in: Annual Conference
on Medical Image Understanding and Analysis, Springer, 2024, pp. 158–
172.
[24] B. Wu, Y. Xie, Z. Zhang, J. Ge, K. Yaxley, S. Bahadir, Q. Wu, Y. Liu, M.-
S. To, Bhsd: A 3d multi-class brain hemorrhage segmentation dataset,
in: International Workshop on Machine Learning in Medical Imaging,
Springer, 2023, pp. 147–156.
[25] Z. Zhang, X. Qi, B. Zhang, B. Wu, H. Le, B. Jeong, Z. Liao, Y. Liu,
J. Verjans, M.-S. To, et al., Segreg: Segmenting oars by registering mr
images and ct annotations, in: 2024 IEEE International Symposium on
Biomedical Imaging (ISBI), IEEE, 2024, pp. 1–5.
[26] S. Tan, Z. Zhang, Y. Cai, D. Ergu, L. Wu, B. Hu, P. Yu, Y. Zhao,
Segstitch: Multidimensional transformer for robust and efficient medical
imaging segmentation, arXiv preprint arXiv:2408.00496 (2024).
[27] Z. Zhang, B. Zhang, A. Hiwase, C. Barras, F. Chen, B. Wu, A. J. Wells,
D. Y. Ellis, B. Reddi, A. W. Burgan, M.-S. To, I. Reid, R. Hartley,
Thin-thick adapter: Segmenting thin scans using thick annotations,
OpenReview (2023).
[28] Z. Zhang, N. Yi, S. Tan, Y. Cai, Y. Yang, L. Xu, Q. Li, Z. Yi, D. Ergu,
Y. Zhao, Meddet: Generative adversarial distillation for efficient cervical
disc herniation detection, arXiv preprint arXiv:2409.00204 (2024).
[29] J. Xu, H. Ren, S. Cai, X. Zhang, An improved faster r-cnn algorithm for
assisted detection of lung nodules, Computers In Biology And Medicine
153 (2023) 106470.
[30] Y. Su, D. Li, X. Chen, Lung nodule detection based on faster r-
cnn framework, Computer Methods and Programs in Biomedicine 200
(2021) 105866.
[31] C. Tong, B. Liang, M. Zhang, R. Chen, A. K. Sangaiah, Z. Zheng,
T. Wan, C. Yue, X. Yang, Pulmonary nodule detection based on isodata-
improved faster rcnn and 3d-cnn with focal loss, ACM Transactions on
Multimedia Computing, Communications, and Applications (TOMM)
16 (1s) (2020) 1–9.
[32] N. Venkateswarlu, P. Raju, Fast isodata clustering algorithms, Pattern
recognition 25 (3) (1992) 335–342.
[33] X. Wu, H. Zhang, J. Sun, S. Wang, Y. Zhang, Yolo-msrf for lung nodule
detection, Biomedical Signal Processing and Control 94 (2024) 106318.
[34] Z. Ji, J. Zhao, J. Liu, X. Zeng, H. Zhang, X. Zhang, I. Ganchev, Elct-
yolo: an efficient one-stage model for automatic lung tumor detection
based on ct images, Mathematics 11 (10) (2023) 2344.
[35] Z. Zhang, K. A. Ahmed, M. R. Hasan, T. Gedeon, M. Z. Hossain, A
deep learning approach to diabetes diagnosis, in: Asian Conference on
Intelligent Information and Database Systems, Springer, 2024, pp. 87–
99.
[36] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks, in: Proceed-
ings of the IEEE conference on computer vision and pattern recognition,
2018, pp. 7132–7141.
[37] S. Woo, J. Park, J.-Y. Lee, I. S. Kweon, Cbam: Convolutional block at-
tention module, in: Proceedings of the European conference on computer
vision (ECCV), 2018, pp. 3–19.
[38] Q.-L. Zhang, Y.-B. Yang, Sa-net: Shuffle attention for deep convolutional
neural networks, in: ICASSP 2021-2021 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp.
2235–2239.
[39] Y. Liu, Z. Shao, Y. Teng, N. Hoffmann, Nam: Normalization-based
attention module, arXiv preprint arXiv:2111.12419 (2021).
[40] Q. Hou, D. Zhou, J. Feng, Coordinate attention for efficient mobile net-
work design, in: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, 2021, pp. 13713–13722.
[41] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, Q. Hu, Eca-net: Efficient channel
attention for deep convolutional neural networks, in: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, 2020,
pp. 11534–11542.
[42] X. Wang, R. Girshick, A. Gupta, K. He, Non-local neural networks,
in: Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018, pp. 7794–7803.
[43] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, Y. Bengio,
Graph attention networks, arXiv preprint arXiv:1710.10903 (2017).
[44] J. Park, S. Woo, J.-Y. Lee, I. S. Kweon, Bam: Bottleneck attention
module, arXiv preprint arXiv:1807.06514 (2018).
[45] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, W. Liu, Ccnet:
Criss-cross attention for semantic segmentation, in: Proceedings of the
IEEE/CVF international conference on computer vision, 2019, pp. 603–
612.
[46] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, L. Zhang, Second-order atten-
tion network for single image super-resolution, in: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, 2019,
pp. 11065–11074.
[47] A. V. EGA, W. ARDIATNA, Study on image processing method
and data augmentation for chest x-ray nodule detection with yolov5
algorithm, ELKOMIKA: Jurnal Teknik Energi Elektrik, Teknik Teleko-
munikasi, & Teknik Elektronika 11 (2) (2023) 424.
[48] L. Goel, P. Patel, Improving yolov6 using advanced pso optimizer for
weight selection in lung cancer detection and classification, Multimedia
Tools and Applications (2024) 1–34.
[49] S. Mammeri, M. Amroune, M.-Y. Haouam, I. Bendib, A. Corrˆea Silva,
Early detection and diagnosis of lung cancer using yolo v7, and transfer
learning, Multimedia Tools and Applications 83 (10) (2024) 30965–
30980.
[50] C. S¸aman, S¸. C¸ elikbas¸, Yolov8-based lung nodule detection: A novel
hybrid deep learning model proposal, International Research Journal of
Engineering and Technology 10 (8) (2023) 230–237.
[51] S. Jain, P. Choudhari, M. Gour, Pulmonary lung nodule detection
from computed tomography images using two-stage convolutional neural
network, The Computer Journal 66 (4) (2023) 785–795.
[52] H. Zhang, H. Zhang, Lungseek: 3d selective kernel residual network
for pulmonary nodule diagnosis, The Visual Computer 39 (2) (2023)
679–692.
[53] H. Lu, K. Liu, H. Zhao, Y. Wang, B. Shi, Dual-layer detector spectral
ct-based machine learning models in the differential diagnosis of solitary
pulmonary nodules, Scientific Reports 14 (1) (2024) 4565.
[54] M. Tan, R. Pang, Q. V. Le, Efficientdet: Scalable and efficient object
detection, in: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, 2020, pp. 10781–10790.
[55] X. Zhou, D. Wang, P. Kr¨ahenb¨uhl, Objects as points, arXiv preprint
arXiv:1904.07850 (2019).
[56] K. Liu, Stbi-yolo: A real-time object detection method for lung nodule
recognition, IEEE Access 10 (2022) 75385–75394.
[57] Z. Ji, Y. Wu, X. Zeng, Y. An, L. Zhao, Z. Wang, I. Ganchev, Lung
nodule detection in medical images based on improved yolov5s, IEEE
Access (2023).
APPENDIX
DETAILED ANALYSIS OF TRAINING PROGRESS
The training curves shown in Figure 12 compare the per-
formance of the MSDet model against YOLOv3, YOLOv7,
Scaled YOLOv4, and TPH-YOLOv5 across 200 epochs using
metrics such as mAP0.5 and mAP from 0.5 to 0.95. MSDet
demonstrates rapid convergence and superior performance,
achieving a 60% mAP at 0.5 to 0.95 by epoch 125, high-
lighting its efficient learning and adaptation capabilities.
The graph presents key metrics including mAP at IoU 0.5,
mAP from 0.5 to 0.95, precision, and recall. These metrics
illustrate MSDet’s consistently higher precision and recall,
maintaining smooth and stable progress. The model’s early
high performance, especially in mAP at IoU 0.5, underscores
its effective adaptation to complex image data with fewer
epochs compared to other models.This enhanced performance
is due to MSDet’s sophisticated feature extraction and opti-
mized learning strategies.
Fig. 12.
Training performance comparison of MSDet with YOLO variants
across 200 epochs. The graph demonstrates MSDet’s superior performance
with a clear lead in mAP scores from IoU 0.5 to 0.95, underscoring its
rapid convergence and higher accuracy in detecting more complex features
effectively.
