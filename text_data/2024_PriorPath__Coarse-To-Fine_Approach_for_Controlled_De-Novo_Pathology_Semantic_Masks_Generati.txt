PRIORPATH: COARSE-TO-FINE APPROACH FOR CONTROLLED
DE-NOVO PATHOLOGY SEMANTIC MASKS GENERATION
Nati Daniel1
Technion-IIT, Israel
May Nathan2
Technion – IIT, Israel
Eden Azeroual2
Technion – IIT, Israel
Yael Fisher3
Rambam Health Care Campus, Israel
Yonatan Savir1,2,∗
Technion-IIT, Israel
ABSTRACT
Incorporating artificial intelligence (AI) into digital pathology offers promising prospects for automat-
ing and enhancing tasks such as image analysis and diagnostic processes. However, the diversity
of tissue samples and the necessity for meticulous image labeling often result in biased datasets,
constraining the applicability of algorithms trained on them. To harness synthetic histopathological
images to cope with this challenge, it is essential not only to produce photorealistic images but also
to be able to exert control over the cellular characteristics they depict. Previous studies used methods
to generate, from random noise, semantic masks that captured the spatial distribution of the tissue.
These masks were then used as a prior for conditional generative approaches to produce photorealistic
histopathological images. However, as with many other generative models, this solution exhibits
mode collapse as the model fails to capture the full diversity of the underlying data distribution. In
this work, we present a pipeline, coined PriorPath, that generates detailed, realistic, semantic masks
derived from coarse-grained images delineating tissue regions. This approach enables control over
the spatial arrangement of the generated masks and, consequently, the resulting synthetic images.
We demonstrated the efficacy of our method across three cancer types, skin, prostate, and lung,
showcasing PriorPath’s capability to cover the semantic mask space and to provide better similarity
to real masks compared to previous methods. Our approach allows for specifying desired tissue
distributions and obtaining both photorealistic masks and images within a single platform, thus
providing a state-of-the-art, controllable solution for generating histopathological images to facilitate
AI for computational pathology.
Keywords Deep Learning, Histopathology Image Generation, Image Translation, Representation Learning, Tissue
Mask Generation.
1
Introduction
The increasing use of digital and computational biology, along with the incorporation of AI and machine learning, is
significantly impacting the field of medicine [1]. By analyzing vast amounts of biological data, these technologies
enhance diagnosis accuracy, optimize treatment assignment, and pave the way for personalized medicine. By analyzing
vast amounts of biological data, these technologies enhance diagnosis accuracy, optimize treatment assignment, and
pave the way for personalized medicine [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]. AI algorithms help pathologists process
and interpret large volumes of data more quickly, contributing to faster and more precise diagnoses [12]. Moreover, AI
can be used to identify patterns and anomalies that lead to inferences of new biomarkers [15] and generate predictive
insights [11].
∗Corresponding author, e-mail: yoni.savir@technion.ac.il. 1Department of Physiology, Biophysics, and Systems Biology, Faculty
of Medicine, Technion - IIT, Haifa, Israel. 2Network Biology Research Lab, Faculty of Electrical and Computer Engineering,
Technion – IIT, Haifa, Israel 3 Division of Pathology, Rambam Health Care Campus, Haifa, Israel.
arXiv:2411.16515v1  [eess.IV]  25 Nov 2024
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
Figure 1: Analysis and Illustration of Generative Approaches for High-Resolution Binary Semantic Masks of Tissue
Structure. (a) DCGAN approach: Generates synthetic histopathological fine semantic masks from random noise,
requiring real histopathological semantic masks for discriminator processing during training. (b) DEAPS method
(current SOTA): Enhances DCGAN with discrete adaptive block final activation, multi-scale discriminators, and
spatial noise within hidden layers. (c) PriorPath approach (this work): Produces complex, realistic semantic masks
from coarse-grained semantic masks of regions of interest. (d) Demonstration of DEPAS mode collapse: t-SNE
projection of Inception’s representations for the DEPAS method (blue) and the Real masks (black). The dataset includes
three cancer types with H&E staining: Skin Cutaneous Melanoma (SKCM), Prostate Adenocarcinoma (PRAD), and
Lung Squamous Cell Carcinoma (LUSC), along with a Non-small cell lung carcinoma (NSCLC) dataset with PD-L1
immunohistochemistry.
Yet, the development and deployment of AI applications are limited by data availability. Specifically, in computational
pathology, the shortage of verified, unbiased, datasets compromises the accuracy and generalizability of AI algorithms in
unraveling the complexities of histopathological images and enabling precise diagnostic decision-making [16, 17]. This
data scarcity serves as the driving force behind the generation of synthetic histopathological images [18, 19, 20, 21, 22]
by using Generative Adversarial Networks (GANs) [23].
There are two main approaches to generating histopathological images. The first is to generate images from a noise
[23]. The second uses conditional GANs (cGANs) that recive some prior as in input, particularly cGANs that receive a
semantic label mask as an input and then translate it to an image [36]. The first approach can produce an unlimited
number of synthetic images but lacks the ability to control the distribution of cellular features directly. The second
approach provides the ability to control the cellular features of the synthetic images precisely, but its scalability is
bounded by the quantity and quality of the semantic masks. Manually producing highly detailed synthetic tissue masks
is equally impractical. These limitations highlight the importance of producing high-quality synthetic masks, therefore
underlining the need within histopathology to address this challenge by generating fine-grained semantic masks that
closely replicate the distribution of real data.
Various net architectures, such as the Deep Convolutional Generative Adversarial Network (DCGAN) [24] and De-novo
Pathology Semantic Masks using a generative adversarial model (DEPAS) [25], have been developed for synthetic
mask generation by sampling noise from a given distribution. DCGAN is a common architecture model for creating
de-novo semantic masks (Fig. 1a), but the model’s results are repetitive semantic masks and, as such, are not fully for
histopathology [25]. DEPAS solves this limitation by introducing three main novel additions: injecting spatial noise
into each hidden layer in the generator network, adapting multi-scale discriminators, and using a discrete adaptive block
to generate synthetic fine tissue masks (Fig. 1b). Nevertheless, despite the unbiased wide distribution of the training set
within the entire mask space, the model exhibits some mode collapse (Fig. 1d). That is, synthetic masks do not cover
the entire physiological semantic mask space.
Here, to overcome this challenge, we developed a novel approach based on Image-to-Image (I2I) Translation architecture,
coined PriorPath, that is able to generate complex, realistic synthetic masks based on a coarse grain spatial distribution
of features. We condition the model with manually drawn coarse semantic masks instead of just generating them from
noise. These coarse-grain masks are used to generate a fine-grained mask that can then in turn be used to generate more
diverse and realistic synthetic histopathological images. This provides the pathologists with an interactive control of the
general visual features of the resulting image. Hence, our approach combines scalability, realistic semantic labeling,
and controllability (Fig. 1c).
2
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
2
MATERIALS AND METHODS
2.1
Study population and dataset
In this study, we use four histopathology imaging datasets with two different staining techniques. Hematoxylin and
eosin (H&E) histopathological images derived from three specific cancer types: Prostate Adenocarcinoma (PRAD),
Skin Cutaneous Melanoma (SKCM), and Lung Squamous Cell Carcinoma (LUSC). These data sets are sourced
from the Cancer Genome Atlas (TCGA) research network [26], where only the imaging information was utilized for
all data types. The second staining technique, known as immunohistochemistry (IHC), involved the collection of
histopathological images from patients diagnosed with non-small cell lung carcinoma (NSCLC). This data originated
from a study focused on Immune Checkpoint inhibitor therapy, wherein the identification of programmed death-ligand 1
(PD-L1) in tissue biopsies was crucial for treatment determination [27]. All Whole Slide Images (WSIs) were obtained
from NSCLC patients who underwent a biopsy at the Rambam Health Care Campus. All procedures performed in this
study and involving human participants followed the ethical standards of the institutional research committee of the
Rambam Medical Center, approval 0522-10-RMB, and with the Declaration of Helsinki of 1964 and its subsequent
amendments or comparable ethical standards.
Across all instances, the whole slides, 50 for each H&E staining, and 55 for IHC PD-L1, were partitioned into patches
with dimensions of 512 x 1024 pixels. Patches predominantly comprised of background elements exceeding 85%
coverage were excluded from the analysis. Consequently, a total of ∼6559 −6983 images were utilized from each
H&E dataset, while 5500 images were extracted from the IHC dataset. In all instances, we generated ground truth for
binary tissue semantic masks by converting the patches to grayscale, subsequently applying a high threshold to extract
air pixels. The optimal thresholds identified to distinguish between tissue and air pixels were determined to be 204 and
235, within the range of 0-255, for H&E and IHC, respectively. Table 1 summarizes the main parameters of all the
instances, including the splitting information that was employed for training and evaluation purposes.
Table 1: Main parameters of all four datasets used in the training and evaluation processes.
Parameter / Dataset
PRAD [26]
SKCM [26]
LUSC [26]
NSCLC [19]
# of slides
50
50
50
55
# of participants
50
50
50
4
# of train patches
5,983
5,759
5,559
4,000
# of test patches
1,000
1,000
1,000
1,500
Patch size
512 x 1024
512 x 1024
512 x 1024
512 x 1024
Staining method
H&E
H&E
H&E
IHC
Gender
Male
Male & Female
Male & Female
Male
Year
2015
2015
2015
2019
2.2
Image quality assessment metrics
To quantize the performance of tissue mask generation, we employed three different similarity distance metrics such as
Frechet inception distance (FID) [28], Kolmogorov–Smirnov (K-S) test [29], and Kullback–Leibler (K-L) divergence
[30].
2.3
Generating Paired Coarse-Fine Grain Masks for Training
To be able to train I2I models [31, 32] for various cancer types described in [26], we had to create paired coarse and
fine grains semantic masks. The way we handled it, is to execute over the ground truth of binary tissue semantic masks
(explained in Subsection 2.1). Then, for each fine-grain semantic mask, we applied a set of morphological operations to
create its pair of coarse-grain semantic mask. In particular, we performed a two-stage morphological process; the first
stage is an erosion followed by a dilation, with an opening kernel in size of 5 x 5 pixels. The second stage is a dilation
followed by an erosion, with a closing kernel in size of 10 x 10 pixels.
2.4
Labeling Manual-made Synthetic Semantic Masks for Evaluation
One of the main goals of our study was to equip pathologists with a semantic tool enabling the creation of RGB
tissue images with high controllability. Hence, we suggest generating images based on manually made coarse masks.
3
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
Pathologists have extensive background knowledge regarding natural tissue structure. To compensate for our limited
familiarity with the basic formation of tissues, as a starting point, the raw coarse rudimentary masks were manually
annotated on top of the original RGB tissue images, using an instrument named Labelme [33].
We aimed to create high-quality, semantically richer representations of biological tissue masks. To achieve this, we
developed a method that generates synthetic fine-grained semantic masks that closely match the distribution of real
masks obtained from RGB tissue images. To learn the real mask distribution, we applied K-means - an iterative
algorithm that classifies data into clusters based on centroids and latent distance. For optimal outcomes, we tuned the
hyper-parameter that fixates the number of clusters by visual evaluation. Using K-means clustering, we grouped the
data based on the prominent visual similarities shared by the masks within each cluster. Then, based on these clusters,
we manually drew on an iPad and a smartphone using the Madibeng [34] and Samsung PENUP [35] digital platforms,
where the art of painting was carried out with varying levels of accuracy; consequently, the results represent trustworthy
scribbles of individual pathologists.
Eventually, this resulted in about 100 raw binary sketches per cancer type. To demonstrate the scalability of our
approach, we applied extensive augmentations to the masks, generating a large repository of coarse semantic masks for
each cancer type. Over these outputs, we performed additional processing, such as binarization and resizing.
2.5
The CycleGAN and pix2pix formulation
In this work, we have integrated both pix2pix [31] and CycleGAN [32] in our framework. Both are cGAN frameworks
[36] for I2I translation tasks, to generate synthetic histopathological masks that achieve state-of-the-art (SOTA) results
of fine geometry / pattern mask details and realistic textures. The architecture of pix2pix and CycleGAN is based
on the GAN paradigm with one main modification of conditional input to the generator’s loss function. The pix2pix
architecture includes a generator network, which is based on an Encoder-Decoder architecture, a U-Net [37] (like
models with skip connections), that aims to learn the deep representation of the input mask and then decode it. In
addition, it includes a part of the discriminator network, which is based on PatchGAN that takes a patch mask N · N
and predicts for every pixel of the patch whether it belongs to a real mask or a synthetic mask. In particular, the inputs
of the pix2pix generator, G, architecture are: noise z, coarse grain mask of size 512 x 1024 pixels, x. y is the synthetic
fine-grain mask output of size 512 x 1024 pixels, such that x and y are data pairs. In Discriminator, D, architecture,
we take the pair of ordered coarse and synthetic fine grain masks, x and y, and try to train the network to distinguish
between them. As a result, the G is forced to study the true realistic distribution of information and to reduce the loss
from the discriminator. Thus, it obtains higher-quality masks even in the smallest geometry/pattern details.
Hence, the objective of the pix2pix model is expressed as:
min
G max
D LcGAN[(G, D)] + λ · LL1[(G)]
(1)
where G is a generator, D is a discriminator, and λ represents a regularization parameter.
LcGAN(G, D) is conditional GAN loss (2), LL1(G) is a L1 distance loss (3).
Ex,y[log D(x, y)] + Ex,z[log (1 −D(x, G(x, z))]
(2)
Ex,y,z[||y −G(x, z)||1]
(3)
where y represents the fine grain mask, x is the coarse grain mask, and G(x, z) is the generated mask given the prior x.
In CycleGAN, the used data is unpaired (i.e., without having explicit pairs), unlike pix2pix. Therefore, there may not
be a meaningful translator learner that can take a pixel from one mask and convert that into another pixel in a second
mask. But rather it learns a cycle representation instead. CycleGAN architecture includes two generator networks,
G and F, and two discriminators, DX and DY : where the first generator, G, takes an input coarse grain mask x and
generates a synthetic fine grain mask y, while the second generator, F, does the vice versa process. The DX and DY
verify that the input grain masks generated by the G and F, are in the same distribution as the masks of the y and x
domains, respectively, consistent with the AKA cycle.
Hence, the objective of the CycleGAN model is expressed as:
min
G,F
max
DX,DY L[(G, F, DX, DY )]
(4)
where L[(G, F, DX, DY )] (5) is a combination of GAN losses (6) and Cycle loss (7).
4
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
LGAN[(G, DY , X, Y )]+
LGAN[(F, DX, Y, X)]+
λ · Lcyc[(G, F)]
(5)
where λ represents a regularization parameter.
Ey[log DY (y)] + Ex[log (1 −DY (G(x))],
Ex[log DX(x)] + Ey[log (1 −DX(G(y))]
(6)
Ex[||F(G(x)) −x||1] + Ey[||G(F(y)) −y||1]
(7)
where y ∈Y represents the fine grain mask domain, and x ∈X is the coarse grain mask domain.
2.6
Coarse-to-Fine Mask Generation Approach
To enhance controllability, the generative process of producing synthetic tissue fine masks is initialized by specifying a
prior of desired coarse-grained semantic masks representing tissue and applying it to the model (Image Translation). The
mechanism is based on the cGAN architecture and includes both paired and unpaired models (described in subsection
2.3). We utilized and trained these models on our paired coarse-fine grain masks (subsection 2.3). On the one hand,
explicit supervision [31] during learning is less flexible and therefore more limiting, but on the other hand, it can be
expected to provide more precise results. Alternatively, the [32] can result in more scalable but may produce unexpected
outcomes.
Comparison of the primary results of both models clearly confirmed that the pix2pix model [31] is more effective at
capturing the characteristics of cellular tissue and improving the diversity in the spatial distribution of fine-grained
tissue masks. Consequently, we have decided to proceed with the pix2pix model only for further experiments and
results analysis.
2.7
Photorealistic RGB Image Generation Model
Paired image translation encompasses a set of tasks aimed at transforming images from one domain to another, using
input-output image training pairs [38]. One such task involves the insertion of a semantic map into an image and
subsequently translating it, leveraging additional information, such as class labels provided alongside the image during
the training phase.
In the subsequent stage of our pipeline, we employed pix2pixHD [39]. This network facilitated the generation of
synthetic histopathological images using the provided fine-grained semantic masks (while directly translating coarse
semantic masks to histopathological images yields low-fidelity results [25]). The generator component of pix2pixHD
comprises convolutional residual layers [40] and operates on a 512 x 1024 pixels semantic mask input to generate
corresponding 512 x 1024 pixels high-resolution RGB images. Furthermore, our approach incorporated two multiscale
discriminators, both employing a CNN architecture that operates at distinct image scales.
2.8
Training Procedure
We employed the PyTorch framework [41] and trained the model on a single NVIDIA GeForce RTX A6000 GPU with
48GB GPU memory.
To ensure optimal convergence for the model, we carefully tuned the hyperparameters using the Adam Solver [42]. We
set beta-1 at 0.5 and beta-2 to 0.999, employed a mini-batch of size 1, and initialized the learning rate to 2e-4. During
the first 50 epochs, we maintained a constant learning rate and then linearly decayed it to zero over the subsequent
50 epochs. We initialized the weights from a Gaussian distribution with a mean of 0 and a standard deviation of 0.02.
We utilized ReLU activation functions for the generator architecture, while the discriminator architectures employed
leaky ReLUs with a slope of 0.2. Additionally, we applied reflection padding, as demonstrated in [43], to enhance the
network’s performance.
Our optimization loss function comprises two terms. The first term is the mean square error (MSE) between the
discriminator’s average predictions for synthetic and real masks. The second term is the classic adversarial loss based
5
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
on binary cross entropy (BCE), supplemented by two feature-based matching losses. These matching losses aim to
enforce the synthetic output mask to resemble the specific real mask, preserving the conditional features. All elements
of the loss function were equally weighted with a value of 1.
Figure 2: Examples of PriorPath tissue mask generation compred with DEPAS. Thie figures show examples of four
types of cancers, and three different organs: skin, prostate, and lung. For each realization, we show PriorPath fine-grain
tissue mask generation from coarse-grain masks. The bottom panels show representative tissue masks taken from real
biopsy patches, AKA Ground Truth (bottom, right), and by the current SOTA baseline, DEPAS (bottom, left). We
show that PriorPath provides tissue masks from a distribution that is closer to the Ground Truth, and is superior in both
quality and control of tissue mask generation rather than DEPAS’s outputs (as quantified in Table 2).
6
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
3
RESULTS
3.1
Coarse-to-fine translation model
We used two different baseline models for the coarse-to-fine mask approach: CycleGAN [32] and pix2pix [31]
(subsection 2.5). Then, we performed quantitative evaluation methodology over the generated fine masks, by calculating
the FID score between the synthetic masks stem from pix2pix to the real histopathological masks. For comparison, we
performed the same evaluation on the tissue masks generated by CycleGAN for the four different datasets shown in
Table 1. Specifically, pix2pix’s FID scores were better than CycleGAN by a factor of 1.316 for H&E staining datasets,
and by a factor of 1.378 for the IHC staining dataset. Furthermore, qualitative results showed that pix2pix succeeded in
learning to generate fine masks compared to CycleGAN, which lacked those fine tissue details and produced relatively
coarse output masks. Thus, suggesting that pix2pix gives better control over the spatial distribution of the tissue masks
(Fig. 2).
3.2
Synthetic Semantic Tissue Masks
We compared PriorPath to the current two SOTA techniques for generating synthetic masks: DCGAN and DEPAS. One
of the main limitations of DEPAS is its mode collapse (Fig. 1d). Fig. 3a demonstrates how PriorPath is able to generate
masks that cover the real physiological space of real masks. To show that PriorPath masks can cover that semantic
phase space without losing their similarity to real ones, we have divided the semantic space into regions and, for each
region, calculated the similarity based on FID. Fig. 3b shows that for each region, there are PriorPath masks that reside
within this region and that their similarity is comparable with that of DEPAS. The overall average local similarity of
PriorPath is much better than that of DEPAS. Table 2 summarises the improvement of ProirPath, compared to trained
models of DEPAS and DCGAN, in three distances: KL, KS, and FID.
Figure 3: Illustration of PriorPath results, demonstrating the benefits of controlling the distribution while preserving
high similarity to the real distribution. (a) A t-SNE projection of inception’s representations for real skin cancer masks
(black), DEPAS synthetic masks (blue), and PriorPath synthetic masks (red). PriorPath covers the real pathology mask
space. The yellow line defines a grid of size 3 x 3 cells. (b) The similarity metric between the DEPAS and PriorPath
synthetic masks and the real ones for each cell inside the grid of 3 x 3. The horizontal lines are the average similarity
scores of the synthetic and real masks when taking into account all the masks (in all the grid’s cells). (c) The local
average similarity over the cell in a grid of size 3 x 3 for all four cancer realizations. The horizontal lines represent the
global average similarity scores calculated over all masks (without dividing the representations into the grid).
3.3
Synthetic Photorealistic RGB Images
In addition to the primary image translation stage that utilizes a coarse-to-fine mask approach, we further evaluate the
full pipeline in the synthetic photorealistic histopathology perspective, by applying the synthetic tissue fine masks to
[39], and training over every cancer realization (subsection 2.1) in a supervised manner. Finally, we compared their
outputs to the real histopathological images. Examples of the different tissue fine-grained masks and RGB images for
various tissue staining techniques (such as H&E and IHC) are shown in (Fig. 4a-b).
To ensure the reliability of the histopathological images generated, we employed a qualitative evaluation methodology
at the RGB level. This assessment verified the absence of visual artifacts in the images and confirmed their capture of
the histopathological geometry and texture characteristics of the real RGB histopathological images.
7
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
Figure 4: Full pipeline photorealistic results. Samples of fine-grain tissue masks and their corresponding histopathologi-
cal photorealistic synthetic RGB images for all four types of cancer realizations. For each realization, pairs of real and
synthetic masks and their corresponding images are shown. (a) Skin prostate tissues. (b) Lung tissues.
8
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
Table 2: Similarity metrics between PriorPath de-novo digital pathology semantic masks and current SOTA masks. a,
H&E stained tissues; b, IHC stained tissues. KS, Kolmogorov–Smirnov test; KL, Kullback–Leibler divergence; FID,
Fréchet inception distance. ↓- symbol indicates lower is better similarity.
Image quality metrics
Method
Dataset
KS ↓
KL ↓
FID ↓
DCGAN [24]
0.112
6.163
554.699
DEPAS [25]
PRADa
0.309
261.584
198.704
PriorPath (ours)
0.056
0.309
71.246
DCGAN [24]
0.251
82.569
495.516
DEPAS [25]
SKCMa
0.314
234.171
138.604
PriorPath (ours)
0.074
0.355
61.892
DCGAN [24]
0.214
51.578
448.568
DEPAS [25]
LUSCa
0.282
260.163
437.812
PriorPath (ours)
0.137
1.772
60.782
DCGAN [24]
0.086
0.904
424.950
DEPAS [25]
NSCLCb
0.264
185.724
138.629
PriorPath (ours)
0.112
1.279
103.348
4
DISCUSSION
One of the primary hurdles in creating synthetic images of tissues is the ability to tightly control the arrangement
of features within them. Paired GANs offer a promising avenue for enhancing the quality of synthetic images by
incorporating semantic masks that capture the spatial characteristics of the tissue. However, controlling the cellular
features in the masks themselves poses a challenge. Unlike other fields such as autonomous vehicles, where there are
simulation tools that can provide control over a scene, simulating semantic masks that faithfully represent the intricate
biological complexity of the image.
A recent study presented a framework designed to produce high-resolution binary masks representing tissue structure,
serving as semantic guidance for image translation models coined DEPAS [25]. Although DEPAS synthetic masks
are realistic for various pathological realizations and yield photorealistic synthetic images, they exhibit mode collapse
and cannot capture the entire space of physiological semantic masks. The reason for that is the masks themself are
generated from noise and fall into a particular region of the semantic space.
In this work, we took a different approach. We aimed to create detailed, realistic semantic masks from a coarse-grain
image that outlines the regions in which the tissue should be. This allowed us to control the spatial distribution of the
generated masks and, by extension, the generated synthetic images. We showed the ability of this approach to cover the
semantic mask space for three types of cancer: skin, prostate, and lung. Not only do the masks generated by PriorPath
relieve the mode collapse, but also their similarity to the real masks is better. Our approach can allow pathologists and
AI developers to define the desired tissue distributions and get photo-realistic masks and images in one platform. This
will facilitate AI development in cases where data is scarce or unbiased. In this work, we focus on generating binary
images that capture the distribution of the tissue of the relevant organs (skin, prostate, or lung) as a whole.
A limitation of the current work is that it generates binary masks. Future work should explore the ability to create
multilabel masks that also capture the distribution of single-cell features and not just tissue regions. In addition, future
work should explore other conditional generative pipelines such as conditional diffusion models [44, 45].
Overall, this study offers a cutting-edge solution to the demanding task of generating synthetic histopathological images
along with their semantic information in a manner that is both scalable and controllable.
Acknowledgments
We thank Yael Abuhatsera for her valuable discussions. This work is supported by the Israeli Ministry of Science and
Technology (MOST) grant #2149.
Data Availability statement
All data supporting this study’s findings are available upon request from the corresponding author.
9
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
References
[1] T. R. Kiehl, Digital and Computational Pathology: A Specialty Reimagined.
Cham: Springer International
Publishing, 2022, pp. 227–250. [Online]. Available: https://doi.org/10.1007/978-3-030-99838-7_12
[2] S. N. Hart, W. Flotte, F. Andrew, K. K. Shah, Z. R. Buchan, T. Mounajjed, T. J. Flotte et al., “Classification of
melanocytic lesions in selected and whole-slide images via convolutional neural networks,” Journal of Pathology
Informatics, vol. 10, no. 1, p. 5, 2019.
[3] T. A. A. Tosta, L. A. Neves, and M. Z. do Nascimento, “Segmentation methods of h&e-stained histological images
of lymphoma: a review,” Informatics in medicine unlocked, vol. 9, pp. 35–43, 2017.
[4] Y. Cui, G. Zhang, Z. Liu, Z. Xiong, and J. Hu, “A deep learning algorithm for one-step contour aware nuclei
segmentation of histopathology images,” Medical & biological engineering & computing, vol. 57, no. 9, pp.
2027–2043, 2019.
[5] J. Wang, J. D. MacKenzie, R. Ramachandran, and D. Z. Chen, “A deep learning approach for semantic segmenta-
tion in histology tissue images,” in International Conference on Medical Image Computing and Computer-Assisted
Intervention.
Springer, 2016, pp. 176–184.
[6] V. Kovalev, A. Kalinovsky, and V. Liauchuk, “Deep learning in big image data: Histology image classification for
breast cancer diagnosis,” in Big Data and Advanced Analytics, Proc. 2nd International Conference, BSUIR, Minsk.
sn, 2016, pp. 44–53.
[7] R. Fakoor, F. Ladhak, A. Nazi, and M. Huber, “Using deep learning to enhance cancer diagnosis and classification,”
in Proceedings of the international conference on machine learning, vol. 28.
ACM, New York, USA, 2013, pp.
3937–3949.
[8] K. J. Geras, S. Wolfson, Y. Shen, N. Wu, S. Kim, E. Kim, L. Heacock, U. Parikh, L. Moy, and K. Cho,
“High-resolution breast cancer screening with multi-view deep convolutional neural networks,” arXiv preprint
arXiv:1703.07047, 2017.
[9] U. Djuric, G. Zadeh, K. Aldape, and P. Diamandis, “Precision histology: how deep learning is poised to revitalize
histomorphology for personalized cancer care,” NPJ precision oncology, vol. 1, no. 1, pp. 1–5, 2017.
[10] A. Larey, E. Aknin, N. Daniel, G. A. Osswald, J. M. Caldwell, M. Rochman, T. Wasserman, M. H. Collins, N. C.
Arva, G.-Y. Yang et al., “Harnessing artificial intelligence to infer novel spatial biomarkers for the diagnosis of
eosinophilic esophagitis,” Frontiers in Medicine, vol. 9, p. 950728, 2022.
[11] T. Czyzewski, N. Daniel, M. Rochman, J. M. Caldwell, G. A. Osswald, M. H. Collins, M. E. Rothenberg,
and Y. Savir, “Machine learning approach for biopsy-based identification of eosinophilic esophagitis reveals
importance of global features,” IEEE open journal of engineering in medicine and biology, vol. 2, pp. 218–223,
2021.
[12] N. Daniel, A. Larey, E. Aknin, G. A. Osswald, J. M. Caldwell, M. Rochman, M. H. Collins, G.-Y. Yang,
N. C. Arva, K. E. Capocelli, M. E. Rothenberg, and Y. Savir, “A deep multi-label segmentation network for
eosinophilic esophagitis whole slide biopsy diagnostics,” in 2022 44th Annual International Conference of the
IEEE Engineering in Medicine & Biology Society (EMBC).
IEEE, 2022, pp. 3211–3217.
[13] A. Janowczyk and A. Madabhushi, “Deep learning for digital pathology image analysis: A comprehensive tutorial
with selected use cases,” Journal of pathology informatics, vol. 7, no. 1, p. 29, 2016.
[14] D. Shen, G. Wu, and H.-I. Suk, “Deep learning in medical image analysis,” Annual review of biomedical
engineering, vol. 19, p. 221, 2017.
[15] A. Larey, E. Aknin, N. Daniel, G. A. Osswald, J. M. Caldwell, M. Rochman, T. Wasserman, M. H. Collins,
N. C. Arva, G.-Y. Yang, M. E. Rothenberg, and Y. Savir, “Harnessing artificial intelligence to infer novel spatial
biomarkers for the diagnosis of eosinophilic esophagitis,” Frontiers in Medicine, vol. 9, 2022. [Online]. Available:
https://www.frontiersin.org/articles/10.3389/fmed.2022.950728
[16] A. Serag, A. Ion-Margineanu, H. Qureshi, R. McMillan, M.-J. Saint Martin, J. Diamond, P. O’Reilly, and
P. Hamilton, “Translational ai and deep learning in diagnostic pathology,” Frontiers in medicine, vol. 6, p. 185,
2019.
[17] H. R. Tizhoosh and L. Pantanowitz, “Artificial intelligence and digital pathology: challenges and opportunities,”
Journal of pathology informatics, vol. 9, no. 1, p. 38, 2018.
[18] A. C. Quiros, R. Murray-Smith, and K. Yuan, “Pathologygan: Learning deep representations of cancer tissue,” in
Medical Imaging with Deep Learning.
PMLR, 2020, pp. 669–695.
10
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
[19] N. Daniel, E. Aknin, A. Larey, Y. Peretz, G. Sela, Y. Fisher, and Y. Savir, “Between generating noise and
generating images: Noise in the correct frequency improves the quality of synthetic histopathology images for
digital pathology,” in 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology
Society (EMBC).
IEEE, 2023, pp. 1–7.
[20] K. Chen, M. Wang, and Z. Song, “Generative adversarial networks for pre-training of medical image segmentation
networks,” Research Square, 2020.
[21] J. T. Guibas, T. S. Virdi, and P. S. Li, “Synthetic medical images from dual generative adversarial networks,” arXiv
preprint arXiv:1709.01872, 2017.
[22] W. Li, J. Li, J. Polson, Z. Wang, W. Speier, and C. Arnold, “High resolution histopathology image generation and
segmentation through adversarial training,” Medical Image Analysis, vol. 75, p. 102251, 2022.
[23] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,
“Generative adversarial networks,” Communications of the ACM, vol. 63, no. 11, pp. 139–144, 2020.
[24] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep convolutional generative
adversarial networks,” arXiv preprint arXiv:1511.06434, 2015.
[25] A. Larey, N. Daniel, E. Aknin, Y. Fisher, and Y. Savir, “Depas: De-novo pathology semantic masks using a
generative model,” in 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology
Society (EMBC).
IEEE, 2023, pp. 1–7.
[26] K. Tomczak, P. Czerwi´nska, and M. Wiznerowicz, “Review the cancer genome atlas (tcga): an immeasurable
source of knowledge,” Contemporary Oncology/Współczesna Onkologia, vol. 2015, no. 1, pp. 68–77, 2015.
[27] X. Wang, F. Teng, L. Kong, and J. Yu, “Pd-l1 expression in human cancers and its association with clinical
outcomes,” OncoTargets and therapy, vol. 9, p. 5023, 2016.
[28] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans trained by a two time-scale update
rule converge to a local nash equilibrium,” Advances in neural information processing systems, vol. 30, 2017.
[29] G. Marsaglia, W. W. Tsang, and J. Wang, “Evaluating kolmogorov’s distribution,” Journal of statistical software,
vol. 8, pp. 1–4, 2003.
[30] A. W. Bowman and A. Azzalini, Applied smoothing techniques for data analysis: the kernel approach with S-Plus
illustrations.
OUP Oxford, 1997, vol. 18.
[31] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,”
in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1125–1134.
[32] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent
adversarial networks,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2223–
2232.
[33] Kentaro Wada, “Labelme: Image polygonal annotation with python.” [Online]. Available: https://github.
com/wkentaro/labelme
[34] M. Inc, “Medibang inc,” mediBang Paint - the free digital painting and manga creation software. [Online].
Available: https://medibangpaint.com/en/
[35] SAMSUNG, “Samsung penup.” [Online]. Available: https://www.samsung.com/global/galaxy/apps/
pen%-up/
[36] M. Mirza and S. Osindero, “Conditional generative adversarial nets,” arXiv preprint arXiv:1411.1784, 2014.
[37] A. Frangi, “Medical image computing and computer-assisted intervention–miccai 2015,” in Lecture Notes in
Computer Science.
Direct Science, 2015.
[38] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,”
in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1125–1134.
[39] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro, “High-resolution image synthesis and
semantic manipulation with conditional gans,” in Proceedings of the IEEE conference on computer vision and
pattern recognition, 2018, pp. 8798–8807.
[40] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 770–778.
[41] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga
et al., “Pytorch: An imperative style, high-performance deep learning library,” Advances in neural information
processing systems, vol. 32, 2019.
11
PriorPath: Controlled De-Novo Pathology Semantic Masks Generation
DANIEL N ET AL.
[42] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.
[43] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro, “High-resolution image synthesis and
semantic manipulation with conditional gans,” in Proceedings of the IEEE conference on computer vision and
pattern recognition, 2018, pp. 8798–8807.
[44] A. B. Levine, J. Peng, D. Farnell, M. Nursey, Y. Wang, J. R. Naso, H. Ren, H. Farahani, C. Chen, D. Chiu et al.,
“Synthesis of diagnostic quality cancer pathology images by generative adversarial networks,” The Journal of
pathology, vol. 252, no. 2, pp. 178–188, 2020.
[45] P. A. Moghadam, S. Van Dalen, K. C. Martin, J. Lennerz, S. Yip, H. Farahani, and A. Bashashati, “A morphology
focused diffusion probabilistic model for synthesis of histopathology images,” in Proceedings of the IEEE/CVF
winter conference on applications of computer vision, 2023, pp. 2000–2009.
12
