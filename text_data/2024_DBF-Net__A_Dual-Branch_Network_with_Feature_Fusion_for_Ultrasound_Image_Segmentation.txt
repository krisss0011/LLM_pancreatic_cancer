arXiv:2411.11116v1  [eess.IV]  17 Nov 2024
DBF-Net: A Dual-Branch Network with Feature Fusion
for Ultrasound Image Segmentation
Guoping Xua, Xiaming Wua, Wentao Liaoa, Xinglong Wua, Qing Huanga,
Chang Lib
aHubei Key Laboratory of Intelligent Robotics, School of Computer Science and
Engineering, Wuhan Institute of Technology, Wuhan, 430205, Hubei, China
bDepartment of Biomedical Engineering, Hefei University of
Technology, Hefei, 230009, Anhui, China
Abstract
Objective: Accurately segmenting lesions in ultrasound images remains a
challenging task due to the inherent ambiguity in distinguishing boundaries
between lesions and adjacent tissues. Although deep learning has made sig-
niï¬cant progress in accurate ultrasound image segmentation, there is still a
lack of attention to the quality of boundary segmentation and its relation-
ship with the body. As a result, the current accuracy of ultrasound image
segmentation still has room for improvement in enhancing the precision of
lesion delineation. We aim to improve the overall segmentation accuracy and
the quality of boundary segmentation by exploring the relationship between
the body and the boundary in ultrasound images.
Methods: We introduce a dual-branch structure based on a deep neural
network that allows the model to learn the relationship between the body
and boundary parts under supervision, leading to an improvement in overall
segmentation accuracy and boundary segmentation quality. Additionally, we
propose a novel feature fusion module aimed at facilitating the integration
and interaction of body and boundary information. Based on the dual-branch
structure and feature fusion strategy, we have designed UBBS-Net for ultra-
sound image segmentation.
Results: The proposed approach is evaluated on three challenging public
ultrasound image datasets. Our experimental results demonstrate the supe-
riority of the proposed method compared to existing state-of-the-art meth-
ods. Speciï¬cally, we achieve a Dice Similarity Coeï¬ƒcient of 81.05Â±10.44% for
breast cancer segmentation, 76.41Â±5.52% for brachial plexus nerves segmen-
Preprint submitted to Nuclear Physics B
November 19, 2024
tation, and 87.75Â±4.18% for infantile hemangioma segmentation on BUSI,
UNS and UHES datasets, respectively.
Conclusions: We propose UBBS-Net, a novel network for ultrasound image
segmentation that combines body and boundary information with a proposed
feature fusion module. Our method outperforms existing approaches on three
challenging public datasets, demonstrating its eï¬€ectiveness for ultrasound im-
age segmentation. Our code will be available on: https://github.com/apple1986/DBF-
Net.
Keywords:
Image segmentation, ultrasound, feature fusion.
1. Introduction
Image segmentation plays a vital role in medical imaging by enabling
physicians to identify and analyze diverse anatomical structures such as
tumors, organs, and blood vessels[1][2].
Ultrasound (US) imaging ranks
among the foremost modalities in medical diagnostics.US imaging employs
high-frequency sound waves to provide real-time, non-invasive visualization
of internal human body structures.In contrast to other imaging modali-
ties such as computed tomography and magnetic resonance imaging, ultra-
sound (US) imaging is radiation-free, cost-eï¬€ective, and portable, and widely
available[3][4][5]. These advantages make US imaging widely adopted in clin-
ical diagnosis and treatment.
Semantic segmentation allocates individual pixels within an image to
a speciï¬c object class.
Since the advent of fully convolutional networks
(FCN)[6], the encoder-decoder architecture (Fig. 1(a)) has been widely used
for this task, including architectures such as U-Net [7] and LinkNet [8]. With
these advancements, researchers have developed robust and accurate segmen-
tation models that can handle challenges posed by ultrasound images, such as
speckle noise, shadows, and artifacts[9][10]. Recent studies have also shown
the eï¬ƒcacy of deep learning methods in segmenting various tissues and or-
gans in ultrasound images, such as breast [10][11][12], thyroid [13][14], kidney
[15] and liver [16] et al.
Although much progress has been achieved, the segmentation perfor-
mance still has much space to improve, especially the boundary of the objects
or lesions[43]. The shape of the boundary is crucial in clinical diagnosis and
treatment, such as judging the benign or malignant lesion. Despite its im-
portance, boundary segmentation has often been overlooked in prior studies.
2
One of the main reasons is the low resolution of US images, making it dif-
ï¬cult to distinguish the boundary between the lesions and nearby tissues
or objects.
Finally, it is prone to result in over-segmentation or under-
segmentation.
Therefore, it is critical to enhance boundary segmentation
accuracy to achieve optimal segmentation accuracy [17][18].
Feature Encoder
Feature Decoder
Ü½
Feature Encoder
Boundary 
Decoder
Body 
Decoder
Feature interaction 
and enhancement
Ü¿
Feature Encoder
Boundary 
Decoder
Body 
Decoder
Ü¾
Figure 1: Three diï¬€erent convolution neural network architectures: (a) the classic encoder-
decoder structure, (b) a network structure with added boundary information, and (c) the
proposed structure in this paper, which do feature interaction and enhancement between
body decoder and boundary decoder.
The red curve delineates the boundaries of the
lesions.
To address the problem mentioned above, some studies have utilized edge
information as complementary information to enhance the performance of se-
mantic segmentation (Fig. 1(b)). Typically, Some studies have introduced
boundary-aware losses [19][20] to regulate network training. For instance, a
new loss function was devised to achieve balanced weighting between edge
and non-edge pixels[19].Other studies focus on enhancing edge information
using additional sub-networks dedicated to edges. For example, HD-Net [21]
introduced a two-dimensional boundary decoder that embeds boundary rep-
resentations to guide the segmentation process.The ET-Net [22]utilizes multi-
ple branches to integrate edge and multi-scale information, thereby improving
segmentation accuracy. The RF-Net [11] introduced a technique to improve
segmentation accuracy by training residual representations speciï¬cally for re-
gions containing challenging-to-predict pixels. DSN-OB [23] employed deep
3
supervision for detecting both boundaries and objects across layers with ï¬ne
and coarse resolutions.
However, those studies ignore the correlation be-
tween boundary and body. More precisely, the boundary deï¬nes the body
part, while the body part delineates the contour of the boundary. Incorpo-
rating these relationships into the segmentation model could le ad to a more
eï¬€ective and coherent segmentation pipeline.
Building upon this perspective, we propose a novel approach that inte-
grates ultrasound lesion body segmentation and ultrasound lesion boundary
segmentation. Notably, we enable interaction between the body decoder and
boundary decoder (Fig. 1(C)). Adhering to the classical encoder-decoder
architecture, we design a Feature Fusion and Supervision block (FFS) ded-
icated to concurrently processing body and boundary information. Speciï¬-
cally, the features of the boundary and body are supervised by the ground
truth and fused by the proposed feature fusion block. This approach enables
the acquisition of representative feature information, capturing nuances in
both the body and boundary aspects. Preliminary ï¬ndings from this pa-
per were presented at the IEEE International Conference on Bioinformatics
and Biomedicine (BIBM) in 2022[26]. However, this paper overlooked the
correlation between the boundary and body.
To be more precise, it did
not incorporate feature interaction and fusion concerning the boundary and
body during training. Moreover, the experiments were carried out on a wider
range of datasets, leading to signiï¬cant advancements compared to the pre-
vious method. Consequently, the entire approach has undergone substantial
transformation.
In summary, the primary contributions of our work are as follows:
1. We introduce a novel feature interaction module, termed Feature Fusion
for Segmentation (FFS), designed to leverage the relationship between
body and boundary features for enhanced fusion.
2. We devise an innovative architecture, named DBF-Net (Dual-Branch
Fusion Network), aimed at fusing features from the body and boundary
branches for segmenting ultrasound images. We integrate the proposed
FFS module to explore the correlation between body and boundary in
ultrasound image segmentation.
3. The experimental results, tested in three public US datasets, demon-
strate that the proposed method achieves state-of-the-art performance
when compared to existing methods.
4
ASP_OC
Encoder block 3
Encoder block 2
Encoder block 1 
Encoder block 4
Encoder block 5
C
Feature 
Fusion
+
Body
Bound
Bound
Body
Ü®à­ à­­à­³à­¬à­¢
Ü®à­ à­­à­³à­¬à­¢
Ü®à¯•à¯¢à¯—à¯¬
Ü®à¯•à¯¢à¯—à¯¬
FFS-2
C
Feature 
Fusion
+
Body
Bound
Bound
Body
FFS-1
8 Ã—
2 Ã—
Figure 2: The proposed DBF-Net. 8x and 2x are upsampling ratios. C and + means con-
catenation and addition, respectively. Our model adopts the traditional encoder-decoder
architecture. After the encoder, we utilize an ASP OC module to extract multi-scale fea-
tures. The feature fusion and supervision block (FFS) focuses on explicitly modeling body
and boundary at the feature level through a set of convolution operations, feature fusion
block, and supervision.
2. Methods
2.1. Network structure
As depicted in Fig. 2, our network follows the classic encoder-decoder
structure. For the encoder, we use ï¬ve successive convolution blocks. We
denote the output feature representation of each encoder block as Ei âˆˆ
RCÃ—(H/2iâˆ’1)Ã—(W/2iâˆ’1), where C, H and W are channel, input width, and input
height, respectively. The i is the i-th layer of the model. Each convolution
block comprises two convolutional layers, followed by a batch normalization
layer and ReLU activation. More detailed information about the encoder
5
Table 1: Detailed information of the encoder of DBF-Net
Encoder number
kernel size
Dilation
Input channel
Output channel
1
3
1
3
32
2
3
2
32
64
3
3
3
64
128
4
3
5
128
256
5
3
7
256
256
can be seen in Table. 1. After the successive convolution block, we use an
ASP OC block [24] to capture multi-level information. For the decoder, it
consists of two feature fusion and supervision blocks (FFS). ASP OC applies
object context pooling and four dilated convolutions, identical to the original
ASPP, with rates representing dilation rates. The ï¬ve output feature maps
are then concatenated for the ï¬nal output. Each convolution is followed by
a group of BN and ReLU operations. In FFS, there are two branches that
extract body and boundary information from feature maps with deep super-
vision, respectively. Generally, The encoder blocks E1 and E2 focus more on
low-level features, like texture and boundary, while E3 âˆ¼E5 are in favor of
semantic information. Therefore, the outputs from encoder blocks E1 and E2
are inputted into FFS to assist in extracting information about the boundary
and body of segmented objects.
2.2. Label generation
To cater to diverse supervision requirements, we generate boundary and
body labels based on ground truth data. A binary ground truth, denoted as
Gfinal, is composed of foreground Gfg and background Gbg. The initial step
involves applying distance transformation to Gfg to yield a distance map,
representing the shortest distance from pixels in Gfg to Gbg. In this distance
map, distances equal to or less than a speciï¬ed hyper-parameter are identiï¬ed
as part of the boundary, denoted as Gbound, while distances exceeding the
hyper-parameter are assigned to the body, labeled as Gbody. Notably, the
labels for the remaining pixels in both Gbound and Gbody are unequivocally
designated as background. This process can be succinctly expressed as:
Gj
fg â‡’
 Gj
fg âˆˆGbody, if Î·(Gj
fg, Gbg) > Î±
Gj
fg âˆˆGbound,
if Î·(Gj
fg, Gbg) â‰¤Î±
(1)
Gfinal = Gbound âˆªGbody
(2)
6
where Î·(Gj
fg, Gbg) represents the distance from pixel Gj
fg to Gbg, and the Gj
fg
represent pixels belonging to foreground. The hyper-parameter Î± is set to
the default value of 1.
2.3. Feature fusion and supervision block
As depicted in Fig.2, the designed FFS block primarily comprises of three
parts: body and boundary pre-generation, feature fusion, and body and
boundary supervision. For each FFS block, we leverage detailed and seman-
tic information from the feature maps of both the encoder and decoder. To
achieve comprehensive feature maps Fi, we concatenate the feature maps
from the encoder with those from either the ASP OC block or the preceding
FFS block. The encoderâ€™s feature maps contain extensive detailed informa-
tion, whereas the decoderâ€™s feature maps from the preceding layer incorpo-
rate more semantic information. These contributions to the segmentation
of boundaries and objects diï¬€er signiï¬cantly. Therefore, we ï¬rst process the
concatenated feature map through two parallel convolutional branches, which
pre-generate some boundary and body features. The feature map generated
by two parallel convolutional layers can be represented as:
Fbody,i, Fbound,i = Conv (Fi)
(3)
where Conv represents the matrix of two 3Ã—3 convolutions.
Fbody,i and
Fbound,i denote pre-generation feature maps of body and boundary of the
i-th layer, respectively. The next section will provide a detailed introduction
to the feature fusion module.
The pre-generated feature maps undergo feature fusion and enhancement
through a feature fusion module â„œ(â€¢, â€¢).
This process aims to integrate
and enhance the boundary and body features previously generated by the
convolutional branches. The â„œ(â€¢, â€¢) module performs the fusion by consid-
ering the relationship between body and boundary, which aims at enhancing
segmentation quality. The process can be expressed as:
(F âˆ—
body,i, F âˆ—
bound,i) = â„œ(Fbody,i, Fbound,i)
(4)
where F âˆ—
body,i and F âˆ—
bound,i represent the body and boundary feature maps after
enhancement of the i-th layer, respectively. These enhanced feature maps are
crucial in the training process as they are used for supervision. During the
training phase, these feature maps are used to compute the loss, which is
then optimized to enhance segmentation performance.
7
Å½Å¶Ç€
Å½Å¶Ç€
Fâ€¢â€¢â€¢â€¢
âˆ—
Fâ€¢â€¢â€¢â€¢â€¢
Fâ€¢â€¢â€¢â€¢â€¢
âˆ—
Fâ€¢â€¢â€¢â€¢
&ÄÄ‚ÆšÆµÆŒÄ &ÆµÆÅÅ½Å¶
Figure 3: Details of boundary and body feature fusion block.
Conv means successive
convolution operations.
Finally, given two feature maps F âˆ—
body,i and F âˆ—
bound,i, we propose to fuse
them by a trainable parameter as the output of FFS. Speciï¬cally, the train-
able parameter is continuously updated during the training process through
backpropagation. Compared with previous works that directly use addition
or concatenation for feature fusion[7][8], the proposed FFS introduces the
interaction between the features from the boundary and body in training.
Fusion by trainable parameter can be expressed as :
Fout,i = Î»F âˆ—
body,i + F âˆ—
bound,i
(5)
where Fout,i represents the output feature map of FFS of the i-th layer. In
particular, the output of the ï¬nal FFS module is denoted as Ë†F.
Î» is a
trainable parameter, and the initial value is set to 1.
2.4. Feature fusion module
Here We give a detailed introduction of the feature fusion module (FFM)â„œ(â€¢, â€¢)
in FFS. Due to the close relationship between semantic segmentation and
boundary segmentation tasks, we introduce a novel feature fusion block. This
block facilitates bidirectional information ï¬‚ow between the body and bound-
ary streams.
FFM is illustrated in Fig. 3. It receives two input feature
maps Fbody and Fbound from parallel convolution layers. The Fbody and Fbound
are forwarded to a convolution layer, respectively. Those features are sub-
sequently added back to the original features Fbody and Fbound to derive the
8
complementary features F âˆ—
body and F âˆ—
bound. The process in the feature fusion
block is as follows:
F âˆ—
body = Fbody + Conv(Fbound)
(6)
F âˆ—
bound = Fbound + Conv(Fbody)
(7)
where Conv means a convolution layer.
2.5. Loss function
As shown in Fig. 2, there are multiple tasks for the proposed network,
including body map F âˆ—
body, boundary map F âˆ—
bound, and ï¬nal segmentation
map Ë†F. Specially, we adopt auxiliary supervised losses for F âˆ—
body and F âˆ—
bound,
receptively. The total loss L is computed as:
L =Lseg( Ë†F, Gfinal) +
X
i
(Lbody(F âˆ—
body,i, Gbody)+
Lbound(F âˆ—
bound,i, Gbound))
(8)
where Lseg, Lbody, and Lbound represent the loss for the ï¬nal segmentation
map, body map, and boundary map, respectively.
For simplicity in optimization, we employ the same loss function across all
training stages. The loss Lseg, Lbody and Lbound are given by two terms: Lwbce
is the weight binary cross-entropy loss, while represents Dice loss. The weight
of Ldice is inversely proportional to the ratio of foreground to background
pixels in the label, aiming to prioritize attention on the foreground region
within the network. The formulations are shown as follows:
Lâˆ—(P, G) = Î»1Lwbce(P, G) + Î»2Ldice(P, G)
(9)
Lwbce(G, P) = âˆ’
X
i
wi (Gi Â· log(Pi) + (1 âˆ’Gi) Â· log(1 âˆ’Pi))
(10)
Ldice = 1 âˆ’2|G âˆ©P|
|G| + |P|
(11)
where Î»1 and Î»2 are two hyper-parameters that control the weight among the
two losses and we set them 1 and 10, respectively, as default. Lâˆ—represents
Lseg, Lbody and Lbound. P is the prediction map, G is the mask. The deï¬nition
of the weight wi is as follows:
wi = (log(Î² +
Gi
P Gi
))âˆ’1
(12)
where Î² is a hyperparameter, and we set 1 as the default value.
9
3. Experiments
This section describes the experimental settings and datasets used for ul-
trasound image segmentation to evaluate and compare the proposed method.
3.1. Experiment settings
The experiments were conducted using PyTorch 1.9.1 on Ubuntu 18.04.5,
with training performed on a single RTX 1080Ti GPU. Optimization was
carried out using the Adam algorithm, starting with an initial learning rate
of 0.001. we implemented a polynomial learning rate policy with a power
of 0.9 to adjust the learning rate. Batch sizes were set to 2, 4, and 6, and
maximum epochs were conï¬gured to 300, 100, and 350 for datasets BUSI,
UNS, and UHES, respectively.
Online augmentation techniques included
random resizing with a scale of 0.75 to 1.5, random cropping, and random
horizontal ï¬‚ips.
3.2. Dataset
To validate the eï¬€ectiveness of our proposed method, we selected three
ultrasound image datasets featuring various lesions and tissues for compre-
hensive experiments, including segmentation tasks for breast cancer, nerve,
and hemangioma. we
1. BUSI [28]: The BUSI dataset, constituting a repository of breast can-
cer data, comprises 780 images obtained from 600 female patients, with
an average image resolution of 500Ã—500. Image acquisition was con-
ducted using the LOGIQ E9 US system and LOGIQ E9 Agile US. The
BUSI dataset includes 133 instances of normal breast masses, 437 be-
nign cases, and 210 malignant occurrences.
In accordance with the
conï¬guration speciï¬ed in the UNeXt [? ], both the training and test
sets were resized to dimensions of 512Ã—512 for consistency during both
phases.
2. UNS [29] : The ultrasound nerve segmentation (UNS) Kaggle chal-
lenge in 2016 involved identifying and marking the brachial plexus (BP)
nerves in ultrasound images. This was a diï¬ƒcult task because many of
the training images did not contain the BP area. The challenge used
a dataset of 5,635 training images and 5,508 test images with a size of
580Ã—420 pixels.
10
3. UHES: The ultrasound fetal hemangioma segmentation (UHES) dataset
was graciously made available by the Childrenâ€™s Hospital of Chongqing
Medical University, comprising a total of 215 annotated images. In light
of the practicalities associated with image processing and the average
dimensions of the dataset images, a judicious decision was made to re-
size each image to a standardized resolution of 448Ã—256. This resizing
strategy was implemented to enhance the uniformity and facilitate the
subsequent analyses of the dataset.
Table 2: The segmentation results of diï¬€erent CNN-based networks on BUSI, UNS, and
UHES datasets are provided, highlighting the best-performing results in bold. Mean and
standard deviation values are included. * mean the oï¬ƒcial result
Method
Metric
BUSI
UNS
UHES
U-Net[7]
DSC(%)
65.19Â±7.19
70.86Â±10.45
78.24Â±24.00
HD(mm)
9.93Â±0.17
3.88Â±1.25
9.06Â±0.51
DeepLabV3+[25]
DSC(%)
77.76Â±8.92
69.61Â±12.32
85.02Â±9.69
HD(mm)
7.66Â±0.27
3.98Â±1.56
8.30Â±0.30
LinkNet[8]
DSC(%)
72.70Â±9.77
74.68Â±11.46
82.97Â±6.19
HD(mm)
8.41Â±0.70
3.61Â±0.95
8.55Â±0.22
DBBS-Net[26]
DSC(%)
79.34Â±9.43
75.69Â±8.65
84.85Â±10.49
HD(mm)
8.05Â±0.32
3.55Â±0.85
9.84Â±0.37
UNeXt[27]
DSC(%)
78.17Â±2.57
69.24Â±9.92
83.11Â±5.74
HD(mm)
8.16Â±0.39
4.06Â±1.95
8.57Â±0.27
MSSA-Netâˆ—[37]
DSC(%)
80.65
-
-
HD(mm)
-
-
-
V2-CE-CDâˆ—[38]
DSC(%)
80.23
-
-
HD(mm)
-
-
-
HEAT-Netâˆ—[39]
DSC(%)
74.1
-
-
HD(mm)
-
-
-
EHA-Netâˆ—[40]
DSC(%)
80.64
-
-
HD(mm)
-
-
-
DSEU-netâˆ—[41]
DSC(%)
78.51 Â± 1.87
-
-
HD(mm)
-
-
-
NU-netâˆ—[42]
DSC(%)
78.62 Â± 1.38
-
-
HD(mm)
-
-
-
DBF-Net(Ours)
DSC(%)
81.05Â±10.44
76.41Â±5.52
87.75Â±4.18
HD(mm)
7.35Â±0.27
3.52Â±0.52
7.35Â±0.27
11
Input
U-Net
DeepLabV3+
LinkNet
UNext
DBBS-Net
Ours
Figure 4: Comparison of qualitative results between U-Net, DeepLabV3+, LinkNet, UN-
eXt, DBBS-Net and the proposed method for breast cancer segmentation using BUSI
dataset, brachial plexus nerves segmentation using UNS, infantile hemangioma segmen-
tation using UHES. The red curve outlines the boundaries of the lesions. The ï¬rst and
second rows show results from the BUSI dataset, while the third and fourth rows depict
results from UNS, and the ï¬fth and sixth rows present results from UHES.
3.3. Comparison with CNN-based methods
To quantitatively assess the segmentation performance of various meth-
ods in ultrasound image analysis, two widely employed segmentation metrics
are utilized: the Dice Similarity Coeï¬ƒcient (DSC) and the Hausdorï¬€dis-
tance (HD). To ensure fairness, all experiments are conducted using the same
pipeline. We employ ï¬ve-fold cross-validation to generalize the modelsâ€™ per-
formance. To evaluate the robustness and eï¬ƒcacy of the proposed method
in this paper, we initially conducted a comparative analysis against state-of-
the-art convolutional methods. The comparative methods include U-Net[7],
DeepLabV3+[25], LinkNet [8], UNeXt [27], and DBBS-Net [26]. In addition,
we also present some networks speciï¬cally designed for segmenting ultrasound
images, such as MSSA-Net[37], V2-CE-CD[38], HEAT-Net[39], EHA-Net[40],
DSEU-net[41], and NU-net[42]. The average performance of these models is
12




5HFDOO

'%)1HW0$3
/LQN1HW0$3
81HW0$3
'HHSODE90$3
81H;W0$3
'%%61HW0$3







3UHFLVLRQ
35&XUYHRI%86,




5HFDOO

'%)1HW0$3
/LQN1HW0$3
81HW0$3
'HHSODE90$3
81H;W0$3
'%%61HW0$3







3UHFLVLRQ
35&XUYHRI816




5HFDOO

'%)1HW0$3
/LQN1HW0$3
81HW0$3
'HHSODE90$3
81H;W0$3
'%%61HW0$3







3UHFLVLRQ
35&XUYHRI8+(6






)DOVH3RVLWLYH5DWH






7UXH3RVLWLYH5DWH
52&&XUYHRI%86,
'%)1HW$8&
/LQN1HW$8&
81HW$8&
'HHSODE9$8&
81H;W$8&
'%%61HW$8&






)DOVH3RVLWLYH5DWH






7UXH3RVLWLYH5DWH
52&&XUYHRI816
'%)1HW$8&
/LQN1HW$8&
81HW$8&
'HHSODE9$8&
81H;W$8&
'%%61HW$8&




)DOVH3RVLWLYH5DWH

'%)1HW$8&
/LQN1HW$8&
81HW$8&
'HHSODE9$8&
81H;W$8&
'%%61HW$8&







7UXH3RVLWLYH5DWH
52&&XUYHRI8+(6
Figure 5: P-R and ROC curves of DBF-Net, LinkNet, U-Net, DeepLabV3+, UNeXt, and
DBBS-Net on BUSI, UNS, and UHES.
presented in Table 2. From Table 2, it is evident that that DBF-Net achieves
superior DSC of 81.05Â±10.44%, 76.41Â±5.52 and 87.75Â±4.18 on breast cancer,
hemangioma and brachial plexus nerves segmentation, respectively. The HD
values of DBF-Net on BUSI, UNS, and UHES are 7.35Â±0.27, 3.52Â±0.52, and
7.35Â±0.27, respectively. Fig. 4 presents the qualitative results of our model
and state-of-the-art methods on three datasets. In comparison to alternative
techniques, the segmentation outcomes of DBF-Net, our proposed approach,
closely resemble the ground truth.
Fig. 5 displays the precision-recall (P-R) and Receiver Operating Char-
acteristic (ROC) curves for several CNN-based methods on BUSI, UNS, and
UHES, along with the AUC and MAP scores on the corresponding curves.
Our method outperforms other approaches by attaining the highest AUC
and MAP values on BUSI, UNS, and UHES datasets. Comparison of the
precision-recall (P-R) and Receiver Operating Characteristic (ROC) curves
indicates that DBF-Net achieves the highest conï¬dence levels in segmenting
BUSI, UNS, and UHES.
3.4. Comparison with transformer-based methods
We also compared DBF-Net with some transformer-based models, like
LeViT-UNet-192 [30], MedT [31] and TransUNet[32]. It should be noted that
we adjusted the input resolution of images to 224Ã—224 due to the GPUâ€™s lim-
13
Input
LeVIiT-Unet-192
MedT
Trans-Unet
Ours
Figure 6: Comparison of qualitative results between LeViT-UNet-192, MedT, TransUnet
and the proposed method for breast cancer segmentation using BUSI dataset, brachial
plexus nerves segmentation using UNS, infantile hemangioma segmentation using UHES.
The red curve represents the boundary of the breast tumor. The ï¬rst and second rows
show the results for the BUSI dataset, the third and fourth rows depict the results for
UNS, and the ï¬fth and sixth rows present the results for UHES.
14





5HFDOO






3UHFLVLRQ
35 &XUYH RI %86,
8%%61HW0$3
/H9L781HW0$3
0HG70$3
7UDQV81HW0$3
'%)1HW0$3
/H9L781HW0$3
0HG70$3
7UDQV81HW0$3






5HFDOO






3UHFLVLRQ
35&XUYHRI816
'%)1HW0$3
/H9L781HW0$3
0HG70$3
7UDQV81HW0$3
 



5HFDOO

'%)1HW0$3
/H9L781HW0$3
0HG70$3
7UDQV81HW0$3







3UHFLVLRQ
35 &XUYH RI 8+(6






)DOVH3RVLWLYH5DWH






7UXH3RVLWLYH5DWH
52&&XUYHRI%86,
'%)1HW$8&
/H9L781HW$8&
0HG7$8&
7UDQV81HW$8&






)DOVH3RVLWLYH5DWH






7UXH3RVLWLYH5DWH
52&&XUYHRI816
'%)1HW$8&
/H9L781HW$8&
0HG7$8&
7UDQV81HW$8&




)DOVH3RVLWLYH5DWH

'%)1HW$8&
/H9L781HW$8&
0HG7$8&
7UDQV81HW$8&







7UXH3RVLWLYH5DWH
52&&XUYHRI8+(6
Figure 7: P-R and ROC curves of DBF-Net, LeViT-UNet-192, MedT and TransUNet on
BUSI, UNS and UHES.
Table 3: This graph shows the segmentation outcomes of various transformer networks on
BUSI, UNS, and UHES with the best results highlighted in bold text. Mean and standard
deviation are also included.
Method
Metric
BUSI
UNS
UHES
LeViT-UNet-192[30]
DSC(%)
44.37Â±13.31
40.07Â±16.85
64.01Â±15.09
HD(mm)
8.71Â±0.32
4.25Â±0.78
8.00Â±0.15
MedT[31]
DSC(%)
52.27Â±11.1
37.69Â±17.84
63.15Â±14.25
HD(mm)
7.95Â±0.31
4.84Â±0.64
8.92Â±0.08
TransUNet[32]
DSC(%)
31.40Â±8.95
36.68Â±15.23
69.20Â±4.09
HD(mm)
10.03Â±0.05
4.51Â±0.84
7.98Â±0.26
DBF-Net(Ours)
DSC(%)
55.49Â±2.12
56.49Â±10.12
70.20Â±4.09
HD(mm)
6.67Â±0.04
2.76Â±0.34
7.63Â±0.16
ited memory.Table 3 presents the quantitative evaluation results of diï¬€erent
segmentation methods. From Table 3, it is clear that DBF-Net consistently
outperforms the compared transformer-based methods across both Dice Sim-
ilarity Coeï¬ƒcient (DSC) and Hausdorï¬€Distance (HD) metrics. In Fig. 6, we
provide qualitative results of DBF-Net and transformer-based networks on
BUSI, UNS, and UHES. Comparing our proposed method to other segmen-
tation methods, we ï¬nd it to be more eï¬€ective visually. In Fig.7, we draw
the PR curves and ROC curves of LeViTUNet-192, MedT, Trans-Unet and
15
the proposed method on three datasets. According to the P-R curves, the
proposed method outperformed LeViT-UNet-192, MedT, and Trans-Unet on
UHES and ranked second on BUSI and UNS. However, the ROC curves in-
dicate that the proposed method performs suboptimally.
Table 4: Comparison of a number of parameters.
Method
U-Net
DeepLabV3+
LinkNet
DBBS-Net
UNeXt
Params(M)
17.8
5.8
11.5
2.7
1.47
Method
LeViT-UNet-192
MedT
Trans-Unet
-
Ours
Params(M)
15.9
1.6
91.7
-
3.2
Table 5: Ablation experiments for body and boundary supervision block and feature fusion
strategy. baseline means keeping the encoder and ASP OC block in DBF-Net and using
output stride 16 to generate the segmentation map
FFS number and setting
Dice(%)
HD(mm)
1
2
Feature fusion
Baseline
79.91Â±9.43
7.39Â±0.35
+FFS
âœ“
79.85Â±10.03
7.41Â±0.29
âœ“
80.73Â±7.68
7.37Â±0.22
âœ“
âœ“
80.34Â±10.75
7.38Â±0.21
âœ“
âœ“
81.05Â±10.44
7.35Â±0.27
3.5. Complexity analysis
In Table 4, the number of parameters for the proposed method and other
networks compared are tabulated.Comparing the proposed method with U-
Net, we ï¬nd that it has approximately three times fewer parameters. More-
over, even when compared to other lightweight networks like UNeXt, our
model exhibits a competitive parameter count. Our ï¬ndings demonstrate
that our proposed method achieves an optimal balance between model com-
plexity and segmentation accuracy.
3.6. Ablation study
Table 5 shows ablation studies on body and boundary supervision block
and feature fusion strategy. We perform all experiments on the BUSI dataset.
From Table 5, we can note the following: (I) The proposed FFS module en-
hances the networkâ€™s performance over the baseline.(II) Furthermore, we ob-
tain additional performance improvement by incorporating the feature fusion
16
module into the FFS module. Results presented in Table 5 indicate that the
optimal conï¬guration for ultrasound image segmentation is achieved with two
FFS modules combined with feature fusion operations as illustrated in Fig.
2. As a result of these ï¬ndings, it appears that the FFS module, speciï¬cally
when paired with feature fusion, can signiï¬cantly enhance the segmentation
performance of neural networks. Qualitative results of the feature fusion and
ï¬nal output can be seen in Fig. 8. We can see that F âˆ—
body,1 and F âˆ—
bound,1 are
complementary to each other.
Ü¨à¯•à¯¢à¯—à¯¬,à¬¶
×›
Ü¨à¯•à¯¢à¯¨à¯¡à¯—,à¬¶
×›
Ü¨à¯•à¯¢à¯—à¯¬,à¬µ
×›
Ü¨à¯•à¯¢à¯¨à¯¡à¯—,à¬µ
×›
à· Ü¨
Input
Figure 8: Example output of feature fusion module and ï¬nal output for ultrasound image
segmentation.
Table 6: Ablation experiments on body and boundary supervision.
Method
Lseg
Lbody
Lbound
DSC(%)
HD(mm)
DBF-Net
âœ“
78.79Â±11.65
7.59Â±0.30
âœ“
âœ“
79.91Â±9.43
7.39Â±0.35
âœ“
âœ“
79.79Â±9.93
7.67Â±0.27
âœ“
âœ“
âœ“
81.05Â±10.44
7.35Â±0.27
The DBF-Net architecture integrates body and boundary supervision as
a vital component. In this research, we assess the eï¬ƒcacy of this supervision
scheme and provide the associated quantitative outcomes in Table 6.Our
ï¬ndings reveal that the incorporation of additional loss functions, speciï¬cally
Lbody and Lbound, can eï¬€ectively enhance the segmentation performance, with
a more pronounced eï¬€ect observed when both are utilized in combination.
17
Table 7: Ablation experiments for the parameter Î» in Equation (5).
Method
Cross-validation
Î» in FFS-1
Î» in FFS-2
w/ FFM
1
0.94
1.05
2
1.01
1.06
3
1.10
1.04
4
1.14
1.01
5
1.18
0.99
w/o FFM
1
1.21
1.38
2
1.40
1.40
3
1.49
1.45
4
1.55
1.43
5
1.61
1.32
In Table 7, we present the conclusive outcomes derived from the training
of the trainable parameter Î» on the BUSI dataset. Through a meticulous
examination of the ï¬‚uctuations exhibited by Î» under conditions involving
both the inclusion and exclusion of the Feature Fusion and Supervision block
(FFM), a discernible pattern emerges. Our scrutiny reveals that upon the
incorporation of FFM, the disparities in weights between the boundary and
the main body components are rendered non-signiï¬cant. This compelling
observation strongly indicates that, under the inï¬‚uence of FFM, an eï¬€ective
integration of weights between these two crucial components takes place.
Conversely, in the absence of FFM, our discernment exposes a noteworthy
pattern wherein the value of Î» exceeds 1. This conspicuous ï¬nding implies a
discernible imbalance, with the weight attributed to the main body features
surpassing that assigned to the boundary features.
Such a circumstance
underscores a heightened and more substantial contribution of the main body
features to the ultimate outcome, thereby emphasizing the pivotal role of
FFM in harmonizing the inï¬‚uence of both components in the parameter-
tuning process.
4. Discussion
In order to rigorously evaluate our proposed modelâ€™s eï¬ƒcacy, we con-
ducted a comparative evaluation with various CNN-based and Transformer-
based models. We demonstrate in Table 2 and Table 3 that our approach
18
outperforms established state-of-the-art models in ultrasound image segmen-
tation.
It is imperative to underscore that, despite the outstanding results achieved,
Transformer-based methods exhibited a comparatively diminished perfor-
mance in comparison to CNN-based methods. This discrepancy can be as-
cribed to several factors. Firstly, Transformer-based methods conventionally
demand extensive datasets for training purposes, as underscored in literature
[33]. In the context of ultrasound image segmentation, the dataset size is rel-
atively modest, potentially limiting the capacity of Transformer architectures
to unfold their full potential. Furthermore, our employment of identical train-
ing settings for both Transformer-based and CNN-based models might not
be optimal for the distinctive characteristics of the Transformer architecture,
as highlighted by prior research [34][35][36].
In light of the insights gleaned from the results presented in Fig.
7,
our model exhibits a lower Area Under the Curve (AUC) compared to the
Transformer-based model. This discrepancy is attributed to the integration
of numerous shallow features in our model, potentially laden with superï¬‚u-
ous noise and extraneous information. Consequently, this may impede the
modelâ€™s discernment between relevant and irrelevant features, thereby inï¬‚u-
encing the AUC. Nevertheless, these ï¬ndings serve as a catalyst for further
enhancements in our model, advocating for the exploration of alternative
feature selection techniques. These considerations oï¬€er valuable insights for
future research directions in this burgeoning area.
5. Conclusion
This study introduces DBF-Net, a novel dual-branch network with body
and boundary supervision and feature fusion for ultrasound image segmen-
tation. Our approach incorporates a novel Feature Fusion and Supervision
(FFS) module aimed at enhancing segmentation performance in ultrasound
images by integrating body and boundary information and facilitating their
interaction. We thoroughly evaluate DBF-Net against several state-of-the-art
deep learning-based segmentation methods on challenging datasets: BUSI,
UNS, and UHES. The experimental results demonstrate that DBF-Net out-
performs existing approaches in ultrasound image segmentation, highlighting
its potential to advance the state-of-the-art in this ï¬eld.
19
Conï¬‚ict of interest
This work has not been published and has not been submitted for publi-
cation elsewhere while under consideration. The authors declare no potential
conï¬‚ict of interest.
Acknowledgments
This work is supported by the Guangdong Provincial Key Laboratory of
Human Digital Twin (No. 2022B1212010004), the Fundamental Research
Funds for the Central Universities of China (No. PA2023IISL0095), and the
Hubei Key Laboratory of Intelligent Robot in Wuhan Institute of Technology
(No. HBIRL 202202).
References
[1] R. M. Sigrist, J. Liau, A. El Kaï¬€as, M. C. Chammas, and J. K. Will-
mann, Ultrasound elastography: review of techniques and clinical ap-
plications, Theranostics 7, 1303 (2017).
[2] Z. Akkus, J. Cai, A. Boonrod, A. Zeinoddini, A. D. Weston, K. A.
Philbrick, and B. J. Erickson, A survey of deep-learning applications
in ultrasound: Artiï¬cial intelligenceâ€“powered ultrasound for improving
clinical workï¬‚ow,
Journal of the American College of Radiology 16,
1318â€“1328 (2019).
[3] M. Tanter and M. Fink, Ultrafast imaging in biomedical ultrasound,
IEEE transactions on ultrasonics, ferroelectrics, and frequency control
61, 102â€“119 (2014).
[4] K. Hynynen and R. M. Jones, Image-guided ultrasound phased arrays
are a disruptive technology for non-invasive therapy, Physics in Medicine
& Biology 61, R206 (2016).
[5] Z. Wang, Deep learning in medical ultrasound image segmentation: A
review, arXiv preprint arXiv:2002.07703 (2020).
[6] J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks
for semantic segmentation, in Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 3431â€“3440, 2015.
20
[7] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional net-
works for biomedical image segmentation, in Medical Image Computing
and Computer-Assisted Interventionâ€“MICCAI 2015: 18th International
Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234â€“241, Springer, 2015.
[8] A. Chaurasia and E. Culurciello, Linknet: Exploiting encoder represen-
tations for eï¬ƒcient semantic segmentation, in 2017 IEEE visual com-
munications and image processing (VCIP), pages 1â€“4, IEEE, 2017.
[9] Z. Ning, S. Zhong, Q. Feng, W. Chen, and Y. Zhang, SMU-net: saliency-
guided morphology-aware U-net for breast lesion segmentation in ultra-
sound image, IEEE transactions on medical imaging 41, 476â€“490 (2021).
[10] Q. Huang, Y. Huang, Y. Luo, F. Yuan, and X. Li, Segmentation of breast
ultrasound image with semantic classiï¬cation of superpixels, Medical
Image Analysis 61, 101657 (2020).
[11] K. Wang, S. Liang, and Y. Zhang, Residual feedback network for breast
lesion segmentation in ultrasound image, in Medical Image Computing
and Computer Assisted Interventionâ€“MICCAI 2021: 24th International
Conference, Strasbourg, France, September 27â€“October 1, 2021, Proceed-
ings, Part I 24, pages 471â€“481, Springer, 2021.
[12] Y. Hu, Y. Guo, Y. Wang, J. Yu, J. Li, S. Zhou, and C. Chang, Automatic
tumor segmentation in breast ultrasound images using a dilated fully
convolutional network combined with an active contour model, Medical
physics 46, 215â€“228 (2019).
[13] J. Chen, H. You, and K. Li, A review of thyroid gland segmentation and
thyroid nodule segmentation methods for medical ultrasound images,
Computer methods and programs in biomedicine 185, 105329 (2020).
[14] A. Shahroudnejad, X. Qin, S. Balachandran, M. Dehghan, D. Zonoobi,
J. Jaremko, J. Kapur, M. Jagersand, M. Noga, and K. Punithaku-
mar, TUN-Det: a novel network for thyroid ultrasound nodule detec-
tion, in Medical Image Computing and Computer Assisted Interventionâ€“
MICCAI 2021:
24th International Conference, Strasbourg, France,
September 27â€“October 1, 2021, Proceedings, Part I 24, pages 656â€“667,
Springer, 2021.
21
[15] S. Yin, Q. Peng, H. Li, Z. Zhang, X. You, K. Fischer, S. L. Furth, G. E.
Tasian, and Y. Fan, Automatic kidney segmentation in ultrasound im-
ages using subsequent boundary distance regression and pixelwise clas-
siï¬cation networks, Medical image analysis 60, 101602 (2020).
[16] W. Qin, J. Wu, F. Han, Y. Yuan, W. Zhao, B. Ibragimov, J. Gu, and
L. Xing, Superpixel-based and boundary-sensitive convolutional neu-
ral network for automated liver segmentation, Physics in Medicine &
Biology 63, 095017 (2018).
[17] T. Takikawa, D. Acuna, V. Jampani, and S. Fidler, Gated-scnn: Gated
shape cnns for semantic segmentation, in Proceedings of the IEEE/CVF
international conference on computer vision, pages 5229â€“5238, 2019.
[18] D. Marin, Z. He, P. Vajda, P. Chatterjee, S. Tsai, F. Yang, and
Y. Boykov, Eï¬ƒcient segmentation: Learning downsampling near seman-
tic boundaries, in Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pages 2131â€“2141, 2019.
[19] G. Xu, H. Cao, and G. Jiang,
Boundary-attention Loss Function in
Neural Network for Pathological Lymph Nodes Segmentation based on
PET/CT Images, in 2020 9th International Conference on Bioinformat-
ics and Biomedical Science, pages 90â€“94, 2020.
[20] C. Wang, Y. Zhang, M. Cui, P. Ren, Y. Yang, X. Xie, X.-S. Hua, H. Bao,
and W. Xu, Active boundary loss for semantic segmentation, in Pro-
ceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 36,
pages 2397â€“2405, 2022.
[21] H. Jia, Y. Song, H. Huang, W. Cai, and Y. Xia, HD-Net: hybrid dis-
criminative network for prostate segmentation in MR images, in Medical
Image Computing and Computer Assisted Interventionâ€“MICCAI 2019:
22nd International Conference, Shenzhen, China, October 13â€“17, 2019,
Proceedings, Part II 22, pages 110â€“118, Springer, 2019.
[22] Z. Zhang, H. Fu, H. Dai, J. Shen, Y. Pang, and L. Shao, Et-net: A
generic edge-attention guidance network for medical image segmenta-
tion, in Medical Image Computing and Computer Assisted Interventionâ€“
MICCAI 2019: 22nd International Conference, Shenzhen, China, Octo-
ber 13â€“17, 2019, Proceedings, Part I 22, pages 442â€“450, Springer, 2019.
22
[23] D. Mishra, S. Chaudhury, M. Sarkar, and A. S. Soin, Ultrasound image
segmentation: a deeply supervised network with attention to bound-
aries,
IEEE Transactions on Biomedical Engineering 66, 1637â€“1648
(2018).
[24] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang, OC-
Net: Object context for semantic segmentation, International Journal
of Computer Vision 129, 2375â€“2398 (2021).
[25] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroï¬€, and H. Adam, Encoder-
decoder with atrous separable convolution for semantic image segmen-
tation, in Proceedings of the European conference on computer vision
(ECCV), pages 801â€“818, 2018.
[26] W. Liao, G. Xu, X. Wu, X. Zhang, X. He, and C. Li,
Dual-branch
body and boundary supervision network for ultrasound image segmen-
tation, in 2022 IEEE International Conference on Bioinformatics and
Biomedicine (BIBM), pages 3071â€“3077, IEEE, 2022.
[27] J. M. J. Valanarasu and V. M. Patel, Unext: Mlp-based rapid medical
image segmentation network, in Medical Image Computing and Com-
puter Assisted Interventionâ€“MICCAI 2022: 25th International Confer-
ence, Singapore, September 18â€“22, 2022, Proceedings, Part V, pages
23â€“33, Springer, 2022.
[28] W. Al-Dhabyani, M. Gomaa, H. Khaled, and A. Fahmy,
Dataset of
breast ultrasound images, Data in brief 28, 104863 (2020).
[29] A. Montoya, Hasnin, shirzad, W. Cukierski, Ultrasound Nerve Segmen-
tation, Kaggle, (2016).
[30] G. Xu, X. Wu, X. Zhang, and X. He,
Levit-unet: Make faster en-
coders with transformer for medical image segmentation, arXiv preprint
arXiv:2107.08623 (2021).
[31] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, Med-
ical transformer: Gated axial-attention for medical image segmenta-
tion, in Medical Image Computing and Computer Assisted Interventionâ€“
MICCAI 2021:
24th International Conference, Strasbourg, France,
September 27â€“October 1, 2021, Proceedings, Part I 24, pages 36â€“46,
Springer, 2021.
23
[32] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,
and Y. Zhou, Transunet: Transformers make strong encoders for medical
image segmentation, arXiv preprint arXiv:2102.04306 (2021).
[33] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,
Transformers in vision: A survey, ACM computing surveys (CSUR) 54,
1â€“41 (2022).
[34] M. Popel and O. Bojar, Training tips for the transformer model, arXiv
preprint arXiv:1804.00247 (2018).
[35] L. Liu, X. Liu, J. Gao, W. Chen, and J. Han, Understanding the diï¬ƒ-
culty of training transformers, arXiv preprint arXiv:2004.08249 (2020).
[36] Y. Zhao, G. Wang, C. Tang, C. Luo, W. Zeng, and Z.-J. Zha, A battle
of network structures: An empirical study of cnn, transformer, and mlp,
arXiv preprint arXiv:2108.13002 (2021).
[37] M. Xu, K. Huang, Q. Chen, and X. Qi,
Mssa-net: Multi-scale self-
attention network for breast ultrasound image segmentation, in 2021
IEEE 18th International Symposium on Biomedical Imaging (ISBI),
2021, pp. 827â€“831.
[38] H. Â¨Uzen, Convmixer-based encoder and classiï¬cation-based decoder ar-
chitecture for breast lesion segmentation in ultrasound images, Biomed-
ical Signal Processing and Control, vol. 89, p. 105707, 2024.
[39] T. Jiang, W. Xing, M. Yu, and D. Ta,
A hybrid enhanced atten-
tion transformer network for medical ultrasound image segmentation,
Biomedical Signal Processing and Control, vol. 86, p. 105329, 2023.
[40] X. Huang, Q. Wang, J. Chen, L. Chen, and Z. Chen, Eï¬€ective hybrid
attention network based on pseudo-color enhancement in ultrasound im-
age segmentation, Image and Vision Computing, vol. 137, p. 104742,
2023.
[41] G. Chen, Y. Liu, J. Qian, J. Zhang, X. Yin, L. Cui, and Y. Dai, Dseu-
net: A novel deep supervision seu-net for medical ultrasound image
segmentation, Expert Systems with Applications, vol. 223, p. 119939,
2023.
24
[42] G. Chen, L. Li, J. Zhang, and Y. Dai, Rethinking the unpretentious
u-net for medical ultrasound image segmentation, Pattern Recognition,
p. 109728, 2023.
[43] Feng Cheng, Cheng Chen, Yukang Wang, Heshui Shi, Yukun Cao, Dan-
dan Tu, Changzheng Zhang, and Yongchao Xu. Learning directional
feature maps for cardiac mri segmentation. In Medical Image Comput-
ing and Computer Assisted Interventionâ€“MICCAI 2020: 23rd Interna-
tional Conference, Lima, Peru, October 4â€“8, 2020, Proceedings, Part
IV, pages 108â€“117. Springer, 2020.
25
