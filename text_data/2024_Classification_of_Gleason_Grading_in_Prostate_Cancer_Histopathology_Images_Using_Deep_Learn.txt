CLASSIFICATION OF GLEASON GRADING IN PROSTATE CANCER
HISTOPATHOLOGY IMAGES USING DEEP LEARNING
TECHNIQUES: YOLO, VISION TRANSFORMERS, AND VISION
MAMBA
Amin Malekmohammadia, Ali Badiezadehb, Seyed Mostafa Mirhassanic, Parisa Gifanid, Majid Vafaeezadehb,∗
a School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran
b School of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran
c School of Electrical Engineering, Shahrood Branch, Islamic Azad University, Shahrood, Iran
d Medical Sciences and Technologies Department, Science and Research Branch, Islamic Azad University, Tehran, Iran
ABSTRACT
Purpose: Prostate cancer ranks among the leading health issues impacting men, with the Gleason
scoring system serving as the primary method for diagnosis and prognosis. This system relies on
expert pathologists to evaluate samples of prostate tissue and assign a Gleason grade, a task that
requires significant time and manual effort. To address this challenge, artificial intelligence (AI)
solutions have been explored to automate the grading process. In light of these challenges, this study
evaluates and compares the effectiveness of three deep learning methodologies—YOLO, Vision
Transformers, and Vision Mamba—in accurately classifying Gleason grades from histopathology
images. The goal is to enhance diagnostic precision and efficiency in prostate cancer management.
Methods: This study utilized two publicly available datasets, Gleason2019 and SICAPv2, to train and
test the performance of YOLO, Vision Transformers, and Vision Mamba models. Each model was
assessed based on its ability to classify Gleason grades accurately, considering metrics such as false
positive rate, false negative rate, precision, and recall. The study also examined the computational
efficiency and applicability of each method in a clinical setting.
Results: Vision Mamba demonstrated superior performance across all metrics, achieving high preci-
sion and recall rates while minimizing false positives and negatives. YOLO showed promise in terms
of speed and efficiency, particularly beneficial for real-time analysis. Vision Transformers excelled
in capturing long-range dependencies within images, although they presented higher computational
complexity compared to the other models.
Conclusions: Vision Mamba emerges as the most effective model for Gleason grade classification
in histopathology images, offering a balance between accuracy and computational efficiency. Its
integration into diagnostic workflows could significantly enhance the precision of prostate cancer
diagnosis and treatment planning. Further research is warranted to optimize model parameters and
explore the applicability of these deep learning techniques in broader clinical contexts. The code is
available at https://github.com/Swiman/mamba-medical-classification.
Keywords Gleason Grading · Prostate Cancer · Deep Learning · Histopathology Images · YOLO · Vision Transformers ·
Vision Mamba
1
Introduction
Gleason cancer, a type of prostate cancer, is characterized by its aggressiveness and progression rate, making precise
diagnosis and grading imperative for effective treatment planning [1]. The current gold standard for diagnosing
∗corresponding author
arXiv:2409.17122v2  [eess.IV]  2 Oct 2024
Gleason cancer relies heavily on histopathological examination of prostate tissue samples, a process that demands
considerable expertise, involves lengthy procedures, and may be subject to inconsistencies in interpretation among
medical professionals [2]. This has led to a pressing need for automated, objective, and reliable diagnostic tools that
can complement or even surpass the capabilities of human pathologists [3]. The classification of histopathology images
for Gleason cancer stands out for its potential to significantly enhance diagnostic accuracy and patient outcomes [4].
The advent of artificial intelligence (AI) has revolutionized the diagnosis and prognosis of prostate cancer [5]. Early
machine learning techniques for prostate carcinomas were built around manually crafted feature extraction, followed by
feature selection, and concluding with traditional classification algorithms [6]. However, advancements in technology
have led to the emergence of deep learning systems, which utilize multi-layered neural networks as an innovative
approach to these traditional feature-based methods for medical image analysis in any modalities [7, 8, 9, 10]. Unlike
their predecessors, deep learning techniques do not rely on pre-defined features but can autonomously identify and
learn intricate, relevant features straight from the data [11]. This capability mirrors the human brain’s ability to process
information and generate decision-making patterns. Innovations in neural network design and training methodologies
have unlocked the potential to tackle complex learning challenges that were once deemed unsolvable. Consequently,
recent studies have concentrated on leveraging deep learning as the cutting-edge technique in machine learning.
YOLO (You Only Look Once) [12] has emerged as a pivotal model in the realm of real-time object detection, offering
a unique blend of speed and accuracy that distinguishes it from many other models. Originally designed for general
object detection tasks, YOLO’s architecture has been adapted to meet the specific demands of histopathology image
classification [13], where rapid analysis is often as critical as the accuracy of the diagnosis itself. By leveraging
YOLO’s ability to process images in a single pass, researchers and clinicians can significantly accelerate the detection
of pathological features associated with Gleason’s cancer, thereby facilitating more timely and accurate diagnoses.
Transformers, initially designed for natural language processing tasks, have revolutionized the field of computer vision
and medical image analysis [14, 15], including histopathology image analysis [16]. These models, inspired by the
mechanism of attention in neural networks, excel in capturing complex relationships between elements within an image.
By adapting transformers for image classification in histopathology, researchers aim to leverage their ability to focus on
relevant features and ignore irrelevant ones, potentially leading to more accurate and nuanced diagnoses of conditions
like Gleason’s cancer.
In the quest to enhance the analytical capabilities of histopathology image analysis, Mamba stands out as a cutting-edge
tool [17, 18, 19]. Originating from the realm of time series analysis, Mamba offers a sophisticated approach to modeling
dynamic data, which is particularly pertinent in the context of histopathology where changes over time can significantly
influence disease progression and diagnosis. This section delves into the application of Mamba in histopathology
image analysis, highlighting its unique ability to capture and analyze temporal patterns within images. By focusing on
Mamba’s potential to handle dynamic data, we explore how this state-of-the-art method can contribute to more insightful
and accurate interpretations of histopathological images, ultimately aiding in the early detection and management of
diseases such as Gleason’s cancer.
In this article, we explore three cutting-edge architectures for classifying Gleason cancer images from Tissue Microarray
(TMA), prostate samples: the YOLO structure, Vision Transformer structure, and state-space model (Mamba) structures.
These methods, rooted in the latest advancements in deep network technologies, showcase unparalleled efficiency
and promise in dissecting the complexities inherent in histopathology images. By harnessing the power of these
state-of-the-art techniques, we aim to illuminate the potential of employing deep networks in enhancing the accuracy
and reliability of Gleason cancer diagnoses. Consequently, this article is structured to explore each method, offering an
overview of their applications, benefits, and limitations within the context of histopathology image analysis.
Our article is divided into five sections. In the Literature Review section, we review the most recent works on Gleason
grading classification based deep learning methods. The Materials section, we present the datasets used in our study. In
the Methods section, we discuss the deep learning methodologies YOLO, Vision Transformers, and Vision Mamba,
explaining their appropriateness for this task. In In the results section, we reveal the performance of each method
in diagnosing Gleason’s cancer. This aids in understanding the advantages and disadvantages of each deep learning
method. In the discussion section, we examine the implications of our findings, evaluating the pros and cons of these
methodologies in histopathological analysis. Finally, in the conclusion section, we aim to provide a comprehensive
summary of our investigative process, concluding with our findings.
2
Literature Review
A review study introduced [3] the significant role of AI in enhancing prostate cancer diagnosis and treatment through
digital pathology and whole-slide imaging. It emphasized the potential of AI to collaborate with pathologists, reducing
workload and aiding in treatment decisions. Additionally, the paper outlined the development process and challenges
of AI pathology models for prostate cancer, providing access to public datasets and open-source codes to foster
research advancements. In [4] the researchers conducted a systematic literature review using specific keywords in
2
major databases to gather relevant information on prostate cancer, active surveillance, and biomarkers. A systematic
comparison has been proposed in [2] wherein various learning approaches on heterogeneous datasets for Gleason
grading and scoring, achieved high performance and generalization. The dataset included nine heterogeneous datasets,
allowing for a comprehensive evaluation of the methods. The results showed that fully-supervised learning excelled in
patch-level classification tasks, while Multiple-Instance Learning (MIL) methods, particularly Clustering-constrained
Attention Multiple Instance Learning (CLAM) [20], achieved the highest performance in image-level classification
tasks.
Arvaniti et al.’s [21] study introduces a deep learning method for automating Gleason grading of prostate cancer tissue
microarrays through a convolutional neural network (CNN) trained on 641 patients and validated on another 245
patients. The model demonstrated inter-annotator agreement akin to human pathologists, suggesting its potential for
enhancing objectivity and reproducibility in prostate cancer grading. It also showed promise in stratifying patients into
distinct prognostic groups using disease-specific survival data. Nonetheless, the study identified limitations, including
occasional misclassification in unrecognized stromal areas, highlighting the need for additional preprocessing steps
to exclude such regions. The inherent subjectivity of the Gleason scoring system and potential discrepancies among
pathologists from different institutions underscore the requirement for broader, multicenter studies to improve the
model’s reliability and applicability.
In [22] the authors proposed Unsupervised Confidence Approximation (UCA) to address training with noisy labels
and selective prediction simultaneously. They evaluated the efficacy of UCA on CheXpert and Gleason-2019 datasets
[23], showing performance improvements in both noisy label training and selective prediction aspects. Their method’s
notable strength is that it provides concurrent solutions for training with noisy labels and selective prediction. The
approach developed by the researchers in [24] employs an accessible AI-powered image recognition framework
called DeepGleason, which utilizes a tile-by-tile classification strategy incorporating a ConvNeXt architecture and
sophisticated image refinement techniques. This system was developed and tested using a proprietary prostate cancer
database consisting of 34,264 labeled tiles derived from 369 prostate carcinoma specimens. The data source comprised
comprehensive histopathological images obtained from prostate tissue samples. The outcomes demonstrated that
DeepGleason attained a macro-averaged F1-score of 0.806, area under the curve (AUC) of 0.991, and accuracy rate
of 0.974. In comparison to other architectures, ConvNeXt exhibited superior performance on this particular dataset.
The primary strength of this method lies in its exceptional precision and dependability in Gleason grading assessments.
Overcoming the limitations posed by insufficient and unbalanced databases during model training, the method proposed
in [25] presents a conditional Progressive Growing GAN framework named ProGleason-GAN for synthesizing prostate
tissue patches with any Gleason Grade. The dataset utilized is SICAPv2 [26], and the results indicate that the proposed
ProGleason-GAN framework achieved a weighted Frechet Inception Distance (FID) metric of 77.85 for all Gleason
grades. A notable advantage of this method is its capacity to generate realistic prostate tissue patches with varying
Gleason Grades.
In [27] the authors utilized transfer learning with pre-trained CNNs on Gleason-2019 datasets images, achieving the
best performance with the NasnetLarge architecture with an accuracy of 0.93 and an area under the curve of 0.98. Their
method’s salient feature is the ability to automate Gleason grading effectively.
Lucas et in [28] proposed a method for detection of prostate biopsies. Their method involved re-training a CNN for
Gleason pattern detection in prostate biopsies, achieving high accuracy in differentiating between Gleason patterns,
with a dataset of 96 biopsies from 38 patients. The results demonstrated 92% accuracy in distinguishing non-atypical
from malignant areas (GP ≥3) and 90% accuracy in differentiating between GP ≥4 and GP ≤3. The method’s good
point is the high accuracy, sensitivity, and specificity in Gleason pattern classification.
In [29] the authors used transfer learning from AlexNet and GoogleNet on a dataset from the Gleason-2019 Challenge
to classify 6000 cropped images, achieving 85.51% accuracy for AlexNet and 74.75% for GoogleNet when compared
against the majority vote of pathologists. A hallmark of their method lies in successful classification using pre-trained
deep-learning networks.
in [5] a method has been proposed based on visually meaningful features to differentiate between low and high-grade
prostate cancer, achieving high training and testing accuracies of 93.0% and 97.6%, respectively. The dataset consisted
of 43 ROI images from TCGA for training and 88 ROIs from an independent dataset for testing. The method’s key
strengths are its ability to achieve expert-level accuracy in prostate cancer diagnosis and prognosis tasks, while also
providing visually interpretable features that can assist pathologists in understanding the AI’s decision-making process.
A recent investigation [30] centered on examining discrepancies in interpretations among medical experts using a
localized collection of 80 comprehensive histopathology images marked by specialists and developing convolutional
neural network architectures based on this dataset. The researchers utilized a localized collection of 80 comprehensive
histopathology images marked by specialists. The results showed an inter-observer variability of 0.6946 with a 46%
discrepancy in area size of annotations by pathologists, while the best trained models achieved a performance of
0.826±0.014 on the test set. The method’s standout feature is its successful demonstration of how AI-driven approaches
can potentially mitigate inconsistencies in diagnoses among specialists. Fabian León and Fabio Martínez [31] introduced
a novel approach to address the challenges of Gleason score classification in HARVARD Data-verse [21], comprising
3
886 hematoxylin and eosin-stained tissue samples, leveraging a multitask deep learning framework. This method
combines a triplet loss scheme with a cross-entropy task to create a robust embedding representation that captures
the high inter- and intra-class variability inherent in Gleason scoring. The proposed approach significantly improves
upon existing methods by attaining an average accuracy of 66% and 64% across evaluations by two medical specialists,
with no statistical difference, and an impressive 73% accuracy in patches where both specialists agreed. Despite these
strengths, the method still faces limitations, particularly in capturing detailed variations in annotator segmentations,
indicating potential areas for future improvement to enhance its effectiveness. In another research work [32] the authors
proposed a PSPNet for automatic prostate Gleason scoring, utilizing a dataset of 321 biopsy samples from the Vancouver
Prostate Centre, achieving top ranking in the Gleason-2019 prostate segmentation benchmark. Their method excelled in
distinguishing benign from malignant and high-risk (Gleason pattern 4, 5) from low-risk (Gleason pattern 3) cancer.
One of the notable strengths of this technique lies in its ability to excel in both image segmentation and classification
tasks, demonstrating its versatility and effectiveness across different medical image analysis domains. Additionally, the
method’s effectiveness in extracting features from digital histopathology images, combined with its robustness and
generalizability across high-resolution images and varying cell and tissue heterogeneities, positions it as a valuable tool
in prostate cancer diagnosis. However, the paper also identifies key weaknesses, such as the method’s dependency on
high-quality expert annotations and challenges in differentiating specific Gleason grades, highlighting areas for further
improvement and integration with clinical practices.
In other study [33], a novel federated learning framework, Federated Attention Consistent Learning (FACL) framework
was used with a dataset of 19,461 whole-slide images of prostate cancer from multiple centers. FACL achieved an AUC
of 0.9718 in the diagnosis task and a Kappa score of 0.8463 in the Gleason grading task, outperforming multiple centers.
The method’s good point lies in its ability to maintain high accuracy and robustness across large-scale, multicenter
pathological image datasets while preserving patient privacy. Its weaknesses include the complexity of implementing
attention consistency and differential privacy, which might require advanced technical expertise.
The field of artificial intelligence (AI) has witnessed remarkable advancements in recent years, significantly impacting
various domains, including healthcare. Within the realm of histopathology image analysis, AI has emerged as a powerful
tool for enhancing diagnostic accuracy and efficiency. While YOLO, Transformers, and Mamba represent significant
strides in applying AI to histopathology, the broader landscape of AI methods extends far beyond these innovations.
The next paragraphs aim to illuminate the diversity of AI techniques available for histopathology image analysis,
encompassing a range of approaches from the latest deep learning architectures. By exploring the full spectrum of
AI methods applicable to histopathology, we seek to underscore the multifaceted nature of AI’s contribution to this
critical aspect of medical diagnostics, paving the way for further innovation and refinement in disease detection and
characterization.
3
Materials
Our study leverages two comprehensive datasets, namely the Gleason 2019 dataset [23] and the SICAPv2 dataset [26],
to explore the nuances of prostate cancer tissue imagery. Each dataset offers unique insights, yet they come with their
own set of limitations that necessitate careful consideration during analysis.
The study identifies five primary repositories for prostate cancer tissue imagery, with the Cancer Genome Atlas initiative
holding the largest collection of approximately 720 biopsy slides [34]. However, the lack of Gleason grade annotations
at both slide and biopsy levels limits these datasets’ applicability [26]. In contrast, the database by Arvaniti et al. [21]
offers detailed pixel-level annotations for Gleason patterns across 886 smaller slide segments, though these do not fully
capture the diversity of patterns in localized prostate cancer and benign conditions, hindering their utility for slide-level
Gleason assessments. The PANDA challenge [35] introduced a substantial dataset, albeit with biopsy-level Gleason
score annotations, not aligning with the research’s goals. Additionally, two datasets, the Gleason19 challenge and
SICAPv2, are thoroughly discussed as follows:
The SICAPv2 database, with 155 biopsies from 95 consenting participants, addresses concerns of inter-observer
variability through collaborative Gleason score assignments by Valencia hospital pathologists. Additionally, the
Gleason19 challenge dataset, featuring 331 annotated cores from 244 individuals, provides meticulously prepared and
annotated Prostate cancer TMA images, contributing valuable resources for prostate.
4
Methods
4.1
Data Preparation
Gleason2019
The Gleason 2019 dataset comprises TMA images with a resolution of 5120 x 5120 pixels, which are
annotated by several experts. After calculating the final annotation map for each image by pixel-level majority voting,
we opt for smaller patches inside original images to train our model. Specifically, we extract 512 x 512 patches with a
4
stride of 256 in each image, and assign a single label for them based on pixel annotations at their central areas. If all of
the pixels residing in this 250 x 250 area do not belong to the background and share an identical label, the patch label is
set accordingly, otherwise the patch is discarded.
After generating the patches, we use a patient-based cross-validation approach. This involves reserving 20% of the data
for final testing, while 10% of the remaining training set is randomly chosen for validation during the training process.
SICAPv2
The SICAPv2 dataset offers whole slide histology images with both global Gleason scores and patch-level
grades. Like the Gleason2019 dataset, the patches are 512 x 512 pixels with a 50% overlap. We implemented a
cross-validation strategy in which each patient and their associated patches are assigned to a single fold, resulting in
four folds and a separate test subset. Notably, they meticulously selected these patients to guarantee the class balance
between cross-validation subsets. In our experiments we have followed their method for preparing our training data.
Figure 1 illustrates some instances of TMA images in both datasets on 4 classes (i.e. benign, G3, G4, G5). Differentiating
between these grades presents a significant challenge, even for seasoned pathologists, due to the subtle variations in
cellular morphology and tissue architecture characteristic of each grade.
Figure 1: Examples of TMA images showcasing Gleason grades from the Gleason2019 dataset (top row) and the
SICAPv2 dataset (bottom row): a) benign tissue, b) Gleason grade 3, c) Gleason grade 4, and d) Gleason grade 5.
Table.1 presents the count of patches with regard to their Gleason grading categories for Gleason2019 and SICAPv2
datasets.
Table 1: Number of patches corresponding to Gleason categories in Gleason2019 and SICAPv2 datasets.
Benign
Grade 3
Grade 4
Grade 5
Total
Gleason2019
2767
10073
15502
312
28654
SICAPv2
4417
1636
3622
665
10340
4.2
YOLO and Its Application in Histopathology Images
The YOLO (You Only Look Once) algorithm [12], renowned for its efficiency and accuracy in real-time object detection,
operates by dividing an input image into a grid. Each grid cell predicts a certain number of bounding boxes along
with the confidence scores for those boxes, all within a single forward pass through the network. This approach
significantly reduces the computational cost compared to traditional methods that involve multiple stages of detection
and classification. YOLO’s grid-based approach enables it to detect small objects effectively, and its use of anchor
boxes allows the model to handle objects of varying scales within the same image. Moreover, YOLO is designed to
efficiently utilize GPU resources, facilitating faster training and inference times.
Despite its primary design for object detection and potential inaccuracies compared to other methods for specific tasks,
YOLO’s advantages in real-time detection and overall efficiency make it a compelling choice for applications in medical
imaging, where rapid and accurate analysis of complex visual data is paramount. Given these strengths, including its
speed, accuracy, and ability to handle multiple scales, YOLO was chosen as one of the three deep learning methods
5
employed in this study for the classification of histopathology images.
Salman’s groundbreaking article represents a pivotal moment in integrating artificial intelligence (AI), specifically
the YOLO algorithm, into the domain of digital pathology [36]. This trailblazing research aims to automate the
detection and classification of prostate cancer in histopathology images, capitalizing on AI’s advanced capabilities to
boost diagnostic precision and efficiency. The study uniquely explores YOLO’s applicability in analyzing Gleason
histopathology images, marking a first-of-its-kind endeavor in academic literature. The research methodology is
meticulously crafted around the assembly of a custom dataset, meticulously selected to balance quantity and quality.
This dataset includes annotated prostate tissue biopsy images, meticulously annotated by pathologists to uphold the
utmost accuracy. Through strategic data augmentation techniques, the dataset is enlarged, introducing a diversity of
examples to mimic real-world conditions and bolster the model’s learning capability. As the investigation unfolds,
the YOLO model undergoes fine-tuning to discern the unique patterns linked to various Gleason grades, allowing it
to distinguish between benign and malignant tissues with exceptional precision. This flexible strategy caters to the
subtleties of histopathological image analysis, demonstrating YOLO’s flexibility and versatility in managing intricate
visual data. In summary, the adoption of YOLO for histopathology image classification in Salman’s study leverages
the algorithm’s real-time detection capabilities, high accuracy, and scalability to tackle the complex task of prostate
cancer diagnosis. Despite its limitations, particularly in detecting small objects and variations in lighting conditions,
YOLO’s strengths in speed and efficiency position it as a valuable tool in the field of digital pathology, contributing to
the advancement of AI-driven diagnostics.
In this study, we have employed YOLOv8x for the classification of Gleason grades in prostate cancer images. YOLOv8x,
the most advanced variant in the YOLOv8 family developed by Ultralytics, offers state-of-the-art performance for
image classification tasks. This model variant leverages sophisticated neural network architectures and training
techniques to achieve the highest accuracy among YOLOv8 models. While computationally demanding, YOLOv8x’s
superior capabilities make it particularly well-suited for complex medical image analysis tasks such as Gleason grade
classification, where precision is paramount.
4.3
Transformers in Histopathology Image Analysis
Transformers have emerged as a pivotal technology in the realm of artificial intelligence, particularly in the analysis
of medical images and their classification [14, 15]. Initially designed for natural language processing (NLP) tasks,
transformers have been ingeniously adapted for image classification, marking a significant advancement in the field of
computer vision. At the heart of transformers is the concept of transforming or changing an input series into an output
series by acquiring contextual understanding and tracking relationships between series elements. This capability is
particularly beneficial in medical imaging, where understanding the context and relationships within images is crucial
for precise diagnosis and therapeutic planning. Vision transformers (ViT) [37] represent a groundbreaking adaptation
of the transformer paradigm for image classification tasks. By treating an image as a sequence of fixed-size patches
rather than a grid of pixels, ViTs enable the model to detect associations between any pair of segments, irrespective of
their location. This approach mimics the way words are treated in a sentence, enabling the model to understand the
context and relationships within the image data. The addition of positional embeddings ensures that spatial information
is maintained, further enhancing the model’s capacity for accurate image classification.
The application of transformers in medical imaging offers numerous benefits. Their ability to process all elements of the
input sequence in parallel, combined with their capacity to capture long-range dependencies, makes them exceptionally
suited for analyzing complex medical images. This efficiency holds particular significance in healthcare settings, where
rapid analysis of medical images can significantly impact patient outcomes.
Moreover, transformers’ adaptability and scalability unveiled fresh pathways for research and innovation in medical
imaging. Their success in tasks such as object detection, image classification, and even segmentation suggests a
promising future for their integration into clinical workflows, potentially revolutionizing diagnostics and treatment
strategies. Vision transformers have found applications in the examination of histopathology images, particularly for
the classification of Gleason cancer. A notable example is the development of a weakly-supervised machine learning
model for prostate cancer assessment and Gleason grading of histopathology images [38]. This model employs a novel
approach that integrates transformers with other techniques to address the challenges of Gleason grading. Specifically, it
uses transformers to extract discriminative areas in histopathology images through a Multiple Instance Learning (MIL)
algorithm. Following this, a graph is constructed using these discriminative patches, and a Graph Convolutional Neural
Network (GCN) is developed based on the gated attention mechanism to classify the image into its Gleason grades.
Furthermore, the application of transformers extends beyond Gleason grading to encompass the broader field of prostate
cancer detection in ultrasound images [39]. Researchers have explored the use of several transformer architectures,
including Vision Transformers (ViT), Compact Convolutional Transformers (CCT), and Pyramid Vision Transformers
(PvT), for detecting cancer in small regions of interest (ROI) within ultrasound images. These models are pretrained
using VICReg and then fine-tuned with an additional MLP classifier to detect cancer in individual ROIs. The aggregated
predictions from each patch within a core are averaged to produce the final output for the core. This approach highlights
6
the versatility of transformers in processing medical image data, demonstrating their potential to improve the accuracy
of prostate cancer detection from ultrasound images.
In this paper we applied the ViT_base_patch16_224_in21k as a pre-trained Vision Transformer model trained on
the ImageNet21k dataset, featuring a 16x16 patch size and a 224x224 input size. ImageNet21k, unlike the standard
ImageNet-1K, offers a broader array of images and classes, making it more comprehensive and varied. However, its
complexity, limited accessibility, and underestimated benefits lead to its infrequent use for pretraining deep learning
models for computer vision tasks.
The Vision Transformer’s image classification process comprises dividing the image into fixed-size patches, linearly
embedding each patch, adding position embeddings, encoding the sequence with a Transformer, and utilizing the
encoded output for classification. Position embeddings, representing the spatial location of each patch, are incorporated
into the patch embeddings, typically learned alongside other model parameters during training.
Linearly embedding a patch in Vision Transformer involves transforming the pixel values of the patch into a high-
dimensional vector space using a fully connected layer with learnable weights and biases, resulting in the patch
embedding.
The Vision Transformer’s primary advantage over CNN models lies in its ability to train end-to-end on large datasets
without the need for manually engineered features or data augmentation, thanks to its foundation in the Transformer,
adept at processing sequential data. Moreover, it has achieved state-of-the-art performance on several benchmark image
classification datasets, including ImageNet.
4.4
Vision Mamba for State-Space Modeling in Histopathology Images
Vision Mamba is a novel approach for image classification, particularly tailored for medical images [40]. This method
leverages the principles of Mamba, originally designed for large language models, to tackle the challenges of computer
vision tasks, including image classification.
State Space Sequence Models (SSMs) are systems designed to map 1-dimensional sequences using a linear Ordinary
Differential Equation (ODE). They offer benefits from previous architectures, such as parallel processing of data like
transformers and linear complexity during inference similar to recurrent layers. However, their adoption has been
limited due to issues like vanishing/exploding gradients and higher memory use compared to CNNs.
Structured State Space Sequence Models (S4) enhance basic SSMs by structuring the state matrix. This matrix is
initialized with a High-Order Polynomial Projection Operator, enabling the creation of deep models with effective
long-range reasoning. Mamba builds on SSMs for discrete data modeling (e.g., text and genome) by introducing two
key innovations. First, it uses an input-dependent selection mechanism, unlike traditional time- and input-invariant
SSMs, which improves information filtering by parameterizing SSM parameters based on input data. Second, Mamba
incorporates a hardware-aware algorithm that scales linearly with sequence length, making it faster on modern hardware.
The Mamba architecture combines SSM blocks with linear layers for a simpler design and has achieved state-of-the-art
performance in long-sequence tasks, such as language and genomics, due to its efficiency in both training and inference.
The SS2D comprises three components: a scan expanding operation, an S6 block, and a scan merging operation.
As depicted in Figure 2, the scan expanding operation unfolds the input image into sequences along four directions
(top-left to bottom-right, bottom-right to top-left, top-right to bottom-left, and bottom-left to top-right). These sequences
undergo feature extraction in the S6 block, ensuring comprehensive scanning and capturing diverse features from
various directions. Afterwards, the scan merging operation sums and merges the sequences from all four directions,
restoring the output image to its original size.
Figure 2: Main operations of 2D-selective-scan.
7
Figure 3: The architecture of MedMamba, cComprising Batch Normalization (BN), Layer Normalization (LN), Sigmoid
Linear Unit (SiLU), Rectified Linear Unit (ReLU), linear layers, Point-wise Convolutions (PWConv), and Depth-wise
Convolutions (DWConv). SS2D: Selective State Space Model 2D, B: Batch size, H: Height dimension, W: Width
dimension, C: Channel dimension.
The S6 block, inspired by Mamba [41], incorporates a selective mechanism atop S4, adapting the SSM’s parameters
based on the input. This allows the model to differentiate and retain relevant information while filtering out the irrelevant.
The pseudo-code for the S6 block, given an input x with batch size b, token length l, and dimension d, can be outlined
as follows:
∆, B, C = Linear(x), Linear(x), Linear(x)
A = exp(∆A)
B = (∆A)−1(exp(∆A) −I) · ∆B
ˆht = Aht−1 + Bxt
yt = Cht + Dxt
y = [y′
1, y′
2, . . . , y′
t]
(1)
Where A, B, C, and D are learnable parameters, Linear(.) denotes the linear projection layer, and y is the output feature
map with the same shape as input. The MedMamba model is a deep learning architecture designed for medical image
classification tasks. It is inspired by the VMamba architecture [42] and incorporates a novel hybrid basic block called
SS-Conv-SSM, which stands for State Space Convolution - State Space Model. This block is the core element of the
MedMamba model and is designed to efficiently handle both local feature extraction and the capture of long-range
dependencies within medical images.
As depicted in Figure 3, Overall architecture of MedMamba includes several key components: Patch Embedding
Layer: This layer is responsible for transforming the input medical images into a format suitable for processing by
the subsequent layers of the model. It typically involves dividing the image into patches and embedding them into a
lower-dimensional space. Stacked SS-Conv-SSM Blocks: These are the building blocks of the MedMamba model.
Each SS-Conv-SSM block consists of the following:
Channel-Split: Divides the input channels into groups.
Convolutional Layers: Extract local features from the input.
8
SSM Layers: Capture long-range dependencies using state space models.
Channel-Shuffle: Re-arranges the channels to mix the features extracted by different groups.
Patch Merging Layers: These layers are used for down-sampling the feature maps, which helps in reducing the
computational complexity and capturing hierarchical features at different scales.
Feature Classifier: The final component of the MedMamba model is a classifier that takes the high-level features
extracted by the previous layers and uses them to classify the input image into one of the predefined categories.
The MedMamba model also employs a grouped convolution strategy and channel-shuffle operation to reduce the
number of parameters and lower the computational burden, making it efficient for medical artificial intelligence
applications. This design allows MedMamba to maintain excellent performance while being computationally efficient,
addressing the limitations of both CNNs and ViTs in the context of medical image classification.
Vision Mamba establishes a new baseline for medical image classification, providing valuable insights for developing
more powerful SSM-based artificial intelligence algorithms and application systems in the medical field. To conduct a
fair comparison, we have employed Mamba and other methods based on their reported configurations and trained them
on our datasets.
5
Results
We employ several classification metrics for quantitative evaluation of competing methods. These metrics are defined as
follows:
Precision =
TP
TP + FP
(2)
Recall =
TP
TP + FN
(3)
F-Score = 2 × (Precision × Recall)
Precision + Recall
(4)
Accuracy =
TP + TN
TP + TN + FP + FN
(5)
With TP, FP, TN, and FN indicating true positives, false positives, true negatives, and false negatives, respectively.
These metrics highlight the challenge in differentiating between four classes in Gleason2019 and SICAPv2 dataset. It is
worth mentioning that due to severe class imbalance in these datasets, we opt for a weighted average over class metrics
to provide a more comprehensive measurement of overall performance. This approach ensures that the performance
metrics reflect the proportion of each class in the dataset, rather than being skewed by the performance on less frequent
classes.
To compare the performance of aforementioned models, we have employed two public datasets and reported the
results in Table.3 and Table.4. It can be seen that MedMamba outperforms other competing methods with regards
to all quantitative metrics. Specifically, by achieving a high precision and recall, Medmamba showcases its ability
in identifying True Positives while maintaining a low False Positive and False Negative rate. Compared to that,
ViT presents inferior metrics with much higher computational complexity. Additionally, due to high resolution of
histopathology images, long-range feature extraction plays a crucial role in overall model performance. Compared to
CNNs, Mamba and ViT architectures excel in this regard and deliver better results.
Table 3: Results of competing methods on Gleason2019 dataset based on weighted metrics.
Precision
Recall
F1-score
Accuracy
Mamba
85.82
84.87
85.34
85.13
YOLO
83.56
82.24
82.89
83.69
Vision Transformer
83.84
83.16
83.49
84.07
Figure 4 represents the normalized confusion matrices for competing methods on two datasets, with the first and second
row corresponding to Gleason2019 and SICAPv2 datasets, respectively. These tables provide a comprehensive visual
representation of models’ performance. It is evident that the diagonal of the tables, which represent the True positive
predictions are lower for SICAPv2 dataset compared to Gleason, which can be attributed to its challenging samples.
9
Table 4: Results of competing methods on SICAPv2 dataset based on weighted metrics.
Precision
Recall
F1-score
Accuracy
Mamba
64.61
58.23
61.25
69.98
YOLO
56.96
53.58
55.21
64.7
Vision Transformer
62.26
58.86
60.51
66.02
Overall, Mamba performs strongly on benign cases and well on the others, though it consistently struggles with higher
Gleason grades, particularly g5. On the Gleason dataset, ViT encounters more difficulty, especially with g4 and g5,
showing greater confusion between these classes than Mamba. YOLO surpasses ViT in handling benign cases, but still
encounters challenges with g4 and g5. Additionally, it excels at distinguishing higher grades, particularly g5, in the
SICAPv2 dataset.
Figure 4: Normalized confusion matrices on SICAPv2 (top) and Gleason2019 (bottom) datasets. Matrices from left to
right correspond to Mamba, ViT, and YOLO models. Ben: Benign, g3: Grade 3, g4: Grade 4, g5: Grade 5.
6
Discussion
In recent years, the field of prostate cancer diagnosis and prognosis has witnessed significant advancements through the
integration of artificial intelligence (AI) technologies. Researchers have developed novel AI-based methods that not
only enhance the accuracy of diagnosis but also improve the prediction of patient outcomes.
One of the key breakthroughs in AI-based methods for prostate cancer diagnosis is the achievement of expert-level
performance in lesion detection and Gleason grading. By leveraging deep learning algorithms, these methods have
demonstrated capabilities comparable to human pathologists in accurately identifying lesions and classifying tissue
samples based on Gleason patterns.
In the context of histopathology image analysis, YOLO’s speed and efficiency are particularly advantageous. The
rapid turnaround times enabled by YOLO can facilitate immediate feedback during surgical procedures or expedite
the triage of biopsy samples, potentially leading to quicker diagnosis and treatment initiation. Moreover, YOLO’s
architecture allows for the simultaneous detection of multiple objects within an image, a feature that is highly beneficial
in histopathology where multiple types of cells and tissues may be present in a single slide.
However, while YOLO’s speed and flexibility offer significant advantages, the model’s performance in histopathology
image classification is contingent upon careful tuning and validation. The nuances of histopathological images,
characterized by subtle variations in cellular morphology and tissue composition, pose unique challenges that
necessitate the optimization of YOLO’s parameters and the selection of appropriate training datasets. Furthermore, the
balance between speed and accuracy remains a critical consideration, as overly aggressive optimizations aimed at
increasing speed may compromise the model’s ability to accurately distinguish between different Gleason grades or
other pathological features.
10
Despite these challenges, the integration of YOLO into the pipeline for histopathology image analysis represents
a promising direction for advancing diagnostic workflows. By leveraging YOLO’s strengths in real-time object
detection, researchers and clinicians can explore new paradigms for integrating AI into clinical practice, potentially
revolutionizing the way prostate cancer is diagnosed and managed.
CNNs and ViTs both have limitations when it comes to medical image classification tasks. CNNs are adept at extracting
local features due to their convolutional layers, but they struggle with capturing global context and long-range
dependencies because of their inherently limited local receptive fields. This limitation can lead to insufficient feature
extraction and suboptimal classification results.
On the other hand, ViTs, which were originally designed for natural language processing, represent an input image as a
sequence of image patches and use self-attention mechanisms to capture long-range dependencies effectively. However,
ViTs lack the ability to handle spatial image hierarchies natively, and their self-attention mechanism has a high
quadratic complexity, which can degrade local feature details. This complexity results in a significant computational
burden, especially for high-resolution medical images, making it challenging to deploy ViT models in clinical settings
with limited computational resources.
MedMamba integrates the advantages of CNNs and SSMs for efficient medical image classification by introducing a
novel hybrid basic block called SS-Conv-SSM. This block combines the capabilities of convolutional layers to extract
local features with the strengths of SSMs in capturing long-range dependencies. The SS-Conv-SSM block is designed
to efficiently process medical images by using a combination of channel-split, convolutional layers, SSM layers, and
channel-shuffle operations.
The convolutional layers in the SS-Conv-SSM block are responsible for capturing local spatial features from the
input images, which is a task that CNNs are well-suited for due to their ability to learn hierarchical representations
of visual data. On the other hand, the SSM layers within the block are adept at modeling long-range dependencies,
which is crucial for understanding the global context of medical images. By stacking multiple SS-Conv-SSM blocks,
MedMamba can build a deep architecture that effectively captures both local and global features from medical images
of different modalities.
Furthermore, MedMamba employs a grouped convolution strategy and channel-shuffle operation to reduce the number
of parameters and lower the computational burden, making it efficient for medical artificial intelligence applications.
This design allows MedMamba to maintain excellent performance while being computationally efficient, addressing the
limitations of both CNNs and ViTs in the context of medical image classification.
7
Conclusion
This study has comprehensively evaluated and compared the performance of three deep learning methodolo-
gies(i.e.,YOLO, Vision Transformers, and Vision Mamba) in the classification of Gleason grades from histopathology
images. The goal was to enhance diagnostic precision and efficiency in prostate cancer management, a critical issue
given the prevalence of prostate cancer and the reliance on Gleason grading for diagnosis and prognosis.
The results demonstrate that Vision Mamba emerged as the most effective model for Gleason grade classification,
offering a balance between accuracy and computational efficiency. It achieved high precision and recall rates while
minimizing false positives and negatives, showcasing its superiority across all evaluated metrics. YOLO, despite
its speed and efficiency advantages, particularly in real-time analysis, was found to be less accurate in this specific
task. Vision Transformers, while capable of capturing long-range dependencies within images, exhibited higher
computational complexity compared to the other models.
These findings suggest that Vision Mamba could significantly enhance the precision of prostate cancer diagnosis
and treatment planning by integrating into diagnostic workflows. Its ability to accurately classify Gleason grades
from histopathology images could streamline the diagnostic process, potentially leading to earlier and more accurate
diagnoses, improved patient outcomes, and reduced healthcare costs.
Further research is recommended to optimize model parameters and explore the applicability of these deep learning
techniques in broader clinical contexts. Such efforts could pave the way for the widespread adoption of AI in pathol-
ogy, revolutionizing the field and improving patient care. The availability of the code used in this study on GitHub
(https://github.com/Swiman/mamba-medical-classification) encourages further exploration and experimentation by the
scientific community, fostering innovation in the application of AI to medical imaging.
In conclusion, the integration of deep learning techniques, particularly Vision Mamba, into the diagnostic workflow
for prostate cancer holds great promise. It not only enhances the accuracy and efficiency of Gleason grading but also
underscores the potential of AI to transform healthcare delivery. As the field continues to evolve, it will be essential to
continue investigating and refining these methodologies to ensure they remain at the forefront of diagnostic innovation.
11
References
[1] Donald F Gleason. Histologic grading of prostate cancer: a perspective. Human pathology, 23(3):273–279, 1992.
[2] Juan P Dominguez-Morales, Lourdes Duran-Lopez, Niccolò Marini, Saturnino Vicente-Diaz, Alejandro Linares-
Barranco, Manfredo Atzori, and Henning Müller. A systematic comparison of deep learning methods for gleason
grading and scoring. Medical Image Analysis, 95:103191, 2024.
[3] Lingxuan Zhu, Jiahua Pan, Weiming Mou, Longxin Deng, Yinjie Zhu, Yanqing Wang, Gyan Pareek, Elias Hyams,
Benedito A Carneiro, Matthew J Hadfield, et al. Harnessing artificial intelligence for prostate cancer management.
Cell Reports Medicine, 2024.
[4] Rui Bernardino, Rashid K Sayyid, Ricardo Leão, Alexandre R Zlotta, Theodorus van der Kwast, Laurence Klotz,
and Neil E Fleshner. Using active surveillance for gleason 7 (3+ 4) prostate cancer: A narrative review. Canadian
Urological Association Journal, 18(4):135, 2024.
[5] M Khalid Khan Niazi, Keluo Yao, Debra L Zynger, Steven K Clinton, James Chen, Mehmet Koyutürk, Thomas
LaFramboise, and Metin Gurcan. Visually meaningful histopathological features for automatic grading of prostate
cancer. IEEE journal of biomedical and health informatics, 21(4):1027–1038, 2016.
[6] Saqib Iqbal, Ghazanfar Farooq Siddiqui, Amjad Rehman, Lal Hussain, Tanzila Saba, Usman Tariq, and
Adeel Ahmed Abbasi. Prostate cancer detection using deep learning and traditional techniques. IEEE Ac-
cess, 9:27085–27100, 2021.
[7] Parisa Gifani, Ahmad Shalbaf, and Majid Vafaeezadeh. Automated detection of covid-19 using ensemble of
transfer learning with deep convolutional neural network based on ct scans. International journal of computer
assisted radiology and surgery, 16:115–123, 2021.
[8] Parisa Gifani, Majid Vafaeezadeh, Mahdi Ghorbani, Ghazal Mehri-Kakavand, Mohamad Pursamimi, Ahmad
Shalbaf, and Amirhossein Abbaskhani Davanloo. Automatic diagnosis of stage of covid-19 patients using an
ensemble of transfer learning with convolutional neural networks based on computed tomography images. Journal
of Medical Signals & Sensors, 13(2):101–109, 2023.
[9] Ahmad Shalbaf, Parisa Gifani, Ghazal Mehri-Kakavand, Mohamad Pursamimi, Mahdi Ghorbani, Amirhossein Ab-
baskhani Davanloo, and Majid Vafaeezadeh. Automatic diagnosis of severity of covid-19 patients using an
ensemble of transfer learning models with convolutional neural networks in ct images. Polish Journal of Medical
Physics and Engineering, 28(3):117–126, 2022.
[10] Majid Vafaeezadeh, Hamid Behnam, Ali Hosseinsabet, and Parisa Gifani. A deep learning approach for the
automatic recognition of prosthetic mitral valve in echocardiographic images. Computers in Biology and Medicine,
133:104388, 2021.
[11] Majid Vafaeezadeh, Hamid Behnam, Ali Hosseinsabet, and Parisa Gifani. Automatic morphological classification
of mitral valve diseases in echocardiographic images based on explainable deep learning methods. International
Journal of Computer Assisted Radiology and Surgery, 17(2):413–425, 2022.
[12] J Redmon. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016.
[13] Maisun Mohamed Al Zorgani, Irfan Mehmood, and Hassan Ugail. Deep yolo-based detection of breast cancer
mitotic-cells in histopathological images. In Proceedings of 2021 International Conference on Medical Imaging
and Computer-Aided Diagnosis (MICAD 2021) Medical Imaging and Computer-Aided Diagnosis, pages 335–342.
Springer, 2022.
[14] Majid Vafaeezadeh, Hamid Behnam, Ali Hosseinsabet, and Parisa Gifani. Carpnet: Transformer for mitral valve
disease classification in echocardiographic videos. International Journal of Imaging Systems and Technology,
33(5):1505–1514, 2023.
[15] Majid Vafaeezadeh, Hamid Behnam, and Parisa Gifani. Ultrasound image analysis with vision transformers.
Diagnostics, 14(5):542, 2024.
[16] Hongming Xu, Qi Xu, Fengyu Cong, Jeonghyun Kang, Chu Han, Zaiyi Liu, Anant Madabhushi, and Cheng Lu.
Vision transformers for computational histopathology. IEEE Reviews in Biomedical Engineering, 2023.
[17] Ali Nasiri-Sarvi, Vincent Quoc-Huy Trinh, Hassan Rivaz, and Mahdi S Hosseini. Vim4path: Self-supervised
vision mamba for histopathology images. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 6894–6903, 2024.
[18] Ruiwen Ding, Kha-Dinh Luong, Erika Rodriguez, Ana Cristina Araujo Lemos da Silva, and William Hsu.
Combining graph neural network and mamba to capture local and global tissue spatial relationships in whole slide
images. arXiv preprint arXiv:2406.04377, 2024.
12
[19] Shu Yang, Yihui Wang, and Hao Chen. Mambamil: Enhancing long sequence modeling with sequence reordering
in computational pathology. arXiv preprint arXiv:2403.06800, 2024.
[20] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood. Data-
efficient and weakly supervised computational pathology on whole-slide images. Nature biomedical engineering,
5(6):555–570, 2021.
[21] Eirini Arvaniti, Kim S Fricker, Michael Moret, Niels Rupp, Thomas Hermanns, Christian Fankhauser, Norbert
Wey, Peter J Wild, Jan H Rueschoff, and Manfred Claassen. Automated gleason grading of prostate cancer tissue
microarrays via deep learning. Scientific reports, 8(1):12054, 2018.
[22] Navid Rabbani and Adrien Bartoli. Unsupervised confidence approximation: Trustworthy learning from noisy
labelled data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4609–4617,
2023.
[23] Guy Nir, Soheil Hor, Davood Karimi, Ladan Fazli, Brian F Skinnider, Peyman Tavassoli, Dmitry Turbin, Carlos F
Villamil, Gang Wang, R Storey Wilson, et al. Automatic grading of prostate cancer in digitized histopathology
images: Learning from multiple experts. Medical image analysis, 50:167–180, 2018.
[24] Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Jonas Bäcker, Samantha Cramer, Christoph
Wengenmayr, Bruno Märkl, Ralf Huss, Iñaki Soto-Rey, et al. Deepgleason: a system for automated gleason
grading of prostate cancer using deep neural networks. arXiv preprint arXiv:2403.16678, 2024.
[25] Alejandro Golfe, Rocío Del Amor, Adrián Colomer, María A Sales, Liria Terradez, and Valery Naranjo. Progleason-
gan: Conditional progressive growing gan for prostatic cancer gleason grade patch synthesis. Computer Methods
and Programs in Biomedicine, 240:107695, 2023.
[26] Julio Silva-Rodríguez, Adrián Colomer, María A Sales, Rafael Molina, and Valery Naranjo. Going deeper through
the gleason scoring scale: An automatic end-to-end system for histology prostate grading and cribriform pattern
detection. Computer methods and programs in biomedicine, 195:105637, 2020.
[27] Parisa Gifani and Ahmad Shalbaf. Transfer learning with pretrained convolutional neural network for automated
gleason grading of prostate cancer tissue microarrays. Journal of Medical Signals & Sensors, 14(1):4, 2024.
[28] Marit Lucas, Ilaria Jansen, C Dilara Savci-Heijink, Sybren L Meijer, Onno J de Boer, Ton G van Leeuwen,
Daniel M de Bruin, and Henk A Marquering. Deep learning for automatic gleason pattern classification for grade
group determination of prostate biopsies. Virchows Archiv, 475:77–83, 2019.
[29] Mircea-Sebastian ¸Serb˘anescu, Carmen-Nicoleta Oancea, Costin Teodor Streba, Iancu Emil Ple¸sea, Daniel Pirici,
Liliana Streba, and R˘azvan Mihail Ple¸sea. Agreement of two pre-trained deep-learning neural networks built with
transfer learning with six pathologists on 6000 patches of prostate cancer from gleason2019 challenge. Romanian
Journal of Morphology and Embryology, 61(2):513, 2020.
[30] José M Marrón-Esquivel, Lourdes Duran-Lopez, Alejandro Linares-Barranco, and Juan P Dominguez-Morales. A
comparative study of the inter-observer variability on gleason grading against deep learning-based approaches for
prostate cancer. Computers in Biology and Medicine, 159:106856, 2023.
[31] Fabian León and Fabio Martínez. A multitask deep representation for gleason score classification to support grade
annotations. Biomedical Physics & Engineering Express, 8(3):035021, 2022.
[32] Yali Qiu, Yujin Hu, Peiyao Kong, Hai Xie, Xiaoliu Zhang, Jiuwen Cao, Tianfu Wang, and Baiying Lei. Automatic
prostate gleason grading using pyramid semantic parsing network in digital histopathology. Frontiers in oncology,
12:772403, 2022.
[33] Fei Kong, Xiyue Wang, Jinxi Xiang, Sen Yang, Xinran Wang, Meng Yue, Jun Zhang, Junhan Zhao, Xiao Han,
Yuhan Dong, et al. Federated attention consistent learning models for prostate cancer diagnosis and gleason
grading. Computational and Structural Biotechnology Journal, 23:1439–1449, 2024.
[34] John N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Shaw, Brad A Ozenberger, Kyle Ellrott, Ilya
Shmulevich, Chris Sander, and Joshua M Stuart. The cancer genome atlas pan-cancer analysis project. Nature
genetics, 45(10):1113–1120, 2013.
[35] Wouter Bulten, Kimmo Kartasalo, Po-Hsuan Cameron Chen, Peter Ström, Hans Pinckaers, Kunal Nagpal, Yuannan
Cai, David F Steiner, Hester Van Boven, Robert Vink, et al. Artificial intelligence for diagnosis and gleason
grading of prostate cancer: the panda challenge. Nature medicine, 28(1):154–163, 2022.
[36] Mehmet Emin Salman, Gözde Çakirsoy Çakar, Jahongir Azimjonov, Mustafa Kösem, and ˙Ismail Hakkı Cedim. o˘glu.
Automated prostate cancer grading and diagnosis system using deep learning-based yolo object detection algorithm.
Expert Systems with Applications, 201:117148, 2022.
13
[37] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
[38] Mohammad Mahdi Behzadi, Mohammad Madani, Hanzhang Wang, Jun Bai, Ankit Bhardwaj, Anna Tarakanova,
Harold Yamase, Ga Hie Nam, and Sheida Nabavi. Weakly-supervised deep learning model for prostate cancer
diagnosis and gleason grading of histopathology images. Biomedical Signal Processing and Control, 95:106351,
2024.
[39] Mohamed Harmanani, Paul FR Wilson, Fahimeh Fooladgar, Amoon Jamzad, Mahdi Gilany, Minh Nguyen Nhat
To, Brian Wodlinger, Purang Abolmaesumi, and Parvin Mousavi. Benchmarking image transformers for prostate
cancer detection from ultrasound data. In Medical Imaging 2024: Image-Guided Procedures, Robotic Interventions,
and Modeling, volume 12928, pages 245–251. SPIE, 2024.
[40] Yubiao Yue and Zhenzhang Li. Medmamba: Vision mamba for medical image classification. arXiv preprint
arXiv:2403.03849, 2024.
[41] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint
arXiv:2312.00752, 2023.
[42] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu.
Vmamba: Visual state space model. ArXiv, abs/2401.10166, 2024.
14
