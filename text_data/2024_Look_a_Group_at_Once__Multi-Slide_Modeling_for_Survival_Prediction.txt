Look a Group at Once: Multi-Slide Modeling for Survival Prediction
Xinyang Li1∗, Yi Zhang1∗, Yi Xie2, Jianfei Yang3, Xi Wang4, Hao Chen5†, Haixian Zhang1†
Sichuan University1, Duke-NUS Medical School2,
Nanyang Technological University3, The Chinese University of Hong Kong4,
The Hong Kong University of Science and Technology5
Abstract
Survival prediction is a critical task in pathology. In clinical
practice, pathologists often examine multiple cases, lever-
aging a broader spectrum of cancer phenotypes to enhance
pathological assessment. Despite significant advancements
in deep learning, current solutions typically model each
slide as a sample, struggling to effectively capture compa-
rable and slide-agnostic pathological features. In this pa-
per, we introduce GroupMIL, a novel framework inspired
by the clinical practice of collective analysis, which models
multiple slides as a single sample and organizes groups of
patches and slides sequentially to capture cross-slide prog-
nostic features. We also present GPAMamba, a model de-
signed to facilitate intra- and inter-slide feature interac-
tions, effectively capturing local micro-environmental char-
acteristics within slide-level graphs while uncovering es-
sential prognostic patterns across an extended patch se-
quence within the group framework.
Furthermore, we
develop a dual-head predictor that delivers comprehen-
sive survival risk and probability assessments for each pa-
tient. Extensive empirical evaluations demonstrate that our
model significantly outperforms state-of-the-art approaches
across five datasets from The Cancer Genome Atlas.
1. Introduction
Survival prediction is a fundamental task in pathology, fo-
cused on evaluating patient prognosis through comprehen-
sive analysis of tissue slides and pathological features [25].
It plays a pivotal role in formulating treatment plans, assess-
ing therapeutic efficacy, and ultimately improving patient
outcomes [12].
In clinical practice, it is observed that pathologists often
benefit from reviewing multiple cases [21, 42], as shown
in Fig. 1(a).
Pathologists suggest that the rationale be-
hind is that different slides reveal a broader range of can-
*These authors contributed equally to this work.
†Co-corresponding authors. Hao Chen: jhc@ust.hk; Haixian Zhang:
zhanghaixian@scu.edu.cn
…
Evaluate
Reference
…
…
…
…
c
b
a
Compare
…
…
…
…
…
…
…
…
…
…
Figure 1. (a) illustrates the process by which pathologists refer-
ence other slides and compare phenotypes to enhance their assess-
ments. (b) depicts the conventional procedure of processing WSIs
individually. (c) presents our proposed group modeling.
cer phenotypes and patient-agnostic prognostic character-
istics, which contribute to a non-isolated and comparable
prognostic analysis. However, as shown in Fig. 1(b), exist-
ing survival prediction models typically process and predict
outcomes for each patient individually, treating each Whole
Slide Image (WSI) as a separate sample [2, 4, 16, 23, 30,
32, 37, 38, 40, 45]. WSIs are segmented into patches, from
which features are extracted and aggregated using Multiple
Instance Learning (MIL). While these methods learn from
individual slides, we assume that the “collective analysis”
can enhance the accuracy and reliability of automatic prog-
nostic evaluation. In this work, as depicted in Fig. 1(c), we
propose to learn from multiple slides at once, rather than
from each slide individually, which we refer to as a “group”
of slides.
arXiv:2411.11487v2  [cs.CV]  24 Nov 2024
To effectively learn from slide groups, two key issues
should be addressed:
1) learning from a multitude of
patches within a group, and 2) capturing distinctive prog-
nostic features of each slide in the group.
To address
these issues, we propose: 1) modeling a group of patches
as a sequence and introducing the Position and Attention
Mamba (PAMamba), which leverages Mamba’s [13] ro-
bust ability for handling extremely long sequences and cap-
ture distant dependencies, to scan extended patch sequences
based on coordinate order and prognostic relevance rank-
ing; and 2) representing each slide as a graph, and devel-
oping Graph PAMamba (GPAMamba), which clamps PA-
Mamba by Graph Neural Networks (GNNs), to identify
slide-specific characteristics.
GPAMamba alternates be-
tween aggregating graphs and scanning the sequence, pro-
viding a comprehensive understanding of cancer pheno-
types and allowing prognostic assessments for each slide to
benefit from collective group insights. Furthermore, consid-
ering the correspondence between survival probability [41]
and patient risks [5], we design a dual-head predictor and a
combined measure to comprehensively quantify the risk of
patients.
The main contributions of this work can be summarized
as follows:
• Inspired by clinical diagnosis, we propose GroupMIL, the
first group-level survival prediction framework to enable
cross-slide prognostic analysis.
• To capture intra- and inter-slide prognostic features
within our group framework, a GNN-empowered two-
branch SSM (GPAMamba), is developed.
• A dual-head predictor, with an additive loss function, is
designed to jointly measure the risk score and survival
probability distribution.
• Extensive experiments are conducted on five public
TCGA datasets, and empirical results show that our
model outperforms the state-of-the-art models, demon-
strating promise in interpretability, patient stratification,
and clinical consistency.
2. Related Work
Survival prediction using WSI was approached as a regres-
sion problem [44], utilizing pathologist-annotated Regions
of Interest (RoIs). This task was later reformulated as a MIL
problem, eliminating the need for manual RoI delineation.
In these studies, some utilized plain images, while others
structurized the images as graphs or sequences.
Plain Image-based Models.
Convolutional Neural Net-
works (CNNs) are employed to learn from images. For in-
stance, Zhu et al. [44] and Yao et al. [39] sampled patches
from WSIs and clustered them based on phenotypes, mak-
ing final predictions using the fused features of these phe-
notype clusters. Similarly, Shao et al. [30] employed a grid-
based approach to obtain patches, a method that is now
widely adopted. Fan et al. [9] introduced a pairwise learning
strategy that combines features from multiple WSI images
of a pair of patients for contrastive learning, enhancing the
model’s discriminative ability.
Graph-based Models.
Considering the significance of
the topological features of WSIs, which plain image-based
models tend to overlook, Li et al. [23], Chen et al. [2],
and Mackenzie et al. [27] modeled WSI images as graphs
based on the locations of patches and utilized Graph Neu-
ral Networks (GNNs) to learn from pre-extracted features.
Wang et al. [37] constructed multi-scale graphs that span
from cells to patches, enabling a hierarchical learning ap-
proach for WSI images. Additionally, Wang et al. [36] pro-
posed a coarse-to-fine two-stage graph construction strategy
to refine key graph nodes.
Sequence-based Models.
Given the large number of
patches, some studies model WSIs as sequences and employ
sequence models such as Transformers [4, 7, 10, 24, 32] and
Mamba [38] to handle them. Specifically, Chen et al. [4]
adopted a hierarchical fusion strategy of “cell-patch-region”
to learn information from the entire WSI, with each level
processed by a vision transformer [8]. A similar approach
was utilized by Shao et al. [32]. Fan et al. [10] expanded
their previous work using Transformers [35]. Additionally,
studies focused on multimodal fusion predictions based on
Transformers include [7] and [24]. Recently, the emergence
of Mamba [13] has pushed the boundaries of sequence mod-
els, with Yang et al. [38] employing it to achieve improved
patch sequence learning.
Graph models are suited for aggregating local informa-
tion, enhancing the understanding of micro-environmental
features, while sequence models capture broader correla-
tions between patches. In this paper, we model WSIs as
graphs to capture intra-slide topological information and
represent groups as sequences to account for inter-slide
global associations, enabling a more comprehensive prog-
nostic analysis.
3. Method
3.1. Preliminaries
…
…
…
…
…
…
extractor
aggregator
predictor
Figure 3. The standard pipeline for survival prediction.
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
…
GPAMamba 
Mamba
…
…
Patching
Encoding
Patch Group Learning
Slide Group Learning
Predicting
A Group of WSIs
Risk
Prob
Figure 2. In our framework, each group of slides is segmented into patches, encoded, and represented as independent graphs before being
input into GPAMamba. Within GPAMamba, node features are aggregated and sequentially scanned (details in Sec. 3.3). The resulting
graphs are then pooled to obtain slide representations, which are also arranged sequentially before being analyzed by a Mamba module.
Finally, these representations are evaluated using the dual-head predictor, comprising Hrisk and Hprob (details in Sec. 3.4).
The standard pipeline for survival prediction treats each
slide as a sample, processing them in parallel, as illustrated
in Fig. 3. WSIs are segmented into patches that are encoded
into embeddings using a feature extractor. These embed-
dings are passed through an aggregator to generate slide-
level representations, which are subsequently fed into a pre-
dictor for the final prediction. The extractor, aggregator, and
predictor are described in detail below.
Extractor. Given a set of WSIs {w1, . . . , wn}, each slide
wi is segmented into Ni patches, where i denotes the in-
dex of the i-th WSI. These patches are encoded into Ni D-
dimensional embeddings, resulting in each wi being repre-
sented by a slide feature si ∈RNi×D. Details on our seg-
mentation and encoding process can be found in Sec. 4.3.
Aggregator. There are two common approaches to process
slide features: 1) each si in the set {s1, . . . , sn} is input into
survival models individually, as Ni varies, with the models
processing s ∈R1×Ni×D one at a time [2, 13, 16, 32]. 2)
Alternatively, some studies [9, 10] filter some of patches
to ensure a consistent number, which may lead to infor-
mation loss, allowing the model to process a batch of slide
features s ∈RB×N×D in parallel, where B represents the
batch size and N represents the fixed number of patches per
slide. Subsequently, a pooling operator is used to aggregate
s into slide representations s′ (for vision transformer-based
models, these representations are the class tokens), with
s′ ∈R1×1×D or s′ ∈RB×1×D, where each s′
i ∈R1×D.
Two common practices follow: 1) using s′ directly for pre-
diction [37, 39, 40]; or 2) passing s′ through an additional
module for enhancement beforehand [10, 28, 32].
In conclusion, prevailing methodologies treat each wi
as a sample for computation, making them unsuitable for
holistic prognostic analysis across multiple slides.
Predictor. The prediction head can be categorized into two
types: the risk score head Hrisk and the survival probability
head Hprob [34, 41]. Hrisk models survival prediction as a re-
gression problem, mapping each D-dimensional slide rep-
resentation s′ to a scalar, the risk score r, which quantifies
the patient’s risk. The loss function of Hrisk, Cox Loss [5],
is defined as:
Lrisk = −
X
i:Ei=1
(ri −log
X
j:Tj≥Ti
erj),
(1)
On the other hand, Hprob models the task as a classifica-
tion problem, mapping each s′ to a K-dimensional vector
representing survival probabilities across K time intervals,
predicting the probability of a death event occurring within
each interval. The loss function of Hprob, DT Loss [41], is
defined as:
Lprob = −
X
i:Ei=1
log (P(T = ti | Xi))−
X
j:Ej=0
log(S(tj | Xj)).
(2)
Both heads are widely used. However, Hrisk may show
sensitivity to censored patients, while Hprob discretizes con-
tinuous survival time into intervals, which can potentially
lead to information loss and boundary effects.
In this paper, we aim to bring new understandings into
the design of both aggregators and predictors, exploring
how the group can enhance model performance. First, we
introduce the concept of group modeling and describe the
organization of two distinct groups in Sec. 3.2. Next, we de-
tail the design of the aggregator, including the GPAMamba
and group fusion module, and how they process their re-
spective groups in Sec. 3.3. Finally, we describe our predic-
tor in Sec. 3.4. An overview of the proposed framework is
provided in Fig. 2.
3.2. Group Modeling
Inspired by clinical practice [21, 42], we propose the con-
cept of “group” to enhance cross-slide prognostic analysis.
Specifically, given a dataset of WSIs with the same type
of cancer, we define every B random WSIs {w1, . . . , wb}
as a group and treat the entire group, instead of each in-
dividual slide, as a single sample. Furthermore, recogniz-
ing that pathologists first observe and analyze subtle phe-
notypes and then compare and draw conclusions about the
entire WSI, we model the group hierarchically at both the
patch and slide levels.
Patch group.
For a group containing B slide features
s = {s1, . . . , sb}, where each si has Ni patches, we se-
quentialize s to form the patch group g ∈R1×PB
i=1 Ni×D,
which represents a single and extended sequence of patches
with a length of PB
i=1 Ni. This sequentialization ensures
that all patches within the group are visible. Our proposed
model, PAMamba, then reorders and rearranges this long
patch sequence to uncover slide-agnostic phenotype corre-
lations.
Slide group. Similarly, we construct the slide group g′ ∈
R1×B×D by sequentializing {s′
1, . . . , s′
b}, which is also a
single sequence of slides of length B. Note that g′ differs
fundamentally in theory from traditional s′ ∈RB×1×D for
sequence models. This setting allows for all slides within
the group to remain visible. Additionally, we employ an-
other Mamba module to traverse this sequence, thereby en-
riching g′ with cross-slide prognostic information.
By sequentializing the patches and slides hierarchically,
we enable collaborative analysis of multiple WSIs, allowing
survival models to achieve comprehensive and comparable
assessments.
3.3. Patch Group and Slide Group Learning
{s1, s2, ..., sb}
Conv1D
SSM
SSM
GC
⊗
⊗
PAMamba
�
⊗
Conv1D
�
�
�
GC
⊗
…
Restore
Position Scan
Attention Scan
Linear
Reorder
SiLU
�
Sequentialize
Desequentialize
{s1, s2, ..., sb}
�
GPAMamba
�
Figure 4.
Overview of GPAMamba.
GC layers convolve the
graphs {s1, ..., sb}, while PAMamba scans the sequence g. The
red lines indicate the position scanning branch, and the blue lines
represent the attention scanning branch.
PAMamba. The number of patches Ni segmented from
a single WSI can reach thousands (at 10× magnification),
and that of those within a group can be even greater.
Given this, and inspired by Mamba’s efficient handling of
long sequences, we design PAMamba to employ position-
and attention-based sequential scanning, establishing spa-
tial and prognostic relevance relationships within this ex-
tremely long patch sequence. For position sequence gpos,
PAMamba scans patches in their spatial order (original or-
der) on the WSI, moving from top to bottom and left to
right, connecting WSIs within a group end-to-end. Since
groups often contain many patches from normal tissue that
may lack strong prognostic significance, PAMamba scans
attention sequence gatt, which reorders g by the learned
weights, to emphasize “key” patches. Each sequence is pro-
cessed by a distinct ϕ(x), namely ϕpos and ϕatt, respectively,
as defined in Eq. 3:
ϕ(x) = S(σ(C(L(x)))) + σ(L(x)),
(3)
where L and C represent a linear layer and a 1D convolu-
tion layer, respectively, σ denotes the SiLU activation func-
tion [15, 29], and S represents the SSM operator [13].
Algorithm 1 GPAMamba block
Require: A group of node features s = {s1, . . . , sb},
where si ∈RNi×D; A group of adjacency matrices
A = {A1, . . . , Ab}, where Ai ∈BNi×Ni; Scanning
operators ϕpos(·) and ϕatt(·); GC operators G1(·) and
G2(·); Linear layer L : RD →R1.
Ensure: Updated node features s = {s1, . . . , sb}.
1: for each graph i = 1 to b do
2:
si ←G1(Ai, si)
▷Aggregate node features
3:
g ←g.stack(si)
▷Sequentialize s
4: end for
5: gpos ←ϕpos(g)
▷Scan g by ϕpos
6: w ←L(g)
▷Calculate attention weights
7: idx ←w.argsort()
▷Get ascending order
8: gatt ←g[idx]
▷Reorder g
9: gatt ←ϕatt(gatt)
▷Scan gatt by ϕatt
10: gatt[idx] ←gatt
▷Restore order
11: g ←(gpos + gatt)
▷Update g
12: for each graph i = 1 to b do
13:
si ←g[:, Pi−1
j=1 Nj : Pi
j=1 Nj, :]
▷Desequentialize g
14:
si ←G2(Ai, si)
▷Update node features
15: end for
16: Return s
The process of PAMamba is outlined in the 5 to 11 lines
(L) of Alg. 1. The position sequence gpos is handled by the
ϕpos operator for spatial order learning (L5). Meanwhile, a
linear layer generates the attention weights (L6), which re-
flect the prognostic relevance of the patches (visualization
can be found in Sec. 4.6). g is then reordered in ascending
order according to these weights (L7 and L8), and the re-
sulting attention sequence gatt is processed by the ϕatt oper-
ator for relevant phenotypes learning (L9). Subsequently, to
align the corresponding patches between the two sequences,
PAMamba restores the processed attention sequence to the
original order (L10), and merges the two aligned feature
sequences to create a unified and updated group feature g
(L11). In summary, PAMamba performs dual-branch long-
sequence modeling, enhancing the group-level feature in-
teractions.
Patch group learning and GPAMamba. The patch group
sequentializes multiple WSIs into a single sample (L3). To
preserve the structural integrity and relative independence
of each slide, we represent slides as distinct, structurally
fixed graphs, where nodes correspond to patches and edges
reflect their adjacency relationships (details can be found
in Sec. 4.3). Each slide corresponds to one graph. Ac-
cordingly, Graph Convolutional (GC) layers are used to ex-
change information between a patch and its neighboring
patches within the same slide (L2 and L14), enabling intra-
slide micro-environmental learning.
As shown in Fig. 4, the combination of “GC-PAMamba-
GC”, along with a desequentialization process to reverse
the sequence into graphs (L13), forms the core building
block of GPAMamba. This innovative design enables GPA-
Mamba to alternately learn from the B graphs and a single
sequence, embodying an “intra-inter-intra” learning process
within the group.
Slide group learning. The carefully crafted organization
and computation procedure of GPAMamba effectively man-
age patch groups. Here, we employ ϕint, as defined in Eq. 3,
to integrate the sequence of group representation g′, facili-
tating the predictor to deliver comparable predictions.
3.4. Dual-head Predictor and Loss Function
To harness the rich information within g′, we design a dual-
head predictor, Hdual, for joint prognostic prediction. As
depicted in Fig. 2, Hdual is an intuitive combination of the
Hrisk and Hprob defined in Sec. 3.1. Since the cumulative
survival probabilities over intervals can serve as a proxy for
negative risk, we follow the approach in [2, 3, 32], convert-
ing the probability distribution predicted by Hprob into a risk
score. This score is then summed with the predicted risk
score ri output by Hrisk, creating a comprehensive measure
R that quantifies prognostic outcomes:
Ri = ri −
K
X
k=1
P(T > tk | wi),
(4)
where wi denotes a slide in a group, T represents survival
time, and k enumerates K time intervals t. Thus, R com-
bines the predicted risk score with probability distributions
across each time interval, providing a robust risk assessment
with probabilistic detail. Accordingly, the dual-head predic-
tor, Hdual, is optimized by a hybrid loss function defined as:
Loverall = α ∗Lrisk + (1 −α) ∗Lprob,
(5)
where Lrisk and Lprob are defined in Sec. 3.1.
4. Experiments
4.1. Datasets
We use five publicly available datasets from The Can-
cer Genome Atlas (TCGA) [18], including pathological
slides from different cancers:
BReast invasive CArci-
noma (BRCA), BLadder urothelial CArcinoma (BLCA),
GlioBlastoMa & Lower Grade Glioma (GBM&LGG),
LUng ADenocarcinoma (LUAD), and Uterine Corpus En-
dometrial Carcinoma (UCEC).
4.2. Evaluation Metrics
The concordance index (C-index) [33] is employed as the
metric for survival prediction performance of the models.
C-index measures the order consistency between event time
labels and the predicted outcomes among patients. C-index
ranges from 0 to 1, where values between 0 and 0.5 indicate
poor predictive performance, 0.5 represents complete ran-
domness, and values closer to 1 indicate better predictive
performance.
4.3. Implementation Details
The empirical studies in this work are conducted using the
PyTorch 2.0 framework on an NVIDIA GeForce RTX 4090
GPU. We follow CLAM [26] for patch segmentation and
encoding.
Patch segmentation is performed on WSIs at
10× magnification, with each patch sized 512×512. These
patches are encoded by ResNet50 [14] pretrained on Ima-
geNet [6], resulting in patch embeddings with a dimension-
ality D of 1024. For graph construction, each patch is con-
nected to its 24 nearest neighbors in a (5×5) grid, based on
experimental results. During model training and validation,
we treat every 6 slides as a group. We divide the patients’
survival time into four intervals. We use gradient accumu-
lation with a step size of 32 and the Adam optimizer [20]
with a learning rate of 2 × 10−4. The α of the loss function
Loverall is set to 0.5.
4.4. Comparative Experiments
To evaluate our model’s performance, we conduct com-
parative experiments with several SOTA models using
the same five-fold cross-validation splits.
These in-
clude CNN-based models such as ABMIL [17], DeepAttn-
MISL [40], DSMIL [22], and DTFDMIL [43]; graph-based
models like DeepGraphConv [23] and Patch-GCN [2];
transformer-based models such as TransMIL [31] and HVT-
Surv [32]; and SSM-based models like S4MIL [11] and
MambaMIL [38]. As shown in Tab. 1, our model outper-
forms all these models across five datasets.
https://portal.gdc.cancer.gov/
Models
BRCA
BLCA
GBM&LGG
LUAD
UCEC
Mean
DeepGraphConv [23]
0.577 ± 0.021
0.580 ± 0.048
0.721 ± 0.031
0.580 ± 0.023
0.567 ± 0.072
0.605
DeepAttnMISL [40]
0.504 ± 0.042
0.524 ± 0.043
0.734 ± 0.029
0.548 ± 0.050
0.597 ± 0.059
0.581
Patch-GCN [2]
0.598 ± 0.038
0.586 ± 0.029
0.803 ± 0.021
0.545 ± 0.012
0.603 ± 0.064
0.628
ABMIL [17]
0.536 ± 0.038
0.564 ± 0.050
0.787 ± 0.028
0.559 ± 0.060
0.625 ± 0.057
0.614
DSMIL [22]
0.603 ± 0.031
0.549 ± 0.036
0.801 ± 0.023
0.594 ± 0.025
0.620 ± 0.089
0.634
DTFDMIL [43]
0.643 ± 0.061
0.626 ± 0.033
0.823 ± 0.038
0.644 ± 0.057
0.752 ± 0.111
0.697
TransMIL [31]
0.643 ± 0.027
0.610 ± 0.020
0.850 ± 0.017
0.620 ± 0.027
0.711 ± 0.751
0.687
HVTSurv [32]
0.624 ± 0.034
0.580 ± 0.069
0.817 ± 0.021
0.620 ± 0.032
0.622 ± 0.019
0.653
S4MIL [11]
0.657 ± 0.048
0.590 ± 0.059
0.850 ± 0.023
0.642 ± 0.031
0.717 ± 0.109
0.691
MambaMIL [38]
0.654 ± 0.042
0.642 ± 0.038
0.861 ± 0.015
0.652 ± 0.027
0.743 ± 0.055
0.710
Ours
0.713 ± 0.052
0.675 ± 0.018
0.870 ± 0.020
0.666 ± 0.033
0.760 ± 0.047
0.737
Table 1. We evaluate the C-index performance of our model against other state-of-the-art models across five datasets. The comparison is
conducted with five-fold cross-validation, and the results are reported as the mean and standard deviation across the folds.
4.5. Ablation Studies
In this section, we evaluate the effectiveness of each compo-
nent on the same five-fold splits. The results are presented
in Tab. 2 and Tab. 3.
a. Patch Group and Slide Group
PG SG
BRCA
BLCA
G&L
LUAD
UCEC Mean
×
× .680.042 .662.021 .855.017 .661.029 .751.055 .722
✓
× .706.057 .665.034 .868.028 .660.036 .750.090 .729
×
✓
.689.048 .661.029 .862.018 .671.012 .748.094 .726
✓
✓
.713.052 .675.018 .870.020 .666.033 .760.047 .737
b. Graph Representation
Graph
BRCA
BLCA
G&L
LUAD
UCEC Mean
w/o
.721.075 .667.021 .843.033 .646.035 .706.072 .717
w/
.713.052 .675.018 .870.020 .666.033 .760.047 .737
c. Prediction Head
Head
BRCA
BLCA
G&L
LUAD
UCEC Mean
Hrisk
.705.025 .658.036 .853.017 .662.027 .753.070 .720
Hprob
.702.077 .666.024 .862.021 .658.022 .713.057 .726
Hdual
.713.052 .675.018 .870.020 .666.033 .760.047 .737
d. Group Learning Module
Module
BRCA
BLCA
G&L
LUAD
UCEC Mean
Trans
.632.043 .663.029 .860.037 .611.032 .710.046 .695
Mamba .713.052 .675.018 .870.020 .666.033 .760.047 .737
Table 2. Ablation experiments across five datasets: (a) effect of
including or excluding patch and slide groups, with no changes
to model structure; (b) comparison of models with and without
graph-based WSI representation and GC layers; (c) evaluation of
different predictor choices; (d) assessment of the sequence learn-
ing module.
We validate the effectiveness of the proposed Patch
Group (PG) and Slide Group (SG). As shown in Tab. 2a, re-
sults indicate that PG and SG independently improve mean
C-index by 0.8% and 0.4%, respectively, with a combined
improvement of 1.5%.
Additionally, we removed both
the graph representations and GC layers, leaving only PA-
Mamba. As shown in Tab. 2b, this led to a 2% performance
decline, underscoring the necessity of graph representation
for slides in our group framework. Furthermore, we assess
the impact of the prediction heads by activating the risk pre-
diction head Hrisk and the probability prediction head Hprob
separately, using original Cox Loss and DT Loss for super-
vision, respectively. The results in Tab. 2c indicate that the
model obtains higher mean C-index with the dual-head pre-
dictor Hdual.
We also compared the performance of Mamba with that
of Transformer (Trans) [35] in learning extended patch se-
quences. To be specific, we replaced all the three Mamba
modules (ϕpos, ϕatt, ϕint) with Transformer modules that
have a comparable number of parameters. The results, pre-
sented in Tab. 2d, demonstrate that the Mamba-based model
outperforms the Transformer-based model by 4.2% overall.
Additionally, the FLOPs and memory usage are compared
under different node counts, with the results available in
Fig. 5. The Mamba-based model demonstrates significant
advantages in computational efficiency and resource usage.
We investigate the impact of the two scanning methods
in Tab. 3e. The results indicate a 0.5% difference between
the methods individually, while their combination yields an
improvement of 1.4%. Lastly, we show the effect of group
size in Tab. 3f.
4.6. Group Visualization and Interpretability
We visualized the attention sequence gatt and discussed our
findings with pathologists. Fig. 6 presents a example from a
Figure 5. The figure shows a comparison of FLOPs and GPU
memory usage between Transformer-based and Mamba-based
modules in our framework.
e. Sequence Scanning Method
Method BRCA
BLCA
G&L
LUAD
UCEC Mean
Pos
.706.046 .659.022 .861.029 .641.031 .748.055 .723
Att
.709.038 .663.024 .870.026 .643.045 .753.048 .728
Both
.713.052 .675.018 .870.020 .666.033 .760.047 .737
f. Group Size
Size
BRCA
BLCA
G&L
LUAD
UCEC Mean
4
.709.025 .662.031 .865.021 .667.027 .764.049 .733
6
.713.023 .675.018 .870.020 .666.033 .760.047 .737
8
.704.027 .672.019 .872.035 .654.056 .760.037 .732
Table 3. Continuation of Table 2: (e) comparison of sequence
scanning methods; (f) impact of group size.
Low Attention
High Attention
…
A Group of WSIs
…
Attention 
Ordering
…
A0GB
A1C9
A2K5
Figure 6. Visualization of a random group from the UCEC dataset.
In the heatmaps, red regions indicate areas of high prognostic rel-
evance, while blue regions correspond to lower relevance areas.
Patches with the same color border are from the same WSI.
random group in the UCEC dataset, where the attention dis-
tribution of the patches is illustrated through heatmaps. The
pathologists noted that the attention weights of gatt reflect
the prognostic relevance of the patches. Patches with low
attention are typically derived from normal tissues, such as
stroma and muscle, while high-attention patches predomi-
nantly originate from tumor and necrotic regions. This dis-
tinction highlights the model’s capacity to differentiate be-
tween benign and malignant tissues, providing visual evi-
dence for survival prediction.
4.7. Patient Stratification and Clinical Consistency
Stratifying patients into different risk categories is essential
for targeted treatments and effective patient management.
To evaluate our model’s capability in this regard, we classify
patients into high- and low-risk categories for each cancer
type based on the median R, which is outlined in Eq. 4.
We generate Kaplan-Meier (KM) curves [19] and conduct
log-rank tests [1]. The results are illustrated in Fig. 7. The
p-values across all five datasets are below 0.05, indicating
a statistically significant difference between the high- and
low-risk cohorts.
Furthermore, as demonstrated in Fig. 8, we examine the
relationship between patients’ pathological stages and their
corresponding R values. Due to the unavailability of stage
labels for the GBM&LGG dataset, four plots are presented.
The findings reveal a strong correlation between R and clin-
ical pathological staging (p-value < 0.05). This alignment
underscores the model’s potential to yield reliable prog-
nostic outcomes that align with established clinical assess-
ments.
5. Discussion
How to organize the group? In our implementation, we
randomly group slides of the same cancer type. However,
we believe that this grouping method requires further explo-
ration. Organizing slides according to factors such as age,
gender, race, region, and genetic information could deepen
our understanding of how these variables relate to prognos-
tic predictions, offering valuable insights for cancer diagno-
sis and treatment.
Does group benefit from a larger batch size?
Due to
limitations in the code structure, most survival prediction
and computational pathology (CPath) models, as discussed
in Sec. 3.1, are restricted to a batch size of only 1. This
prompts the question:
is the enhanced performance of
our proposed methods merely attributable to incorporating
more than one sample within the group? To investigate this,
we restructured the code for several models to accommo-
date batch sizes greater than 1, aligning batch size (bs) with
group size (gs). Given that these models do not possess an
inherent “slide-level learning module” — such as ϕint in our
framework — that precedes the predictor, we deliberately
excluded the slide group here. For a fair comparison, we
also omitted ϕint from our framework; thus, eliminating the
entire “slide group learning” component. All these exper-
iments employ an identical one-head predictor supervised
by DT Loss. The results, displayed in Tab. 4, do not show
p-value: 1.91e-02
p-value: 1.52e-02
p-value: 8.41e-14
p-value: 8.43e-04
p-value: 7.83e-03
Figure 7. Kaplan-Meier survival curves for high- and low-risk categories (HR and LR) based on the median R across the validation folds
of five cancer datasets. Smaller p-values indicate better patient stratification.
p-value:7.30e-03
p-value:9.70e-03
p-value:3.29e-02
p-value:1.90e-03
Figure 8. Box plots of the model’s prognostic outcomes, categorized by pathological stages.
Model bs gs BRCA
BLCA
G&L
LUAD
UCEC Mean
Ours
1 / .663.057 .646.021 .862.025 .658.030 .768.094 .719
6 / .680.028 .644.028 .858.015 .656.051 .757.064 .719
/ 6 .676.056 .637.030 .868.020 .664.043 .774.074 .724
Deep-
Graph-
Surv
[23]
1 / .577.021 .580.048 .721.031 .580.023 .567.072 .605
6 / .600.021 .569.065 .753.032 .569.043 .526.046 .603
/ 6 .585.046 .582.057 .795.023 .570.045 .596.052 .626
Patch-
GCN
[2]
1 / .598.038 .586.029 .803.021 .548.012 .603.064 .628
6 / .588.050 .546.036 .807.021 .543.039 .621.076 .621
/ 6 .667.031 .603.046 .784.034 .603.015 .673.043 .666
Trans-
MIL
[31]
1 / .643.027 .610.020 .850.017 .620.027 .711.071 .687
6 / .664.001 .607.021 .846.016 .647.039 .732.068 .699
/ 6 .665.022 .626.016 .845.019 .637.035 .747.052 .704
S4-
MIL
[11]
1 / .657.048 .590.059 .850.023 .642.031 .717.109 .691
6 / .648.013 .583.028 .856.028 .639.028 .715.104 .688
/ 6 .650.047 .591.025 .858.031 .639.037 .739.038 .695
Mamba-
MIL
[38]
1 / .654.042 .642.038 .861.015 .652.027 .743.055 .710
6 / .656.025 .631.065 .865.037 .657.046 .759.023 .714
/ 6 .666.035 .648.022 .874.024 .660.033 .752.038 .720
Table 4. Model performance under the original settings, batch size
(bs) set to 6 and group size (gs) set to 6.
a strong correlation between batch size and model perfor-
mance. Nevertheless, our proposed method achieved the
highest mean C-index under identical sizes, underscoring
the effectiveness of the group.
Can group enhance other models? We evaluated the ef-
fectiveness of the group method on several two-stage mod-
els, including the graph-based models DeepGraphSurv [23]
and Patch-GCN [2], as well as the sequence-based models
TransMIL [31], S4MIL [11], and MambaMIL [38]. The
results presented in Tab. 4 show that all these models ben-
efit from the group, which may indicate its potential ver-
satility. Furthermore, since “collective analysis” is a com-
mon practice in clinical pathological assessment, we believe
that group modeling could also benefit related tasks, such as
staging and subtyping, in CPath. Relevant work is currently
underway, and we hope this modeling approach will inspire
researchers and provide pathologists with more comprehen-
sive support across various clinical scenarios.
6. Conclusion
In this paper, we propose a novel group-level survival pre-
diction framework that is in line with clinical observation.
Our framework regards a group of slides as a single sample,
sequences patches and slides to facilitate inter-slide evalua-
tion. This approach enables each slide to draw on the col-
lective insights of the group, ultimately improving patient
outcomes.
To tackle the problems related to extended patch se-
quences and capturing slide-specific features, we introduce
GPAMamba to enhance intra- and inter-slide feature inter-
action in turn and capture prognostically relevant character-
istics. We also develop a dual-head predictor that provides
a comprehensive assessment of both risk scores and prob-
ability distributions. Extensive experiments carried out on
various datasets demonstrate the superiority of our model
and the effectiveness of its components. Furthermore, we
visualize the WSIs and patches within a group to confirm
the model’s ability to identify tumor and necrotic regions
and their relevance among patients. We also validate the
consistency of the model outputs with patient stratification
and cancer staging, highlighting its robustness and align-
ment with clinical assessments.
In the future, we plan to extend our GroupMIL frame-
work to more CPath tasks. As a general concept to address
challenges in CPath and MIL problems, group modeling
may offer a solution from new perspectives.
References
[1] J Martin Bland and Douglas G Altman. The logrank test.
Bmj, 328(7447):1073, 2004. 7
[2] Richard
J
Chen,
Ming
Y
Lu,
Muhammad
Shaban,
Chengkuan Chen, Tiffany Y Chen, Drew FK Williamson,
and Faisal Mahmood.
Whole slide images are 2d point
clouds: Context-aware survival prediction using patch-based
graph convolutional networks. In Medical Image Computing
and Computer Assisted Intervention–MICCAI 2021: 24th In-
ternational Conference, Strasbourg, France, September 27–
October 1, 2021, Proceedings, Part VIII 24, pages 339–349.
Springer, 2021. 1, 2, 3, 5, 6, 8
[3] Richard J Chen, Ming Y Lu, Wei-Hung Weng, Tiffany Y
Chen, Drew FK Williamson, Trevor Manz, Maha Shady, and
Faisal Mahmood. Multimodal co-attention transformer for
survival prediction in gigapixel whole slide images. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision, pages 4015–4025, 2021. 5
[4] Richard J Chen, Chengkuan Chen, Yicong Li, Tiffany Y
Chen, Andrew D Trister, Rahul G Krishnan, and Faisal
Mahmood. Scaling vision transformers to gigapixel images
via hierarchical self-supervised learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 16144–16155, 2022. 1, 2
[5] David R Cox. Regression models and life-tables. Journal of
the Royal Statistical Society: Series B (Methodological), 34
(2):187–202, 1972. 2, 3
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009. 5
[7] Kexin Ding, Mu Zhou, Dimitris N Metaxas, and Shaoting
Zhang. Pathology-and-genomics multimodal transformer for
survival outcome prediction. In International Conference on
Medical Image Computing and Computer-Assisted Interven-
tion, pages 622–631. Springer, 2023. 2
[8] Alexey Dosovitskiy.
An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 2
[9] Lei Fan, Arcot Sowmya, Erik Meijering, and Yang Song.
Learning visual features by colorization for slide-consistent
survival prediction from whole slide images.
In Medi-
cal Image Computing and Computer Assisted Intervention–
MICCAI 2021: 24th International Conference, Strasbourg,
France, September 27–October 1, 2021, Proceedings, Part
VIII 24, pages 592–601. Springer, 2021. 2, 3
[10] Lei Fan, Arcot Sowmya, Erik Meijering, and Yang Song.
Cancer survival prediction from whole slide images with
self-supervised learning and slide consistency. IEEE Trans-
actions on Medical Imaging, 2022. 2, 3
[11] Leo Fillioux, Joseph Boyd, Maria Vakalopoulou, Paul-Henry
Courn`ede, and Stergios Christodoulidis.
Structured state
space models for multiple instance learning in digital pathol-
ogy. In International Conference on Medical Image Com-
puting and Computer-Assisted Intervention, pages 594–604.
Springer, 2023. 5, 6, 8
[12] Wolf H Fridman, Laurence Zitvogel, Catherine Saut`es-
Fridman, and Guido Kroemer. The immune contexture in
cancer prognosis and treatment. Nature reviews Clinical on-
cology, 14(12):717–734, 2017. 1
[13] Albert Gu and Tri Dao.
Mamba: Linear-time sequence
modeling with selective state spaces.
arXiv preprint
arXiv:2312.00752, 2023. 2, 3, 4
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 5
[15] Dan Hendrycks and Kevin Gimpel.
Gaussian error linear
units (gelus). arXiv preprint arXiv:1606.08415, 2016. 4
[16] Wentai Hou, Yan He, Bingjian Yao, Lequan Yu, Rongshan
Yu, Feng Gao, and Liansheng Wang. Multi-scope analysis
driven hierarchical graph transformer for whole slide image
based cancer survival prediction. In International Confer-
ence on Medical Image Computing and Computer-Assisted
Intervention, pages 745–754. Springer, 2023. 1, 3
[17] Maximilian Ilse,
Jakub Tomczak,
and Max Welling.
Attention-based deep multiple instance learning. In Inter-
national conference on machine learning, pages 2127–2136.
PMLR, 2018. 5, 6
[18] Cyriac Kandoth, Michael D McLellan, Fabio Vandin, Kai
Ye, Beifang Niu, Charles Lu, Mingchao Xie, Qunyuan
Zhang, Joshua F McMichael, Matthew A Wyczalkowski,
et al. Mutational landscape and significance across 12 major
cancer types. Nature, 502(7471):333–339, 2013. 5
[19] Edward L Kaplan and Paul Meier. Nonparametric estima-
tion from incomplete observations. Journal of the American
statistical association, 53(282):457–481, 1958. 7
[20] Diederik P Kingma. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980, 2014. 5
[21] Vinay Kumar, Abul K Abbas, and Jon C Aster. Robbins basic
pathology e-book. Elsevier Health Sciences, 2012. 1, 3
[22] Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple
instance learning network for whole slide image classifica-
tion with self-supervised contrastive learning. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 14318–14328, 2021. 5, 6
[23] Ruoyu Li, Jiawen Yao, Xinliang Zhu, Yeqing Li, and Jun-
zhou Huang. Graph cnn for survival analysis on whole slide
pathological images. In International Conference on Med-
ical Image Computing and Computer-Assisted Intervention,
pages 174–182. Springer, 2018. 1, 2, 5, 6, 8
[24] Zhe Li, Yuming Jiang, Mengkang Lu, Ruijiang Li, and Yong
Xia.
Survival prediction via hierarchical multimodal co-
attention transformer: A computational histology-radiology
solution.
IEEE Transactions on Medical Imaging, 42(9):
2678–2689, 2023. 2
[25] Jianfang Liu, Tara Lichtenberg, Katherine A Hoadley,
Laila M Poisson, Alexander J Lazar, Andrew D Cherniack,
Albert J Kovatich, Christopher C Benz, Douglas A Levine,
Adrian V Lee, et al. An integrated tcga pan-cancer clinical
data resource to drive high-quality survival outcome analyt-
ics. Cell, 173(2):400–416, 2018. 1
[26] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J
Chen, Matteo Barbieri, and Faisal Mahmood. Data-efficient
and weakly supervised computational pathology on whole-
slide images. Nature biomedical engineering, 5(6):555–570,
2021. 5
[27] Callum Christopher Mackenzie, Muhammad Dawood, Si-
mon Graham, Mark Eastwood, et al. Neural graph modelling
of whole slide images for survival ranking. In Learning on
Graphs Conference, pages 48–1. PMLR, 2022. 2
[28] Ramin Nakhli, Puria Azadi Moghadam, Haoyang Mi, Hos-
sein Farahani, Alexander Baras, Blake Gilks, and Ali
Bashashati. Amigo: Sparse multi-modal graph transformer
with shared-context processing for representation learning of
giga-pixel images. arXiv preprint arXiv:2303.00865, 2023.
3
[29] Prajit Ramachandran,
Barret Zoph,
and Quoc V Le.
Searching
for
activation
functions.
arXiv
preprint
arXiv:1710.05941, 2017. 4
[30] Wei Shao, Tongxin Wang, Zhi Huang, Zhi Han, Jie Zhang,
and Kun Huang. Weakly supervised deep ordinal cox model
for survival prediction from whole-slide pathological im-
ages. IEEE Transactions on Medical Imaging, 40(12):3739–
3747, 2021. 1, 2
[31] Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian
Zhang, Xiangyang Ji, et al. Transmil: Transformer based
correlated multiple instance learning for whole slide image
classification.
Advances in neural information processing
systems, 34:2136–2147, 2021. 5, 6, 8
[32] Zhuchen Shao, Yang Chen, Hao Bian, Jian Zhang, Guojun
Liu, and Yongbing Zhang.
Hvtsurv: Hierarchical vision
transformer for patient-level survival prediction from whole
slide image. In Proceedings of the AAAI Conference on Ar-
tificial Intelligence, pages 2209–2217, 2023. 1, 2, 3, 5, 6
[33] Harald Steck, Balaji Krishnapuram, Cary Dehing-Oberije,
Philippe Lambin, and Vikas C Raykar. On ranking in sur-
vival analysis: Bounds on the concordance index. Advances
in neural information processing systems, 20, 2007. 5
[34] Gerhard Tutz, Matthias Schmid, et al.
Modeling discrete
time-to-event data. Springer, 2016. 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 2, 6
[36] Han Wang, Dan Jiang, Haixian Zhang, Yujue Wang, Li Yang,
David J Kerr, and Yi Zhang.
An explainable coarse-to-
fine survival analysis method on multi-center whole slide
images. IEEE Transactions on Artificial Intelligence, 5(3):
1316–1327, 2023. 2
[37] Zichen Wang, Jiayun Li, Zhufeng Pan, Wenyuan Li, An-
thony Sisk, Huihui Ye, William Speier, and Corey W Arnold.
Hierarchical graph pathomic network for progression free
survival prediction.
In Medical Image Computing and
Computer Assisted Intervention–MICCAI 2021: 24th Inter-
national Conference, Strasbourg, France, September 27–
October 1, 2021, Proceedings, Part VIII 24, pages 227–237.
Springer, 2021. 1, 2, 3
[38] Shu Yang, Yihui Wang, and Hao Chen.
Mambamil: En-
hancing long sequence modeling with sequence reordering in
computational pathology. arXiv preprint arXiv:2403.06800,
2024. 1, 2, 5, 6, 8
[39] Jiawen Yao, Xinliang Zhu, and Junzhou Huang. Deep multi-
instance learning for survival prediction from whole slide
images.
In Medical Image Computing and Computer As-
sisted Intervention–MICCAI 2019: 22nd International Con-
ference, Shenzhen, China, October 13–17, 2019, Proceed-
ings, Part I 22, pages 496–504. Springer, 2019. 2, 3
[40] Jiawen Yao, Xinliang Zhu, Jitendra Jonnagaddala, Nicholas
Hawkins, and Junzhou Huang. Whole slide images based
cancer survival prediction using attention guided deep multi-
ple instance learning networks. Medical Image Analysis, 65:
101789, 2020. 1, 3, 5, 6
[41] Shekoufeh Gorgi Zadeh and Matthias Schmid. Bias in cross-
entropy-based training of deep survival networks.
IEEE
transactions on pattern analysis and machine intelligence,
43(9):3126–3137, 2020. 2, 3
[42] Mark D Zarella, Douglas Bowman, Famke Aeffner, Navid
Farahani, Albert Xthona, Syeda Fatima Absar, Anil Par-
wani, Marilyn Bui, and Douglas J Hartman.
A practical
guide to whole slide imaging: a white paper from the digital
pathology association. Archives of pathology & laboratory
medicine, 143(2):222–234, 2019. 1, 3
[43] Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao,
Xiaoyun Yang, Sarah E Coupland, and Yalin Zheng. Dtfd-
mil: Double-tier feature distillation multiple instance learn-
ing for histopathology whole slide image classification. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, pages 18802–18812, 2022. 5,
6
[44] Xinliang Zhu, Jiawen Yao, and Junzhou Huang. Deep con-
volutional neural network for survival analysis with patho-
logical images. In 2016 IEEE International Conference on
Bioinformatics and Biomedicine (BIBM), pages 544–547.
IEEE, 2016. 2
[45] Xinliang Zhu, Jiawen Yao, Feiyun Zhu, and Junzhou
Huang. Wsisa: Making survival prediction from whole slide
histopathological images. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
7234–7242, 2017. 1
