Queryable Prototype Multiple Instance Learning with Vision-Language Models
for Incremental Whole Slide Image Classification
Jiaxiang Gou1, Luping Ji1*, Pei Liu1, Mao Ye1
1University of Electronic Science and Technology of China
Abstract
Whole Slide Image (WSI) classification has very significant
applications in clinical pathology, e.g., tumor identification
and cancer diagnosis. Currently, most research attention is fo-
cused on Multiple Instance Learning (MIL) using static WSI
datasets. One of the most obvious weaknesses of these meth-
ods is that they cannot efficiently preserve and utilize previ-
ously learned knowledge. With any new data arriving, clas-
sification models are required to be re-trained on both previ-
ous and current new data. To overcome this shortcoming and
break through traditional vision modality, this paper proposes
the first Vision-Language-based framework with Queryable
Prototype Multiple Instance Learning (QPMIL-VL) specially
designed for incremental WSI classification. This framework
mainly consists of two information processing branches: one
is for generating bag-level features by prototype-guided ag-
gregation of instance features, while the other is for enhanc-
ing class features through a combination of class ensem-
ble, tunable vector and class similarity loss. The experiments
on four public WSI datasets demonstrate that our QPMIL-
VL framework is effective for incremental WSI classification
and often significantly outperforms other compared methods,
achieving state-of-the-art (SOTA) performance.
Introduction
Histopathology Whole Slide Image (WSI) is crucial for
the diagnosis and treatment of tumor diseases (Bera et al.
2019; Song et al. 2023). To model gigapixel WSIs (e.g.,
90, 000 × 90, 000 pixels) for training clinical-grade models,
many methods based on Multiple Instance Learning (MIL)
have been extensively studied in the field of computational
pathology (CPATH) (Campanella et al. 2019; Lu et al. 2021;
Lin et al. 2023, 2024; Liu et al. 2024c). A shared character-
istic of these methods is that they are specially designed for
capturing a static data distribution (e.g., a given WSI dataset
with two lung cancer subtypes), still following conventional
machine learning paradigm. However, in the real-world, the
distribution of WSI data could be dynamic due to the arrival
of new datasets or the discovery of emerging cancer types
(Van der Laak, Litjens, and Ciompi 2021; Derakhshani et al.
2022). As a result, a well-trained WSI classification model
that is suitable for previous data often cannot adapt to new
data. Therefore, the model must be re-trained on the entire
*Corresponding author: Luping Ji (jiluping@uestc.edu.cn).
Image 
Encoder
WSI  Data Stream
Buffer
Instance features
(a)
Prompts
(b)
Store
Aggregator
WSI  Data Stream
Vision
Image 
Encoder
Text 
Encoder
Prototype 
Pool
Instance features
Aggregator
Class features
Enhancing
Query
Classify
Classify
Vision
Language
Class 
texts
Replay
Figure 1: (a) Existing visual modality framework for in-
cremental WSI classification with buffer dependency; (b)
our proposed Vision-Language-based framework with a
queryable prototype pool and class feature enhancement.
dataset (including both previous and new data), leading to
a significant increase in training costs. This presents the ne-
cessities of new methods well-tailored to dynamic data dis-
tributions.
Incremental Learning (IL) (Li and Hoiem 2017), also re-
ferred to as continual learning or lifelong learning, is exactly
such approach to filling the aforementioned gap. As a new
learning paradigm, it can encourage models to maintain the
memory stability on old data distributions while adapting to
a new one (Wang et al. 2024), thus mitigating the notori-
ous catastrophic forgetting problem (McCloskey and Cohen
1989). Owing to such practical benefit, this new paradigm
has attracted great attention and has shown strong poten-
tial for application in the real-world (Li et al. 2019; Cac-
cia et al. 2021; Wang, Huang, and Hong 2022). Similarly,
in CPATH, IL strategies are also noticed recently and show
promising performance in learning from dynamic WSI data.
Concretely, ConSlide (Huang et al. 2023a) is proposed for
arXiv:2410.10573v1  [cs.CV]  14 Oct 2024
the first time to study incremental WSI classification, as il-
lustrated in Fig. 1 (a). (1) Like most existing methods, Con-
Slide captures image-level features for WSI classification,
relying solely on a traditional vision modality. (2) Moreover,
to retain certain past data, an additional buffer is utilized
to enable the model to review previous knowledge while
learning new information. This may result in privacy con-
cerns (Shokri and Shmatikov 2015) and higher computa-
tional costs, leaving a gap to the resource efficiency goal
pursued in IL (Wang et al. 2024).
Recent works, e.g., PLIP (Huang et al. 2023b), Prov-
GigaPath (Xu et al. 2024), and CONCH (Lu et al. 2024),
have deeply demonstrated the significant potential of Vision-
Language Models (VLMs) in CPATH. Breaking through tra-
ditional vision modality, this kind of new models could ef-
fectively exploit pathology features from both vision and
language modalities. All of these motivate us to leverage
foundational pathology VLMs to study incremental learning
for WSI classification.
To extend the pure vision frameworks, we propose a new
Vision-Language-based framework with Queryable Pro-
totype Multiple Instance Learning (QPMIL-VL) for in-
cremental WSI classification, as shown in Fig. 1 (b). Specif-
ically, (1) in the vision branch, inspired by L2P (Wang
et al. 2022), we design a Queryable Prototype MIL (QPMIL)
module comprising a prototype pool and prototype-guided
aggregation. This module encourages the model to incre-
mentally learn a set of prototypes corresponding to each
dataset through a prototype-query mechanism, effectively
mitigating catastrophic forgetting without relying on extra
buffer. (2) In the language branch, we propose a Class Fea-
ture Enhancement (CFE) module. We employ CFE to in-
crease the diversity of text descriptions via a class ensem-
ble approach and further refine it with tunable vector and a
class similarity loss. Our experiments show that QPMIL-VL
could often surpass other state-of-the-art (SOTA) methods
by a large margin in incremental WSI classification tasks.
The contributions of this work are as follows:
I) To our knowledge, beyond traditional pure vision
frameworks, we propose the first Vision-Language-based
framework for incremental WSI classification.
II) We devise a new Queryable Prototype Multiple In-
stance Learning (QPMIL) strategy to alleviate catastrophic
forgetting. In this strategy, instance features are matched
with a set of prototypes through querying to guide the gen-
eration of WSI bag-level feature.
III) Extensive comparison and ablation experiments are
performed on four public WSI datasets. The results demon-
strate the superiority of our QPMIL-VL over the existing
methods.
Related Work
Multiple Instance Learning for WSI Classification
Multiple Instance Learning (MIL) focuses on learning from
weakly-annotated data, where only an unknown subset of
instances within each input bag is relevant to the label. Due
to the unique data characteristics of WSI, the MIL paradigm
is widely applied in WSI classification (Campanella et al.
2019; Li, Li, and Eliceiri 2021; Shao et al. 2021; Zhang
et al. 2022; Zheng et al. 2023; Liu et al. 2024b). Among
these methods, ABMIL (Ilse, Tomczak, and Welling 2018)
proposes an attention-based instance feature aggregation.
By leveraging ResNet for instance-level feature extraction,
CLAM (Lu et al. 2021) introduces an interpretable weakly-
supervised learning method, focused on data-efficient WSI
processing. However, most of these studies focus on static
MIL classification tasks and rarely consider the incremental
WSI classifcation with MIL.
Incremental Learning
Recently, Incremental Learning (IL) has gained signifi-
cant attention, aiming to enable deep models to continu-
ally acquire knowledge like humans. IL algorithms can be
classified into three main categories. Regularization-based
methods (Kirkpatrick et al. 2017; Li and Hoiem 2017)
aim to reduce catastrophic forgetting by limiting changes
to key parameters but often fall short of optimal results.
Architecture-based methods (Rusu et al. 2016; Mallya and
Lazebnik 2018; Li et al. 2019; Ke, Liu, and Huang 2020)
train an independent module for each task but typically
apply only to task-incremental learning scenario requiring
task identity during inference. Additionally, although preva-
lent Rehearsal-based methods (Chaudhry et al. 2018, 2019;
Prabhu, Torr, and Dokania 2020; Buzzega et al. 2020; Cac-
cia et al. 2021) could achieve SOTA performance on various
benchmarks (Parisi et al. 2019; Mai et al. 2022), these meth-
ods rely on additional buffer to store past data. When buffer
size is limited, the performance of such these methods dete-
riorates (Cha, Lee, and Shin 2021). They will even become
inapplicable when historical data is unavailable (Shokri and
Shmatikov 2015). Currently, most of these studies focus
more on natural images, not medical gigapixel WSIs, except
ConSlide (Huang et al. 2023a).
Vision-Language Models
Recent research has made significant success in devel-
oping Vision-Language Models (VLMs). For example,
CLIP (Radford et al. 2021) learns SOTA image represen-
tations, by the training on 400 million (image, text) pairs.
Coca (Yu et al. 2022) employs contrastive and captioning
losses to pre-train a foundation model comprising an image-
text encoder-decoder. Additionally, some specialized VLMs
like PLIP (Huang et al. 2023b), Prov-GigaPath (Xu et al.
2024) and CONCH (Lu et al. 2024) have been developed in
CPATH. These VLMs need large datasets for pre-training, so
they could often show excellent generalizability capabilities.
Moreover, some VLMs combined with certain fine-tuning
methods (Zhou et al. 2022; Yu et al. 2023; Gao et al. 2024)
are widely transferred to various downstream tasks (Wang
et al. 2023; Qu et al. 2024; Li et al. 2024; Liu et al. 2024a).
At present, to our knowledge, the potential of VLMs for in-
cremental WSI classification remains unexplored.
Preliminaries
Problem Formulation
Incremental WSI classification requires a model to contin-
uously learn new knowledge from sequential tasks on non-
stationary datasets, while retaining the knowledge learned
from prior tasks. We define a sequence of WSI datasets
D
= {D1, · · · , DT }, in which the t-th dataset Dt
=
{(xt
i, yt
i)}nt
i=1 contains the tuples of sample xt
i and its cor-
responding bag-level label yt
i, where 1 ≤t ≤T and nt
is the sample number in Dt. When training is made on
current dataset Dtc, the data from previous datasets (i.e.,
D1, · · · , Dtc−1) may be limited or even unavailable.
For a given WSI sample xt
i, in class-incremental learn-
ing scenario, the model only uses xt
i to predict its bag-level
label yt
i. In contrast, in task-incremental learning scenario,
the model combines sample xt
i with its corresponding task
identity t together to predict label yt
i.
VLM Pre-training and Inference
In our work, we choose a SOTA VLM in pathology, i.e.,
CONCH (Lu et al. 2024), as our image and text encoders.
Pre-training. CONCH uses the contrastive learning on
diverse histopathology images, biomedical text, and over
1.17 million image-caption pairs (not including TCGA
Datasets). Based on CoCa (Yu et al. 2022) framework,
CONCH combines an image encoder Eimg with f(·, θ), a
text encoder Etxt with g(·, ϕ), and a multi-modal fusion de-
coder.
Zero-shot Patch Inference. Since CONCH is pre-trained
on patch-level WSI image-text pairs, after training is com-
pleted, zero-shot inference can be fulfilled on patch-level
samples. Specifically, given a patch p and class text token
set {tc}C
c=1, where C denotes total class number. The clas-
sification can be conducted based on the cosine similarity
between patch feature f(p, θ) and class feature g(tc, ϕ).
Proposed Method
To effectively fulfil incremental WSI classification, on tra-
ditional MIL framework, we introduce Vision-Language
Model and propose a queryable prototype MIL framework,
as shown in Fig. 2. It consists of two branches, one for gen-
erating bag-level feature (QPMIL), and the other for enhanc-
ing class text feature (CFE). In inference, the features from
both branches will be used to compute classification proba-
bility.
Queryable Prototype Multiple Instance Learning
(QPMIL)
As usual, one of the most important objectives for WSI
classification is to generate discriminative bag-level feature
representation. However, unlike traditional static classifica-
tion, our incremental classification task requires that all sub-
sequent model learning specifically avoids causing catas-
trophic impacts on the tasks learned earlier.
Moreover, it has been proved that the instances in differ-
ent WSIs, even in different WSI datasets, could often appear
high similarities, in terms of cell shape, staining response,
and tissue arrangement (Song et al. 2024).Therefore, similar
instances, even if they are from different datasets, could be
clustered into the same instance prototype. In view of this,
to generate more effective WSI bag-level feature, we intro-
duce VLM and instance-level feature prototypes, designing
a queryable prototype pool and prototype-guided aggrega-
tion.
Prototype Pool. Specifically, the prototype pool consists
of M (key, prompt) prototype pairs {(ki, P i)}M
i=1, ki ∈
RDf , P i ∈RLP ×De, where Df is the output feature dimen-
sion of encoder Eimg and Etxt, De represents the embed-
ding dimension of Etxt and LP indicates the total number of
learnable vectors in P i. In a prototype pair, the key works as
a prototype identifier, used by WSIs for querying, while the
prompt acts as a prototype descriptor, utilized to describe the
specific visual feature of instance prototype within WSIs.
Given a WSI sample x from Dt, it is first divided into n
non-overlapping patches {pi}n
i=1, i.e., instances. Then, all
instances are encoded by image encoder Eimg with f(·, θ),
generating n instance features Z ∈Rn×Df , as shown in
Fig. 2. Next, we apply MaxPooling to features Z to obtain
the query vector of sample x, i.e., z = MaxPooling(Z) ∈
RDf . MeanPooling is also tried and its experimental result
is provided in our Supplementary Material.
Subsequently, a query metric function q : RDf × RDf →
R is defined to evaluate how close the query vector z
matches the key of each prototype pair. We employs the co-
sine distance for this evaluation. Finally, we are able to find
the most matched prototype keys through
Kx = Top-N min{q(z, ki)}M
i=1,
(1)
where Top-N min denotes the operation of choosing the top-
N prototype keys with minimal z-key distances. However,
in prototype matching on different WSI datasets by Eq. 1,
we observe that almost all samples tend to choose the same
set of N prototypes from pool. This could usually lead to
catastrophic forgetting in incremental learning. To effec-
tively avoid this tendency, we define a penalty factor ptc
i to
optimize prototype choosing:







Kx = Top-N min{q(z, ki) · ptc
i }M
i=1
Ptc =
1
tc −1
tc−1
X
t=1
Ft, tc ⩾2,
(2)
where tc is the index of current WSI dataset Dtc, Ft =
[f t
1, · · · , f t
M] denotes the matching frequency table of M
prototype keys on dataset Dt and ptc
i represents the i-th ele-
ment of penalty table Ptc. In essence, this penalty factor am-
plifies the distances of z to all the prototype keys that have
high frequency matching on those datasets prior to Dtc.
Prototype-guided Aggregation. After key matching, the
prompts Px = {P x
i }N
i=1 of top-N prototype pairs will be
further fed into text encoder Etxt to generate N prototype
features F p ∈RN×Df .
Next, we compute the cosine similarity matrix S ∈Rn×N
between instance features Z and prototype features F p.
Since each column in S exactly corresponds to a prototype, a
Softmax normalization along column direction is performed
Data Stream
90,000 90,000

WSI x
( , )
f


Instance features  Z
Prototype Pool
p
F
Aggregation 
matrix
Bag-level 
feature
bf
ClassName + Template
+
txt
F
txt
f
…
txtf
img
E
 
1
2
3
T
t
Query vector z
Pooling
Query
Prompts
Prototype features
W
( , )
g


txt
E
lung adenocarcinoma
adenocarcinom of the lung
a histopathological image 
of ClassName
an H&E stained image 
showing ClassName
Class feature matrix
Averaging Class feature
+
Tunable v
Enhanced 
class feature
Language
Vision
Similarity
 
x
( , )
g


txt
E
key-prompt
Figure 2: The framework of QPMIL-VL. The prompts in the prototype pool enable an efficient incremental learning process by
gradually capturing the visual feature descriptions of instance prototypes present in the sequential WSI datasets.
to obtain a weighted aggregation matrix W
∈Rn×N.
Finally, we obtain the prototype-guided bag-level features
through W , and their average yields the bag-level feature
f b ∈RDf for the WSI sample x. The procedure can be
represented as
f b = mean(Softmax(Z × F p
⊤)⊤× Z).
(3)
Ideally, in incremental training, different datasets will
match a non-overlapped set of prototype pairs. In practice,
some prototype pairs could often be shared with low fre-
quencies by different datasets. Our method doesn’t impose
a strict independence of matched prototype pairs. Instead,
it allows WSI datasets to choose their optimal prototypes,
providing more flexibility and adaptability.
Class Feature Enhancement (CFE)
As shown in Figure 2, class text feature enhancing branch is
designed to generate an enhanced class feature f ′
txt.
Class Ensemble. For a specific subtype of cancer, such as
“lung adenocarcinoma”, it can also be expressed as “adeno-
carcinoma of the lung”. Therefore, we adopt class ensemble
to obtain better class feature. For each cancer subtype, we
combine several common class names and templates to ob-
tain m different class text descriptions. All text descriptions
are encoded by text encoder Etxt with g(·, ϕ) to obtain initial
class feature matrix F txt ∈Rm×Df , and by averaging F txt,
we get the class feature f txt ∈RDf for this subtype. The
entire calculation process can be expressed as
f txt = mean(g(T , ϕ)),
(4)
where T ∈Rm×L represents the text tokens corresponding
to the text descriptions and L is the length of text token.
Tunable Vector. To further refine class feature f txt, re-
ferring to the TaskRes (Yu et al. 2023), we design a tunable
(i.e., learnable) vector v ∈RDf of the same dimension for
each class. The final enhanced class feature f ′
txt ∈RDf is
calculated through
f ′
txt = f txt + α · v,
(5)
where α (default constant 0.5) is an additional amplitude
factor to control the superposition scale of vector v on fea-
ture f txt.
Prediction and Optimization
Class Prediction. The probability of predicting the WSI
sample x as the class y can be computed as
p(y|x) =
eτ·⟨f b,[f ′
txt]y⟩
PC
c=1 eτ·⟨f b,[f ′
txt]c⟩,
(6)
where C indicates the total number of classes, [f ′
txt]c rep-
resents the c-th class feature, τ is a temperature parameter
learned by CONCH (Lu et al. 2024) and ⟨·, ·⟩denotes the
cosine similarity.
Optimization Objective. To appropriately reduce the
similarity between features of different classes, we firstly
define a class similarity loss, which is calculated through
LS =
2
C · (C −1)
C
X
i=1
C
X
j=i+1
⟨[f ′
txt]i, [f ′
txt]j⟩+ 1.
(7)
Moreover, there are another two losses. One is classifica-
tion loss. Based on Eq. 6, it is computed by
LC = −log
eτ·⟨f b,[f ′
txt]y⟩
PC
c=1 eτ·⟨f b,[f ′
txt]c⟩.
(8)
IL Type
Method
ACC (↑)
Upper-bound Ratio (↑)
Forgetting (↓)
BWT (↑)
Params (M)
Masked ACC (↑)
Baseline
JointTrain (Upper)
0.908 ± 0.022
1.000 ± 0.000
-
-
0.502
0.937 ± 0.022
FineTune (Lower)
0.308 ± 0.045
0.336 ± 0.045
0.841 ± 0.054
-0.841 ± 0.054
0.844 ± 0.038
Regularization-based
EWC (Kirkpatrick et al. 2017)
0.309 ± 0.051
0.338 ± 0.055
0.836 ± 0.064
-0.836 ± 0.064
0.502
0.840 ± 0.035
LwF (Li and Hoiem 2017)
0.378 ± 0.081
0.412 ± 0.083
0.731 ± 0.116
-0.731 ± 0.116
0.916 ± 0.031
Rehearsal-based
A-GEM/30 (Chaudhry et al. 2018)
0.436 ± 0.058
0.477 ± 0.070
0.670 ± 0.072
-0.670 ± 0.072
0.502
0.879 ± 0.045
ER-ACE/30 (Caccia et al. 2021)
0.666 ± 0.049
0.732 ± 0.051
0.075 ± 0.057
-0.070 ± 0.059
0.917 ± 0.023
DER++/30 (Buzzega et al. 2020)
0.749 ± 0.055
0.823 ± 0.059
0.219 ± 0.059
-0.214 ± 0.062
0.904 ± 0.029
ER/30 (Chaudhry et al. 2019)
0.790 ± 0.040
0.870 ± 0.049
0.186 ± 0.044
-0.186 ± 0.044
0.894 ± 0.036
ConSlide/30 (Huang et al. 2023a)
0.659 ± 0.022
-
0.076 ± 0.030
-0.075 ± 0.030
39.446
0.861 ± 0.017
ER/100 (Chaudhry et al. 2019)
0.827 ± 0.028
0.910 ± 0.034
0.143 ± 0.034
-0.142 ± 0.032
0.502
0.918 ± 0.028
Pre-trained VLM-based
AttriCLIP (Wang et al. 2023)
0.616 ± 0.056
0.677 ± 0.071
0.285 ± 0.059
-0.285 ± 0.059
0.111
0.844 ± 0.026
MI-Zero (Lu et al. 2023)
0.839 ± 0.034
0.927 ± 0.044
-
-
-
0.909 ± 0.019
PQMIL-VL (ours)
0.890 ± 0.021
0.982 ± 0.032
0.027 ± 0.014
-0.027 ± 0.014
0.365
0.930 ± 0.018
Table 1: Main results in class-incremental learning scenario on forward-order training. The best performances are highlighted
as bold. “Params” represents the number of learnable parameters. Gray numbers are directly cited from published paper. “/30”
and “/100” mean buffer 30 WSIs and buffer 100 WSIs, respectively. We present the Masked ACC reflecting task-incremental
learning scenario for reference.
The other is matching loss to make the matched keys
Kx = {kx
i }N
i=1 closer to the query vector z of WSI sam-
ple x. It is defined as
LM = 1
N
N
X
i=1
q(z, kx
i ).
(9)
To optimize our QPMIL-VL model in training, based on
the three losses above, total loss LO is computed by
LO = LC + λ · LM + β · LS,
(10)
where balance factors λ and β are set to constant 0.5. There-
fore, our optimization objective is to minimize LO.
Experiments
Following the setup of previous representative work (Van de
Ven and Tolias 2019), we mainly evaluate QPMIL-VL in the
class-incremental learning scenario and additionally report
the results in the task-incremental scenario.
Experimental Details
Datasets. Following ConSlide (Huang et al. 2023a), we
use four public WSI datasets from The Cancer Genome
Atla (TCGA) repository: non-small cell lung carcinoma
(NSCLC), invasive breast carcinoma (BRCA), renal cell car-
cinoma (RCC) and esophageal carcinoma (ESCA). With the
arrival of each dataset, we add two additional classes to the
model for training and evaluation.
We use CLAM (Lu et al. 2021) to crop non-overlapping
256 × 256 patches from the segmented tissue at 10×
magnification. Then, pre-trained image encoder Eimg in
CONCH (Lu et al. 2024) is used to extract instance features.
Evaluation Metrics. We assess all incremental learning
methods with Average Accuracy (ACC), Upper-bound Ra-
tio, Forgetting and Backward Transfer (BWT) (Lopez-Paz
and Ranzato 2017; Hayes et al. 2018; Fini et al. 2022). Be-
sides, we provide the Masked Average Accuracy (Masked
ACC) metric, which is used for reference only to evaluate
performance in task-incremental learning scenario by mask-
ing the logits of irrelevant classes. All results are reported by
the mean and standard deviation of ten-fold cross-validation.
Training Details. In all experiments, we set the num-
ber of training epoch to 12 for each dataset and use the
Adam (Kingma and Ba 2014) optimizer with lr = 0.001 and
weight decay = 0.0005. The mini-batch sizes on forward-
order and reverse-order training are set to 16 and 8, re-
spectively. For QPMIL-VL, we set M = 20, N = 5 and
LP = 24. All experiments are conducted on a machine with
two NVIDIA GeForce RTX 3090 GPUs.
Refer to our Supplementary Material for more details.
Comparison Results
Compared Methods. It is often necessary to compare with
the upper and lower bounds of classification methods. We
employ the classic ABMIL (Ilse, Tomczak, and Welling
2018) as the aggregator for the vision branch, serving as a
Baseline. Its performances by jointly training on all datasets
(JointTrain, non-incremental) are taken as upper bound,
and those by naively fine-tuning sequential datasets (Fine-
Tune) are taken as lower bound. In addition, we compare
Regularization-based methods: EWC (Kirkpatrick et al.
2017) and LwF (Li and Hoiem 2017); Rehearsal-based
methods: A-GEM (Chaudhry et al. 2018), ER (Chaudhry
et al. 2019), DER++ (Buzzega et al. 2020), ER-ACE (Cac-
cia et al. 2021) and ConSlide (Huang et al. 2023a); and
Pre-trained VLM-based methods: AttriCLIP (Wang et al.
2023) and MI-Zero (Lu et al. 2023). To ensure a fair compar-
ison, all compared methods are adapted to VLM. Specially,
they use the same instance features and text encoder Etxt in
CONCH.
Results on Forward-order Training. This set of com-
parisons on NSCLC→BRCA→RCC→ESCA is shown in
Table 1. From this table, we have three empirical findings.
(1) Our QPMIL-VL method is consistently superior to
all other methods, with parameters only slightly larger
than the smallest one. PQMIL-VL achieves an ACC of
0.890 ± 0.021 with an Upper-bound Ratio as high as 0.982
± 0.032, winning SOTA performance metrics. Its Upper-
bound Ratio is even 5.5% higher than that of MI-Zero, the
second-best method. Additionally, our model has a small
learnable parameter size of 0.365M, which is only slightly
IL Type
Method
ACC (↑)
Upper-bound Ratio (↑)
Forgetting (↓)
BWT (↑)
Masked ACC (↑)
Baseline
JointTrain (Upper)
0.908 ± 0.022
1.000 ± 0.000
-
-
0.937 ± 0.022
FineTune (Lower)
0.234 ± 0.008
0.262 ± 0.010
0.927 ± 0.023
-0.927 ± 0.023
0.803 ± 0.060
Regularization-based
EWC (Kirkpatrick et al. 2017)
0.235 ± 0.011
0.264 ± 0.011
0.928 ± 0.020
-0.928 ± 0.020
0.833 ± 0.069
LwF (Li and Hoiem 2017)
0.236 ± 0.016
0.265 ± 0.012
0.908 ± 0.030
-0.908 ± 0.030
0.900 ± 0.041
Rehearsal-based
A-GEM/30 (Chaudhry et al. 2018)
0.536 ± 0.047
0.591 ± 0.053
0.527 ± 0.067
-0.527 ± 0.067
0.872 ± 0.026
ER-ACE/30 (Caccia et al. 2021)
0.703 ± 0.049
0.777 ± 0.053
0.281 ± 0.062
-0.279 ± 0.063
0.889 ± 0.041
DER++/30 (Buzzega et al. 2020)
0.684 ± 0.055
0.755 ± 0.063
0.310 ± 0.069
-0.307 ± 0.072
0.910 ± 0.043
ER/30 (Chaudhry et al. 2019)
0.644 ± 0.028
0.711 ± 0.033
0.387 ± 0.050
-0.386 ± 0.049
0.901 ± 0.035
ConSlide/30 (Huang et al. 2023a)
0.499 ± 0.025
-
0.058 ± 0.032
-0.021 ± 0.039
0.854 ± 0.039
Pre-trained VLM-based
AttriCLIP (Wang et al. 2023)
0.694 ± 0.058
0.766 ± 0.061
0.207 ± 0.063
-0.207 ± 0.063
0.861 ± 0.019
MI-Zero (Lu et al. 2023)
0.839 ± 0.034
0.927 ± 0.044
-
-
0.909 ± 0.019
PQMIL-VL (ours)
0.859 ± 0.032
0.946 ± 0.028
0.064 ± 0.031
-0.064 ± 0.031
0.925 ± 0.018
Table 2: Main results in class-incremental learning scenario on reverse-order training.
QPMIL
CFE
ACC (↑)
Forgetting (↓)
Tunable Vector
Class Ensemble
Class Similarity Loss
0.308 ± 0.045
0.841 ± 0.054
✓
0.720 ± 0.029
0.158 ± 0.025
✓
✓
0.747 ± 0.039
0.149 ± 0.037
✓
✓
✓
0.874 ± 0.033
0.052 ± 0.030
✓
✓
✓
✓
0.890 ± 0.021
0.027 ± 0.014
Table 3: Ablation studies on main components.
Ablation
ACC (↑)
Upper-bound Ratio (↑)
Forgetting (↓)
BWT (↑)
w/o key
0.675 ± 0.029
0.742 ± 0.045
0.332 ± 0.034
-0.332 ± 0.034
w/o matching penalty
0.659 ± 0.029
0.724 ± 0.037
0.356 ± 0.048
-0.356 ± 0.048
QPMIL-VL
0.890 ± 0.021
0.982 ± 0.032
0.027 ± 0.014
-0.027 ± 0.014
Table 4: Ablation studies on prototype pool.
larger than the smallest size of 0.111M in AttriCLIP. How-
ever, in terms of performance metrics, our method is far su-
perior to the lowest-parameter method, i.e., AttriCLIP. Be-
sides, in task-incremental experiments, our Masked ACC
could reach up to 0.930 ± 0.018, which is also significantly
higher than any achieved by other compared methods.
(2) Two regularization-based methods are almost in-
effective in WSI class-incremental learning scenario.
For example, as a regularization-based method, LwF only
achieves an ACC of 0.378 ± 0.081, which is slightly higher
than the lower bound of 0.308 ± 0.045, resulting in a very
low Upper-bound Ratio of 0.412 ± 0.083. Meawhile, EWC
also shows almost ineffective in incremental classification.
(3) Most rehearsal-based and pre-trained VLM-based
methods could achieve good classification results. The
rehearsal-based DER++ acquires an ACC of 0.749 ± 0.055
with an Upper-bound Ratio of 0.823 ± 0.059, and the
pre-trained VLM-based MI-Zero obtains an ACC of 0.839
± 0.034 with an Upper-bound Ratio of 0.927 ± 0.044.
Their Upper-bound Ratio results exceed 80%. Therefore,
these two types of methods are believed to be more effec-
tive than the former, consistent with natural image obser-
vations. Notably, although AttriCLIP performs well in the
natural image-based IL setting, its performance in this task
is lower than some rehearsal-based methods, reflecting the
unique challenges of analyzing gigapixel WSI. We consider
the main reasons are the poor bag-level feature (obtained
through MaxPooling) and the overly similar class features
derived from the same set of prompts.
Results on Reverse-order Training. In incremental clas-
sification, the training order of the dataset could influence
the model’s performance. The results on reverse-order train-
ing (ESCA→RCC→BRCA→NSCLC) are shown in Ta-
ble 2. By comparison, we have three observations, which
are almost the same as those from Table 1.
(1) Our QPMIL-VL method is consistently superior
to all other methods in both class-incremental and task-
incremental learning scenarios, maintaining a SOTA posi-
tion. (2) Regularization-based methods are still ineffective
in class-incremental learning scenario due to suffering from
severe catastrophic forgetting. (3) Most rehearsal-based and
pre-trained VLM-based methods often achieve good classi-
fication performance. Except for MI-Zero and our PQMIL-
VL, the best ACC among these two types of methods reaches
up to 0.703 ± 0.049 with an Upper-bound Ratio of 0.777 ±
0.053, achieved by ER-ACE.
Comparison between Forward-order and Reverse-
order Training. Additionally, we observe that on reverse-
order training, the performance of most methods is notice-
ably lower than on forward-order training. For example, our
PQMIL-VL decreases to an ACC of 0.859 ± 0.032. In con-
trast, its ACC is 0.890 ± 0.021 on forward-order training (as
shown in Table 1). One possible explanation to this decline is
that in incremental learning, later datasets can affect earlier
ones. ESCA, with only 150 slides, is the first to be learned,
while NSCLC, with 965 slides, is the last. NSCLC is dom-
inant in the total number of training samples, which could
cause a greater influence on the earlier datasets, leading to
more forgetting.
Ablation and Analysis
Ablation Studies. The following are several ablation analy-
ses of QPMIL-VL.
I) on Main Components To analyze the impact of the
main components on performance, we conduct a set of abla-
tion studies on forward-order training. The results are shown
in Table 3, where the first row represents our baseline (Fine-
Tune). By comparison, two obvious findings can be ob-
served as follows.
(1) Each component in QPMIL-VL contributes to the per-
formance improvement. For example, starting from the base-
line, QPMIL can significantly boost the ACC from 0.308
± 0.045 to 0.720 ± 0.029. Similarly, incrementally apply-
ing Class Similarity Loss raises the ACC from 0.874 ±
0.033 to 0.890 ± 0.021. (2) Among all components, both
QPMIL and Class Ensemble have a more significant im-
pact on performance than the other two. This is because the
prototype query mechanism in QPMIL effectively mitigates
catastrophic forgetting, while the Class Ensemble signifi-
cantly optimizes class feature encoding in the VLM by in-
creasing the diversity of class text descriptions. These com-
parisons imply that each component is effective.
II) on Prototype Pool Furthermore, we conduct another
set of ablation studies specifically on the queryable proto-
type pool. Our experimental results are shown in Table 4.
(1) “w/o key” indicates the removal of the key from proto-
type pair, implying that incremental training always uses the
same set of prompts. In this situation, the knowledge from
different datasets is encoded into the same set of prompts,
leading to catastrophic forgetting and a significant perfor-
mance decline (ACC dropping to 0.675 ± 0.029). (2) “w/o
matching penalty” presents that only Eq. 1 is used for
matching prototype pairs. Under this ablation, the ACC met-
ric decreases to 0.659 ± 0.029 because different datasets
tend to match the same set of prototypes during training,
which consequently results in catastrophic forgetting. Only
when both key and matching penalty are applied together
in our QPMIL-VL does the ACC reach its highest value of
0.890 ± 0.021. This ablation demonstrates that both are ben-
eficial for incremental classification.
Further Performance Analysis. Additionally, we con-
duct additional experiments to further analyze our method.
I) Effect of Hyper-parameters As shown in Fig. 3, there
are three important parameters in the prototype pool. The re-
sults indicate that inappropriate prototype pool capacity ((a):
N = 7, (b): M < 20) may result in knowledge interference
and too few prompts ((a): N = 1) makes it difficult to de-
scribe the various instance prototype visual features present
in WSIs.
(a)
(b)
Figure 3: (a) ACC w.r.t length of each prompt (LP ) and size
of matching keys (N), size of prototype pool M = 20; (b)
ACC w.r.t M, LP = 24 and N = 5.
II) Prototype Key Matching To analyze the matching re-
sults of keys on incremental datasets, we compute the match-
ing frequency of each prototype pair in the first fold of ex-
periments, as illustrated in Fig. 4 (a). It can be observed that
each dataset tends to stably match 5 keys. Moreover, the 5
keys predominantly matched by different datasets are often
(a)
(b)
Figure 4: (a) Prototype key matching frequency histogram;
(b) prototype feature visualization.
non-overlapping due to the matching penalty.
III) Prototype Feature Visualization As depicted in
Fig. 4 (b), we present the distribution visualization of 200
prototype features using t-SNE (Van der Maaten and Hin-
ton 2008). These features are derived from the ten-fold
cross-validation experiments conducted on four datasets,
i.e., 200 = 10 × 4 × 5. Their distributions show that most
prototypes learned from the same dataset exhibit clear clus-
tering properties. From another perspective, they also imply
that our model can learn distinct prototype features from dif-
ferent datasets. Additionally, from Fig. 4 (a) and (b), we ob-
serve a small number of outliers. They can be regarded as
shared prototypes.
IV) Class Feature Enhancement Analysis To show the
effectiveness of CFE, we visualize four classes of WSI sam-
ples and their corresponding class features, as shown in
Fig. 5. More experimental results can be found in our Sup-
plementary Material.
From the alteration in feature similarity, we can observe
that the class features are noticeably improved by the en-
hancement. For example, in the case of the class sample
ESCA-ESCC, the initial cosine similarity between the cen-
ter point feature and the class feature is only 0.078, but it
increases to 0.447 after enhancement, resulting in an im-
provement of 0.369. This alteration suggests that the CFE
is effective in enhancing the quality of class features.
NSCLC—LUAD
ESCA—ESCC
0.535
0.476
0.705
0.652
0.447
0.078
0.246
0.238
BRCA—ILC
RCC—CCRCC
Figure 5: Class features visualization, only one class per
dataset. Red ⋆is the center, i.e., the average of features, for
the instances of interest (orange points, determined by the
cosine similarity between instance features and learned pro-
totype features). Green ◦and • denote the cosine similarities
before and after feature enhancement, respectively.
Conclusions
This paper proposes the first Vision-Language-based frame-
work for incremental WSI classification, QPMIL-VL,
breaking through traditional vision modality. By a pool of
learnable prototype pairs, it aggregates the instance features
from pre-trained image encoder to generate bag-level fea-
ture under the guidance of matched prototype prompts. Class
probability is predicted by the Consine similarity between
bag-level feature and enhanced class feature. Extensive ex-
periments demonstrate that our method and its components
are effective. It obviously outperforms other methods in in-
cremental WSI classification, achieving SOTA results.
References
Bera, K.; Schalper, K. A.; Rimm, D. L.; Velcheti, V.; and
Madabhushi, A. 2019.
Artificial intelligence in digital
pathology—new tools for diagnosis and precision oncology.
Nature reviews Clinical oncology, 16(11): 703–715.
Buzzega, P.; Boschini, M.; Porrello, A.; Abati, D.; and
Calderara, S. 2020. Dark experience for general continual
learning: a strong, simple baseline. Advances in neural in-
formation processing systems, 33: 15920–15930.
Caccia, L.; Aljundi, R.; Asadi, N.; Tuytelaars, T.; Pineau, J.;
and Belilovsky, E. 2021. New insights on reducing abrupt
representation change in online continual learning. arXiv
preprint arXiv:2104.05025.
Campanella, G.; Hanna, M. G.; Geneslaw, L.; Miraflor, A.;
Werneck Krauss Silva, V.; Busam, K. J.; Brogi, E.; Reuter,
V. E.; Klimstra, D. S.; and Fuchs, T. J. 2019.
Clinical-
grade computational pathology using weakly supervised
deep learning on whole slide images.
Nature medicine,
25(8): 1301–1309.
Cha, H.; Lee, J.; and Shin, J. 2021. Co2L: Contrastive Con-
tinual Learning. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), 9516–9525.
Chaudhry, A.; Ranzato, M.; Rohrbach, M.; and Elhoseiny,
M. 2018.
Efficient lifelong learning with a-gem.
arXiv
preprint arXiv:1812.00420.
Chaudhry, A.; Rohrbach, M.; Elhoseiny, M.; Ajanthan, T.;
Dokania, P. K.; Torr, P. H.; and Ranzato, M. 2019.
On
tiny episodic memories in continual learning. arXiv preprint
arXiv:1902.10486.
Derakhshani, M. M.; Najdenkoska, I.; van Sonsbeek, T.;
Zhen, X.; Mahapatra, D.; Worring, M.; and Snoek, C. G.
2022. Lifelonger: A benchmark for continual disease clas-
sification. In International Conference on Medical Image
Computing and Computer-Assisted Intervention, 314–324.
Springer.
Fini, E.; Da Costa, V. G. T.; Alameda-Pineda, X.; Ricci, E.;
Alahari, K.; and Mairal, J. 2022. Self-supervised models are
continual learners. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, 9621–
9630.
Gao, P.; Geng, S.; Zhang, R.; Ma, T.; Fang, R.; Zhang, Y.; Li,
H.; and Qiao, Y. 2024. Clip-adapter: Better vision-language
models with feature adapters. International Journal of Com-
puter Vision, 132(2): 581–595.
Hayes, T. L.; Kemker, R.; Cahill, N. D.; and Kanan, C.
2018.
New metrics and experimental paradigms for con-
tinual learning.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops,
2031–2034.
Huang, Y.; Zhao, W.; Wang, S.; Fu, Y.; Jiang, Y.; and Yu,
L. 2023a. ConSlide: Asynchronous Hierarchical Interaction
Transformer with Breakup-Reorganize Rehearsal for Con-
tinual Whole Slide Image Analysis. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
21349–21360.
Huang, Z.; Bianchi, F.; Yuksekgonul, M.; Montine, T. J.;
and Zou, J. 2023b.
A visual–language foundation model
for pathology image analysis using medical twitter. Nature
medicine, 29(9): 2307–2316.
Ilse, M.; Tomczak, J.; and Welling, M. 2018.
Attention-
based deep multiple instance learning. In International con-
ference on machine learning, 2127–2136. PMLR.
Ke, Z.; Liu, B.; and Huang, X. 2020. Continual learning of a
mixed sequence of similar and dissimilar tasks. Advances in
neural information processing systems, 33: 18493–18504.
Kingma, D. P.; and Ba, J. 2014.
Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980.
Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Des-
jardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho, T.;
Grabska-Barwinska, A.; et al. 2017.
Overcoming catas-
trophic forgetting in neural networks. Proceedings of the
national academy of sciences, 114(13): 3521–3526.
Li, B.; Li, Y.; and Eliceiri, K. W. 2021. Dual-stream multiple
instance learning network for whole slide image classifica-
tion with self-supervised contrastive learning. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition, 14318–14328.
Li, H.; Chen, Y.; Chen, Y.; Yu, R.; Yang, W.; Wang, L.;
Ding, B.; and Han, Y. 2024.
Generalizable Whole Slide
Image Classification with Fine-Grained Visual-Semantic In-
teraction. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 11398–11407.
Li, X.; Zhou, Y.; Wu, T.; Socher, R.; and Xiong, C. 2019.
Learn to grow: A continual structure learning framework for
overcoming catastrophic forgetting. In International confer-
ence on machine learning, 3925–3934. PMLR.
Li, Z.; and Hoiem, D. 2017. Learning without forgetting.
IEEE transactions on pattern analysis and machine intelli-
gence, 40(12): 2935–2947.
Lin, T.; Yu, Z.; Hu, H.; Xu, Y.; and Chen, C.-W. 2023. Inter-
ventional bag multi-instance learning on whole-slide patho-
logical images. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 19830–19839.
Lin, W.; Zhuang, Z.; Yu, L.; and Wang, L. 2024. Boost-
ing Multiple Instance Learning Models for Whole Slide Im-
age Classification: A Model-Agnostic Framework Based on
Counterfactual Inference. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 38, 3477–3485.
Liu, P.; Ji, L.; Gou, J.; Fu, B.; and Ye, M. 2024a.
Inter-
pretable Vision-Language Survival Analysis with Ordinal
Inductive Bias for Computational Pathology. arXiv preprint
arXiv:2409.09369.
Liu, P.; Ji, L.; Ye, F.; and Fu, B. 2024b. Advmil: Adver-
sarial multiple instance learning for the survival analysis on
whole-slide images. Medical Image Analysis, 91: 103020.
Liu, P.; Ji, L.; Zhang, X.; and Ye, F. 2024c. Pseudo-Bag
Mixup Augmentation for Multiple Instance Learning-Based
Whole Slide Image Classification. IEEE Transactions on
Medical Imaging, 43(5): 1841–1852.
Lopez-Paz, D.; and Ranzato, M. 2017. Gradient episodic
memory for continual learning. Advances in neural infor-
mation processing systems, 30.
Lu, M. Y.; Chen, B.; Williamson, D. F.; Chen, R. J.; Liang,
I.; Ding, T.; Jaume, G.; Odintsov, I.; Le, L. P.; Gerber, G.;
et al. 2024. A visual-language foundation model for compu-
tational pathology. Nature Medicine, 30(3): 863–874.
Lu, M. Y.; Chen, B.; Zhang, A.; Williamson, D. F.; Chen,
R. J.; Ding, T.; Le, L. P.; Chuang, Y.-S.; and Mahmood, F.
2023.
Visual language pretrained multiple instance zero-
shot transfer for histopathology images. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, 19764–19775.
Lu, M. Y.; Williamson, D. F.; Chen, T. Y.; Chen, R. J.; Bar-
bieri, M.; and Mahmood, F. 2021. Data-efficient and weakly
supervised computational pathology on whole-slide images.
Nature biomedical engineering, 5(6): 555–570.
Mai, Z.; Li, R.; Jeong, J.; Quispe, D.; Kim, H.; and Sanner,
S. 2022. Online continual learning in image classification:
An empirical survey. Neurocomputing, 469: 28–51.
Mallya, A.; and Lazebnik, S. 2018. Packnet: Adding mul-
tiple tasks to a single network by iterative pruning. In Pro-
ceedings of the IEEE conference on Computer Vision and
Pattern Recognition, 7765–7773.
McCloskey, M.; and Cohen, N. J. 1989. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem.
In Psychology of learning and motivation, vol-
ume 24, 109–165. Elsevier.
Parisi, G. I.; Kemker, R.; Part, J. L.; Kanan, C.; and Wermter,
S. 2019. Continual lifelong learning with neural networks:
A review. Neural networks, 113: 54–71.
Prabhu, A.; Torr, P. H.; and Dokania, P. K. 2020. Gdumb:
A simple approach that questions our progress in continual
learning. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceed-
ings, Part II 16, 524–540. Springer.
Qu, L.; Fu, K.; Wang, M.; Song, Z.; et al. 2024. The rise of ai
language pathologists: Exploring two-level prompt learning
for few-shot weakly-supervised whole slide image classifi-
cation. Advances in Neural Information Processing Systems,
36.
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
et al. 2021. Learning transferable visual models from natural
language supervision. In ICML.
Rusu, A. A.; Rabinowitz, N. C.; Desjardins, G.; Soyer, H.;
Kirkpatrick, J.; Kavukcuoglu, K.; Pascanu, R.; and Had-
sell, R. 2016. Progressive neural networks. arXiv preprint
arXiv:1606.04671.
Shao, Z.; Bian, H.; Chen, Y.; Wang, Y.; Zhang, J.; Ji, X.;
et al. 2021. Transmil: Transformer based correlated multiple
instance learning for whole slide image classification. Ad-
vances in neural information processing systems, 34: 2136–
2147.
Shokri, R.; and Shmatikov, V. 2015.
Privacy-preserving
deep learning. In Proceedings of the 22nd ACM SIGSAC
conference on computer and communications security,
1310–1321.
Song, A. H.; Chen, R. J.; Ding, T.; Williamson, D. F.; Jaume,
G.; and Mahmood, F. 2024. Morphological prototyping for
unsupervised slide representation learning in computational
pathology. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 11566–11578.
Song, A. H.; Jaume, G.; Williamson, D. F.; Lu, M. Y.;
Vaidya, A.; Miller, T. R.; and Mahmood, F. 2023. Artificial
intelligence for digital and computational pathology. Nature
Reviews Bioengineering, 1(12): 930–949.
Van de Ven, G. M.; and Tolias, A. S. 2019. Three scenarios
for continual learning. arXiv preprint arXiv:1904.07734.
Van der Laak, J.; Litjens, G.; and Ciompi, F. 2021. Deep
learning in histopathology: the path to the clinic. Nature
medicine, 27(5): 775–784.
Van der Maaten, L.; and Hinton, G. 2008. Visualizing data
using t-SNE. Journal of machine learning research, 9(11).
Wang, L.; Zhang, X.; Su, H.; and Zhu, J. 2024. A com-
prehensive survey of continual learning: theory, method and
application. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence.
Wang, R.; Duan, X.; Kang, G.; Liu, J.; Lin, S.; Xu, S.;
L¨u, J.; and Zhang, B. 2023. Attriclip: A non-incremental
learner for incremental knowledge learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, 3654–3663.
Wang, Y.; Huang, Z.; and Hong, X. 2022. S-prompts learn-
ing with pre-trained transformers: An occam’s razor for do-
main incremental learning. Advances in Neural Information
Processing Systems, 35: 5682–5695.
Wang, Z.; Zhang, Z.; Lee, C.-Y.; Zhang, H.; Sun, R.; Ren,
X.; Su, G.; Perot, V.; Dy, J.; and Pfister, T. 2022. Learn-
ing to prompt for continual learning.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 139–149.
Xu, H.; Usuyama, N.; Bagga, J.; Zhang, S.; Rao, R.; Nau-
mann, T.; Wong, C.; Gero, Z.; Gonz´alez, J.; Gu, Y.; et al.
2024. A whole-slide foundation model for digital pathology
from real-world data. Nature, 1–8.
Yu, J.; Wang, Z.; Vasudevan, V.; Yeung, L.; Seyedhos-
seini, M.; and Wu, Y. 2022.
Coca: Contrastive caption-
ers are image-text foundation models.
arXiv preprint
arXiv:2205.01917.
Yu, T.; Lu, Z.; Jin, X.; Chen, Z.; and Wang, X. 2023. Task
residual for tuning vision-language models. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, 10899–10909.
Zhang, H.; Meng, Y.; Zhao, Y.; Qiao, Y.; Yang, X.; Coup-
land, S. E.; and Zheng, Y. 2022. Dtfd-mil: Double-tier fea-
ture distillation multiple instance learning for histopathol-
ogy whole slide image classification.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, 18802–18812.
Zheng, Y.; Li, J.; Shi, J.; Xie, F.; Huai, J.; Cao, M.; and Jiang,
Z. 2023.
Kernel attention transformer for histopathology
whole slide image analysis and assistant cancer diagnosis.
IEEE Transactions on Medical Imaging, 42(9): 2726–2739.
Zhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2022. Learning
to prompt for vision-language models. International Journal
of Computer Vision, 130(9): 2337–2348.
Supplementary Material
A. Dataset Details
The benchmark consists of four public WSI datasets from
TCGA repository: non-small cell lung carcinoma (NSCLC),
invasive breast carcinoma (BRCA), renal cell carcinoma
(RCC) and esophageal carcinoma (ESCA), as shown in Ta-
ble 5.
Dataset
Tumor Type
Cases
Slides
NSCLC
Lung adenocarcinoma (LUAD)
449
512
Lung squamous cell carcinoma (LUSC)
419
453
BRCA
Invasive ductal (IDC)
713
761
Invasive lobular carcinoma (ILC)
178
191
RCC
Clear cell renal cell carcinoma (CCRCC)
493
499
Papillary renal cell carcinoma (PRCC)
245
269
ESCA
Esophageal adenocarcinoma (ESAD)
64
64
Esophageal squamous cell carcinoma (ESCC)
84
86
Table 5: The statistics of incremental WSI analysis bench-
mark.
B. Metric Details
After completing the incremental training, we can obtain an
Accuracy performance matrix, as illustrated in Table 6. Sub-
sequently, the relevant metrics reported in our experiments
are calculated using this performance matrix.
After Training
Test on
Dataset 1
Dataset 2
· · ·
Dataset T
Dataset 1
R1,1
-
· · ·
-
Dataset 2
R2,1
R2,2
· · ·
-
· · ·
· · ·
· · ·
· · ·
· · ·
Dataset T
RT,1
RT,2
· · ·
RT,T
JointTrain (Upper)
Rjoint,1
Rjoint,2
· · ·
Rjoint,T
Table 6: The Accuracy performance matrix during incre-
mental training.
ACC. Average Accuracy (ACC) represents the model’s
average performance across all datasets, which is calculated
by
ACC = 1
T
T
X
t=1
RT,t.
(11)
Upper-bound Ratio. Upper-bound Ratio enables an in-
crementally trained model to be compared relative to a
jointly trained model (upper bound). It can be computed by
Upper-bound Ratio = 1
T
T
X
t=1
RT,t
Rjoint,t
.
(12)
Forgetting. Forgetting quantifies how much knowledge
the model has forgotten about previous datasets, which is
defined as
Forgetting =
1
T −1
T −1
X
t=1
max
i∈{1,··· ,T } Ri,t −RT,t.
(13)
BWT. Backward Transfer (BWT) reflects the model’s
ability to mitigate catastrophic forgetting (i.e., memory sta-
bility). The calculation formula is as follows:
BWT =
1
T −1
T −1
X
t=1
RT,t −Rt,t.
(14)
C. More Details of Class Ensemble
Table 7 provides all the class names and templates used in
our implementation of the class ensemble. They are adapted
from those used in CONCH.
D. Analysis on Accuracy Performance Matrix
The performance variations of deep models across different
datasets during incremental learning cannot be thoroughly
analyzed solely by relying on final metrics such as ACC.
Therefore, we present detailed evaluation results of QPMIL-
VL on both forward-order and reverse-order incremental
training in Tables 8 and 9, respectively. Through our anal-
ysis, there are three findings.
(1) Forgetting phenomena are commonly observed in
the old datasets. On forward-order training, the Accuracy
of NSCLC decreases from 0.906 ± 0.027 to 0.837 ± 0.046,
while on reverse-order training, the Accuracy of ESCA
drops from 0.967 ± 0.045 to 0.807 ± 0.092. This reveals the
prevalent phenomenon of forgetting during the incremental
learning process.
(2) Our QPMIL-VL demonstrates competitiveness in
static WSI classification when compared to the baseline
(JointTrain). For example, on forward-order training, af-
ter QPMIL-VL is trained on the fourth dataset, ESCA, it
reaches an Accuracy of 0.928 ± 0.083, which is higher than
the 0.913 ± 0.055 of JointTrain. However, due to forgetting
caused by incremental learning, QPMIL-VL shows a notice-
able drop in performance on earlier datasets, so its overall
performance ends up being lower than that of JointTrain.
(3) The primary reason for the performance degra-
dation on reverse-order training compared to forward-
order training lies in the greater impact of learning the
fourth dataset on the performance of the first dataset.
After training on the fourth dataset, the Accuracy of the
first dataset decreases by 0.055 (0.892 ± 0.024 →0.837
± 0.046) and 0.148 (0.955 ± 0.060 →0.807 ± 0.092) on
forward-order and reverse-order training, respectively. Dur-
ing reverse training, NSCLS significantly affects the perfor-
mance of ESCA, this may be because the NSCLS sample
size (965) is about six times larger than that of ESCA (150),
making it easier for NSCLS to interfere with ESCA. In con-
trast, during forward training, ESCA has a much smaller im-
pact on NSCLS.
E. Query Vector Generation Strategies
MaxPooling vs MeanPooling. To investigate the impact of
different pooling operations on the performance of QPMIL-
VL when generating query vector, we conduct comparative
experiments using MaxPooling and MeanPooling. Table 10
presents the experimental results.
Template
Dataset
Tumor Type
ClassName
“ClassName”
“a photomicrograph showing ClassName”
“a photomicrograph of ClassName”
“an image of ClassName”
“an image showing ClassName”
“an example of ClassName”
“ClassName is shown”
“this is ClassName”
“there is ClassName”
“a histopathological image showing ClassName”
“a histopathological image of ClassName”
“a histopathological photograph of ClassName”
“a histopathological photograph showing ClassName”
“shows ClassName”
“presence of ClassName”
“ClassName is present”
“an H&E stained image of ClassName”
“an H&E stained image showing ClassName”
“an H&E image showing ClassName”
“an H&E image of ClassName”
“ClassName, H&E stain”
“ClassName, H&E”
NSCLC
LUAD
“lung adenocarcinoma”
“adenocarcinoma of the lung”
“LUAD”
LUSC
“lung squamous cell carcinoma”
“squamous cell carcinoma of the lung”
“LUSC”
BRCA
IDC
“invasive ductal carcinoma”
“breast invasive ductal carcinoma”
“invasive ductal carcinoma of the breast”
“invasive carcinoma of the breast, ductal pattern”
“breast IDC”
ILC
“invasive lobular carcinoma”
“breast invasive lobular carcinoma”
“invasive lobular carcinoma of the breast”
“invasive carcinoma of the breast, lobular pattern”
“breast ILC”
RCC
CCRCC
“clear cell renal cell carcinoma”
“renal cell carcinoma, clear cell type”
“renal cell carcinoma of the clear cell type”
“clear cell RCC”
PRCC
“papillary renal cell carcinoma”
“renal cell carcinoma, papillary type”
“renal cell carcinoma of the papillary type”
“papillary RCC”
ESCA
ESAD
“esophageal adenocarcinoma”
“adenocarcinoma of the esophageal”
“ESAD”
ESCC
“esophageal squamous cell carcinoma”
“squamous cell carcinoma of the esophageal”
“ESCC”
Table 7: The class names and templates used in our class ensemble.
After Training
Test on
NSCLC
BRCA
RCC
ESCA
NSCLC
0.906 ± 0.027
-
-
-
BRCA
0.895 ± 0.027
0.878 ± 0.024
-
-
RCC
0.892 ± 0.024
0.876 ± 0.025
0.931 ± 0.022
-
ESCA
0.837 ± 0.046
0.868 ± 0.029
0.929 ± 0.023
0.928 ± 0.083
JointTrain (Upper)
0.891 ± 0.041
0.908 ± 0.029
0.921 ± 0.045
0.913 ± 0.055
Table 8: The Accuracy performance matrix of QPMIL-VL
on forward-order training.
After Training
Test on
ESCA
RCC
BRCA
NSCLC
ESCA
0.967 ± 0.045
-
-
-
RCC
0.967 ± 0.045
0.932 ± 0.029
-
-
BRCA
0.955 ± 0.060
0.930 ± 0.030
0.873 ± 0.030
-
NSCLC
0.807 ± 0.092
0.913 ± 0.039
0.860 ± 0.028
0.856 ± 0.029
JointTrain (Upper)
0.913 ± 0.055
0.921 ± 0.045
0.908 ± 0.029
0.891 ± 0.041
Table 9: The Accuracy performance matrix of QPMIL-VL
on reverse-order training.
The results show that QPMIL-VL performs better over-
all with MaxPooling compared to MeanPooling, particularly
on forward-order training (the ACC increases from 0.877
± 0.034 to 0.890 ± 0.021). These results reflect that the
query vectors obtained through MaxPooling could possess
greater discriminative ability across different datasets (refer
to Fig. 6).
F. Prototype Visualization on Whole Slide Images
The pre-defined prompts in the prototype pool are designed
to describe the visual features of the instance prototypes of
interest present in WSIs. To validate this design principle,
we calculate the cosine similarity between the prompt fea-
tures (i.e., prototype features) learned by QPMIL-VL and
the instance features, and use this similarity as the basis to
determine the prototype category to which each instance be-
longs. The experimental results are presented in Fig. 7, 8, 9
and 10.
By observing the results, we can clearly see that prompts
can correctly identify the target instance prototypes (i.e., the
lesion areas in the WSIs). Additionally, different prompts
can locate different instance prototypes to some extent. This
experimental phenomenon is consistent with our expecta-
tions.
Training Order
Pooling Type
ACC (↑)
Upper-bound Ratio (↑)
Forgetting (↓)
BWT (↑)
Masked ACC (↑)
MeanPooling
0.877 ± 0.034
0.966 ± 0.039
0.045 ± 0.019
-0.044 ± 0.019
0.929 ± 0.027
Forward-order
MaxPooling
0.890 ± 0.021
0.982 ± 0.032
0.027 ± 0.014
-0.027 ± 0.014
0.930 ± 0.018
MeanPooling
0.859 ± 0.042
0.946 ± 0.042
0.070 ± 0.054
-0.070 ± 0.055
0.935 ± 0.015
Reverse-order
MaxPooling
0.859 ± 0.032
0.946 ± 0.028
0.064 ± 0.031
-0.064 ± 0.031
0.925 ± 0.018
Table 10: The comparative results of MaxPooling and MeanPooling on both forward-order and reverse-order training.
MeanPooling
MaxPooling
Figure 6: The visualization of the query vectors from the
four datasets obtained through MeanPooling (left) and Max-
Pooling (right).
G. More Analysis on Class Feature Enhancement
Since in QPMIL-VL, class features guide the learning of
WSI bag-level features, we further validate the effectiveness
of the CFE module by visualizing the distribution of the bag-
level features learned by the model before and after using the
CFE module in four different incremental learning stages.
The results are shown in Fig. 11.
From these results, we could find that our CFE makes i)
intra-class bag-level features more compact (dashed box)
and ii) inter-class ones more distinguishable (solid box).
This finding suggests that the class features used for super-
vising bag-level feature learning are improved.
LUAD
LUSC
Figure 7: Prototype visualization on Whole Slide Images (NSCLC).
IDC
ILC
Figure 8: Prototype visualization on Whole Slide Images (BRCA).
CCRCC
PRCC
Figure 9: Prototype visualization on Whole Slide Images (RCC).
ESAD
ESCC
Figure 10: Prototype visualization on Whole Slide Images (ESCA).
Incremental Learning Stage
Class Feature Enhancement
Figure 11: Bag-level features visualization with or without our Class Feature Enhancement (CFE) in four incremental learning
stages.
