Scalable likelihood-based estimation and variable selection for the
Cox model with incomplete covariates
Ngok Sang Kwok and Kin Yau Wong∗
Department of Applied Mathematics, The Hong Kong Polytechnic University
Abstract
Regression analysis with missing data is a long-standing and challenging problem, particularly
when there are many missing variables with arbitrary missing patterns. Likelihood-based methods,
although theoretically appealing, are often computationally inefficient or even infeasible when dealing
with a large number of missing variables. In this paper, we consider the Cox regression model with
incomplete covariates that are missing at random. We develop an expectation-maximization (EM)
algorithm for nonparametric maximum likelihood estimation, employing a transformation technique
in the E-step so that it involves only a one-dimensional integration.
This innovation makes our
methods scalable with respect to the dimension of the missing variables. In addition, for variable
selection, we extend the proposed EM algorithm to accommodate a LASSO penalty in the likelihood.
We demonstrate the feasibility and advantages of the proposed methods over existing methods by
large-scale simulation studies and apply the proposed methods to a cancer genomic study.
Keywords: EM algorithm; LASSO; missing data; nonparametric maximum likelihood estimation; penalized
regression; survival analysis.
1
Introduction
In many public health and medical studies, we study the association between covariates, such as treatment
received, demographic information, or other personal characteristics, and times to disease events or death. One
complication that often arises in practice is that the covariates may not be available for all study subjects. For
example, The Cancer Genome Altas (TCGA) collected genomic and clinical data for many types of cancer, but
for a substantial number of subjects, protein expressions were not measured. Another example is the North
Staffordshire Osteoarthritis Project (Wilkie et al., 2019), where researchers investigated the association between
mortality and symptomatic osteoarthritis. Covariates such as walking frequency, depression, and BMI, collected
via questionnaires, had missing responses from some subjects.
Missing data pose significant theoretical and
computational challenges for regression analysis.
There are multiple popular approaches for handling missing covariates in regression analyses, many of which
have been applied to survival analysis. One is the likelihood approach, where we incorporate a model for the
missing covariates into the likelihood. An advantage of this approach is that maximum likelihood estimation
(MLE) is efficient. Herring and Ibrahim (2001) considered the Cox model with incomplete covariates, which
could be categorical or continuous, and developed an expectation-maximization (EM) algorithm (Dempster et al.,
1977) for computation. Zhou et al. (2022) implemented the EM algorithm with two-stage data augmentation for
the Cox model with interval-censored survival time. However, MLE becomes computationally intensive or even
infeasible with a large number of missing covariates.
Another approach is inverse-probability weighting (Martinussen et al., 2016), where only subjects with com-
plete observations are used and are weighted according to the probability of complete observation. Thiessen et al.
∗Corresponding author; email: kin-yau.wong@polyu.edu.hk
1
arXiv:2410.11482v1  [stat.ME]  15 Oct 2024
(2022) proposed a two-step estimator for the Cox model with MAR incomplete covariates, where two inverse-
probability weighted estimators are combined to form a more efficient estimator. Inverse probability weighting,
however, is in general inefficient as it discards information contained in subjects with partial observations.
A third approach is (multiple) imputation, where the missing values are first imputed based on the observed
data, and standard analysis methods can be applied to the completed data (Rubin, 1987). White and Royston
(2009) considered multiple imputation with the Cox model. A popular version of multiple imputation is mul-
tiple imputation by chained equation (MICE), also known as a “fully conditional specification,” which involves
specifying the conditional density for each incomplete covariate (van Buuren and Groothuis-Oudshoorn, 2011;
Raghunathan et al., 2001; Azur et al., 2011). Another imputation technique is matrix-completion, which imputes
the missing values through solving a rank restricted optimization problem (Hastie et al., 2015). However, the
theoretical properties of estimators based on imputed data are not guaranteed.
In addition to missing data, high-dimensional covariates present another challenge. When the number of
available covariates is large, standard approaches that regress on all covariates, such as MLE or estimating
equations based on inverse-probability weighting, may suffer from over-fitting and difficulties in interpretations,
or may even be infeasible. In such cases, we are often interested in selecting a subset of covariates that are
associated with the outcome.
Penalized regression methods, such as LASSO (Tibshirani, 1996), are popular
approaches to reduce over-fitting and to perform variable selection.
Penalized regression or variable selection with missing data is highly challenging, with limited research in this
area. For likelihood-based methods, Garcia et al. (2010) developed EM algorithms for the Cox model with LASSO,
adaptive LASSO (Zou, 2006), and the smoothly clipped absolute deviation (Fan and Li, 2001) penalties. Sabbe
et al. (2013) studied variable selection in logistic regression using a LASSO penalty via a stochastic EM algorithm.
For inverse probability weighting, Johnson et al. (2008) and Wolfson (2011) incorporated a penalty term to inverse-
probability-weighted estimating equations for performing variable selection. For multiple imputation, Wood et al.
(2008) investigated methods for combining variable selection results from multiply imputed datasets. Deng et al.
(2016) extended the MICE approach to high-dimensional settings by fitting a penalized regression model for each
missing covariate. Liang et al. (2024) developed an iterative imputation method based on matrix completion
and a randomized LASSO method based on bootstrap.
However, these approaches suffer the shortcomings
of their unpenalized counterparts, such as computational or estimation inefficiency and the lack of theoretical
justifications. Also, they may require computationally intensive tuning.
In this paper, we study the likelihood approach and develop a novel algorithm for MLE that overcomes the
computational challenges of existing methods. In particular, we consider the Cox proportional hazards model
with incomplete covariates and develop an EM algorithm for computation, which is computationally feasible
even when the missing pattern is arbitrary and a large number of covariates are missing for a subject. The
algorithm involves a subject-specific transformation of the missing covariates, which results in a one-dimensional
numerical integration in the E-step.
This is a major advance over existing likelihood-based methods, which
involve numerical integration of the same dimension as the covariates and thus are feasible only for a small
number of missing covariates.
Under the likelihood framework, we can naturally perform variable selection by incorporating a penalty term.
In particular, we consider a LASSO penalty and develop an EM algorithm for computation. Employing the
transformation technique, the E-step involves only a one-dimensional numerical integration, as in the unpenalized
case. In the M-step, we perform quadratic approximation of the log-partial likelihood and adopt the coordinate-
descent algorithm. Consequently, the estimation and variable selection procedure remain computationally feasible
even with many missing covariates.
This paper is structured as follows. In Section 2, we describe the proposed model and formulate the EM
algorithm for both the unpenalized and penalized cases. In Section 3, we perform large-scale simulation studies
and compare the performance of the proposed methods with existing methods. In Section 4, we demonstrate
the feasibility and advantages of the proposed method on a cancer genomics study. We provide some concluding
remarks and possible extensions in Section 5.
2
2
Methods
2.1
Model and likelihood
Let T be an event time of interest and X be a p-vector of covariates. Assume that T given X follows the Cox
proportional hazards model, with the hazard function given by
λ(t | X) = λ(t)eXTβ,
where β is a vector of regression parameters, and λ(·) is a nonparametric baseline hazard function. We assume
that X follows the multivariate normal distribution with mean µ and covariance matrix Σ. Here, we for simplicity
of presentation impose a parametric model on all components of X, but the proposed methods can be easily
generalized to the milder condition that a subset of components of X, XS, is multivariate normal given the
remaining components, X−S, if X−S is always observed. Suppose that T may be subject to right-censoring. Let
C be the censoring time, Y = min(T, C), and ∆= I(T ≤C).
We allow components of X to be missing. Let R ≡(R1, . . . , Rp)T denote a vector of missing indicators, where
Rj = 1 if Xj is missing and Rj = 0 if otherwise. Assume missing at random, such that R and X are independent
given {Xj : P(Rj = 0) = 1}, Y , and ∆. Also, assume that T and C are independent given {Xj : P(Rj = 0) = 1}.
For a sample of size n, the observed data consist of Oi ≡{Yi, ∆i, Ri, Xi,−Ri} for i = 1, . . . , n, where Xi,−Ri
denote the subvector of Xi consisting of components that correspond to Rij = 0. Let Λ(t) =
R t
0 λ(s) ds and
θ ≡(β, Λ, µ, Σ) denote the set of all unknown parameters. The (observed-data) likelihood is
Lobs(θ) =
n
Y
i=1
Z 
λ(Yi)eXT
i β	∆ie−Λ(Yi)eXT
i β
|Σ|−1/2e−1
2 (Xi−µ)TΣ−1(Xi−µ) dXi,Ri,
where Xi,Ri denote the subvector of Xi consisting of the components that correspond to Rij = 1.
We adopt the nonparametric likelihood estimation (NPMLE) approach. Let t1 < · · · < tm be the ordered
unique observed event times, where m = Pn
i=1 ∆i. We set Λ to be a step function that jumps only at t1, . . . , tm
and let the corresponding jump sizes be λ1, . . . , λm. In the likelihood, we replace λ(Yi) by the corresponding
jump size. In the sequel, we use Lobs to denote this nonparametric version of the likelihood.
2.2
EM algorithm for unpenalized estimation
When the dimension of X is low and we are not interested in variable selection, we estimate θ by the NPMLE
(bβ, bΛ, bµ, bΣ), which is the maximizer of Lobs. We adopt the EM algorithm to compute the NPMLE, with Xi,Ri
treated as missing data for i = 1, . . . , n. The complete-data log-likelihood is
log Lcom(θ) =
n
X
i=1
n
∆i(log λj(i) + XT
i β) −
X
j:tj≤Yi
λjeXT
i β −1
2 log |Σ| −1
2(Xi −µ)TΣ−1(Xi −µ)
o
,
where j(i) is such that Yj(i) = tj for i ∈{i = 1, . . . , n : ∆i = 1}. In the E-step, we evaluate the conditional
expectation of log Lcom(θ) given the observed data at the current parameter estimate. In the M-step, we maximize
the expected complete-data log-likelihood. In particular, at the (k + 1)th iteration, we update
µ(k+1) = 1
n
n
X
i=1
bE(k)(Xi)
(1)
Σ(k+1) = 1
n
n
X
i=1
bE(k)(XiXT
i ) −

µ(k+1) 
µ(k+1)T
,
(2)
where bE(k) denote conditional expectation given the observed data, evaluated at the parameter estimate at the
kth iteration. After profiling out λ1, . . . , λm, β maximizes the following “complete-data log-partial likelihood”
Q(k)(β) =
n
X
i=1
∆i

bE(k)(Xi)Tβ −log

X
j:Yj≤Yi
bE(k)(eXT
j β)

.
3
Note that
∂Q(k)(β)
∂β
=
n
X
i=1
∆i



bE(k)(Xi) −
P
j:Yj≤Yi bE(k)(eXT
j βXj)
P
j:Yj≤Yi bE(k)(eXT
j β)



∂2Q(k)(β)
∂β∂βT
= −
n
X
i=1
∆i
"P
j:Yj≤Yi bE(k)(eXT
j βXjXT
j )
P
j:Yj≤Yi bE(k)(eXT
j β)
−
(P
j:Yj≤Yi bE(k)(eXT
j βXj)
P
j:Yj≤Yi bE(k)(eXT
j β)
)⊗2#
.
We update β by the one-step Newton method:
β(k+1) = β(k) −
 
∂2Q(k)(β)
∂β∂βT

β=β(k)
!−1  
∂Q(k)(β)
∂β

β=β(k)
!
,
(3)
where β(k) denote the estimate of β at the kth iteration. Finally, we update the baseline hazard function using
the Breslow-like estimator:
λ(k+1)
j(i)
=
1
P
j:Yj≤Yi bE(k)(eXT
j β(k+1))
(4)
for i such that ∆i = 1.
The major computational challenge of the EM algorithm is that the conditional distribution of Xi does not
have a closed form, and direct numerical integration for the expectations is infeasible when the dimension of Xi,Ri
is moderately high. To avoid multi-dimensional numerical integrations, we propose a transformation approach
under which the expectations can be computed using at most one-dimensional numerical integrations. Let βRi
and β−Ri denote the subvector of β consisting of components that correspond to Rij = 1 and 0 respectively.
The same method is used to denote subvectors of µ. Let ΣAi1,Ai2 denote the submatrix of Σ with rows indexed
by Ai1 and columns indexed by Ai2, and Aij is either Ri or −Ri for j = 1, 2. The expectations that need to be
computed in the E-step are in one of the following forms:
E

exp(XT
i,Riβ(k)
Ri )|Oi
	
(5)
E{g(XT
i,Riβ(k)
Ri )Xi,Ri|Oi}
(6)
E{g(XT
i,Riβ(k)
Ri )Xi,RiXT
i,Ri|Oi}
(7)
E

exp(XT
i,Riβ(k+1)
Ri
)|Oi
	
,
(8)
where g is either the exponential function or the constant function g(·) = 1. Note that the expectations are
evaluated at θ = θ(k).
First, if β(k)
Ri = 0, then the conditional distribution of Xi,Ri given Oi is a multivariate normal distribution
that does not depend on (Yi, ∆i). The expectations (5)–(8) have simple closed-form expressions.
For β(k)
Ri ̸= 0, we define an orthogonal matrix Ψi with the first row being (β(k)
Ri )T/∥β(k)
Ri ∥and let f
Xi =
ΨiXi,Ri, where ∥· ∥denote the L2-norm. Note that the first component of f
Xi is e
Xi1 = XT
i,Riβ(k)
Ri /∥β(k)
Ri ∥. Let
ηi, and νi denote the mean and variance of f
Xi given X−Ri, where
ηi = ΨiµRi + ΨiΣRi,−RiΣ−1
−Ri,−Ri(Xi,−Ri −µ−Ri)
νi = ΨiΣRi,RiΨT
i −ΨiΣRi,−RiΣ−1
−Ri,−RiΣ−Ri,RiΨT
i .
Let f
Xi,−1 denote the subvector of f
Xi consisting of all but the first component.
Although the conditional
distribution of f
Xi given the observed data does not have a simple form, f
Xi,−1 given the observed data and e
Xi1
(at θ = θ(k)) follows the multivariate normal distribution:
f
Xi,−1 | (Yi, ∆i, e
Xi1, Xi,−Ri) ∼N
 
(ηi)−1 + (νi)−1,1
e
Xi1 −(ηi)1
(νi)1,1
, (νi)−1,−1 −(νi)⊗2
−1,1
(νi)1,1
!
≡N(mi( e
Xi1), V i),
where (νi)1,1 is the upper left element of νi, (νi)−1,1 is the first column of νi with the first component removed,
4
and (νi)−1,−1 is the lower right submatrix of νi, with the first row and column of νi removed. The conditional
density of e
Xi1 given Oi is proportional to
f(exi1; Oi) ≡exp

∆i∥β(k)
Ri ∥exi1 −Λ0(Yi)e
∥β(k)
Ri ∥exi1+XT
i,−Ri β−Ri −
1
2(νi)1,1 (exi1 −(ηi)1)2

.
Therefore, conditional expectations of functions of Xi,Ri ≡ΨT
i f
Xi can be computed by first further conditioning
on e
Xi1, where the conditional expectations have closed-form expressions, and then taking the expectation over
e
Xi1, which can be performed by numerical integration.
Specifically, the expectation (5) is equal to E

exp(∥β(k)
Ri ∥e
Xi1)|Oi
	
. The expectation (6) is equal to
ΨT
i E

g(∥β(k)
Ri ∥e
Xi1)f
Xi|Oi
	
= ΨT
i E
(
g(∥β(k)
Ri ∥e
Xi1)
 
e
Xi1
mi( e
Xi1)
! Oi
)
.
The expectation (7) is equal to
ΨT
i E

g(∥β(k)
Ri ∥e
Xi1)f
Xif
X
T
i |Oi
	
Ψi
= ΨT
i E
(
g(∥β(k)
Ri ∥e
Xi1)
 
e
X2
i1
e
Xi1mi( e
Xi1)T
e
Xi1mi( e
Xi1)
V i + mi( e
Xi1)mi( e
Xi1)T
! Oi
)
Ψi.
Finally, to evaluate (8), let
ϕi( e
Xi1; a) = E

exp(f
X
T
i,−1a) | e
Xi1, Oi
	
= exp
n
aTmi( e
Xi1) + 1
2aTV ia
o
for any vector a of an appropriate dimension. We can write (8) as
E

exp(f
X
T
i Ψiβ(k+1)
Ri
)|Oi
	
= E

exp((Ψiβ(k+1)
Ri
)1 e
Xi1)ϕi( e
Xi1; (Ψiβ(k+1)
Ri
)−1) | Oi
	
.
Therefore, all expectations involved in the E-step can be computed using one-dimensional numerical integrations
over the conditional distribution of e
Xi1. In particular, for any function h, we have
E{h( e
Xi1) | Oi} =
R
h(exi1)f(exi1; Oi) dexi1
R
f(exi1; Oi) dexi1
.
The integrations can be approximated using the adaptive Gauss–Hermite quadrature (Liu and Pierce, 1994).
The proposed algorithm is summarized in Algorithm 1.
Algorithm 1: NPMLE
Input
: {Oi}i=1,2,...,n.
1 Initialize (β(0), λ(0), µ(0), Σ(0)).
2 Calculate (5), (6), and (7) for i = 1, 2, . . . , n and in turn the gradient and Hessian of Q(k)(β).
3 Update µ and Σ by (1) and (2), respectively.
4 Update β by (3).
5 Calculate (8) for i = 1, 2, . . . , n.
6 Update Λ by (4).
7 Repeat Steps 2–6 until convergence.
Output: (bβ, bΛ, bµ, bΣ).
2.3
EM algorithm for penalized estimation
When the number of covariates is large, it is often desirable to select a subset of covariates that are associated with
the survival time. We propose a penalization approach with the following penalized observed-data log-likelihood:
pℓ(θ) = log Lobs(θ) −nγ∥β∥1,
5
where γ > 0 is tuning parameter. The penalized NPMLE is the maximizer of pℓ(θ). Note that we assume that
the sample size is sufficiently larger than the number of covariates, so no penalty is imposed for the covariance
matrix Σ.
To compute the penalized NPMLE, we adopt the proposed EM algorithm for the unpenalized estimator with
some modifications. The E-step for the penalized estimator is the same as the previous algorithm. In the M-step,
the estimators of µ and Σ are the same as before.
After profiling out the baseline hazard function, β maximizes the (expected) penalized complete-data log-
partial likelihood n−1Q(k)(β)−γ∥β∥1. Clearly, there is no closed-form solution, and the objective function is not
differentiable. To update β, we first approximate the objective function using a second-order Taylor expansion:
n−1Q(k)(β) −γ∥β∥1 ≈−1
2βTAβ −P Tβ −γ∥β∥1 + const,
(9)
where
A = −1
n
 
∂2Q(k)(β)
∂β∂βT

β=β(k)
!
P = 1
n
( 
∂2Q(k)(β)
∂β∂βT

β=β(k)
!
β(k) −
 
∂Q(k)(β)
∂β

β=β(k)
!)
.
Then, to maximize the right-hand side of (9), we adopt the coordinate-descent algorithm (Simon et al., 2011).
For j = 1, . . . , p, we update βj with β−j fixed at the current estimates by setting
βj = −S(Aj,−jβ−j + P j, γ)
Aj,j
,
(10)
where S(x, γ) = sgn(x)(|x| −γ)+. We iterate over components of β until convergence. After updating β, we
update Λ using the same Breslow-like estimator as before. This completes a single M-step. Let βγ be the LASSO
estimator corresponding to γ and Bγ denote the active set of βγ. Since the LASSO estimator is biased, we refit
the model using NPMLE, with only the coefficients in the active set Bγ allowed to be nonzero. We summarize
the procedure in Algorithm 2.
Algorithm 2: Penalized NPMLE
Input
: {Oi}i=1,2,...,n and γ.
1 Initialize (β(0), λ(0), µ(0), Σ(0)).
2 Calculate (5), (6), and (7) for i = 1, 2, . . . , n and in turn the gradient and Hessian of Q(k)(β).
3 Update µ and Σ by (1) and (2), respectively.
4 Iteratively update each component of β through (10) until convergence.
5 Calculate (8) for i = 1, 2, . . . , n.
6 Update Λ through (4).
7 Repeat Steps 2–6 until convergence.
8 Refit NPMLE over the active set.
Output: (bβ
refit
γ
, bΛrefit
γ
, bµrefit
γ
, bΣ
refit
γ
).
We specify a grid of tuning parameters and calculate the penalized NPMLE corresponding to each γ. To
search for the optimal tuning parameter γ∗, we choose the Bayesian information criterion (BIC) as our model
selection criterion:
BIC(γ) = −2 log bLobs(Bγ) + log(n)|Bγ|,
where bLobs(Bγ) is the maximum value of observed likelihood for the active set Bγ.
6
3
Simulation studies
3.1
Unpenalized estimation
In this subsection, we evaluate the empirical performance of the proposed unpenalized methods and two existing
methods, namely complete-case analysis and single imputation.
We set p = 4 and generated X from the multivariate normal distribution with µ = 0 and Σ ≡(0.5|i−j|)i,j=1,...,p.
We set β = (0.5, 0.5, 0.5, 0.5)T and Λ(t) = 0.04t5/4.
We set the censoring time to be min{C∗, 50}, where
C∗∼Exp(0.03); the censoring rate is approximately 34%.
We considered a sample size of n = 500 or 1000. For each subject, either all covariates are observed, or the
first two covariates are missing. We considered two missing mechanisms, namely MCAR and MAR. For MCAR,
subjects with missing data are randomly assigned. For MAR, we mimic a case-cohort study, where a subcohort
consisting of 30% of the whole sample was set to have observed covariates. Then, we randomly selected subjects
outside the subcohort with ∆= 1 to have observed covariates. If all subjects with ∆= 1 were selected and the
missing proportion is still higher than the desired level, then we randomly selected subjects with ∆= 0 to yield
the desired missing proportion. We considered missing proportions (pM) of 20% and 40%.
We considered the proposed NPMLE, complete-case analysis under the standard Cox regression, and single
imputation. For single imputation, we estimated µ and Σ using MLE based on the fully observed Xi’s, imputed
the missing entries by their estimated conditional mean given the partially observed Xi’s, and then fitted the
standard Cox model on the imputed data. For the proposed method, we use bootstrap to obtain standard error
estimators and confidence intervals for the regression parameters, with 500 bootstrap replicates. We considered
500 simulation replicates.
NPMLE
Complete Case Single Imputation
Setting
Parameter
Bias
SE
SEE
CP
Bias
SE
Bias
SE
n = 500
β1
−0.0022 0.0771 0.0764 0.93 −0.0017 0.0782 −0.0206
0.0752
pM = 20%
β2
0.0059 0.0856 0.0854 0.93
0.0064 0.0857 −0.0130
0.0816
β3
0.0061 0.0772 0.0801 0.94
0.0051 0.0824 −0.0303
0.0772
β4
0.0051 0.0701 0.0724 0.95
0.0062 0.0752 −0.0241
0.0698
n = 500
β1
−0.0009 0.0901 0.0886 0.93
0.0014 0.0914 −0.0356
0.0843
pM = 40%
β2
0.0061 0.0965 0.0995 0.94
0.0076 0.0982 −0.0290
0.0896
β3
0.0082 0.0825 0.0859 0.95
0.0086 0.0949 −0.0586
0.0836
β4
0.0049 0.0773 0.0776 0.93
0.0078 0.0860 −0.0486
0.0750
n = 1000
β1
0.0055 0.0547 0.0534 0.94
0.0057 0.0553 −0.0148
0.0532
pM = 20%
β2
−0.0006 0.0587 0.0592 0.95 −0.0005 0.0585 −0.0210
0.0551
β3
0.0066 0.0529 0.0558 0.95
0.0075 0.0580 −0.0305
0.0525
β4
0.0007 0.0514 0.0502 0.92
0.0007 0.0548 −0.0287
0.0513
n = 1000
β1
0.0056 0.0629 0.0615 0.93
0.0062 0.0636 −0.0304
0.0588
pM = 40%
β2
0.0000 0.0689 0.0681 0.94
0.0007 0.0695 −0.0361
0.0633
β3
0.0081 0.0554 0.0596 0.95
0.0092 0.0660 −0.0594
0.0548
β4
0.0005 0.0547 0.0536 0.92
0.0006 0.0625 −0.0530
0.0528
Note: “Bias” is the empirical bias; “SE” is the empirical standard error; “SEE” is the
average standard error estimate; “CP” is the empirical coverage probability of a 95% con-
fidence interval.
Table 1: Results for unpenalized estimators of β under MCAR
The results for MCAR are shown in Table 1. Both the NPMLE and complete-case analysis yield unbiased
estimation, whereas single imputation yields noticeably larger bias than the other two methods, especially under
a missing proportion of 40%.
This is because the imputed values are necessarily not as associated with the
event time as the actual values, so the estimators are biased towards zero. Under all settings, single imputation
yields the smallest standard error, followed by NPMLE. This is because single imputation trades some bias for
7
efficiency, and the NPMLE uses more subjects than the complete-case analysis. For NPMLE, the standard errors
of bβ3 and bβ4 tend to be smaller than those of bβ1 and bβ2, because X3 and X4 have more observations than X1
and X2 and thus have a larger “effective sample size.”
——– True
NPMLE
Complete Case
Single Imputation
Figure 1: Results for unpenalized estimators of Λ under MCAR.
In Figure 1, we present the average value of Λ over the replicates for different methods under MCAR. In this
case, both the NPMLE and complete-case analysis are unbiased for Λ, whereas single imputation yields a biased
estimator.
The simulation results for MAR are shown in Table 2.
Under MAR, NPMLE is unbiased, whereas the
complete-case analysis and single imputation are biased. Similar to the MCAR setting, single imputation yields
the smallest standard errors overall. Under NPMLE, the standard errors of bβ3 and bβ4 tend to be smaller than
those of bβ1 and bβ2. Note that NPMLE yields the smallest mean squared error among all three methods in all
settings, under MCAR and MAR.
In Figure 2, we present the average value of Λ over the replicates for different methods under MAR. The
NPMLE yields unbiased estimation, whereas both the complete-case analysis and single imputation are biased.
3.2
Penalized estimation
In this subsection, we compare the performance of penalized methods. We considered a sample size of n = 500
or 1000 and a number of covariates of p = 100. We draw the covariates from the multivariate normal distribution
with µ = 0 and Σ = diag(Σ1, Σ2), where Σ1 = (0.2|i−j|)i,j=1,...,50 and Σ2 = (0.5|i−j|)i,j=1,...,50. For the survival
model, we set β = (0.25, . . . , 0.25
|
{z
}
4
, 0, . . . , 0
| {z }
192
, 0.25, . . . , 0.25
|
{z
}
4
)T and Λ(t) = 0.04t5/4. We set the censoring time to
be min{C∗, 50}, where C∗∼Exp(0.035); the censoring rate is approximately 34%. For each subject, either
all covariates are observed, or only the covariates with even indices (i.e., X2, X4, . . . , X100) are observed. We
considered missing mechanisms of MCAR and MAR, generated in the same way as the unpenalized case.
We considered the penalized NPMLE, the complete-case analysis, and single imputation. For single imputa-
tion, we imputed the missing values in the same way as for the unpenalized case. For the complete-case analysis
and single imputation, we obtained the active sets using maximum penalized partial likelihood estimation with a
8
NPMLE
Complete Case Single Imputation
Setting
Parameter
Bias
SE
SEE
CP
Bias
SE
Bias
SE
n = 500
β1
−0.0012 0.0773 0.0751 0.93 −0.0524 0.0709 −0.0510
0.0710
pM = 20%
β2
0.0070 0.0848 0.0839 0.93 −0.0455 0.0778 −0.0654
0.0792
β3
0.0046 0.0761 0.0791 0.94 −0.0486 0.0753
0.0181
0.0770
β4
0.0046 0.0691 0.0707 0.94 −0.0484 0.0682 −0.0157
0.0719
n = 500
β1
−0.0017 0.0906 0.0867 0.92 −0.0541 0.0838 −0.0678
0.0794
pM = 40%
β2
0.0071 0.0943 0.0973 0.94 −0.0464 0.0871 −0.0877
0.0865
β3
0.0078 0.0824 0.0848 0.94 −0.0458 0.0896
-0.0079
0.0837
β4
0.0046 0.0726 0.0758 0.95 −0.0458 0.0763 −0.0412
0.0735
n = 1000
β1
0.0043 0.0517 0.0522 0.94 −0.0479 0.0484 −0.0467
0.0490
pM = 20%
β2
−0.0027 0.0591 0.0582 0.95 −0.0550 0.0532 −0.0757
0.0534
β3
0.0076 0.0523 0.0549 0.96 −0.0450 0.0504
0.0215
0.0550
β4
0.0009 0.0505 0.0491 0.92 −0.0517 0.0495 −0.0195
0.0520
n = 1000
β1
0.0051 0.0621 0.0600 0.92 −0.0481 0.0572 −0.0629
0.0549
pM = 40%
β2
−0.0016 0.0702 0.0670 0.93 −0.0559 0.0636 −0.0962
0.0628
β3
0.0074 0.0569 0.0587 0.95 −0.0476 0.0603 −0.0066
0.0577
β4
0.0009 0.0523 0.0524 0.94 −0.0511 0.0568 −0.0457
0.0512
Note: See Note to Table 1.
Table 2: Results for unpenalized estimators of β under MAR
——– True
NPMLE
Complete Case
Single Imputation
Figure 2: Results for unpenalized estimators of Λ under MAR.
LASSO penalty using the observed or completed data and then refitted the model over the active sets. BIC was
used in choosing the best model for all three methods. We report the true positive rate (TPR), false negative rate
(FDR), and mean squared error (MSE) for each method. These statistics, based on 500 simulation replicates,
are summarized in Table 3. The results, based on 500 simulation replicates, are summarized in Table 3.
9
pM = 20%
pM = 40%
n
Pattern
Method
TPR
FDR
MSE
TPR
FDR
MSE
500
MCAR Penalized NPMLE 0.9335 0.1012 0.1013 0.8647 0.1233 0.1517
Complete Case
0.9160 0.1012 0.1219 0.8472 0.1231 0.1934
Single Imputation 0.9322 0.0917 0.0978 0.8502 0.0889 0.1420
500
MAR
Penalized NPMLE 0.9165 0.1092 0.1126 0.8417 0.1283 0.1680
Complete Case
0.8985 0.1092 0.1126 0.8112 0.1283 0.1785
Single Imputation 0.8867 0.0810 0.1207 0.7800 0.0902 0.1805
1000 MCAR Penalized NPMLE 0.9948 0.0814 0.0367 0.9820 0.0921 0.0498
Complete Case
0.9903 0.0810 0.0432 0.9738 0.0917 0.0677
Single Imputation 0.9948 0.0760 0.0352 0.9785 0.0681 0.0472
1000
MAR
Penalized NPMLE 0.9918 0.0770 0.0373 0.9725 0.0926 0.0532
Complete Case
0.9845 0.0768 0.0439 0.9605 0.0926 0.0672
Single Imputation 0.9793 0.0538 0.0452 0.9355 0.0654 0.0730
Note: “TPR” is the true positive rate; “FDR” is the false negative rate; “MSE”
is the mean squared error.
Table 3: Results for penalized estimators of β under MCAR and MAR
The penalized NPMLE has the highest TPR and FDR among all three methods.
This implies that the
penalized NPMLE tends to select more covariates, both relevant and irrelevant ones. In term of MSE, penalized
NPMLE is uniformly better than the complete-case analysis, especially for MCAR. Single imputation could yield
smaller MSE than penalized NPMLE under MCAR, probably because single imputation is biased towards zero,
as demonstrated in the simulation studies for the unpenalized case. By contrast, single imputation always yields
higher MSE than the penalized NPMLE under MAR.
3.3
Unpenalized and penalized estimation under a misspecified covariate
distribution
To evaluate the sensitivity of the proposed methods to the normality assumption, we conduct simulation studies
with a misspecified covariate distribution. In particular, we generated the multivariate normal random vector as
described above. Then, we transformed each component of the normal vector by F −1
5
◦Φ and set the transformed
value as a covariate, where Φ and F5 are the cumulative distribution functions of the standard normal distribution
and the t distribution with five degrees of freedom, respectively. As a result, each covariate marginally follows a
t distribution. The event and censoring times were then generated in the same way as the above. We considered
both the unpenalized and penalized estimators. The results are shown in Tables 4, 5, and 6.
For the unpenalized methods, the results are similar in pattern as those under a correctly-specified covariate
distribution.
In particular, NPMLE and the complete-case analysis are unbiased under MCAR, while single
imputation tends to be biased. However, the standard errors under NPMLE and single imputation are of similar
level, and single imputation does not have noticeable smaller standard errors than the other methods. In term
of MSE, NPMLE dominates the other two methods under all settings.
For the penalized methods, similar to the results in Table 3, the penalized NPMLE has overall the highest
TPR and FDR. Penalized NPMLE always has a smaller MSE than the complete-case analysis. Under MCAR,
penalized NPMLE and single imputation have similar values of MSE. Under MAR, single imputation has the
largest MSE among all three methods.
4
Real data analysis
We analyzed a dataset of kidney renal clear cell carcinoma (KIRC) from TCGA. The dataset, released in November
2015, was downloaded through the RTCGA package (Kosinski et al., 2016) in R. In the study, times to new tumor
10
NPMLE
Complete Case Single Imputation
Setting
Parameter
Bias
SE
SEE
CP
Bias
SE
Bias
SE
n = 500
β1
−0.0020 0.0637 0.0632 0.93 −0.0005 0.0648 −0.0266
0.0615
pM = 20%
β2
0.0032 0.0710 0.0703 0.94
0.0053 0.0711 −0.0221
0.0668
β3
0.0070 0.0653 0.0669 0.95
0.0051 0.0686 −0.0456
0.0695
β4
0.0072 0.0595 0.0605 0.94
0.0072 0.0636 −0.0323
0.0596
n = 500
β1
−0.0026 0.0745 0.0732 0.94
0.0023 0.0765 −0.0464
0.0681
pM = 40%
β2
0.0014 0.0780 0.0817 0.95
0.0066 0.0789 −0.0432
0.0704
β3
0.0107 0.0710 0.0727 0.94
0.0087 0.0798 −0.0815
0.0777
β4
0.0077 0.0657 0.0658 0.92
0.0094 0.0732 −0.0624
0.0640
n = 1000
β1
0.0041 0.0448 0.0439 0.92
0.0057 0.0454 −0.0225
0.0429
pM = 20%
β2
−0.0004 0.0487 0.0483 0.95
0.0012 0.0487 −0.0269
0.0447
β3
0.0054 0.0452 0.0461 0.94
0.0046 0.0489 −0.0485
0.0488
β4
0.0019 0.0418 0.0417 0.94
0.0012 0.0443 −0.0381
0.0452
n = 1000
β1
0.0030 0.0513 0.0505 0.93
0.0068 0.0524 −0.0424
0.0467
pM = 40%
β2
−0.0023 0.0557 0.0554 0.93
0.0020 0.0568 −0.0476
0.0501
β3
0.0082 0.0479 0.0501 0.93
0.0059 0.0551 −0.0863
0.0520
β4
0.0026 0.0455 0.0453 0.94
0.0019 0.0507 −0.0685
0.0483
Note: See Note to Table 1.
Table 4: Results for unpenalized estimators of β with a misspecified distribution under
MCAR
NPMLE
Complete Case Single Imputation
Setting
Parameter
Bias
SE
SEE
CP
Bias
SE
Bias
SE
n = 500
β1
−0.0011 0.0625 0.0610 0.93 −0.0445 0.0587 −0.0480
0.0581
pM = 20%
β2
0.0083 0.0681 0.0680 0.94 −0.0371 0.0636 −0.0590
0.0642
β3
0.0044 0.0643 0.0652 0.94 −0.0395 0.0632
0.0027
0.0684
β4
0.0052 0.0564 0.0583 0.94 −0.0385 0.0549 −0.0219
0.0629
n = 500
β1
0.0002 0.0759 0.0703 0.92 −0.0452 0.0711 −0.0690
0.0664
pM = 40%
β2
0.0050 0.0773 0.0783 0.94 −0.0422 0.0733 −0.0935
0.0717
β3
0.0090 0.0680 0.0706 0.94 −0.0386 0.0714 −0.0327
0.0741
β4
0.0048 0.0610 0.0634 0.94 −0.0428 0.0655 −0.0543
0.0657
n = 1000
β1
0.0047 0.0423 0.0421 0.93 −0.0394 0.0403 −0.0431
0.0403
pM = 20%
β2
0.0009 0.0481 0.0467 0.95 −0.0439 0.0445 −0.0663
0.0442
β3
0.0049 0.0432 0.0448 0.94 −0.0382 0.0422
0.0039
0.0481
β4
0.0004 0.0407 0.0404 0.93 −0.0425 0.0411 −0.0283
0.0435
n = 1000
β1
0.0044 0.0497 0.0483 0.93 −0.0426 0.0465 −0.0667
0.0445
pM = 40%
β2
−0.0011 0.0561 0.0536 0.94 −0.0489 0.0518 −0.0995
0.0502
β3
0.0065 0.0464 0.0484 0.95 −0.0411 0.0494 −0.0338
0.0530
β4
0.0007 0.0427 0.0437 0.95 −0.0459 0.0451 −0.0607
0.0456
Note: See Note to Table 1.
Table 5: Results for unpenalized estimators of β with a misspecified distribution under
MAR
events and death were collected, which were potentially subject to right censoring. Also, omic variables including
gene expressions, measured by RNA sequencing, and protein expressions, measured by reverse-phase protein
array, were collected for some or most subjects. In this paper, we focus on the association between time to death
since initial diagnosis and the omic variables.
The dataset contains 20531 gene expressions and 217 protein expressions. There are 530 subjects with both
11
pM = 20%
pM = 40%
n
Pattern
Method
TPR
FDR
MSE
TPR
FDR
MSE
500
MCAR Penalized NPMLE 0.9848 0.1034 0.0535 0.9465 0.1238 0.0823
Complete Case
0.9773 0.1034 0.0645 0.9430 0.1238 0.1094
Single Imputation 0.9830 0.0877 0.0503 0.9300 0.0953 0.0833
500
MAR
Penalized NPMLE 0.9790 0.1077 0.0558 0.9285 0.1285 0.0939
Complete Case
0.9715 0.1075 0.0582 0.9255 0.1282 0.0967
Single Imputation 0.9603 0.0841 0.0660 0.8692 0.0943 0.1175
1000 MCAR Penalized NPMLE 0.9988 0.0756 0.0214 0.9965 0.0958 0.0267
Complete Case
0.9988 0.0752 0.0242 0.9948 0.0954 0.0379
Single Imputation 0.9988 0.0675 0.0208 0.9960 0.0635 0.0265
1000
MAR
Penalized NPMLE 0.9985 0.0852 0.0216 0.9953 0.0976 0.0281
Complete Case
0.9988 0.0848 0.0254 0.9935 0.0972 0.0362
Single Imputation 0.9950 0.0670 0.0274 0.9760 0.0739 0.0443
Note: See Note to Table 3.
Table 6: Results for penalized estimators of β with a misspecified distribution
under MCAR and MAR
survival data and gene expression measurements.
Among these subjects, 475 have measurements in protein
expressions. Following Zhao et al. (2015), we filtered out gene expressions with 0 median absolute deviation,
resulting in the removal of 2865 genes. Then, following The Cancer Genome Atlas Research Network (2013),
we selected the top 1500 gene expressions with the largest maximum absolute deviation. We then performed
the log(1 + x)-transformation on the gene expressions. In addition, we removed 5 protein expressions that were
missing for over 90% of the subjects. After the above preliminary processing, we performed supervised screening
by fitting a separate Cox model for time to death against each gene or protein expression and selected the top
150 covariates with the smallest p-values; here, complete-case analysis was used in the presence of missing values.
This resulted in 16 protein expressions and 134 gene expressions selected for downstream analyses. The missing
proportion for protein expressions is 10%, and the censoring rate is 58%.
We performed the penalized NPMLE, complete-case analysis, and single imputation approaches on the pro-
cessed data. Note that we obtained the active set based on standardized covariates and refitted our models on
the original unstandardized covariates. The analysis results are presented in Table 7. The penalized NPMLE,
complete-case analysis, and single imputation selected 8, 4, and 5 covariates, respectively. This is consistent with
the findings in the simulation studies that the penalized NPMLE tends to select the largest number of features.
Variable
Penalized NPMLE Complete Case Single Imputation
Protein – MAPK pT202 Y204
−0.3288
−0.3209
−0.3266
Gene – CDCA3 (83461)
0.0797
0.1487
0.1172
Gene – SHOX2 (6474)
0.1089
0.1608
0.1432
Gene – LOC286467 (286467)
0.0883
0.1642
0.1338
Gene – DNASE1L3 (1776)
−0.0976
·
−0.1261
Gene – BRD9 (65980)
0.1717
·
·
Gene – PHF21A (51317)
0.4149
·
·
Gene – CARS (833)
0.1477
·
·
Note: For gene expressions, the Entrez IDs are given in the parentheses.
Table 7: Regression parameter estimates for the KIRC data
We evaluate the prediction performance of the three methods as follows. We randomly split the data into
training and testing sets with a 7:3 ratio of sample sizes and performed the three estimation procedures on the
training data. Then, to facilitate evaluation of the fitted models, we imputed the missing values in the testing
12
data by single imputation, where the whole data set was used to estimate the imputation model. We calculated
the concordance index (C-index) (Harrell et al., 1982) between the event time and the estimated XTβ on the
(imputed) testing data. The above procedure was repeated 100 times. Note that we imputed the testing data in
the exact same way as in the single imputation method in the simulation studies. The average C-index values over
the 100 splits for penalized NPMLE, complete-case analysis, and single imputation are 0.660, 0.656, and 0.658,
respectively. The C-index values are similar due to the small missing proportion, with the proposed method
having a slight advantage.
5
Discussion
In this paper, we propose a likelihood-based approach for (penalized) estimation of the Cox proportional hazards
model, where covariates could be missing. We devise a novel EM algorithm that enables efficient computation un-
der arbitrary missing patterns and a large number of missing covariates. Instead of performing multi-dimensional
numerical integration over all dimensions of the missing covariates, we propose a linear transformation on the
covariates, so that the expectations of all but one components of the transformed variables have closed-form
expressions.
As for likelihood-based methods in general, we need to impose modeling assumptions on the missing covariates
(except when only a few covariates are involved, in which case a full nonparametric model can be fit). The
proposed methods depend crucially on the Gaussian assumptions on the covariates; without these assumptions,
the transformation approach to reduce the dimension of numerical integration is not applicable. One possible
approach to relax the Gaussian assumptions is to assume that the observed covariates are transformed values of
underlying Gaussian variables, that is, Xj = gj(Zj) for some transformation function gj and Gaussian variable Zj.
We then assume that the dependence between the outcome and covariates is mediated through Z ≡(Z1, . . . , Zp)T,
such that λ(t | X, Z) = λ(t)eβTZ. In this scenario, the proposed transformation approach can still be adopted.
The proposed transformation technique can also be applied to random effect models with Gaussian latent
variables (Papageorgiou et al., 2019; Sun et al., 2019; Wong et al., 2022). In general, we can accommodate an
outcome variable that follows a survival model or a generalized linear model that regresses on a linear combination
of Gaussian latent variables. This outcome variable can also be jointly modelled with other Gaussian outcomes
that regress linearly on the random effects. To compute the MLE, we can develop a similar EM algorithm, where
in the E-step, we transform the latent variable vector such that the first component is the linear combination
present in the survival or generalized linear model.
Due to its flexibility, multiple imputation is a popular approach for handling missing data. The proposed
methods have advantages over multiple imputation approaches, especially when variable selection is desirable, in
two key respects. First, under multiple imputation, it is often difficult to explicitly define the estimator, as it is
typically the limit of some iterative algorithm. This makes theoretical studies of the estimator very challenging.
By contrast, the proposed estimator is the maximizer of the (penalized) likelihood, and existing techniques (such
as Wang and Leng, 2007) can be applied to establish the theoretical properties. Second, multiple imputation is
not amenable to simultaneous variable selection and estimation. One advantage of penalization methods is that
they perform variable selection and estimation simultaneously: penalization shrinks the estimators towards zero,
and some estimators are shrunk to exactly zero, thereby eliminating the corresponding covariates. However, even
though penalized estimation can be performed on each imputed dataset to yield a sparse estimator, the final
estimator that combines results from all imputed datasets is generally not sparse, as a variable would be selected
even if it is selected in just one of the imputed dataset. By contrast, because the proposed method imposes a
penalty on a single likelihood, it performs simultaneous variable selection and estimation.
In the proposed methods, we fit an unstructured covariance matrix for the covariates and estimate it by
unpenalized MLE. As a result, we cannot accommodate a high-dimensional setting with p > n, as the variance
estimator would be non-positive definite. To accommodate high-dimensional data, one can consider shrinkage
estimators for covariance estimation (Ledoit and Wolf, 2004; Warton, 2008). Alternatively, we could impose
structures on the covariance matrix to facilitate estimation. For example, we may fit a factor model for X, such
that Σ can be decomposed into a low-rank matrix plus a sparse or diagonal matrix (Fan et al., 2008). These
13
approaches would require modifications to the M-step of the proposed algorithm, but the E-step remains the
same.
References
Azur, M. J., Stuart, E. A., Frangakis, C., and Leaf, P. J. (2011). Multiple imputation by chained equations:
What is it and how does it work? International Journal of Methods in Psychiatric Research, 20:40–49.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39:1–22.
Deng, Y., Chang, C., Ido, M. S., and Long, Q. (2016). Multiple imputation for general missing data patterns in
the presence of high-dimensional data. Scientific Reports, 6:21689.
Fan, J., Fan, Y., and Lv, J. (2008). High dimensional covariance matrix estimation using a factor model. Journal
of Econometrics, 147:186–197.
Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal
of the American Statistical Association, 96:1348–1360.
Garcia, R. I., Ibrahim, J. G., and Zhu, H. (2010). Variable selection in the Cox regression model with covariates
missing at random. Biometrics, 66:97–104.
Harrell, F. E., Califf, R. M., Pryor, D. B., Lee, K. L., and Rosati, R. A. (1982). Evaluating the yield of medical
tests. JAMA, 247:2543–2546.
Hastie, T., Mazumder, R., Lee, J. D., and Zadeh, R. (2015). Matrix completion and low-rank SVD via fast
alternating least squares. Journal of Machine Learning Research, 16:3367–3402.
Herring, A. H. and Ibrahim, J. G. (2001). Likelihood-based methods for missing covariates in the Cox proportional
hazards model. Journal of the American Statistical Association, 96:292–302.
Johnson, B. A., Lin, D. Y., and Zeng, D. (2008).
Penalized estimating functions and variable selection in
semiparametric regression models. Journal of the American Statistical Association, 103:672–680.
Kosinski, M., Biecek, P., and Chodor, W. (2016).
RTCGA: The Cancer Genome Atlas Data Integration. R
package version 1.34.0.
Ledoit, O. and Wolf, M. (2004). A well-conditioned estimator for large-dimensional covariance matrices. Journal
of Multivariate Analysis, 88:365–411.
Liang, L., Zhuang, Y., and Yu, P. L. H. (2024). Variable selection for high-dimensional incomplete data. Com-
putational Statistics & Data Analysis, 192:107877.
Liu, Q. and Pierce, D. A. (1994). A note on Gauss–Hermite quadrature. Biometrika, 81:624–629.
Martinussen, T., Holst, K. K., and Scheike, T. H. (2016). Cox regression with missing covariate data using a
modified partial likelihood method. Lifetime Data Analysis, 22:570–588.
Papageorgiou, G., Mauff, K., Tomer, A., and Rizopoulos, D. (2019). An overview of joint modeling of time-to-
event and longitudinal outcomes. Annual Review of Statistics and its Application, 6:223–240.
Raghunathan, T. E., Lepkowski, J. M., Van Hoewyk, J., and Solenberger, P. (2001). A multivariate technique
for multiply imputing missing values using a sequence of regression models. Survey Methodology, 27:85–96.
Rubin, D. B. (1987). Multiple imputation for nonresponse in surveys. Wiley: New York.
Sabbe, N., Thas, O., and Ottoy, J.-P. (2013). EMLasso: Logistic lasso with missing data. Statistics in Medicine,
32:3143–3157.
14
Simon, N., Friedman, J., Hastie, T., and Tibshirani, R. (2011). Regularization paths for Cox’s proportional
hazards model via coordinate descent. Journal of Statistical Software, 39:1–13.
Sun, J., Herazo-Maya, J. D., Molyneaux, P. L., Maher, T. M., Kaminski, N., and Zhao, H. (2019). Regularized
latent class model for joint analysis of high-dimensional longitudinal biomarkers and a time-to-event outcome.
Biometrics, 75:69–77.
The Cancer Genome Atlas Research Network (2013). Comprehensive molecular characterization of clear cell
renal cell carcinoma. Nature, 499:43–49.
Thiessen, D. L., Zhao, Y., and Tu, D. (2022). Unified estimation for Cox regression model with nonmonotone
missing at random covariates. Statistics in Medicine, 41:4781–4790.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society
Series B (Statistical Methodology), 58:267–288.
van Buuren, S. and Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in R.
Journal of Statistical Software, 45:1–67.
Wang, H. and Leng, C. (2007). Unified lasso estimation by least squares approximation. Journal of the American
Statistical Association, 102:1039–1048.
Warton, D. I. (2008). Penalized normal likelihood and ridge regularization of correlation and covariance matrices.
Journal of the American Statistical Association, 103:340–349.
White, I. R. and Royston, P. (2009). Imputing missing covariate values for the Cox model. Statistics in Medicine,
28:1982–1998.
Wilkie, R., Parmar, S. S., Blagojevic-Bucknall, M., Smith, D., Thomas, M. J., Seale, B. J., Mansell, G., and Peat,
G. (2019). Reasons why osteoarthritis predicts mortality: Path analysis within a Cox proportional hazards
model. RMD Open, 5:e001048.
Wolfson, J. (2011).
EEBoost: A general method for prediction and variable selection based on estimating
equations. Journal of the American Statistical Association, 106:296–305.
Wong, K. Y., Zeng, D., and Lin, D. Y. (2022). Semiparametric latent-class models for multivariate longitudinal
and survival data. Annals of Statistics, 50:487–510.
Wood, A. M., White, I. R., and Royston, P. (2008). How should variable selection be performed with multiply
imputed data? Statistics in Medicine, 27:3227–3246.
Zhao, Q., Shi, X., Xie, Y., Huang, J., Shia, B., and Ma, S. (2015).
Combining multidimensional genomic
measurements for predicting cancer prognosis: Observations from TCGA. Briefings in Bioinformatics, 16:291–
303.
Zhou, R., Li, H., Sun, J., and Tang, N. (2022). A new approach to estimation of the proportional hazards model
based on interval-censored data with missing covariates. Lifetime Data Analysis, 28:335–355.
Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101:1418–1429.
15
