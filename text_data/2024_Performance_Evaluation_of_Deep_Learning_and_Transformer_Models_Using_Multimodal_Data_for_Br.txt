Performance Evaluation of Deep Learning and
Transformer Models Using Multimodal Data for
Breast Cancer Classification
Sadam Hussain1[0000−0002−3453−0785], Mansoor Ali1[0000−0002−4669−7267],
Usman Naseem2[0000−0003−0191−7171], Beatriz Alejandra Bosques Palomo1,
Mario Alexis Monsivais Molina1, Jorge Alberto Garza Abdala1, Daly
Betzabeth Avendano Avalos4, Servando Cardona-Huerta4, T. Aaron Gulliver3,
and Jose Gerardo Tamez Pena4[0000−0003−1361−5162]
1 Tecnologico de Monterrey, School of Sciences and Engineering, Mexico.
a01753094@tec.mx
2 School of Computing, Macquarie University, Australia.
3 Department of Electrical and Computer Engineering, University of Victoria,
Canada.
4 School of Medical and Health Sciences, Tecnologico de Monterrey, Mexico.
Abstract. Rising breast cancer (BC) occurrence and mortality are ma-
jor global concerns for women. Deep learning (DL) has demonstrated
superior diagnostic performance in BC classification compared to hu-
man expert readers. However, the predominant use of unimodal (digital
mammography) features may limit the current performance of diagnostic
models. To address this, we collected a novel multimodal dataset compris-
ing both imaging and textual data. This study proposes a multimodal
DL architecture for BC classification, utilizing images (mammograms;
four views) and textual data (radiological reports) from our new in-
house dataset. Various augmentation techniques were applied to enhance
the training data size for both imaging and textual data. We explored
the performance of eleven SOTA DL architectures (VGG16, VGG19,
ResNet34, ResNet50, MobileNet-v3, EffNet-b0, EffNet-b1, EffNet-b2, EffNet-
b3, EffNet-b7, and Vision Transformer (ViT)) as imaging feature extrac-
tors. For textual feature extraction, we utilized either artificial neural
networks (ANNs) or long short-term memory (LSTM) networks. The
combined imaging and textual features were then inputted into an ANN
classifier for BC classification, using the late fusion technique. We evalu-
ated different feature extractor and classifier arrangements. The VGG19
and ANN combinations achieved the highest accuracy of 0.951. For pre-
cision, the VGG19 and ANN combination again surpassed other CNN
and LSTM, ANN based architectures by achieving a score of 0.95. The
best sensitivity score of 0.903 was achieved by the VGG16+LSTM. The
highest F1 score of 0.931 was achieved by VGG19+LSTM. Only the
VGG16+LSTM achieved the best area under the curve (AUC) of 0.937,
with VGG16+LSTM closely following with a 0.929 AUC score.
Keywords: Breast Cancer · Feature Fusion · Multi-modal Classification
· Deep Learning
arXiv:2410.10146v1  [eess.IV]  14 Oct 2024
2
Authors Suppressed Due to Excessive Length
1
Introduction
BC is the most prevalent disease among women [20]. Anticipated figures for
2023 suggest that the United States will likely witness 1,958,310 new cancer
diagnoses and 609,820 cancer-related fatalities [8]. Digital mammography and
other imaging modalities have long been used for BC detection [14]. However,
due to the large volume of data, radiologists often struggle to process it in
a timely manner. To assist them, various Computer-Aided Diagnosis (CAD)
systems have been developed [25, 3].
Traditionally, many CAD systems are tailored to process single-modality
data, such as images, text, or audio. However, recent studies indicate that us-
ing multimodal data as input to SOTA DL models can yield promising results
in the medical field, particularly in BC diagnosis and prognosis [13, 22, 7, 19].
Multimodal data is crucial for BC diagnosis and prognosis, as it provides com-
prehensive information about various aspects such as radiomics characteristics,
clinical features, and imaging features of the disease. Integrating different types
of data allows DL methods to achieve better performance in BC classification,
prognosis, and survival prediction than using only a single modality [13].
Multimodal BC classification has received increasing attention of researchers.
Most approaches use imaging data along with textual reports for BC diagnosis
[2, 11] or some studies such as [11] used clinical factors. Some studies employed
multidimensional data for predicting both short-term and long-term BC risk
[19].
In this study, we provide a comparative analysis of various SOTA DL models
with ViT [5]. We also collected a multimodal dataset comprising digital mam-
mography (four views; L-CC, L-MLO, R-CC and R-MLO) and radiological re-
ports. We propose a DL and transformer-based pipeline with different backbone
networks to extract features from the multimodal data, applying a late fusion
technique to combine imaging and textual features before training a classifier for
BC classification. We also evaluated the performance of the transformer-based
architecture, ViT, which has shown promising outcomes on text and imaging
data [6].
The proposed model uses CNNs and ViT for feature extraction from digital
mammography and either ANN or LSTM for feature extraction from radiological
reports. The fused features are then assigned to an ANN classifier for the final
classification of benign or malignant cancer. We observed that models combining
LSTM for textual feature extraction generally performed better than those using
simple ANN. Furthermore, it has been observed that VGG16 and VGG19 models
outperformed other SOTA DL architectures and ViT across all the evaluated
metrics The proposed multimodal pipeline is illustrated in Fig. 1.
2
Proposed Work
In this work, we evaluate the performance of various DL and transformer based
architectures on a newly collected multimodal dataset for BC diagnosis.
Title Suppressed Due to Excessive Length
3
The key contributions of this work are highlighted as follows:
i. A new in-house dataset comprising multi-view mammograms and radiolog-
ical reports has been collected and preprocessed.
ii. A DL and transformer-based multimodal pipeline for BC classification is
employed on a new in-house dataset. In this pipeline, SOTA DL models, such
as VGG16 [18], VGG19 [18], ResNet34 [9], ResNet50 [9], MobileNet-v3 [12],
EffNet-b0 [21], EffNet-b1 [21], EffNet-b2 [21], EffNet-b3 [21], EffNet-b7 [21],
and a transformer-based model, ViT [4], are used to extract imaging features,
with either LSTM or ANN employed to extract textual features.
[4, 1]
[4, 1000]
[4, 400]
[4, 1000]
[4, 1000]
[4, 1000]
METADATA
[4, 4]
...
224
224
4
[4, 3, 224, 224]
L-CC
Multi-view Multimodal
Feature Fusion
64 neurons
1 neuron
4 predictions
224
224
4
[4, 3, 224, 224]
L-MLO
224
224
4
[4, 3, 224, 224]
R-CC
224
224
4
[4, 3, 224, 224]
R-MLO
BIRADS
BREAST DENSITY
AGE
FAMILY RISK
RADIOLOGY
REPORT
[1, 2, 59, 0]
[1, 1, 64, 1]
[3, 3, 53, 0]
4
4
[4, 4, 61, 0]
 
Fig. 1. Proposed Multi-Modal Model for Processing Multi-Dimensional Data
3
Methodology
This section highlights the methodology employed in this study, encompassing
data collection, preprocessing, data augmentation, as well as the training and
4
Authors Suppressed Due to Excessive Length
testing of data. It also details the multimodal architecture utilized, implementa-
tion specifics, classification, and evaluation of the models.
3.1
Data
The dataset used in this investigation was provided by the breast radiology
department at TecSalud Hospital in Monterrey, Mexico. In addition to digital
mammograms, it includes digital breast tomosynthesis (DBT), digital mammog-
raphy, and ultrasound (US) based radiological reports. Anonymized reports and
mammograms were used to ensure anonymity. The reports, which were gath-
ered between January and December 2014 and were initially written in Spanish,
were translated into English using Google Translate. Bilingual radiologists then
confirmed the translation. Two seasoned radiologists wrote and evaluated each
report. One radiologist reviewed the translated reports. In cases where the first
radiologist disagreed with the translation, the second radiologist revised and fi-
nalized the translation. There were a thousand examples in the dataset at first.
Four views (LCC, LMLO, RCC, and RMLO) and related textual data (BIRADS
categories, breast density scores, patient ages, family cancer histories, and lesion
laterality) make up each patient’s data set. Lesion laterality reveals if a patient
has had cancer in one or both breasts. Three values are used to characterize it:
0 for left breast, 1 for right breast, and 2 for cancer in both breasts. Laterality
was only performed to determine whether the patient had malignancy. We used
770 cases with a total of 3080 mammograms and related metadata after prepro-
cessing, which comprised deleting reports with missing values, duplicates, and
BIRADS categories 0 (inconclusive) and 6 (biopsy-proven). There are exactly
equal numbers of positive (385) and negative (385) cases in the dataset. The
final model building and evaluation involved women in all cases, with an average
patient age of 53.
3.2
Data Preprocessing
The original dataset, in Spanish, consisted of 7,904 samples. After translation,
we removed redundant data, missing values, and cases with BIRADS scores of
0 or 6. Following preprocessing, we collected 5,046 samples. Each patient entry
included a brief clinical indication of the study, personal risk history, imaging
description, findings, and diagnostic impression. From these radiological reports,
we extracted BIRADS categories (1-5), breast density scores, patients’ ages, and
family history of cancer, which were ultimately used for model building.
3.3
Data Augmentation
The augmentation techniques applied to the mammogram dataset were carefully
selected to introduce diversity and robustness into the training data. Horizon-
tal and vertical flipping were utilized to mirror the images, exposing the model
to different orientations. Furthermore, the implementation of random resized
Title Suppressed Due to Excessive Length
5
crops, targeting an output size of 224x224 pixels, aimed to instill scale and
translation invariance during model training. Dynamic transformations, such as
shifts, scales, and rotations, were also incorporated, allowing for rotations up to
90 degrees and scale adjustments within the range of 0.8 to 1.2. This diverse set
of augmentations serves the crucial purpose of expanding the effective size of the
training dataset. By exposing the model to varied perspectives, the augmenta-
tion strategy becomes instrumental in preventing overfitting and enhancing the
model’s adaptability to diverse input scenarios, ultimately contributing to better
performance.
3.4
Multimodal Deep Architecture
We introduced a multimodal architecture to classify BC instances as positive or
negative. The proposed architecture consists of independent DL and transformer-
based architectures (pre-trained on ImageNet "IMAGENET1K_V1" weights
provided by the torchvision library) as a backbone for extracting imaging fea-
tures from mammograms. The DL architectures used in this study are variants
of VGG, ResNet, EffNet and MobileNetv3. We chose these techniques because
of their SOTA performance on natural and medical image classification tasks
[21, 24, 15, 4, 17]. For extracting textual features that were extracted from radi-
ological reports as tabular data we used simple ANN or long short-term memory
(LSTM). These simple architectures perform better on tabular data. Afterwards,
the feature vectors extracted from both modalities were fused before the final
classification task. This vector is then passed through a linear classification layer
for the prediction of malignant or benign cancer.
Let I represents mammogram input, where there are four input images one
for each view of the mammogram, therefore, i1, i2, i3 and i4, represents each view
of the mammogram respectively. Features extracted from the four mammogram
views are represented as f1, f2, f3 and f4. Let T represents input tabular data
extracted from radiological reports. Features extracted from tabular data are
shown as ft. Therefore, the concatenated feature vector represented as Fc =
concat(f1, f2, f3, f4, ft). The Fc is then fed to a linear layer for BC classification.
The transformer architecture is an attention-based encoder-decoder model.
The encoder processes the input, and the decoder generates the output. Impor-
tantly, the transformer architecture does not depend on recurrence or convolu-
tions to produce an output. The architecture employs stacked self-attention and
point-wise, fully connected layers for both the encoder and decoder, as detailed
in Vaswani et al., 2017 [23].
The self-attention mechanism is one of the most important parts of the trans-
former architecture. It makes the model to consider the relationships between
all positions in the input simultaneously with:
MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O
(1)
headi = Attention(QW Q
i , KW K
i , V W V
i ),
(2)
6
Authors Suppressed Due to Excessive Length
where Q is the query, K denotes to the key, and V is the value [23].
The ViT is an extension of the transformer architecture, utilizing only the
encoder part. The input image is divided into fixed-size, non-overlapping patches.
Each patch is linearly embedded to form vectors, with an added "classification
token." Thus, the transformer’s input is the classification token combined with
the patch embeddings. The transformer’s output then serves as the input for a
classification head.
3.5
Implementation Details
For training and testing all models, we utilized an NVIDIA RTX A4000 GPU.
This GPU is equipped with 16 GB of memory and supports PCI Express Gen
4. Additionally, it features third-generation tensor CUDA cores. Our network
architecture incorporated ResNet34, ResNet50, VGG16, VGG19, MobileNet_v3,
ViT, and EffNet_b0 as backbones for image encoding, and either ANN or LSTM
for text encoding. The model weights were initialized from scratch rather than
using pretrained models. We employed Adam as the optimizer to minimize loss,
with a learning rate set at 0.0005. The batch size was fixed at 8, and the models
were trained over 100 epochs. The Rectified Linear Unit (ReLU) was used as
the activation function, with a dropout rate of 0.2. The dataset was split into
training and validation sets at a ratio of 80% to 20%, respectively. This data
split was consistent across all experiments. Model performance was evaluated
using the validation set.
4
Results
In our comprehensive evaluation, we used a variety of performance metrics to
assess the effectiveness of our multimodal models, ensuring a thorough under-
standing of their classification capabilities. These metrics included Accuracy,
Precision, Sensitivity, F1 Score, and AUC. Our results demonstrated significant
achievements across different metrics, highlighting the distinct strengths of var-
ious models.
The highest accuracy, at 0.951, was achieved by the VGG19+ANN archi-
tecture, indicating the superior ability to correctly classify instances within the
dataset. This was closely followed by the joint models of VGG19+LSTM, scoring
0.919. For precision, the VGG19 paired with ANN achieved the highest score of
0.95, surpassing other combined model configurations, followed by the ViT and
LSTM combination scoring 0.932. This highlights VGG19’s proficiency in mini-
mizing false positives and ensuring precise classifications. In terms of sensitivity,
the VGG16 and LSTM combination recorded the highest rate at 0.903, followed
by VGG19+LSTM, at 0.900. This indicates their effectiveness in correctly identi-
fying a significant proportion of true positive cases, crucial in applications where
sensitivity is key. The highest F1 Score, reflecting a balance between precision
and recall, was 0.931 using the VGG19+LSTM, with the VGG19+ANN close
behind at 0.922. The AUC analysis showed the VGG16+LSTM model leading
Title Suppressed Due to Excessive Length
7
Table 1. Overview of the Results of Different DL Models for BC Classification
Feature Extractor
Classifier ACC Precision Sensitivity F1 Score AUC
VGG16+LSTM
ANN
0.882 0.902
0.903
0.902
0.937
VGG19+LSTM
ANN
0.919 0.923
0.900
0.931
0.907
ResNet34+LSTM
ANN
0.910 0.782
0.899
0.753
0.739
ResNet50+LSTM
ANN
0.889 0.747
0.899
0.753
0.798
MobileNet_v3+LSTM ANN
0.813 0.835
0.753
0.903
0.679
EffNet_b0+LSTM
ANN
0.834 0.769
0.892
0.723
0.757
EffNet_b1+LSTM
ANN
0.834 0.772
0.895
0.723
0.759
EffNet_b2+LSTM
ANN
0.842 0.893
0.876
0.723
0.807
EffNet_b3+LSTM
ANN
0.827 0.758
0.903
0.736
0.719
EffNet_b7+LSTM
ANN
0.825 0.884
0.872
0.738
0.685
ViT+LSTM
ANN
0.893 0.932
0.863
0.871
0.925
with a score of 0.937, demonstrating its ability to differentiate between positive
and negative cases effectively, followed by the VGG16 and ANN combination
at 0.929. The results of combining SOTA DL architectures with LSTM for BC
classification are presented in Table 1, and those with ANN in Table 2.
The least performing model was the ResNet34 combined with ANN, recording
an accuracy of 0.692. The MobileNet_v3 combined with ANN was the least
accurate in the LSTM category, at 0.813. The EffNet_b3 paired with ANN
had the lowest precision of 0.711, while ResNet50 with LSTM scored the lowest
at 0.747. In terms of sensitivity, the lowest score, 0.643, was recorded by the
MobileNet_v3 and ANN combination, followed by the EffNet_b3 combined with
ANN scoring 0.653. The EffNet_b3 and ANN combination had the lowest F1
Score at 0.652, while the lowest in the LSTM category was 0.723 for EffNet_b0,
EffNet_b1, EffNet_b2. Lastly, the lowest AUC score of 0.678 was achieved by
the MobileNet_v3 and ANN combination, and 0.679 by the ViT and LSTM
combination.
5
Discussion
The data in healthcare is inherently multimodal[1]. It includes scans in the form
of images, doctors’ analyses as notes or radiological reports, and audio data, such
as ECG/EEG, which provide insights for diagnosis, prognosis, and treatment
decisions[16]. However, most healthcare studies focus on a single modality of data
(image, text, or audio) due to the scarcity of multimodal data, often yielding
less than optimal results. Since the last decade, researchers have been combining
different data modalities to achieve better outcomes. Current methods, though,
still fall short in terms of generalizability and accuracy comparable to that of
physicians[10].
In this work, we extracted features from images and textual data, combining
them for the final classification of benign and malignant cancer. We employed
8
Authors Suppressed Due to Excessive Length
Table 2. Overview of the Results of Different DL Models for BC Classification
Feature Extractor Classifier ACC Precision Sensitivity F1 Score AUC
VGG16+ANN
ANN
0.900 0.912
0.893
0.897
0.929
VGG19+ANN
ANN
0.951 0.950
0.884
0.922
0.915
ResNet34+ANN
ANN
0.692 0.731
0.716
0.692
0.748
ResNet50+ANN
ANN
0.721 0.752
0.705
0.681
0.723
MobileNet_v3+ANN ANN
0.754 0.858
0.643
0.668
0.678
EffNet_b0+ANN
ANN
0.781 0.714
0.658
0.668
0.709
EffNet_b1+ANN
ANN
0.781 0.714
0.658
0.668
0.703
EffNet_b2+ANN
ANN
0.782 0.887
0.825
0.851
0.828
EffNet_b3+ANN
ANN
0.781 0.711
0.653
0.652
0.719
EffNet_b7+ANN
ANN
0.781 0.812
0.658
0.687
0.689
ViT+ANN
ANN
0.871 0.903
0.819
0.848
0.844
various SOTA DL and transformer-based architectures for feature extraction
from mammograms and radiological reports. These included VGG16, VGG19,
ResNet34, ResNet50, MobileNet_v3, EffNet_b0, EffNet_b1, EffNet_b2, EffNet_b3,
EffNet_b7, and ViT for mammogram feature extraction. For textual feature
extraction, we used ANN or LSTM techniques. We then merged these features
using a late fusion strategy and employed an ANN architecture for classification.
ViT, in combination with LSTM or ANN, couldn’t achieve better or comparable
results to other SOTA DL models in terms of accuracy, precision, sensitivity,
F1 score and AUC. However, ViT+LSTM ranked second highest in the AUC
curve with a score of 0.925, just behind VGG16+LSTM with the highest score
of 0.937.
We observed that using LSTM for textual feature extraction with any DL ar-
chitecture as a backbone for imaging feature extraction can slightly enhance the
overall model’s performance in terms of sensitivity, F1 score and AUC, compared
to basic ANN. However, ANN combinations outperformed LSTM in metrics like
accuracy and precision. Both ANN and LSTM effectively utilized the rich con-
textual information in textual data, improving the model’s capability to discern
complex patterns and relationships. It is also observed that SOTA DL archi-
tectures performed better than transformer based architecture ViT. In addition,
across all the evaluated metrics, VGG based architectures performed better than
any other architecture used in this study.
6
Conclusion
The aim of this study was to collect and preprocess a new dataset of digital
mammograms and radiological reports and give a baseline of SOTA DL and
transformer-based architecture combined with either ANN or LSTM using a
multimodal fusion approach. The highest accuracy was achieved by the VGG19
Title Suppressed Due to Excessive Length
9
model in conjunction with ANN. For precision, the top score was again attained
by VGG19 associated with ANN, followed by the combined model of ViT and
LSTM. The best sensitivity score was recorded by the VGG16 with LSTM,
followed by the combined models of VGG19 and LSTM fused model. The highest
F1 scores were achieved by VGG19 in conjunction with LSTM, followed by
the joint model of VGG19 and ANN. The best AUC score was achieved by
VGG16 and LSTM combination. The VGG16 and ANN combination secured
the second best AUC score. Our observations indicated that models combining
LSTM with DL architectures either performed better or comparably to those
using ANN in various metrics. The overall performance of the joint ViT model
with LSTM or ANN was less as compared to SOTA DL architectures in most
of the evaluation metrics. The results suggest that among the five evaluation
metrics, SOTA DL models performed better in all evaluation metrics, while the
transformer architecture along with LSTM was able to achieve second best in a
precision metric only. In addition, the VGG16 and VGG19 outperformed all the
SOTA DL architectures and ViT across all metrics which suggest the ability of
VGG architectures in classifying the positive and negative BC instances across
all the evaluation metrics. These findings further indicate that incorporating
metadata extracted from radiological reports alongside images can enhance the
model’s performance in predicting BC. This approach has potential applications
in various medical fields due to the inherently multimodal nature of healthcare
data.
Acknowledgments. The authors would like to thank the Tecnológico de Monterrey
and CONAHCYT for supporting their studies.
Bibliography
[1] Acosta, J.N., Falcone, G.J., Rajpurkar, P., Topol, E.J.: Multimodal biomed-
ical ai. Nature Medicine 28(9), 1773–1784 (2022)
[2] Akselrod-Ballin, A., Chorev, M., Shoshan, Y., Spiro, A., Hazan, A.,
Melamed, R., Barkan, E., Herzel, E., Naor, S., Karavani, E., et al.: Pre-
dicting breast cancer by applying deep learning to linked health records
and mammograms. Radiology 292(2), 331–342 (2019)
[3] Calisto, F.M., Santiago, C., Nunes, N., Nascimento, J.C.: Breastscreening-
ai: Evaluating medical intelligent agents for human-ai interactions. Artificial
Intelligence in Medicine 127, 102285 (2022)
[4] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un-
terthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit,
J., Houlsby, N.: An image is worth 16x16 words: Transformers for image
recognition at scale. In: International Conference on Learning Representa-
tions (ICLR) (2021), https://openreview.net/forum?id=YicbFdNTTy
[5] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Un-
terthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An
image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint arXiv:2010.11929 (2020)
[6] Fields, C., Kennington, C.: Vision language transformers: A survey. arXiv
e-prints pp. arXiv–2307 (2023)
[7] Gao, J., Aksoy, B.A., Dogrusoz, U., Dresdner, G., Gross, B., Sumer, S.O.,
Sun, Y., Jacobsen, A., Sinha, R., Larsson, E., et al.: Integrative analysis of
complex cancer genomics and clinical profiles using the cbioportal. Science
signaling 6(269), pl1–pl1 (2013)
[8] Hansebout, R.R., Cornacchi, S.D., Haines, T.A., Goldsmith, C.H.: How to
use an article about prognosis. Canadian journal of surgery. Journal cana-
dien de chirurgie 52 4, 328–336 (2009)
[9] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recogni-
tion. In: Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 770–778 (2016)
[10] Heiliger, L., Sekuboyina, A., Menze, B., Egger, J., Kleesiek, J.: Beyond med-
ical imaging-a review of multimodal deep learning in radiology. TechRxiv
(19103432) (2022)
[11] Holste, G., Partridge, S.C., Rahbar, H., Biswas, D., Lee, C.I., Alessio, A.M.:
End-to-end learning of fused image and non-image features for improved
breast cancer classification from mri. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 3294–3303 (2021)
[12] Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang,
W., Zhu, Y., Pang, R., Vasudevan, V., et al.: Searching for mobilenetv3. In:
Proceedings of the IEEE/CVF international conference on computer vision.
pp. 1314–1324 (2019)
Title Suppressed Due to Excessive Length
11
[13] Huang, S.C., Pareek, A., Zamanian, R., Banerjee, I., Lungren, M.P.: Mul-
timodal fusion with deep neural networks for leveraging ct imaging and
electronic health record: a case-study in pulmonary embolism detection.
Scientific reports 10(1), 22147 (2020)
[14] Hussain, S., Lafarga-Osuna, Y., Ali, M., Naseem, U., Ahmed, M., Tamez-
Peña, J.G.: Deep learning, radiomics and radiogenomics applications in
the digital breast tomosynthesis: a systematic review. BMC bioinformat-
ics 24(1), 401 (2023)
[15] Ikechukwu, A.V., Murali, S., Deepu, R., Shivamurthy, R.: Resnet-50 vs vgg-
19 vs training from scratch: A comparative analysis of the segmentation
and classification of pneumonia from chest x-ray images. Global Transitions
Proceedings 2(2), 375–381 (2021)
[16] Pei, X., Zuo, K., Li, Y., Pang, Z.: A review of the application of multi-modal
deep learning in medicine: Bibliometrics and future directions. International
Journal of Computational Intelligence Systems 16(1), 44 (2023)
[17] Qian, S., Ning, C., Hu, Y.: Mobilenetv3 for image classification. In: 2021
IEEE 2nd International Conference on Big Data, Artificial Intelligence and
Internet of Things Engineering (ICBAIE). pp. 490–497. IEEE (2021)
[18] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-
scale image recognition. arXiv preprint arXiv:1409.1556 (2014)
[19] Sun, D., Wang, M., Li, A.: A multimodal deep neural network for hu-
man breast cancer prognosis prediction by integrating multi-dimensional
data. IEEE/ACM transactions on computational biology and bioinformat-
ics 16(3), 841–850 (2018)
[20] Sung, H., Ferlay, J., Siegel, R.L., Laversanne, M., Soerjomataram, I., Jemal,
A., Bray, F.: Global cancer statistics 2020: Globocan estimates of incidence
and mortality worldwide for 36 cancers in 185 countries. CA: a cancer jour-
nal for clinicians 71(3), 209–249 (2021)
[21] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional
neural networks. In: International conference on machine learning. pp. 6105–
6114. PMLR (2019)
[22] Tomczak, K., Czerwińska, P., Wiznerowicz, M.: Review the cancer genome
atlas (tcga): an immeasurable source of knowledge. Contemporary Oncol-
ogy/Współczesna Onkologia 2015(1), 68–77 (2015)
[23] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural
information processing systems 30 (2017)
[24] Xu, W., Fu, Y.L., Zhu, D.: Resnet and its application to medical image
processing: Research progress and challenges. Computer Methods and Pro-
grams in Biomedicine 240, 107660 (2023)
[25] Yassin, N.I., Omran, S., El Houby, E.M., Allam, H.: Machine learning
techniques for breast cancer computer aided diagnosis using different im-
age modalities: A systematic review. Computer methods and programs in
biomedicine 156, 25–45 (2018)
