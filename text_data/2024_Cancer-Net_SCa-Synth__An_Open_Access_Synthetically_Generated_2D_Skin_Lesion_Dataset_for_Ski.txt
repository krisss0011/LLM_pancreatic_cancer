Cancer-Net SCa-Synth: An Open Access Synthetically
Generated 2D Skin Lesion Dataset for Skin Cancer
Classification
Chi-en Amy Tai1∗
Oustan Ding1∗
Alexander Wong1
1University of Waterloo
{amy.tai, oustan.ding, alexander.wong}@uwaterloo.ca
Abstract
In the United States, skin cancer ranks as the most commonly diagnosed cancer,
presenting a significant public health issue due to its high rates of occurrence
and the risk of serious complications if not caught early. Recent advancements
in dataset curation and deep learning have shown promise in quick and accurate
detection of skin cancer. However, current open-source datasets have significant
class imbalances which impedes the effectiveness of these deep learning models.
In healthcare, generative artificial intelligence (AI) models have been employed
to create synthetic data, addressing data imbalance in datasets by augmenting
underrepresented classes and enhancing the overall quality and performance of
machine learning models. In this paper, we build on top of previous work by
leveraging new advancements in generative AI, notably Stable Diffusion and
DreamBooth. We introduce Cancer-Net SCa-Synth, an open access synthetically
generated 2D skin lesion dataset for skin cancer classification. Further analysis on
the data effectiveness by comparing the ISIC 2020 test set performance for training
with and without these synthetic images for a simple model highlights the benefits
of leveraging synthetic data to improve performance. Cancer-Net SCa-Synth is
publicly available at https://github.com/catai9/Cancer-Net-SCa-Synth
as part of a global open-source initiative for accelerating machine learning for
cancer care.
1
Introduction
In the United States, skin cancer ranks as the most commonly diagnosed cancer, presenting a
significant public health issue due to its high rates of occurrence and the risk of serious complications
if not caught early [1]. Advancements in dataset curation and associated classification challenges
have shown promise in quick and accurate detection of skin cancer [2]. Methods leveraging deep
learning have recently grown in popularity and were shown to enhance performance of skin cancer
detection [3, 4, 5]. However, the effectiveness of these deep learning models largely depends on the
quality of their training datasets. As seen in Table 1, current open-source datasets have significant
class imbalances which impedes the effectiveness of these deep learning models [6, 7].
In healthcare, generative artificial intelligence (AI) models have been employed to create synthetic
data to address patient privacy concerns and data imbalance in datasets by augmenting underrepre-
sented classes and enhancing the overall quality and performance of machine learning models [9, 10].
For skin cancer, synthetic data was shown to help in skin cancer classification with a study conducted
using semantic and instance masks to augment the lack of annotated data [11]. Other research in
this cancer domain have also shown that using synthetic data from generative adversarial networks
(GANs) can help in skin cancer classification models, but they unfortunately do not publicize their
38th Conference on Neural Information Processing Systems (NeurIPS 2024).
arXiv:2411.05269v1  [cs.CV]  8 Nov 2024
Table 1: Overview of existing skin cancer datasets and their class distribution in the training set with
values from HAM10000 obtained from [8] and ISIC values obtained from [6].
Dataset
Total Images
Total Benign
Total Melanoma
ISIC 2017
2,000
1,626
374
ISIC 2018
10,015
8,902
1,113
ISIC 2019
25,331
20,809
4,522
ISIC 2020
33,126
32,542
584
HAM10000
10,015
8,902
1,113
Cancer-Net SCa-Synth
10,000
5,000
5,000
Figure 1: Process map of the entire model training pipeline from image generation to model evalua-
tion.
synthetically generated image dataset that they used for experiments nor the code to generate these
images [12, 13, 14, 15, 16].
Even though GANs have gained immense popularity and shown incredible promise, they are com-
putationally intensive to train and exhibit limited diversity in their outputs [17]. To overcome these
challenges, Stable Diffusion was developed [17]. While extremely powerful, Stable Diffusion on its
own lacked fine-tuning for personalized content and required a large dataset for generating highly
specific images. As such, DreamBooth is a trainer that considers text as another input and paired
with Stable Diffusion, fine-tunes the text-to-image diffusion model [18].
Despite the fact that these recent improvements on the GAN architecture have shown success in a
multitude of industries for generating synthetic data, there have been no current open-source studies
that analyze the effectiveness of Stable Diffusion with DreamBooth for generating synthetic skin
lesion images for skin cancer classification. In this paper, we build on top of previous work by
leveraging these new advancements in generative AI, notably Stable Diffusion and DreamBooth. We
introduce Cancer-Net SCa-Synth, an open access synthetically generated 2D skin lesion dataset for
skin cancer classification. We provide further analysis on the data effectiveness by comparing the test
set performance for training with and without these synthetic images for a simple model. Cancer-Net
SCa-Synth is publicly available at https://github.com/catai9/Cancer-Net-SCa-Synth as
part of a global open-source initiative for accelerating machine learning for cancer care.
2
Methodology
2.1
Synthetic Image Generation
As shown in the top portion of Figure 1, image generation was conducted using the Stable Diffusion
model with the DreamBooth trainer. A separate Stable Diffusion model was trained for each of
the two skin cancer classes. 300 randomly sampled benign skin cancer images from the ISIC 2020
training set [2] were used to train a Stable Diffusion model with the DreamBooth trainer for the
2
(a) Original Image
(b) Preprocessed Image
Figure 2: Example of an image before (a) and after (b) preprocessing.
benign class and 300 randomly sampled melanoma skin cancer images from the ISIC 2020 training
set were used to train the melanoma model. A single word was used as the prompt for training,
namely “benign" and “melanoma" for the associated models.
The learning rate was set to 5e-6 with a constant learning rate scheduler and no warmup steps.
The image resolution used for training was 512x512 with no batch training used and a gradient
accumulation step of 1. An AdamW optimizer was used with MSE loss and 2 epochs were initially
used with a max 400 training steps. HuggingFace instead of Tensorflow/Keras was used as the
generated images were often overfit and not realistic with Tensorflow/Keras. Using the associated
trained models, 10,000 images were generated (5,000 for benign and 5,000 for melanoma).
2.2
MobileNetV2 Model Training
The images were first standardized into a consistent format of size 224x224 with equal padding added
to the height and/or width in instances with a smaller image. An example of an image before and
after preprocessing is shown in Figure 2.
We compare the performance using Cancer-Net SCa-Synth on the ISIC 2020 test set using a Mo-
bileNetV2 model for these scenarios:
(A) Train only on the ISIC 2020 training set
(B) Train only on Cancer-Net SCa-Synth
(C) Train with Cancer-Net SCa-Synth and fine-tune with ISIC 2020 training set
For the first two scenarios, we used the ImageNet weights as the initial model weights. The pipeline
was adapted from [19] with modifications for the specific dataset, binary classification, and class
imbalance. We set the MobileNetV2 layers to be trainable and included a dropout layer of 0.3. We
compute the area under the ROC curve (AUC) and accuracy and use a loss of binary cross-entropy
with an AdamW optimizer. The initial learning rate was set to 0.0001 with an initial weight decay of
0.004. The data was split with the ratio 60:40 for the training:validation set and stratified with the
skin cancer target label.
The model was trained over 10 epochs with a batch size of 32 and the best model weights (based
on the validation AUC) were used to evaluate the performance on the ISIC 2020 test set. Since
the ISIC 2020 test set labels were not available online, we used the SIIM-ISIC Kaggle competition
platform [20] to evaluate the performance on the test set. This platform provides two scores: a public
score and a private score. The public score reflects the AUC performance on 30% of the test data,
while the private score measures the AUC performance on the remaining 70% of the test data. The
3
specific data split between the two subsets is unknown. The entire model training and evaluation flow
can be seen in the bottom half of Figure 1.
3
Results
Table 2: The performance of various training data on the ISIC 2020 test dataset using a basic adapted
MobileNetV2 architecture. In this table, ISIC 2020 refers to only the training set.
Scenario
Trained
Fine-Tuned
Private Score
Public Score
(A)
ISIC 2020
None
0.6370
0.6475
(B)
Cancer-Net SCa-Synth
None
0.5344
0.5194
(C)
Cancer-Net SCa-Synth
ISIC 2020
0.6776
0.7376
As seen in Table 2, the test set performance was highest for both the private and public score in
Scenario C. Notably, training using Cancer-Net SCa-Synth and fine-tuning with the ISIC 2020
training set outperforms training on either dataset alone by over 0.04 (private) and 0.09 (public).
Sample images generated from the trained benign and melanoma Stable Diffusion models are shown
in the top right of Figure 1.
4
Conclusion
Given the rise of generative AI and its promising results in producing synthetic data for more
effective model training, this paper builds on top of previous work by leveraging new advancements
in generative AI, notably Stable Diffusion and DreamBooth. We introduce Cancer-Net SCa-Synth, an
open access synthetically generated 2D skin lesion dataset containing 10,000 images for skin cancer
classification that is equally distributed between the benign and melanoma skin cancer cases. Using a
simple MobileNetV2 model, we demonstrate the benefits of leveraging synthetic data to improve
performance and obtain an AUC that is higher than training on only the ISIC 2020 dataset by over
0.04 (private) and over 0.09 (public).
5
Future Work
Future work includes a comparison study with other deep learning models beyond MobileNetV2 and
using ControlNet to add conditional control to the text-to-image diffusion models [21]. To illustrate
the impact of training with ControlNet, a Stable Diffusion model was trained using DreamBooth for
melanoma images, using five images to generate four sample images (Figure 3, left). As seen in the
right side of Figure 3, by using a canny ControlNet to inpaint colors of a reference image, higher
quality images can be generated.
References
[1] Paul Gruber and Patrick M Zito. Skin cancer. 2024.
[2] Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein, Liam Caffery, Emmanouil Chousakos,
Noel Codella, Marc Combalia, Stephen Dusza, Pascale Guitera, David Gutman, et al. A patient-centric
dataset of images and metadata for identifying melanomas using clinical context. Scientific data, 8(1):34,
2021.
[3] Adekanmi Adegun and Serestina Viriri. Deep learning techniques for skin lesion analysis and melanoma
cancer detection: a survey of state-of-the-art. Artificial Intelligence Review, 54(2):811–841, 2021.
[4] Walaa Gouda, Najm Us Sama, Ghada Al-Waakid, Mamoona Humayun, and Noor Zaman Jhanjhi. Detection
of skin cancer based on skin lesion images using deep learning. In Healthcare, volume 10, page 1183.
MDPI, 2022.
[5] James Ren Hou Lee, Maya Pavlova, Mahmoud Famouri, and Alexander Wong. Cancer-net sca: tailored
deep neural network designs for detection of skin cancer from dermoscopy images. BMC Medical Imaging,
22(1):1–12, 2022.
4
Figure 3: Comparison of the generated results with and without the use of ControlNet.
5
[6] Bill Cassidy, Connah Kendrick, Andrzej Brodzicki, Joanna Jaworek-Korjakowska, and Moi Hoon Yap.
Analysis of the isic image datasets: Usage, benchmarks and recommendations. Medical image analysis,
75:102305, 2022.
[7] David Wen, Andrew Soltan, Emanuele Trucco, and Rubeta N Matin. From data to diagnosis: skin cancer
image datasets for artificial intelligence. Clinical and Experimental Dermatology, page llae112, 2024.
[8] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of
multi-source dermatoscopic images of common pigmented skin lesions. Scientific data, 5(1):1–9, 2018.
[9] Hajra Murtaza, Musharif Ahmed, Naurin Farooq Khan, Ghulam Murtaza, Saad Zafar, and Ambreen Bano.
Synthetic data generation: State of the art in health care domain. Computer Science Review, 48:100546,
2023.
[10] Antonio J Rodriguez-Almeida, Himar Fabelo, Samuel Ortega, Alejandro Deniz, Francisco J Balea-
Fernandez, Eduardo Quevedo, Cristina Soguero-Ruiz, Ana M Wägner, and Gustavo M Callico. Synthetic
patient data generation and evaluation in disease prediction using small and imbalanced datasets. IEEE
Journal of Biomedical and Health Informatics, 27(6):2670–2680, 2022.
[11] Alceu Bissoto, Fábio Perez, Eduardo Valle, and Sandra Avila. Skin lesion synthesis with generative
adversarial networks. In OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy,
Clinical Image-Based Procedures, and Skin Image Analysis: First International Workshop, OR 2.0 2018,
5th International Workshop, CARE 2018, 7th International Workshop, CLIP 2018, Third International
Workshop, ISIC 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16 and 20,
2018, Proceedings 5, pages 294–302. Springer, 2018.
[12] Pooyan Sedigh, Rasoul Sadeghian, and Mehdi Tale Masouleh. Generating synthetic medical images by
using gan to improve cnn performance in skin cancer classification. In 2019 7th International Conference
on Robotics and Mechatronics (ICRoM), pages 497–502. IEEE, 2019.
[13] Amirata Ghorbani, Vivek Natarajan, David Coz, and Yuan Liu. Dermgan: Synthetic generation of clinical
skin images with pathology. In Machine learning for health workshop, pages 155–170. PMLR, 2020.
[14] Marco La Salvia, Emanuele Torti, Raquel Leon, Himar Fabelo, Samuel Ortega, Beatriz Martinez-Vega,
Gustavo M Callico, and Francesco Leporati. Deep convolutional generative adversarial networks to
enhance artificial intelligence in healthcare: a skin cancer application. Sensors, 22(16):6145, 2022.
[15] Burak Beynek, ¸Sebnem Bora, Vedat Evren, and Aybars Ugur. Synthetic skin cancer image data generation
using generative adversarial neural network. International Journal of Multidisciplinary Studies and
Innovative Technologies, 5(2):147–150, 2021.
[16] Ranpreet Kaur, Hamid GholamHosseini, and Roopak Sinha. Synthetic images generation using conditional
generative adversarial network for skin cancer classification. In TENCON 2021-2021 IEEE Region 10
Conference (TENCON), pages 381–386. IEEE, 2021.
[17] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 10684–10695, 2022.
[18] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages 22500–22510, 2023.
[19] xhlulu.
Training
mobilenet
v2
in
4
min.
https://www.kaggle.com/code/xhlulu/
training-mobilenet-v2-in-4-min/script, 2019. [Accessed 06-09-2024].
[20] Anna Zawacki, Brian Helba, George Shih, Jochen Weber, Julia Elliott, Marc Combalia, Nicholas Kurtansky,
Noel Codella, Phil Culliton, and Veronica Rotemberg. Siim-isic melanoma classification, 2020.
[21] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion
models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836–3847,
2023.
6
