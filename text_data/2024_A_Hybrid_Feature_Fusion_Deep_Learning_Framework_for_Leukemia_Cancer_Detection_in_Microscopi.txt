A Hybrid Feature Fusion Deep Learning
Framework for Leukemia Cancer Detection
in Microscopic Blood Sample Using Gated
Recurrent Unit and Uncertainty
Quantification
Maksuda Akter1, Rabea Khatun2 and Md Manowarul Islam1*
1Department of Computer Science and Engineering, Jagannath
University, Dhaka, Bangladesh.
2Department of Computer Science and Engineering, Green
University of Bangladesh, Dhaka, Bangladesh.
*Corresponding author(s). E-mail(s): manowar@cse.jnu.ac.bd;
Contributing authors: maksudaoni6@gmail.com;
rabea@cse.green.edu.bd;
Abstract
Acute lymphoblastic leukemia (ALL) is the most malignant form of
leukemia. It is considered the most prevalent kind of cancer in both
adults and children. Leukemia is typically identified by analyzing blood
and bone marrow smears under a microscope. Advanced cytochemical
testing can also be performed to confirm and categorize leukemia. How-
ever, these approaches are expensive, time-consuming, and dependent
on the skill and knowledge of the relevant specialists. In recent decades,
deep learning with Convolutional Neural Networks (CNN) has generated
sophisticated methods for image classification by analyzing microscopic
smear images to detect the presence of leukemic cells. These techniques
are easy to use, quick, affordable, and free from expert bias. Nevertheless,
the majority of these approaches are unable to quantify the uncertainty
in their results, which can have devastating consequences. This research
implemented hybrid models (InceptionV3-GRU, EfficientNetB3-GRU,
and MobileNetV2-GRU) for the ALL detection and classification. Fol-
lowing this, to identify an optimal set of hyperparameters and enhance
1
arXiv:2410.14536v1  [eess.IV]  18 Oct 2024
2
the model’s performance, bayesian optimization is utilized. Then, we
use Deep Ensemble uncertainty quantification approach to deal with
uncertainty during leukemia image classification. The outcomes of three
different hybrid deep learning models were then aggregated at the score
level using the sum rule after they had been trained on the two pub-
licly available leukaemia patient blood sample datasets, ALL-IDB1 and
ALL-IDB2. Herein, parallel architecture was considered, which offers a
high degree of confidence in discriminating between ALL and not ALL.
The proposed system has managed to correctly and accurately diagnose
the leukemia patients, respectively, with a detection accuracy rate of
100% for the ALL-IDB1 dataset, 98.07% for the ALL-IDB12 dataset,
and 98.64% for the combined dataset using our proposed method.
Keywords: Acute Lymphoblastic Leukemi, Blood Smear Images, Deep
Learning, Gated recurrent unit, Uncertainty Quantification, Feature Fusion
1 Introduction
Leukemia is the most severe type of blood cancer across all age groups. Exces-
sive and immature blood cell proliferation is the origin of this anomalous
condition[1], which can harm bone marrow, the immune system, and red blood
cells[2, 3]. Blood is a vital component of the human body, consisting of 45%
red blood cells and 55% plasma. Depending on the blood’s size, composition,
texture, color, and shape, there are three primary blood components: thromo-
cytes (platelets), leukocytes which is commonly known as white blood cells
(WBC), and erythrocytes which is known as red blood cells (RBC). RBC rate
ranges from 4,000,000 to 6,000,000 per microliter of blood which carries oxygen
throughout the bodywbc. WBC provide immunity and resistance and protect
our body against infections; their typical concentration in the body is between
4,000 and 11,000 cells per microliter. Blood clotting is carried out by platelets,
which have a density of 150,000 to 450,000 per microliter of blood. Therefore,
alterations in any of the fundamental blood constituents will lead to health
issues for an individual.
Leukemia is a fatal cancer that results in an abnormal increase in immature
WBC in the bone marrow and blood. A high WBC volume encompasses both
RBCs and platelets, resulting in a low level of body immunity. There are several
possible causes of leukemia, including radiation, familial history, and environ-
mental pollutants[4]. However, the true cause of the disease is unknown[5].
Depending on how rapidly it grows, leukemia can be characterized as either
acute or chronic. WBC that are contaminated with acute leukemia cannot
function or behave normally like normal WBC. However, in chronic leukemia,
they may behave normally[6]. Myeloid or lymphoid is the classification given
to leukemia based on the original cell line. Therefore, based on the disease’s
course, severity, and impact, specialists divide it into four categories: Acute
3
Lymphoblastic Leukemia (ALL), Acute Myeloblastic Leukemia, Chronic Lym-
phocytic Leukemia and Chronic Myeloblastic leukemia[7][8]. Among these,
ALL accounts for 70% of all instances of leukemia and is also the most
deadly[9][10].
To reduce medical risks and choose the best course of treatment, it’s crit-
ical to determine whether leukemia exists and what kind it is. Hematologists
analyze bone marrow or blood smears under a microscope in clinical laborato-
ries to detect ALL and its subtypes[11]. The number of distinct blood cells and
their morphological characteristics are usually used to classify leukemia cases.
Generally, these leukemic cells are easy to identify due to their dark-purple
hue, but because of the variances in pattern and texture, assessment and fur-
ther processing become highly intricate. The lymphocytes, or lymphoblasts, in
ALL individuals have a very thin, homogenous border. They also have spherical
particles called nucleoli inside the nucleus and microscopic pockets in the cyto-
plasm called vacuoles. As the disease progresses, it becomes increasingly clear
that the prescribed medication could cause an early death if it is ignored[12].
Cancer, a fatal disease that is primarily caused by metabolic problems
and inherited conditions, is one of the leading causes of death. According to
the World Health Organization (WHO), cancer ranks as the primary cause of
death before the age of 70 in 112 out of 183 nations, and as the third or fourth
in 23 more[13][14]. Children under the age of five have the highest chance
of developing ALL. The risk decreases gradually until the mid-20s and then
starts to rise slowly once more around age 50[15]. Data from WHO 2020 shows
that there were around 474,519 cases of leukemia worldwide, which resulted
in 311,594 fatalities. According to the American Cancer Society’s estimates,
there were around 6,540 new cases of ALL and 1,390 deaths from the dis-
ease in the US in 2023. Ref. [16] reports that 52,995 people died and around
153,315 people worldwide experienced ALL in 2019[12]. With early diagnosis
and treatment, patients chances of survival can rise. As a result, early detec-
tion and successful treatment of ALL are crucial[17][18]. In order to validate
and distinguish common subtypes of leukemia, additional procedures like flow
cytometry, chromosomal analysis, and complex cytochemical testing could be
needed. These intricate procedures are time-consuming, expensive, and depen-
dent on the medical team’s knowledge and proficiency in sample collection,
preparation, and testing. For a routine examination, the complex laboratory
tests are also not that simple[19][20]. In order to overcome the shortcomings
associated with manual screening, hematologists need to develop an automated
approach that can identify or categorize malignant lymphocytes[21][22].
In comparison to several intricate clinical procedures, automated image
processing-based methods are simple, quick, and less expensive while yet pro-
ducing a diagnosis that is accurate and timely. Issues with manual diagnostics
can be fixed by Machine Learning (ML) and Deep Learning (DL) techniques.
DL algorithms are able to automatically extract features from raw data. Tradi-
tional machine learning methods can be outperformed by deep learning models,
which can attain state-of-the-art performance on picture classification tasks.
4
In this study, DL models and hybrid DL models were used to examine the two
datasets, ALL IDB1 and
ALL IDB2, for leukemia diagnosis.
Three DL models, namely the MobileNetV2 GRU (MobileNetV2-GRU),
the InceptionV3 gated recurrent unit (InceptionV3-GRU), and the Efficient-
Netb3 gated recurrent unit (EfficientNetb3-GRU), are used in this study to
carry out the feature extraction and classification tasks for ALL diagnosis in
microscopic images. To avoid making predictions with too much confidence,
uncertainty qunatification (UQ) is utilized. Ultimately, the final decision is
reached by combining the output produced by these three deep learning mod-
els. The primary driving force for the creation of the suggested system is to
create an efficient and effective system that can be developed in a real-world
clinical situation for fast diagnosis and treatment. The main contributions of
the suggested work for automated leukemia detection are outlined below:
• For significantly high accuracy classification of medical imaging data, we put
forth a unique hybrid DL model. wherein the model integrates three hybrid
DL models (MobileNetV2-GRU, InceptionV3-GRU, and EfficientNetb3-
GRU) that precisely identify ALL and not ALL while also automatically
extracting features from the dataset.
• To avoid overconfidence in the diagnosis of the disease, we employed well-
known uncertainty quantification approach, such as Deep Ensemble (DE),
to increase the classification stages confidence level.
• By implementing a Bayesian optimization technique, we optimized hyper-
parameters of deep learning models to ensure that it performs at its
best.
• The final decision in the suggested system is obtained by combining
the results generated from three different hybrid DL models trained on
ALL IDB1 and
ALL IDB2 datasets, using the sum rule at the score-leve.
The rest of the paper is organized as follows: a review of relevant Previous
studies are presented in Section 2. The proposed approach and the classifica-
tion algorithms used in the proposal are explained in Section 3. The experiment
results are presented in Sections 4 and 5, along with a discussion. Section 6
wraps up our study and discusses its shortcomings and future efforts.
2 Related works
An overview of relevant studies that use Deep Learning (DL) and Machine
Learning(ML) for automated Leukemia detection is presented in Tables 1.
K.K. Anilkumar et al. [5] proposed a methodology that made use of the
Directed Acyclic Graph (DAG) networks GoogLeNet, Inceptionv3, MobileNet-
v2, Xception, AlexNet, VGG-16, VGG-19, DenseNet-201, Inception-ResNet-v2
and residual networks ResNet-18, ResNet-50 and ResNet-101 For the purpose
of classification and comparison.
Maryam Bukhari et al. [12] by explicitly modeling channel interdependen-
cies, she proposed squeeze and excitation learning, which iteratively executes
5
recalibration on channel-wise feature outputs, highlighting the channel rela-
tionships on all levels of feature representation.
Ibrahim Abunadi et al. [10] proposed three systems: First, there is the artifi-
cial neural network, feed forward neural network, and support vector machine.
These are based on hybrid characteristics that are derived using fuzzy color his-
togram, local binary pattern, and gray level co-occurrence matrix. The second
system that is being suggested is made up of the transfer learning-based con-
volutional neural network models AlexNet, GoogleNet, and ResNet-18. Hybrid
CNN-SVM technology is used in the third suggested solution.
Jyoti Rawat et al. [23] suggested methods comprise the following modules:
segmentation (which divides each leukocyte cell into its nucleus and cyto-
plasm); feature extraction; feature dimensionality reduction (which maps the
higher feature space to the lower feature space using principal component
analysis); and classification (which uses standard classifiers such as k-nearest
neighbor, probabilistic neural networks, adaptive neuro fuzzy inference sys-
tems, support vector machines, and smooth support vector machines).
Preetham Kumar et al. [24] presented a method for automatically identify-
ing and dividing the white blood cell nucleus from the images of the microscopic
blood smear. K-Means is used for segmentation and clustering, and Support
Vector Machine with feature reduction is used for classification.
Esti Suryani et al. [25] Utilizing the momentum backpropagation tech-
nique for feature extraction and Hue Saturation Intensity for Watershed
Segmentation, automated AML detection was accomplished. The process of
classification involves evaluating the numerical data input derived from the
feature extraction outcomes that have been recorded in the database.
T. T. P. Thanhet al. [26] proposed a Convolutional Neural Network based
method in order to extract features from raw blood cell photos and perform
classification in order to discriminate between normal and abnormal blood cell
images.
Luis H.S. Vogado et al. [27] extracted features from the photos using trans-
fer learning, and then used three CNN architectures for further classification.
The features were chosen based on their gain ratios and fed into the Support
Vector Machine classifier.
Sarmad Shafique et al. [28] deployed pretrained AlexNet which was fine-
tuned. Data augmentation technique was applied to minimize overtraining,
and performance over various color images was evaluated by comparing data
sets with various color models.
Ibrahim Abdulrab Ahmedet al. [29] suggested systems in which three
CNN models (DenseNet121, ResNet50, and MobileNet) were used to extract
WBC-only zones for additional analysis. The Principal Component Analysis
approach selects highly representative features from the high features produced
by CNN models, which are then supplied to the RF and XGBoost classifiers
for classification.
6
Table 1 Summary of related works
Authors
Dataset
Segmentation
Method
Classification model
Accuracy
Rate
Jyoti Rawat et al.
[23]
ALL-IDB
Otsu’s Thresholding
SVM
89.00%
Esti Suryani et al.
Dr. Moewardi
Watershed Distance
Back Propagation
94.285%
[25]
Hospital
Transform
Preetham
Kumar
et al. [24]
ASH
k-means
SVM
95%
Maryam
Bukhari
et al. [12]
ALL-IDB
Not Used
Squeeze and Excitation Learning
CNN
98.3%
Ibrahim Abunadiet
al. [10]
ALL-IDB
Not Used
SVM
98.11%
T. T. P. Thanh et
al. [26]
ALL-IDB
Not Used
CNN
96.6%
Sarmad Shafique et
al. [28]
ALL-IDB
Not Used
AlexNet
96.06%
Luis H.S. Vogado
et al. [27]
Hybrid-Leukocyte
database
Not Used
CNN-SVM
99.00%
Ibrahim Abdulrab
Ahmedet al. [29]
C-NMC 2019
Not Used
RF classifiers and XGBoost
99.1%
3 Methodology
The proposed methodology’s design is shown in Figure 1.
Fig. 1 The basic block diagram of the proposed model for ALL cancer detection.
In summary, the proposed methodologies comprise the following steps.
1. The first step in the suggested framework is to take microscopic pictures
of blood samples from two experimental datasets, namely
ALL IDB1 and
7
ALL IDB2. For the preprocessing, since DL networks demand more data for
training and better performance, the problem of inadequate data is resolved
by employing data augmentation techniques followed by label resizing, class
labeling and feature scaling.
2. Next, Three distinct hybrid models were designed using different pretrained
CNNs and Gated Recurrent Unit Network, such as MobileNetV2-GRU,
InceptionV3-GRU, and EfficientNetb3-GRU. To avoid model overconfi-
dence UQ approaches, such as Deep Ensemble was employed.
3. Then, To optimize hyperparameters of these models Bayesian optimiza-
tion technique was employed which has the benefits to reduce the search
time and improves the model’s performance by finding a better set of
hyperparameters.
4. Finally, feature fusion method based on sum rule at the score-leve is pre-
sented to identify leukemia based on input microscopic images of blood
samples.
The subsequent methodological subsections provide a detailed explanation
of each step:
3.1 Image Dataset Collection
The proposed study utilized images from the ALL-IDB, a publicly accessible
collection that includes microscopic pictures of blood samples. The dataset
focuses on ALL, most lethal kind of leukemia disease. Lymphoma in each pic-
ture was identified and classified by lymphoma experts. All of the images were
captured in JPG format with RGB color space (24 bits) and high resolution
(2592 × 1944) with a Canon PowerShot G5 optical microscope. The ALL-IDB
datasets come in two varieties:
ALL IDB1 and
ALL IDB2. There are 108 pho-
tos in the
ALL IDB1 dataset: 59 of healthy individuals and 49 of lymphomas.
Each image contains nearly 39,000 blood components classified by lymphoma
experts. On the other hand, the
ALL IDB2 dataset has 260 photos, 130 of
which are of lymphomas and 130 of which are of normal cells. Regions from
the
ALL IDB1 dataset that have been cropped from blast cells and normal
cells make up the
ALL IDB2 dataset.
3.2 Image Preprocessing
The main goal of the image preprocessing is to make the data suitable for deep
learning models and improve the initial medical images by getting rid of noise,
air bubbles, and artifacts caused by gel that was sprayed before the picture was
taken. To attain a high classifying rate, we removed artifacts and noise from the
pictures. The elimination of this noise and artefacts was crucial to guaranteeing
high-quality input images since they could mask important features required
for classification. The image was smoothed using noise reduction algorithms,
which also removed any random deviations that could confound the model.
8
3.2.1 Data Augmentation
CNN demonstrated state-of-the-art performance across a range of procedures.
Nonetheless, the amount of training data has significant impact on CNN
performance. Numerous techniques for data augmentation have reduced the
network’s error rate in image-based studies utilizing CNNs by offering hypoth-
esis. Although there are many different microscopic blood sample images
utilized for this study, there are very few blood samples overall in both datasets.
Thus, in order to overcome the problems of short dataset size and overfit-
ting, we have employed a range of data augmentation approaches to artificially
expand the quantity of data used for model training. Figure 2 shows the sam-
ple augmented images that we generate by applying different augmentation
approaches. We used the following seven picture transformations to increase
the sample number:
1. Rotation (45◦): To achieve this, the image was given a random rotation
effect (left or right). This procedure involves moving an image’s pixel values
left, right, up, and down in accordance with a degree value between 0 and
180. The degree value of 45◦was chosen for this study in order to obtain
various images.
2. Height Shift (20%): The image pixels were arbitrarily moved up or down
by 20% to achieve the desired result.
3. Width Shift (20%): During this process, the pixel values were moved by
20% to the right or left. After operations to change the height or width, a
gap appeared in the opposite direction of the shifted direction.
4. Zoom (10%): It brings objects in the picture closer in aspect. It was accom-
plished by enhancing the original image with new pixel values. The nearest
value was found by examining the original pixel values before adding the
values.
5. Horizontal Flip: In this operation, the pixels in the image were transferred
horizontally from one half to the other half. The selection was made for the
pixel values to travel arbitrarily.
6. Vertical Flip: The image was split in half by a line drawn horizontally from
its center.
7. Shearing (20◦): To do this, the picture pixels were moved counter-clockwise
in accordance with the degree of the set angle. This study’s value was set
at 20◦.
9
Fig. 2 Sample of leukemia images after each augmentation.
3.2.2 Resizing
In this study, resizing is another preprocessing technique. Since the proposed
model requires equal image sizes to produce the intended training results, all
of the photographs were resized to have the same dimensions (224 x 224).
3.2.3
Class Labeling
We used label encoding techniques to transform the categorical data into
numerical format because the labels in medical picture datasets are typically
categorical (e.g., different forms of skin cancer). In order to enable the deep
learning model to process and learn from the labels during training, each dis-
tinct category was given a numerical value. The mapping between numerical
and categorical data for the class labeling is as follows: the label 0 denotes
people without ALL, and the label 1 denotes individuals who have ALL.
3.2.4 Feature scaling
The feature scaling method is well recognized in the field of machine learning
and pattern recognition for being used to normalize data. To avoid outliers,
this approach sets all data items to the same scale and thus improves prediction
quality. As the features in cancer datasets have a high variance, one of the
pre-processing strategies for normalization is feature scaling. We divided the
grayscale value of an image by 255 to standardize our image pixels between 0
and 1. As a result, the numbers will be relatively little, and the computation
will be simpler and faster.
3.3 Pretrained Networks
There are several pretrained CNNs that have been made accessible to the
public that were trained on over a million pictures. Once these pretrained
CNNs have been modified and optimized for a standard classification task, they
can be trained using the new dataset. The proposed work used the pretrained
networks MobileNetV2, InceptionV3 and EfficientNetB3.
10
With its inverted residual and linear bottleneck layer, MobileNetv2’s CNN
layer delivers improved accuracy and performance for embedded and mobile
video applications. In general, MobileNetV2’s architecture consists of a fully
convolution layer with 32 filters at the beginning, followed by 19 residual bot-
tleneck layers. This architecture lowers the network’s complexity cost and is
specifically designed for devices with little processing capability. Its capacity
to attain competitive accuracy in comparison to larger and more computa-
tionally expensive models is another impressive feature of this model. Finally,
the modest size of the model allows for faster inference times, which makes it
appropriate for real-time applications.
The InceptionV3 is a DL model based on CNN with 42 layers, which is
used for image classification. It is developed by Google, has a high image
classification performance. It utilized multiple approaches for optimizing the
network in order to improve model adaptation. While maintaining the same
speed, it is more efficient and has a deeper network. It is computationally
less expensive improving the decision functions and allowing the network to
converge quickly.
The EfficientNet family is based on a new way for scaling up CNN mod-
els. It makes use of a basic compound coefficient that is quite effective. This
architecture is founded on the premise that scaling up the process for width,
height, and depth for neural networks must be balanced. Effectively optimiz-
ing every aspect of the network in relation to the resources at hand enhances
overall performance. We suggest utilizing the EfficientNetB3 due to its ability
to offer a favorable balance of processing power, precision, and execution dura-
tion. The design consists of 26 convolution blocks nested after a convolution
layer with swish activation.
3.4 Gated Recurrent Unit(GRU) Network
The goal of GRU is to resolve the vanishing gradient issue that arises when
using a conventional recurrent neural network, as shown in Figure 3. In 2014,
Kyunghyun Cho at el. presented it[30]. Due to its internal cell state and three
major gates, GRU is more efficient than LSTM. The data is kept within the
GRU in a secret state. GRU makes use of update and reset gates to solve
the vanishing gradient problem of typical recurrent neural network. These two
vectors essentially decide which data should be sent to the output. What sets
them apart is their capacity to be trained to remember past data without
deleting it or removing information that is irrelevant to the prediction. The
update gate (U) provides both forward and backward information, while the
reset gate (R) presents prior knowledge. The reset gate is utilized by the current
memory gate to retain the necessary data from the computer’s prior state. It’s
feasible to give the input nonlinearity and zero-mean features at the same time
by using an input modulation gate. The statement that follows states that the
basic GRU of reset and updated gates can be mathematically described as
follows:
Rt = σ(Xt.wxr + Ht-1.whr + br)
11
Ut = σ(Xt.wxz + Ht-1.whz + bz)
where wrx and wxz are weight parameters and br and bz represent bias
vector[31].
Fig. 3 The basic structure of the GRU model.
3.5 Hybrid Models
In this study, we propose the InceptionV3-GRU, EfficientNetB3-GRU, and
MobileNetV2-GRU models for ALL classification. Figure 4(a) depicts the
structure of the suggested MobileNetV2-GRU model. When the image was
first collected, it had the dimensions (224, 224, 3) with 224 pixels for height
and 224 pixels for width in RGB, and three channels. The convolutional lay-
ers of the suggested model process the image in order to extract its features.
In addition, the convolutional layer’s stride and kernel size were 1 and (3 ×
3), respectively. The training parameter was significantly reduced after each
conventional and max-pooling layer, and was followed by dropout and the acti-
vation function (ReLU). The data must be assembled into an array following
the training of the max-pooling and traditional layers in order to be used as
input for the fully connected layer that is built by flattening with training
parameters of size (5,5) and features map of value 1536. Following the com-
pletion of all convolutional layers, 1,024 feature maps were produced using the
dropout method. To tackle the vanishing gradient problem, a GRU model with
a fully connected layer made up of 512 neurons was used. Two fully connected
layers were also employed after the GRU model. Using the last linked layer,
softmax operations were finally implemented.
Figure 4(b) displays the structure and parameters used in the InceptionV3-
GRU model. At first, the image’s input shape was (224, 224, 3) size, with 224
pixels for the image’s height and 224 pixels for its width in RGB, and three
channels. The input shape goes through the convolutional layers of the sug-
gested model in order to extract the features. Furthermore, the convolutional
12
(a) MobileNetV2-GRU
(b) InceptionV3-GRU
(c) EfficientNetB3-GRU
Fig. 4 The general structure of the proposed Hybrid models.
layer’s stride and kernel size were (3 × 3) and 1. The training parameter was
significantly reduced after each conventional and max-pooling layer, and was
followed by dropout and the activation function (ReLU). The data must be
assembled into an ID array following the training of the max-pooling and tradi-
tional layers in order to be used as input for a fully connected layer that is built
by flattening with training parameters of size (7, 7) and features map of value
512. Following the completion of all convolutional layers, 1,024 feature maps
were produced using the dropout method. To tackle the vanishing gradient
problem, a GRU model with a fully connected layer made up of 1024 neurons
was used. Two fully connected layers were also employed after the GRU model.
Using the last linked layer, softmax operations were finally implemented.
The structure and parameters used in the EfficientNetB3-GRU model are
displayed in Figure 4(c). Initially, the image’s input shape was (224, 224, 3)
size, with 224 pixels for the image’s height and 224 pixels for its width in RGB,
and three channels. The input shape goes through the convolutional layers of
the sug- gested model in order to extract the features. Furthermore, the con-
volutional layer’s stride and kernel size were (3 × 3) and 1. Activation function
(ReLU) and drop out were implemented after each conventional and max-
pooling layer, with a significant decrease in the training parameter.The data
must be assembled into an ID array following the training of the max-pooling
and traditional layers in order to serve as input for a fully connected layer that
is created by flattening with training parameters of size (7, 7) and features
map of value 1536. Once all of the convolutional layers had been processed,
1,024 feature maps were produced using the dropout method. To overcome the
vanishing gradient problem, a fully connected layer of 512 neurons in a GRU
13
Fig. 5 EfficientNetB3-GRU network architecture.
model was used. There were two fully connected layers utilized after the GRU
model. Lastly, the last linked layer was used to implement softmax operations.
To more clearly express the internal composition of its network, the structure
of each part of the EfficientNetB3-GRU model is shown in Figure 5.
3.6 Hyperparameter Tuning
Bayesian Optimization (BO) is a probabilistic method utilizes for the hyperpa-
rameter tuning. The Bayesian theorem serves as its foundation. The two main
parts of BO are the acquisition function, which is optimized for selecting the
next sample location, and the surrogate model, which is a prior distribution
that models the unknown objective function. The surrogate model generates
a posterior probability distribution about probable values for H(θ) at a candi-
date configuration θ. The main concept is to update this posterior distribution
every time we observe H at a new point θ. It develops a probabilistic surrogate
model, frequently by taking into account a tree-based model or a Gaussian
process. It applies a Gaussian process prior over the optimization functions.
The Gaussian Process is fully characterized by-
• A mean function µ(θ) : Θ →R to get assumptions about the optimized
function.
• A definite positive covariance function, also called kernel k(θ,θ′) : Θ2 →R.
14
The following mathematical expression states that the function H(θ) follows
a Gaussian process with mean function µ(θ) and covariance (kernel) function
k(θ,θ′).
H(θ) ∼GP(µ(θ)); k(θ, θ′))
The first set of k configurations is used by the BO algorithm (θi)k
i=1 and their
associated function values (yi)k
i=1 with yi = H(Θi). The GP model is updated
using the Bayes rule at each iteration, t ∈{k + 1, ...., N}, to produce the
posterior distribution conditioned on the current training set, St = {(θ, yi)}t
i=1
which contains the previously evaluated configurations and observations. If a
new point θt+1 is selected and evaluated to provide an observation yt+1 =
H(θt+1) , we add the new pair (θt+1, yt+1) to the current training set St ,
obtaining a new training set for the next iteration St+1 = St ∪(θt+1, yt+1)
[32]. In order to choose the next candidate point to be evaluated, an auxiliary
optimization problem of the following general form should be solved:
θt+1 = argmaxUt(θ; St), θ ∈Θ
where Ut is the acquisition function that needs to be maximized.The BO pro-
cess continues to traverse until the maximum value has been reached. By
utilising all the data it receives from the optimization history, BO makes this
search effective[33][34]. The BO pseudocode is provided in Algorithm 1.
Algorithm 1 Bayesian optimization
1: Initialize data y0 using initial design
2: for n do = 1,....,nmax do
3:
Find θn ∈Θ by optimizing the acquisition function Un,
θt+1 = arg max Ut(θ;St)
4:
Evaluate the objective function yn = H(θn)
5:
Augment the data Sn = Sn−1 ∪{θn, yn}
6:
Update the surrogate model
7: end for
3.7 Uncertainty Quantification
Uncertainty Quantification (UQ) is essential for an accurate application of ML
and DL methods. The confidence of the outcomes produced by these techniques
can be raised using a UQ estimation. Deep Ensembles(DE) were proposed
by Lakshminarayanan [35] , which is easy to implement, easily parallelizable,
require minimal hyperparameter adjustment, and produce high-quality predic-
tion uncertainty estimation.The technique is used to train a group of networks
using various random initializations. Every network operates in a similar way
when there is enough training data available, but when there is none at all, the
15
Fig. 6 Deep ensembles uncertainty quantification.
outcomes are quite different. Instead of training a single network, we will train
an ensemble of M networks, using various random initializations as shown in
figure 6 . For a final prediction, we now take all networks and aggregate their
output into a Gaussian mixture distribution, from which we can derive the
variance estimation σ2
c and single mean µc which is mathematically expressed
as follows.
ˆµc(x) = 1
M
M
X
i=1
ˆµi(x)
ˆσ2
c(x) = 1
M
M
X
i=1
ˆσ2
i (x) +
"
1
M
M
X
i=1
ˆµ2
i (x) −ˆµ2
c(x)
#
We evaluate DE on the ALL-IDB1, ALL-IDB2 and combine datasets using the
InceptionV3-GRU, EfficientNetB3-GRU, and MobileNetV2-GRU networks.
An additional advantage of employing an ensemble is that it makes it simple
to pinpoint training samples where the various networks dispute or agree the
most. This disagreement presents an additional helpful qualitative method of
evaluating prediction uncertainty.
3.7.1 Feature Fusion Hybrid Model
Feature fusion methods are computer programs that use scores from different
models to make a decision. A single scalar score is often generated as a result of
this consolidation process. It typically utilizes relatively easy fusion operators
and doesn’t require a lot of computing. It has been discovered that the best
outcomes are obtained when combining various score-level fusion techniques
employing outputs with comparable performances. A reasonably simple score-
level fusion technique that works directly with raw score data is the sum rule.
The sum rule is shown as follows:
fs = xi + xm + xe
16
According to the sensitivity analysis, the sum rule is the most durable
method for error estimation. It has been found that the sum rule performs
better than most other state-of-the-art score-level fusion algorithms.
The final decision of the proposed method is made by combining the
outputs generated from three separate DL models : MobileNetV2-GRU(xm),
EfficientNetB3-GRU(xe) and InceptionV3-GRU(xi) models for the accurate
classification of ALL cancer as shown in Figure 1. We separately train each
hybrid models using input images. During training, we use the BO to deter-
mine the optimal of hyperparameters. The BO application optimizes achieving
better results at the test stage. Well-known uncertainty method, i.e., DE is
employed and tested on ALL-IDB datasets to reduce models uncertainty.
For each input image, three anticipated probability ratings are generated,
and the input image is classified into either the ALL or noncancer class based
on the highest probability score. To combine the results from the three mod-
els, we used the sum rule, taking into account the parallel architecture in the
suggested system. This approach provides hematologists with a high level of
confidence to make the final judgment and accurately discriminate between
cancer-infected and healthy patients. Our proposed hybrid framework signifi-
cantly improves the accuracy of ALL cancer classification when compared to
MobileNetV2, EfficientNetB3 and InceptionV3 models. Our intention is not
only to enhance the overall prediction performance, also at defining a model
with higher confidence in its predictions. Clearly, the suggested model can han-
dle uncertainties in its predictions. This is essential because it makes it easier
to identify leukemia cancer patients from healthy ones. With our feature fusion
approach, we aim to provide a powerful tool for hematologists to improve their
diagnoses and ultimately improve patient outcomes. In our proposed hybrid
model, several of the parameters are amended as follows Table 2.
Table 2 Hyperparameters of different deep learning architectures after using Bayesian
Optimization.
Hyperparameter
InceptionV3
MobileNetV2
EfficientNetB3
Units
512
512
256
Activation
Softmax
Softmax
Softmax
Optimizer
SGD
SGD
RMSprop
Learning rate
0.01
0.01
0.0001
Momentum
0.3
0.9
0.5
Dropout rate
0.4
0.1
0.1
4 Experimental Results Analysis
4.1 Environment Setup
This section provides the suggested model’s results in a range of experimental
contexts, along with comparisons and discussions. Furthermore, the recom-
mended model is implemented using Python in combination with the Keras
17
deep learning framework, and the simulations are carried out on a powerful
computer system equipped with an Intel Core i7 processor, 16 GB of RAM, and
an NVIDIA GeForce MX330 GPU. Two datasets are used in the performance
and setup of the experiment. The BO technique defines every parameter, and
the optimal combination of parameter values is used to report the findings.
4.2 Evaluation Criteria
In order to evaluate the performance of our proposed model, we employ various
performance metrics such as accuracy, precision, recall, F1-measure, specificity,
and a confusion matrix. The confusion matrix is made up of four main terms:
True Negative (TN), True Positive (TP), False Negative (FN), and False Pos-
itive (FP), which offer valuable information on the overall effectiveness of the
model. The performance metrics utilized in this study are listed below:
Accuracy :
This metric represents the total number of classes—that
is, Acute Lymphoblastic Leukemia (ALL) and not Acute Lymphoblastic
Leukemia (not ALL)—that the trained model was able to predict correctly out
of all classes. This measure depicts the proportion of patients with leukemia
and those without. The higher the value of accuracy, the more accurate is the
model. The equation of accuracy is shown in the following formula:
Accuracy =
TP + TN
TP + TN + FP + FN
Precision: This metric determines the percentage of true positives among
all positive cases. What matters in the case of leukemia disease is the model’s
accuracy in identifying those people who are afflicted. Mathematically, it is
defined as in the following equation:
Precision =
TP
TP + FP
Recall : The recall evaluates how well, given the total relevant data, the
model highlights the patients with leukemia. The following formula is used to
calculate it:
Recall =
TP
FN + TP
F1 measure : This metric integrates the recall and precision numbers to
determine the model’s overall efficiency.
F1 = 2 × Precision × Recall
Precision + Recall
Specificity:
Specificity is the metric that evaluates a model’s ability to
predict true negatives of each available category. These metrics apply to any
categorical model. The equations for calculating this metrics are as follows-
18
Specificity =
TN
FP + TN
Confusion Matrix:
The confusion matrix is a technique for assessing
performance in the form of a table that incorporates information about both
actual and expected classes. The dimension of the confusion matrix would be
nxn if the proposed problem to be examined has an n row, with the rows
representing the actual row and the columns representing the predicted row.
For two or more classes, the matrix depicts actual and anticipated values.
Reciever Operating Characteristics: One useful method for predicting
the possibility of a binary result is the ROC curve. On a probability curve, the
true-positive rate (TPR) vs the false-positive rate (FPR) at various thresholds
are displayed. The ROC curve illustrates the trade-off between specificity and
sensitivity. The true positive indicates how effectively the model predicts the
positive class when the actual result is positive. The shape of the curve provides
a variety of information, such as the things that we would find most important
for a certain situation, the expected false positive rate and false-negative rate.
4.3 Results
This section presents the classification results obtained with the proposed
feature fusion hybrid model for the datasets
ALL IDB1 ,
ALL IDB2 and
ALL IDB1+ ALL IDB2 separately. To ensure the accuracy and generalizability
of our model, we employ a rigorous methodology that involves an 80% training
and 20% testing dataset split. Moreover, to prevent overfitting, we randomly
select 20% of the training set as validation set, which is used to determine the
optimal weights that produce the lowest error.
4.3.1 Results of the
ALL IDB1 Database
The suggested model is initially validated using the
ALL IDB1 database in
order to evaluate its performance. As mentioned earlier, there are extremely
few microscopic blood samples available for training that are both ALL and
not ALL. Therefore, in order to enhance the total amount of blood samples
for training, we have used data augmentation approaches. As a result, the
train set has an acceptable amount of samples, which are used to train the
model after data augmentation. Table 3 provides the total number of blood
samples used for training and testing for both ALL and non-ALL classes.
The model is trained using these enhanced photos. The recommended model
extracts features of leukemic cells from the max-pool and convolution lay-
ers. GRU is applied to the reduced gradient vanishing problem, and UQ is
applied to each hybrid model to boost model confidence. As illustrated in table
4, it is observed that EfficientNetB3+GRU+DE method gives an accuracy
of 95.45%, MobileNetV2+GRU+DE method gives an accuracy of 100% and
InceptionV3+GRU+DE gives 100% accuracy. The parallel architecture was
taken into account in the proposed system, giving hematologists a high level
of confidence to make the final decision that correctly diagnosing leukemia
19
patients with 100% accuracy shown in Table 5. Moreover, the F1Score val-
ues, precision, and recall of the extra evaluation metrics are 100%, 100% and
100%, in that order. Moreover, we have also plotted the confusion matrix of
the experiment, as illustrated in Figure 7 (a). The confusion matrix shows the
overall model efficiency for every class category in the dataset. The results
clearly demonstrate that the suggested model performs better when it comes
to dividing the blood samples into ALL and not ALL classes.
Table 3 Training samples and testing samples on the ALL-IDB1 dataset.
Sample Type
NOT ALL
ALL
Total
Training
376
313
689
Test
12
10
22
Total
388
323
711
Table 4 Obtained results of different deep learning and UQ method for the ALL-IDB1,
ALL-IDB2 and ALL-IDB1 + ALL-IDB2 datasets.
Dataset
Method
Accuracy
ALL-IDB1
EfficientNetB3+GRU+DE
95.45%
MobileNetV2+GRU+DE
100%
InceptionV3+GRU+DE
100%
ALL-IDB2
EfficientNetB3+GRU+DE
92.31%
MobileNetV2+GRU+DE
94.23%
InceptionV3+GRU+DE
92.31%
ALL-IDB1 +
EfficientNetB3
97.30%
ALL-IDB2
+GRU+DE
MobileNetV2+GRU+DE
95.95%
InceptionV3+GRU+DE
94.59%
Table 5 Results of the proposed model on both ALL1-IDB1, ALL-IDB2 and ALL-IDB1
+ ALL-IDB2 datasets
Dataset
Accuracy(%)
Precision(%)
Recall(%)
F1-score(%)
ALL-IDB1
100
100
100
100
ALL-IDB2
98.08
100
96.15
98.04
ALL-IDB1 + ALL-IDB2
98.64
100
97.22
98.59
20
(a) ALL-IDB1
(b) ALL-IDB2
(c) ALL-IDB1 + ALL-IDB2
Fig. 7 Confusion Matrix of the proposed model for ALL-IDB1, ALL-IDB2 and ALL-IDB1
+ ALL-IDB2 datasets.
4.4 Results of the
ALL IDB2 Database
In the second validation stage, we utilized the second dataset,
ALL IDB2. The
total number of microscopic blood samples in this dataset is likewise very inad-
equate for training the suggested model. Therefore, this dataset undergoes the
same data augmentation process. Table 6 presents the total count of blood
samples used for training and testing, both for ALL and not ALL classes. After-
wards, the model is trained using the train set that has an excessive amount of
apparent variety. Additionally, the suggested model’s hyperparameters are all
the same as the ones we initially defined with the first database. Table 5 dis-
plays how the suggested model performed on this database. The convolutional
and pooling layers yield the features of microscopic images of blood samples
whose learning increases by using GRU to solve the reduced gradient vanishing
problem, and each hybrid model is given a UQ application to increase model
confidence. As shown in Table 4, it is observed that EfficientNetB3+GRU+DE
method gives an accuracy of 92.31%, MobileNetV2+GRU+DE method gives
an accuracy of 94.23% and InceptionV3+GRU+DE gives 92.31% accuracy.
When compared to EfficientNetB3+GRU+DE, MobileNetV2+GRU+DE or
InceptionV3+GRU+DE model alone, the accuracy of the suggested hybrid
system has been significantly improved. Table 5 demonstrates that the model
21
is performing well on this dataset as well. The model’s yielded an overall accu-
racy of almost 98.08%, while its precision, recall, and F-Score values were 100%,
96.15% and 98.04% respectively. It’s also important to note that, although we
used the complete microscopic images of the blood samples in the first experi-
ment, the cell areas in this dataset have been cropped. The optimal outcomes
on both kinds of image settings are displayed by the suggested model. The
confusion matrix for the experiment using cropped cell pictures is then also
created. Figure 7(b) displays the experiment’s confusion matrix.
Table 6 Training samples and testing samples on the ALL-IDB2 dataset.
Sample Type
NOT ALL
ALL
Total
Training
832
832
1664
Test
26
26
52
Total
858
858
1716
4.5 Results of Combining Both Datasets
In the third experiment,To enhance diversity and boost the quantity of test
photos, we have included the microscopic images from both datasets. Similarly,
we also used data augmentation in this experiment to raise the training size.
Table 7 displays the total number of training and testing samples for ALL and
not ALL classes. The train set with significant data augmentation is provided
as an input to the suggested model, as was previously done for each of the
datasets separately. The leukemic and normal cell features are extracted by
the model from cropped and full-size images, respectively. It’s also encouraging
how accurate this scenario turned out to be that can be perceived from Table 4
and Table 5. Likewise, there has been an improvement in recall, precision, and
F-Score values. In the third experiment, recall is accomplished on ALL classes
at 97.22%, but in the second experiment, recall is only 96.15%. In addition, as
illustrated in Figure 7(c) , the experiment’s confusion matrix is also plotted.
Table 7 Training samples and testing samples by combining both datasets.
Sample Type
NOT ALL
ALL
Total
Training
1208
1145
2353
Test
38
36
74
Total
1246
1181
2427
Furthermore, receiver operating curves (ROC) are also plotted for each
database. The ROC curves are depicted in Figure 8(a), 8(b) and 8(c) for
ALL-IDB1, ALL-IDB2 and combine datasets respectively.
22
(a) ALL-IDB1
(b) ALL-IDB2
(c) ALL-IDB1 + ALL-IDB2
Fig. 8 ROC curve of the proposed model for ALL-IDB1, ALL-IDB2 and ALL-IDB1 +
ALL-IDB2 datasets.
Loss-epoch & accuracy-epoch curves (MobileNetV2-GRU, EfficientNetB3-
GRU and InceptionV3-GRU methods) for the first, second and combined
datasets are presented in Figure 9. InceptionV3-GRU model accuracy loss
curves are perfectly align for three datasets. As the iteration times increase,
the MobileNetV2-GRU model significantly outperforms compared to the
EfficientNetB3-GRU model.
23
Fig. 9 Accuracy-Epoch vs Loss-Epoch curves for MobileNetV2-GRU, EfficientNetB3-GRU
and InceptionV3-GRU methods (a) the first dataset, (b) second dataset and (c)combined
dataset.
5 Discussion
Finally, we have performed CNN models like EfficientNetB3, InceptionV3
and MobileNetV2 on the same dataset but got the best classification rate
from our proposed model. For ALL-IDB1 dataset we got 54.54% for Effi-
cientNetB3, 61.36% for MobileNetV2 and 95.45% accuracy for InceptionV3
respectively. 69.23% for EfficientNetB3, 86.53% for MobileNetV2 and 92.30%
for InceptionV3 accuracy for For ALL-IDB2 dataset and for combine dataset
we got 72.97% for EfficientNetB3, 94.89% MobileNetV2 and 89.47% accuracy
for InceptionV3 model respectively. The results of our proposed model are
compared with these models that are shown in the Tables 8.
The outcomes of the suggested model have also been compared to the
results of research already done on the identification of leukemia malignancy.
There have been a number of methods proposed that have yielded positive
results. The proposed work’s accuracy measure is compared to those of works
using the same dataset as shown in Table 9.
Earlier, several research studies suggested procedures optimize classifica-
tion accuracy. Some of these techniques used segmentation which divides each
leukocyte cell into its nucleus and cytoplasm; feature extraction; feature dimen-
sionality reduction which maps the higher feature space to the lower feature
space using principal component analysis; and classification which makes use
of common classifiers including k-nearest neighbor, probabilistic neural net-
works, adaptive neuro fuzzy inference systems, support vector machines, and
smooth support vector machines [23] achieved accuracy 89% and A technique
based on convolutional neural networks is used to identify normal and infected
24
Table 8 Comparison of Proposed Model with Individual Model.
Dataset
Methodology
Accuracy
ALL-IDB1
EfficientNetB3
54.54%
MobileNetV2
86.36%
InceptionV3
95.45%
Proposed Methodology
100%
ALL-IDB2
EfficientNetB3
69.23%
MobileNetV2
86.53%
InceptionV3
92.30%
Proposed Methodology
98.08%
ALL-IDB1 +
EfficientNetB3
72.97%
ALL-IDB2
MobileNetV2
94.59%
InceptionV3
89.47%
Proposed Methodology
98.64%
Table 9 Comparison of Proposed Model with Related Study.
Authors
Methodology
Dataset
Accuracy
Jyoti Rawat et al. [23]
SVM
ALL-IDB
89.0%
Sarmad Shafique et al. [28]
AlexNet
ALL-IDB
96.06%
Nizar Ahmed et al. [6]
CNN
ALL-IDB
88.25%
T. T. P. Thanh et al. [26]
CNN
ALL-IDB
96.6%
Maryam Bukhari et al. [12]
Squeeze and excitation
based CNN
ALL-IDB
98.3%
Proposed Methodology
ALL-IDB1
100%
ALL-IDB2
98.08%
ALL-IDB1+
98.64%
ALL-IDB2
blood cells by extracting features from raw blood cell images and classifying
them [26] with accuracy 96.6%, while others utilize pretrained AlexNet which
was fine- tuned. Data augmentation was utilized to verify the performance over
different color images and reduce overtraining by comparing data sets with dif-
ferent color models[28] accuracy 96.06%. In [6] Convolutional Neural Network
based method used for classification and obtain accuracy 88.25%.Squeeze and
excitation based CNN is used in [12] and their proposed method achieves an
accuracy of 98.3%. In [5] AlexNet, VGG16, VGG-19, Inceptionv3, MobileNet-
v2, Xception, DenseNet-201, Inception-ResNet-v2, ResNet-18, ResNet-50, and
ResNet-101 are pretrained series networks that are used for classification. With
all of the pre-trained networks utilized in the ALL-IDB1 study, a classification
accuracy of 100% is achieved. All of the techniques employ enhance classifica-
tion accuracy but none of them consider the uncertainty of the model’s output.
Our model also has the advantage of using numerous models rather than just
one during the classification step. In other words, rather than having a single
decision process, our model has a setof them.
25
6 Conclusion
One of the main causes of cancer-related mortality is leukemia. Recent research
studies that have been conducted suggest using deep learning techniques, such
as transfer learning algorithms, to diagnose leukemia malignancy with remark-
ably precise outcomes. However, enhancing deep learning algorithms remains
an ongoing research challenge among various researchers. The main goal of our
work, an improved deep learning model to introduce a new state-of-art deep
learning model, but assess the performance of uncertainty quantification to
improve the performance of computer-aided diagnostic systems. The proposed
study described the use of DL networks for automated leukemia identifica-
tion with the ALL-IDB, a publicly available dataset. Strong, pertinent, and
discriminative features are extracted from leukemic and normal cells using
GRU, which enables each model to reduce the gradient vanishing problem. The
research can distinguish between normal and leukemia-related images in the
dataset with 100% classification accuracy for the ALL-IDB1 dataset, 98.07%
for ALL-IDB2 dataset and 98.64% for combine datasets.These methods have
demonstrated outstanding results, enabling hematologists to diagnose acute
leukemia at an early stage. Leukemia is detected in general by the image
classifications employed in the study; differentiation of leukemia into distinct
types and subtypes is not taken into consideration. Future research can explore
the prospect of classifying leukemia into many types and subtypes by exper-
imenting with different datasets that include images of various forms of the
disease.
Declarations
The authors do not have any conflicts of interest to declare in connection with
the publication of this paper.
References
[1] Oviedo-Garc´ıa, M.´A.: Journal citation reports and the definition of a
predatory journal: The case of the multidisciplinary digital publishing
institute (mdpi). Research evaluation 30(3), 405–419 (2021)
[2] Ghaderzadeh, M., Asadi, F., Hosseini, A., Bashash, D., Abolghasemi,
H., Roshanpour, A.: Machine learning in detection and classification
of leukemia using smear blood images: a systematic review. Scientific
Programming 2021, 1–14 (2021)
[3] Hegde, R.B., Prasad, K., Hebbar, H., Singh, B.M.K., Sandhya, I.: Auto-
mated decision support system for detection of leukemia from peripheral
blood smear images. Journal of digital imaging 33, 361–374 (2020)
26
[4] Henry, J.B.: Clinical diagnosis and management by laboratory methods
(2006)
[5] Anilkumar, K., Manoj, V., Sagi, T.: Automated detection of leukemia
by pretrained deep neural networks and transfer learning: A comparison.
Medical Engineering & Physics 98, 8–19 (2021)
[6] Ahmed, N., Yigit, A., Isik, Z., Alpkocak, A.: Identification of leukemia
subtypes from microscopic images using convolutional neural network.
Diagnostics 9(3), 104 (2019)
[7] Mohapatra, S., Patra, D., Satpathi, S.: Image analysis of blood micro-
scopic images for acute leukemia detection. In: 2010 International Confer-
ence on Industrial Electronics, Control and Robotics, pp. 215–219 (2010).
IEEE
[8] Patel, N., Mishra, A.: Automated leukaemia detection using microscopic
images. Procedia computer science 58, 635–642 (2015)
[9] Sawyers, C.L., Denny, C.T., Witte, O.N.: Leukemia and the disruption of
normal hematopoiesis. Cell 64(2), 337–350 (1991)
[10] Abunadi, I., Senan, E.M.: Multi-method diagnosis of blood microscopic
sample for early detection of acute lymphoblastic leukemia based on deep
learning and hybrid techniques. Sensors 22(4), 1629 (2022)
[11] Rehman, A., Abbas, N., Saba, T., Rahman, S.I.u., Mehmood, Z., Koli-
vand, H.: Classification of acute lymphoblastic leukemia using deep
learning. Microscopy Research and Technique 81(11), 1310–1317 (2018)
[12] Bukhari, M., Yasmin, S., Sammad, S., El-Latif, A., Ahmed, A., et al.:
A deep learning framework for leukemia cancer detection in micro-
scopic blood samples using squeeze and excitation learning. Mathematical
Problems in Engineering 2022 (2022)
[13] Talukder, M.A., Islam, M.M., Uddin, M.A., Akhter, A., Pramanik,
M.A.J., Aryal, S., Almoyad, M.A.A., Hasan, K.F., Moni, M.A.: An effi-
cient deep learning model to categorize brain tumor using reconstruction
and fine-tuning. Expert systems with applications 230, 120534 (2023)
[14] Khatun, R., Akter, M., Islam, M.M., Uddin, M.A., Talukder, M.A., Kam-
ruzzaman, J., Azad, A., Paul, B.K., Almoyad, M.A.A., Aryal, S., et
al.: Cancer classification utilizing voting classifier with ensemble feature
selection method and transcriptomic data. Genes 14(9), 1802 (2023)
[15] Namayandeh, S.M., Khazaei, Z., Najafi, M.L., Goodarzi, E., Moslem, A.:
27
Global leukemia in children 0-14 statistics 2018, incidence and mortal-
ity and human development index (hdi): Globocan sources and methods.
Asian Pacific journal of cancer prevention: APJCP 21(5), 1487 (2020)
[16] Dwivedi, A.K.: Artificial neural network model for effective cancer clas-
sification using microarray gene expression data. Neural Computing and
Applications 29, 1545–1554 (2018)
[17] Haworth, C., Heppleston, A., Jones, P.M., Campbell, R., Evans, D.,
Palmer, M.K.: Routine bone marrow examination in the management of
acute lymphoblastic leukaemia of childhood. Journal of clinical pathology
34(5), 483–485 (1981)
[18] Bain, B.J.: Diagnosis from the blood smear. New England Journal of
Medicine 353(5), 498–507 (2005)
[19] Putzu, L., Caocci, G., Di Ruberto, C.: Leucocyte classification for
leukaemia detection using image processing techniques. Artificial intelli-
gence in medicine 62(3), 179–191 (2014)
[20] Agaian, S., Madhukar, M., Chronopoulos, A.T.: Automated screening
system for acute myelogenous leukemia detection in blood microscopic
images. IEEE Systems journal 8(3), 995–1004 (2014)
[21] Husham, A., Hazim Alkawaz, M., Saba, T., Rehman, A., Saleh Alghamdi,
J.: Automated nuclei segmentation of malignant using level sets.
Microscopy research and technique 79(10), 993–997 (2016)
[22] Iqbal, S., Khan, M.U.G., Saba, T., Rehman, A.: Computer-assisted brain
tumor type discrimination using magnetic resonance imaging features.
Biomedical Engineering Letters 8(1), 5–28 (2018)
[23] Rawat, J., Singh, A., Bhadauria, H.S., Virmani, J., Devgun, J.S.:
Classification of acute lymphoblastic leukaemia using hybrid hierarchi-
cal classifiers. Multimedia Tools and Applications 76(18), 19057–19085
(2017)
[24] Suryani, E., Palgunadi, S., Pradana, T.N., et al.: Classification of acute
myelogenous leukemia (aml m2 and aml m3) using momentum back prop-
agation from watershed distance transform segmented images. In: Journal
of Physics: Conference Series, vol. 801, p. 012044 (2017). IOP Publishing
[25] Kumar, P., Udwadia, S.M.: Automatic detection of acute myeloid
leukemia from microscopic blood smear image. In: 2017 International
Conference on Advances in Computing, Communications and Informatics
(ICACCI), pp. 1803–1807 (2017). IEEE
28
[26] Thanh, T., Vununu, C., Atoev, S., Lee, S.-H., Kwon, K.-R.: Leukemia
blood cell image classification using convolutional neural network. Inter-
national journal of computer theory and engineering 10(2), 54–58 (2018)
[27] Vogado, L.H., Veras, R.M., Araujo, F.H., Silva, R.R., Aires, K.R.:
Leukemia diagnosis in blood slides using transfer learning in cnns and
svm for classification. Engineering Applications of Artificial Intelligence
72, 415–422 (2018)
[28] Shafique, S., Tehsin, S.: Acute lymphoblastic leukemia detection and clas-
sification of its subtypes using pretrained deep convolutional neural net-
works. Technology in cancer research & treatment 17, 1533033818802789
(2018)
[29] Ahmed, I.A., Senan, E.M., Shatnawi, H.S.A., Alkhraisha, Z.M., Al-
Azzam, M.M.A.: Hybrid techniques for the diagnosis of acute lymphoblas-
tic leukemia based on fusion of cnn features. Diagnostics 13(6), 1026
(2023)
[30] Khdhir, R., Belghith, A., Othmen, S.: Pancreatic cancer segmentation and
classification in ct imaging using antlion optimization and deep learning
mechanism. International Journal of Advanced Computer Science and
Applications 14(3) (2023)
[31] Ahmad, S., Ullah, T., Ahmad, I., Al-Sharabi, A., Ullah, K., Khan, R.A.,
Rasheed, S., Ullah, I., Uddin, M.N., Ali, M.S.: A novel hybrid deep learn-
ing model for metastatic cancer detection. Computational Intelligence and
Neuroscience 2022(1), 8141530 (2022)
[32] Galuzzi, B.G., Giordani, I., Candelieri, A., Perego, R., Archetti, F.:
Hyperparameter optimization for recommender systems through bayesian
optimization. Computational Management Science 17, 495–515 (2020)
[33] Shahriari, B., Swersky, K., Wang, Z., Adams, R.P., De Freitas, N.: Taking
the human out of the loop: A review of bayesian optimization. Proceedings
of the IEEE 104(1), 148–175 (2015)
[34] Canayaz, M., S¸ehribano˘glu, S., ¨Ozda˘g, R., Demir, M.: Covid-19 diagnosis
on ct images with bayes optimization-based deep neural networks and
machine learning algorithms. Neural Computing and Applications 34(7),
5349–5365 (2022)
[35] Lakshminarayanan, B., Pritzel, A., Blundell, C.: Simple and scalable pre-
dictive uncertainty estimation using deep ensembles. Advances in neural
information processing systems 30 (2017)
