arXiv:2410.18939v1  [stat.ME]  24 Oct 2024
Adaptive Partition Factor Analysis
Elena Bortolatoâˆ—
Department of Business and Economics, Universitat Pompeu Fabra,
Data Science Center, Barcelona School of Economics
and
Antonio Canale
Department of Statistical Sciences, University of Padova
October 25, 2024
Abstract
Factor Analysis has traditionally been utilized across diverse disciplines to ex-
trapolate latent traits that inï¬‚uence the behavior of multivariate observed variables.
Historically, the focus has been on analyzing data from a single study, neglecting
the potential study-speciï¬c variations present in data from multiple studies. Multi-
study factor analysis has emerged as a recent methodological advancement that ad-
dresses this gap by distinguishing between latent traits shared across studies and
study-speciï¬c components arising from artifactual or population-speciï¬c sources of
variation. In this paper, we extend the current methodologies by introducing novel
shrinkage priors for the latent factors, thereby accommodating a broader spectrum
of scenariosâ€”from the absence of study-speciï¬c latent factors to models in which
factors pertain only to small subgroups nested within or shared between the stud-
ies. For the proposed construction we provide conditions for identiï¬ability of factor
loadings and guidelines to perform straightforward posterior computation via Gibbs
sampling. Through comprehensive simulation studies, we demonstrate that our pro-
posed method exhibits competing performance across a variety of scenarios compared
to existing methods, yet providing richer insights. The practical beneï¬ts of our ap-
proach are further illustrated through applications to bird species co-occurrence data
and ovarian cancer gene expression data.
Keywords: Factor models, Neural networks, Shrinkage priors, Sparsity.
âˆ—
1
1
Introduction
In numerous scientiï¬c and socio-economic domains, the collection of high-dimensional data
has become ubiquitous.
Instances of such data collection include customer preferences
in recommender system applications, single-cell experiments in genomics, high-frequency
trading in ï¬nance, and extensive sampling campaigns in ecology. In these contexts, the
joint analysis of data originating from diverse sources, studies, or technologies is increasingly
critical for enhancing the accuracy of conclusions and ensuring the reproducibility of results
[26].
However, it is common that diï¬€erent studies mix the common latent signal with study-
speciï¬c ones. This issue is particularly pronounced in our motivating contexts of animal
co-occurrence studies and genomics. In the former, zoothologists record the presence of var-
ious animal species across diï¬€erent spatial locations, conducting sampling campaigns over
multiple time periods. While most sites with similar habitats exhibit consistent patterns
of animal occurrences, some sites may display unique characteristics due to possible unob-
served variables [29]. Similarly, in gene expression studies, high-throughput experiments
are known to present variations of both biological and technical nature. Consequently,
eï¬€ects driven by speciï¬c experimental conditions or particular technology can overshadow
the biological signals of interest [27, 28]. Although preprocessing procedures aimed at cor-
recting for batch eï¬€ects help in removing some speciï¬cities, these methods are often not
enough to obtain homogeneous covariance structures across groups [21].
Motivated by these problems, [1] introduced a novel methodological tool, namely multi-
study factor analysis (MSFA), which extends the classical factor analysis to jointly analyze
data from multiple studies.
MSFA aims to separate the signal shared across multiple
studies from the study-speciï¬c components arising from artifactual or population-speciï¬c
2
sources of variation.
To ï¬x the notation, let yis denote the p-dimensional observation
vector corresponding to subject i âˆˆ{1, . . . , n} belonging to study s âˆˆ{1, . . . , S}, where
each study comprises ns subjects and n = PS
s=1 ns. In the MSFA framework, one assumes
the following representation:
yis = Î›Î·is + Î“sÏ•is + Ç«is
(1)
where Ç«is represents a zero-mean idiosyncratic error term and Î·is is a d dimensional set of
shared latent factors, with d â‰ªp and with Î› the corresponding factor loading matrix of
dimension p Ã— d. Similarly, Î“s is a study-speciï¬c factor loading matrix of dimension p Ã— ks,
with ks â‰ªp possibly diï¬€erent in each study, and Ï•is its corresponding latent factors.
To address the high-p small-n problem typical of high-throughput biological data, [8]
propose a Bayesian generalization of [1] that naturally provides the necessary regularization
through the multiplicative gamma prior of [23]. Posterior sampling is performed via Markov
Chain Monte Carlo (MCMC) methods, enabling ï¬‚exible estimation without constraints on
loading matrices. Extensions handling covariates and alternative estimation procedures has
been discussed by [20] and [21]. Similarly, [32] proposed a perturbed factor analysis that
focuses on inferring the shared structure while making use of subject-speciï¬c perturbations.
Motivated by identiï¬cation issues arising from these approaches, [3] recently proposed a
class of subspace factor models which characterize variation across groups at the level of
a lower-dimensional subspace where the study-speciï¬c factor loadings Î“s are obtained as
Î“s = Î›As where As is a d Ã— ds matrix, with ds < d, i.e.
modeling the study-speciï¬c
contributions as lower dimensional perturbations of the shared factor loadings.
In all of these approaches there is however a modeling limitation, i.e. they allow pre-
cisely S study-speciï¬c loading matrices Î“s. In practice, this assumption is often violated.
We provide some examples, but a continuous spectrum of possibilities may be found: (i)
3
two or more studies exhibit high homogeneity and may share identical or nearly identical
latent representations, and (ii) one study involves highly heterogeneous groups of subjects,
potentially resulting in two or more sub-populations with distinct latent structures as de-
scribed in Equation (1). The latter situation might be practically due to slightly diï¬€erent
experimental setups or unmeasured confunders. Additional possibilities include (iii) the
existence of a subset of units from diï¬€erent groups that share common characteristics, or
(iv) instances where, for various reasons, some units may be associated with characteristics
of multiple groups simultaneously.
To address case (i), [19] proposed a model allowing for partially-shared latent factors
using a study-speciï¬c diagonal selection matrix. The pattern of zeros and ones in this
matrix determines which latent factors pertain to each speciï¬c study. The diagonal elements
are then incorporated into a selection matrix governed by an Indian Buï¬€et Process (IBP)
prior [30]. The same work discusses a variant, where study-speciï¬c labels are unknown
and inferred via mixture models. While this approach moves towards addressing case (ii),
it relies on the a priori knowledge of the total number of groups into which the units
are divided and to our knowledge it has not been applied yet. Case (iii) corresponds to a
situation where instead a single partition is insuï¬ƒcient to describe speciï¬c factors patterns.
Hence, despite these recent advancements, a unifying framework that seamlessly spans from
classical factor models to MSFA, encompassing all the above mentioned cases or nuanced
variants, remains absent.
In this paper, we propose an alternative formulation for Bayesian MSFA leveraging the
concept of informed or structured sparsity [14, 15, 31]. This approach allows for ï¬‚exible
modeling of varying degrees of group heterogeneity in terms of latent factors contribu-
tion. Consistently with this, we name the proposed approach Adaptive Partition Factor
4
Analysis (APAFA). APAFA employs shrinkage priors for latent factors, using the infor-
mation contained in study labels, but without enforcing a deterministic structure. This
structured shrinkage is combined with increasing shrinkage priors on the number of latent
factors, following a cumulative shrinkage process approach [16, 22]. This construction en-
sures identiï¬able separation of the shared and speciï¬c contributions while circumventing,
under suitable regularity conditions, the well-known issue of rotational ambiguity in factor
loadings for the speciï¬c factors, thereby allowing straightforward posterior interpretation
of these quantities. Notably, we show that the proposed method can be seen as a par-
ticular neural network model allowing us to exploit further generalizations, including the
integration of subject- or study-speciï¬c continuous covariates in a ï¬‚exible manner.
The next section details APAFA in terms of model speciï¬cation, prior distributions,
and provides practical guidelines for prior elicitation. We also explore the connections to
neural networks and present generalizations. In addition, we address model identiï¬ability
and describe the procedures for posterior computation. Section 3 reports a comprehensive
empirical assessment through simulation studies covering various data-generating scenarios.
Section 4 reports two illustrative data analyses further highlighting the advantages of our
approach in the motivating examples of bird species occurrence and genomics. The ï¬nal
discussion section explores generalizations and potential extensions. The code to reproduce
our analyses available at https://github.com/elenabortolato/APAFA.
5
Y
H
Î›âŠ¤
Î¦
Î“âŠ¤
Ç«
=
+
+
Figure 1: Multi study Factor model: The n Ã— p data matrix Y is written as the product of
the latent factor matrix H of dimension n Ã— d by the factor loading matrix Î› (the shared
parts) plus the product of the latent factor matrix Î¦ of dimension nÃ—k by the factor loading
matrix Î“ (the study-speciï¬c parts) and a random noise Ç«. Diï¬€erent shades of purple, red,
and blue identify the S = 3 studies in Y . The matrix Î¦ presents a pronounced presence of
structural zeros and study-speciï¬c factors.
2
Adaptive partition factor analysis
2.1
Model and prior speciï¬cation
Under the standard assumption that Î·i âˆ¼N(0, Id) and omitting the subscript s, we rewrite
model (1) as
yi = Î›Î·i + Î“Ï•i + Ç«i,
Ç«i âˆ¼N(0, Î£),
(2)
where Î“ = (Î“1, . . . , Î“S) stacks all the study-speciï¬c factor loading matrices into a p Ã— k
matrix with k = PS
s=1 ks and Ï•i is a k-dimensional augmented vector containing the Ï•is
in Equation (1) framed with suitable pattern of zeros. Speciï¬cally, all the latent factors
pertaining to group s will have Psâˆ’1
l=1 kl zeros, followed by subject-speciï¬c Ï•is, followed by
PS
l=s+1 kl zeros. An illustration is depicted in Figure 1.
6
As customary in factor analysis, one can marginalize out the shared factors, obtaining
yi âˆ¼N(Î“Ï•i, Î›Î›âŠ¤+ Î£).
(3)
Under this characterization, one can think to have, for each i, a vector of dummy variables
xi with S entries characterizing the study to which unit i belongs to and then assume
that the h-th element of the vector Ï•i is Ï•ih = ËœÏ•ihÏˆih where ËœÏ•ih is a continuous random
variable and Ïˆih = fh(xi) with fh a deterministic activation function. In [1, 8], for example,
fh(xi) = xâŠ¤
i 1S where 1S is a S dimensional vector of ones. In this paper, we propose to
incorporate the information contained in the xiâ€™s in a more ï¬‚exible manner letting
Ïˆih = fh(xâŠ¤
i Î²h),
(4)
keeping, for the moment, fh as a general, non-linear function and considering the parameters
Î²h as unknown.
It is now evident that equations (3) and (4) transform model (2) into a speciï¬c single
layer neural network where the dummy variables in the xis are the input variables, yi
are the pâˆ’dimensional output variables, Ï•is are the nodes of the hidden layer, fh are the
activation functions, Î²h are the weights between the input and the hidden layer, and the
elements in Î“ are the weights between the hidden layer and the continuous outcome yi.
The neural network representation of model (2) is reported in Figure 2. In recent years,
there has been a growing interest in Bayesian analysis of neural networks. Typically, the
weight parameters are assigned a prior distribution and are learned updating the prior via
Bayes rule. For comprehensive reviews, see [38], [11], and [39]. Bayesian approaches to
neural network models, however, not only apply the Bayesian inferential paradigm to learn
the network parameters but also allow the neurons themselves to be stochastic [34, 35].
Consistently with these approaches, but focusing on our motivating factor analysis
7
x1
x2
x3
...
xS
Ï•1
Ï•2
...
Ï•k
y1
y2
y3
y4
...
yp
Î²11
Î²12
Î²1k
Î²2k
Î²3k
Î²S1
Î²S2
Î²Sk
Î³11
Î³1p
Î³21
Î³2p
Î³1p
Î³2p
Î³k1
Î³k2
Î³kp
Figure 2: Neural Network representation: The input nodes are the categorical variables
associated to the study structure. The ï¬rst layer of latent variables are the latent study-
speciï¬c factors.
context, we now deï¬ne the prior structure for model (2). Speciï¬cally, we let
Ï•ih âˆ¼N(0, Ïˆih(xi)Ï„ Ï•
h ),
(5)
where the variance of Ï•ih is a product of a global scale dependent on the index h and a
local scale dependent on the indexes i and h, and most importantly, on the variable xi
through Equation (4).
We let the global scale parameter Ï„ Ï•
h âˆ¼Ber(1 âˆ’Ïh), where {Ïh} âˆ¼CUSP(Î±Ï•) follows a
cumulative shrinkage process [16], i.e. for h = 1, 2, . . . , Ïh = P
lâ‰¤h wÏ•
l ,
wÏ•
l = vÏ•
l
Q
m<l(1 âˆ’
vÏ•
l ), and vÏ•
l âˆ¼Beta(1, Î±Ï•). This approach facilitates learning the number of study-speciï¬c
factors or, following the neural network interpretation of our model, the number of neurons
in the hidden layer. Notably, this construction, inspired by successful applications in factor
models [16, 14], oï¬€ers valuable insights into performing Bayesian inference for the number
of nodes in the hidden layer of a Bayesian neural network, potentially connecting with
existing literature on this topic [e.g 9, 41, 11, 10, 13, 12].
We now move the discussion on the local scales Ïˆih(xi) that are assumed to be Bernoulli
8
variables with probability depending on xi. Speciï¬cally, we let
Ïˆih(xi) âˆ¼Ber{logitâˆ’1(xâŠ¤
i Î²h)}.
(6)
Note that under this speciï¬cation, we can also write Ï•ih(xi) = ËœÏ•ihÏˆih(xi) and
ËœÏ•ih(xi) âˆ¼N(0, Ï„ Ï•
h ).
(7)
Marginalizing out Ïˆih(xi) with Ï„ Ï•
h and Î²h ï¬xed, we have
Ï•ih(xi) âˆ¼{1 âˆ’logitâˆ’1(xâŠ¤
i Î²h)}Î´0 + logitâˆ’1(xâŠ¤
i Î²h)N(0, Ï„ Ï•
h ),
where Î´a is a Dirac mass at value a. Marginalizing out ËœÏ•ih, having unitary variance leads
to
yi âˆ¼N(0, Î›Î›âŠ¤+ Î“diag{Ïˆi}Î“âŠ¤+ Î£).
(8)
Notably (6) corresponds to a simple form of conditional variational autoencoder that,
diï¬€erently from the standard, allows for the latent covariance structure to be informed
by the independent variables. Variational autoencoders are speciï¬c extension of neural
networks, widely recognized as a descendant of classical factor models [36, 37].
The beneï¬ts of the proposed solution should now be evident. The shrinkage prior on the
elements Ï•ih(xi) enables and promotes, yet does not mandate, the sparse representation in
(2). Furthermore, it accommodates a wide range of scenarios discussed in the Introduction,
including: (i) two or more studies exhibit high homogeneity and share nearly identical
latent representations, (ii) some studies involve highly heterogeneous groups of subjects,
potentially resulting in two or more sub-populations with distinct latent structures, or any
combinations of the above. The random partition of the observation induced by the sparse
structure of the Ïˆi justify the Adaptive Partition Factor Analysis (APAFA) name adopted
for the proposed solution.
9
Another beneï¬t of APAFA is the generalizability oï¬€ered by its neural network interpre-
tation. In fact, it is clear that if other subject-speciï¬c covariates are available, say, zi âˆˆRq
we can include them as input variables, for example
Ïˆih(xi, zi) âˆ¼Ber{logitâˆ’1(xâŠ¤
i Î²(x)
h
+ zâŠ¤
i Î²(z)
h )},
thus allowing the latent factors Ï•i to have subject-speciï¬c, and not just study-speciï¬c,
conditional distributions. For this reasons we will use henceforth the broader term speciï¬c
factors (loadings) to denote Ï• (Î“) rather than the narrower term study-speciï¬c.
We complete the prior speciï¬cation for the study-speciï¬c part assuming standard prior
for the factor loadings. Speciï¬cally for the general element Î³jh of the factor loading matrix
Î“, we let Î³jh âˆ¼N(0, Î¶Î³
h), with Î¶Î³
h âˆ¼IGa(aÎ³, bÎ³).
One remark on the activation function pertains the potential lack of properness of the
conditional posterior distribution for the parameters Î²h, h = 1, . . . , k. This concern is tied
to the issue of perfect separation in logistic regression. Assuming the group structure en-
coded by the xi dummy variables is both accurate and relevant to the problem at handâ€”in
other words, that the original MSFA model by [8] is correctly speciï¬edâ€“ a perfect separation
in the Ïˆih occurs with respect to the xi within the logistic regression. For this reason, the
prior distribution on each parameter Î²h should be suï¬ƒciently concentrated around zero
to prevent the associated conditional posterior from exhibiting a tendency toward mono-
tonicity. To this end, we specify Î²h âˆ¼N(0, B), where B is a S Ã— S diagonal matrix with
elements proportional to nâˆ’1. Thus, when perfect separation in the Ïˆihâ€™s with respect to
the xisâ€™s occurs, the full conditional posterior is
pr(Î²hs|âˆ’) âˆexp

âˆ’ns
 n
2ns
Î²2
hs âˆ’log
eÎ²hs
1 + eÎ²hs

.
10
Writing n = Â¯ns Ã— S where, Â¯ns is the average number of units per group, we have
pr(Î²hs|âˆ’) âˆexp

âˆ’ns
 Â¯ns Ã— S
2ns
Î²2
hs âˆ’log
eÎ²hs
1 + eÎ²hs

from which we can also recognize that if the number of groups S increases the posterior
concentrates close to 0, for any group size. Thus, despite the apparent inconsistency of a
prior that induces skepticism regarding the given group conï¬guration, it facilitates posterior
inference.
We now move to describe the prior speciï¬cation for the shared part. As before, we
include an increasing shrinkage prior for the dimensions of the shared factors, i.e.
Î·ih âˆ¼N(0, 1),
Î»jh âˆ¼N(0, Ï„hÎ¶Î»
h),
Î¶Î»
h âˆ¼IGa(aÎ», bÎ»),
with a cumulative shrinkage prior on {Ï„h} âˆ¼CUSP(Î±Î·). For the diagonal elements of Î£
we adopt an inverse gamma distribution and speciï¬cally Ïƒ2
j âˆ¼IGa(aÏƒ, bÏƒ), which completes
the prior speciï¬cation.
2.2
Model properties
Identiï¬ability of factor models is an overarching issue. On the one hand, in fact, factor
models are appealing in providing possible interpretations and insights on how a small set
of latent factors is connected to a multivariate observation with a possible intricate form
of dependency.
The other side of the medal, however, is that mathematically a factor
analytic model is not identiï¬able in many respect.
See [7] [5] [6] for a comprehensive
review. For instance, loadings are not identiï¬able due to rotation, sign and permutation
invariance.
Rotational invariance refers to transformation of loadings and factors by a
orthogonal matrix, sign switching arises because we can simultaneously change the signs of
and certain elements without aï¬€ecting the overall structure while permutation invariance
occurs because there is no intrinsic ordering in the factor loading matrix columns.
11
Identiï¬cation issues are exacerbated in the MSFA framework of [8] where it may be dif-
ï¬cult to disentangle the signal between shared and study-speciï¬c factors, a problem known
as information switching. For instance, [3] show how a linear combination of study-speciï¬c
factors, can instead be represented and reincorporated as part of the shared component.
Building on these concepts, we provide conditions to identify the shared from the spe-
ciï¬c factors in our setup. More formally, we give the following deï¬nition of information
switching.
Deï¬nition 2.1. Let n be the number of units, and Sn the number of distinct groups in
model (9), i.e. Sn = |âˆªn
i=1â„¦i|. Denote with Î¨ the n Ã— k matrix that stacks in distinct rows
all local scale parameters Ïˆi = (Ïˆi1, . . . , Ïˆik) and with Î¨h its generic column with Î¨h Ì¸= 1n
for all h âˆˆ{1, . . . , k}. Deï¬ne Ws = â„¦s âˆ’Î›Î›âŠ¤âˆ’Î£ = Î“diag{Ïˆs}Î“âŠ¤, for s âˆˆ{1, . . . , Sn}.
The model suï¬€ers from information switching if there exist ËœÎ“ Ì¸= Î“ and ËœÎ¨ Ì¸= Î¨ such that
Ws = ËœÎ“diag{ ËœÏˆs}ËœÎ“ for all s, with ËœÎ¨h = 1n for at least one h.
It is clear that if there is ËœÎ¨h = 1n, the h-th factor could no longer be interpreted as
speciï¬c and should be moved to the shared part and, consistently, Î›Î›âŠ¤would represent
only a fraction of the shared variance. Note that in Deï¬nition 2.1, Sn does not refer to
the number of distinct labels or studies S ï¬xed a priori and embedded in the categorical
variables xi, but rather it is equal to the number of distinct conï¬gurations of the rows of
the matrix Î¨. The following Theorem ensures the identiï¬cation in terms of information
switching.
Theorem 2.2. For the model deï¬ned in (8), if Î“ is of full column rank k < p(p+1)/2 and
Î“Î“âŠ¤is not sparse, then the model is resistant to information switching.
Proof. Since each Ïˆs is a vector of zeroes and ones, we can see that Ws = Î“diag{Ïˆs}Î“âŠ¤=
Î“diag{Ïˆs}diag{Ïˆs}Î“âŠ¤. Deï¬ne Î“s = Î“diag{Ïˆs}. As in any factor analytic decompositions,
12
the elements of Ws are identiï¬able up to multiplication by group-speciï¬c rotation matri-
ces Ps, such that PsP âŠ¤
s
= Ik and |Ps| = 1.
Hence we can write Ws = Î“sPsP âŠ¤
s Î“âŠ¤
s =
Î“diag{Ïˆs}PsP âŠ¤
s diag{Ïˆs}Î“âŠ¤. Hence each element wjl
s of Ws, can be written as
wjl
s =
k
X
h=1
Î³jhÎ³lh Â¯Ïˆhs Â¯ÏˆâŠ¤
hs,
where Â¯Ïˆhs is the h-th row of the product diag{Ïˆs}Ps. Note that the product Â¯Ïˆhs Â¯ÏˆâŠ¤
hs is
either 0 or 1 by construction. Thus we know that there exists ËœÏˆhs = Â¯Ïˆhs Â¯ÏˆâŠ¤
hs for h = 1, . . . k
that lead to the same Ws for each s = 1, . . . , Sn. Hence, it is suï¬ƒcient to check whether
these ËœÏˆs are diï¬€erent from the original Ïˆs for each s. Since we have Snp(p + 1)/2 equations
of the form
k
X
h=1
Î³jhÎ³hl ËœÏˆhs âˆ’wjl
s = 0,
with Snk unknowns and k < p(p+1)/2 by hypothesis, there will be at most one solution.
Based on the identiï¬ability ï¬nding of Theorem 2.2, the practical condition that ensures
resistance to information switching in APAFA are discussed in Lemma 2.3 and 2.4.
Lemma 2.3. Under any continuous prior for the elements of Î“ and truncating its number
of columns to K = p(p + 1)/2 âˆ’1, the information switching for model (8) has zero
probability.
Lemma 2.4. Under any continuous prior for the elements of Î“, for any Î±Ï• < Îµp2/2
and suï¬ƒciently small Îµ > 0, the information switching for model (8) has prior probability
bounded from above by Ç«.
Proof. From Markovâ€™s inequality pr(k > 2(p + 1)/2) â‰¤
Î±Ï•
p(p+1)/2 â‰¤Îµ
A key advantage of our model is that the requirement of k < p(p + 1)/2 is minimal as
it is customary in factor analysis to have latent factors of much lower dimension than the
13
observed data (k < p). Notably, [3], in diï¬€erent settings, necessitate the dimension of the
speciï¬c part to be smaller than that of the shared part implicitly considering the study-
speciï¬c part as a noise or disturbance parameter. Although we are in diï¬€erent settings,
where the number and composition of the groups is not pre-speciï¬ed, we do not restrict k
to be also smaller than d. This is crucial in many contexts. For instance, in cancer studies,
it is possible to study very diï¬€erent types of cancer simultaneously, identifying a range of
speciï¬c factors that describe the speciï¬c characteristics of each type of cancer, while also
uncovering a few common factors that are shared across cancers, regardless of the organ or
tissue aï¬€ected possibly pertaining to fundamental mechanisms of oncogenesis.
Notably, the speciï¬c structure induced by sparsity in the Ïˆs not only ensures identi-
ï¬ability in terms of information switching but also solves the usual rotational ambiguity,
under certain conditions. This is formalized in the following Theorem.
Theorem 2.5. Assume that for at least one i, Ïˆi Ì¸= 0k but at least one element of Ïˆi is
null, i.e. Ïˆih = 0. Let P Ì¸= I be any rotation matrix such that Phl Ì¸= {0, Â±1} (i.e. P is not a
permutation matrix), let Î¨ be as in Deï¬nition 2.1, Î¦ be the nÃ—k matrix with the vectors Ï•i
organized as its rows, ËœÎ¦ the nÃ—k matrix that similarly stacks the ËœÏ•i, such that Î¦ = Î¨âŠ™ËœÎ¦,
where âŠ™denotes the Hadamard matrix multiplication. Then, Î¦Î“âŠ¤Ì¸= [Î¨ âŠ™(ËœÎ¦P)]P âŠ¤Î“âŠ¤.
Proof. The theorem is proved by contradiction. Assume [Î¨ âŠ™ËœÎ¦] = [Î¨ âŠ™(ËœÎ¦P)]P âŠ¤. If we
multiply both sides of this equality by the matrix P, we get [Î¨ âŠ™ËœÎ¦]P = [Î¨ âŠ™(ËœÎ¦P)]. We
can write the generic element in row i, column h in the left hand side of equation above as
{[Î¨ âŠ™ËœÎ¦]P}ih = [Ïˆi âŠ™ËœÏ•i]âŠ¤Ph =
k
X
l=1
Ïˆil ËœÏ•ilPlh,
and each element of the right hand side as meet [Î¨ âŠ™(ËœÎ¦P)]ih = Ïˆih
Pk
l=1 ËœÏ•ilPhl. In partic-
ular, since for at least one i, there exist a Ïˆih = 0 and Ïˆi Ì¸= 0k the equality holds true only
14
if Ph is a suitable combination of 0 or 1 but this is impossible as P is not a permutation
matrix.
Note that the strongest condition in Theorem 2.5 is that there exist at least one ob-
servation i with Ïˆi Ì¸= 0k and at least one element of Ïˆi is null. This means, for example
that if there is one group with no speciï¬c factors and other groups with speciï¬c factor,
the identiï¬cation of Î› is not ensured. In practice, one can check a posteriori whether this
condition is satisï¬ed or not before trying to interpret the results.
Clearly, the shared variability induced by Î› remains susceptible to well-known identiï¬-
ability issues. However, these challenges can be eï¬€ectively addressed using state-of-the-art
methods, such as those proposed by [4] and [7].
2.3
Posterior computation
In this section, we present a Markov Chain Monte Carlo scheme to sample from the posterior
distribution of the parameters of APAFA. We ï¬rst rewrite the model as
yij =
d
X
h=1
Î»jhÎ·ihÏ„h +
k
X
h=1
ËœÏ•ihÏˆihÏ„ Ï•
h + Ç«ij,
(9)
with Ï•ih = Ïˆih ËœÏ•ih with ËœÏ•ih > 0. The parameters are then updated sequentially iterating
the following steps.
1. Update the block of latent factors [Î·i, ËœÏ•i] for each 1 â‰¤i â‰¤n.
â€¢ Sample from [Î·i, ËœÏ•i] âˆ¼Nd+k

Ë†ÂµÎ·,Ï•, Ë†Î£Î·,Ï•

, with
Ë†Î£Î·,Ï• =
 Id+k + ([Î› , Î“Ik(Ïˆi âŠ™Ï„ Ï•)])âŠ¤Î£âˆ’1[Î›, Î“Ik(Ïˆi âŠ™Ï„ Ï•)]
âˆ’1 ,
Ë†ÂµÎ·,Ï• = Ë†Î£Î·,Ï•([Î›, Î“Ik(Ïˆi âŠ™Ï„ Ï•)])âŠ¤Î£âˆ’1yi,
where aâŠ™b indicates the term-by-term product of vectors a, b, Ik(Â·) the identity
matrix, and Ï•i = ËœÏ•i âŠ™Ïˆi.
15
â€¢ Standardize the factors computing Â¯Î·h = nâˆ’1 Pn
i=1 Î·ih V Î·
h = nâˆ’1 Pn
i=1(Î·ih âˆ’Â¯Î·h)2
for each h = 1, . . . , d , and Â¯ËœÏ•h = nâˆ’1 Pn
i=1 ËœÏ•ih, V Ï•
h
= nâˆ’1 Pn
i=1( Ëœ
Ï•ih âˆ’Â¯ËœÏ•h)2,
for h = 1, . . . , k.
Then set Î·h = Î·h/
p
V Î·
h , Î›h = Î›h
p
V Î·
h , ËœÏ•h = ËœÏ•h/
p
V Ï•
h ,
Ï•h = Ï•h/
p
V Ï•
h , Î“h = Î“h
p
V Ï•
h .
2. Update Î£ by sampling the diagonal elements for j = 1, . . . , p,
Ïƒ2
j âˆ¼IGa
(
aÏƒ + n
2, bÏƒ + 1
2
n
X
i=1
 yij âˆ’Î»âŠ¤
j Î·i âˆ’Î³âŠ¤
j Ï•i
2
)
.
3. Update Î²h = (Î²h1, . . . , Î²hs)âŠ¤for h = 1, . . . , k. as follows
â€¢ for h = 1, . . . , k, sample n independent PÂ´olya-Gamma distributed random vari-
ables [18] Ï‰ih âˆ¼PG
 1, xâŠ¤
i Î²h

â€¢ Let Dh denote the n Ã— n diagonal matrix with entries dih(i = 1, . . . , n) and
let B = Ïƒ2
Î²IS be a S Ã— S diagonal matrix. For each h = 1, . . . , H, update Î²h
sampling
Î²h âˆ¼NS
n xTDhx + Bâˆ’1âˆ’1  xTÎºh

,
 xTDhx + Bâˆ’1âˆ’1o
,
where Îºh is a p-dimensional vector with the j-th entry equal to Ïˆih âˆ’0.5.
4. Update the variance of the loadings
Î£Î»
hh âˆ¼IGa(aÎ» + n/2, bÎ» + 1/2
p
X
j=1
Î»2
jh),
Î£Î³
hh = IGa(aÎ³ + n/2, bÎ³ + 1/2
p
X
j=1
Î³2
jh).
5. Update the loadings Î»j and Î³j (the rows of loading matrix) by sampling from the
independent full conditional : deï¬ne Î£âˆ—as the (d + k) Ã— (d + k) covariance matrix
that collects the diagonal elements of Î£Î» and Î£Î³ and yj the j-th columns of y
16
[Î»j, Î³j] âˆ¼Nd+k{(Î£âˆ’1âˆ—+[Î·j âŠ™Ï„, Ï•j âŠ™Ï„ Ï•]âŠ¤Ïƒâˆ’2
j [Î·j âŠ™Ï„, Ï•j âŠ™Ï„ Ï•])âˆ’1[Î·j âŠ™Ï„, Ï•j âŠ™Ï„ Ï•]âŠ¤Ïƒâˆ’2
j yj,
 Î£âˆ’1âˆ—+ [Î·j âŠ™Ï„, Ï•j âŠ™Ï„ Ï•]âŠ¤Ïƒâˆ’2
j [Î·j âŠ™Ï„, Ï•j âŠ™Ï„ Ï•]
âˆ’1}.
6. Update the cumulative shrinkage parameters for the number of shared factors Ï„.
Deï¬ne indicators zÎ·
h with prior pr(zÎ·
h = l) = wÎ·
l for l = 1, . . . , d. Sample from
pr (zÎ·
h = l) âˆ
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
wÎ·
l
Qn
i=1
Qp
j=1 N

yij; Âµ(0)
ijh, Ïƒ2
j

for
l = 1, . . . , h
wÎ·
l
Qn
i=1
Qp
j=1 N

yij; Âµ(1)
ijh, Ïƒ2
j

for
l = h + 1, . . . , d,
where Âµ(z)
ijh = Pd
lÌ¸=h
âˆšÏ„lÎ»jlÎ·il + zÎ»jhÎ·ih + Pk
l=1
p
Ï„ Ï•
l
âˆšÏˆilÎ³jlÏ•il. Set Ï„h = 1 if zÎ·
h > h,
else Ï„h = 0.
7. Update the cumulative shrinkage parameters for the number of speciï¬c factors Ï„ Ï•.
Deï¬ne indicators zÏ•
h with prior pr(zÏ•
h = l) = wÏ•
l for l = 1, . . . , k. Sample from
pr (zÏ•
h = l) âˆ
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³
wÏ•
l
Qn
i=1
Qp
j=1 N

yij; Âµ(0)
ijh, Ïƒ2
j

for
l = 1, . . . , h
wÏ•
l
Qn
i=1
Qp
j=1 N

yij; Âµ(1)
ijh, Ïƒ2
j

for
l = h + 1, . . . , k,
where Âµ(z)
ijh = Pd
l=1
âˆšÏ„lÎ»jlÎ·il + Pk
lÌ¸=h
p
Ï„ Ï•
l
âˆšÏˆilÎ³jlÏ•il + zâˆšÏˆihÎ³jhÏ•ih. Set Ï„ Ï•
h = 1 if
zÏ•
h > h, else Ï„ Ï•
h = 0.
8. For l = 1, . . . , d âˆ’1, sample vÎ·
l from
(vÎ·
l | âˆ’) âˆ¼Be
(
1 +
d
X
h=1
1 (zÎ·
h = l) , Î±Î· + 1 (zÎ·
h > l)
)
,
set vÎ·
d = 1 and update wÎ·
l = vÎ·
l
Qlâˆ’1
m=1 (1 âˆ’vÎ·
m), for l = 1, . . . , d. Proceed analogously
for vÏ•
l and wÏ•
l .
9. Update Ïˆih, for i = 1, . . . , n and h = 1, . . . , k by sampling from the full conditional
17
(A, Aâˆ—)
(B)
(C)
(D)
(A)
(Aâˆ—)
(C)
(D)
Figure 3: Scenariosâ€™ true sparsity pattern (ï¬rst four plots) and posterior estimate for a
generic replicate (last four plots)
distributions
pr (Ïˆih = u) âˆ
ï£±
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£³

1 âˆ’logitâˆ’1  xâŠ¤
i Î²h
	 Qp
j=1 N

yij; Âµ(u)
ijh, Ïƒ2
j

for u = 0
logitâˆ’1  xâŠ¤
i Î²h
 Qp
j=1 N

yij; Âµ(u)
ijh, Ïƒ2
j

for u = 1
with Âµ(u)
ijh =
d
X
l=1
âˆšÏ„lÎ»jlÎ·il +
k
X
lÌ¸=h
q
Ï„ Ï•
l
p
ÏˆilÎ³jlÏ•il +
q
Ï„ Ï•
h
âˆšuÎ³jhÏ•ih.
10. Reorder the columns of Î“ and Î› and factors Î·, Ï†, and Ïˆ according to active and non
active components of Ï„ and Ï„ Ï•. Move to shared components factors s.t. Ïˆh = 1n.
3
Simulation study
We assess the performance of APAFA in a simulation experiment and its relative merits
with respect to the approach of [19] (TETRIS).
18
We consider two main conï¬guration for what concern the data dimensionality: tall data
where n = 60 and p = 10 and large data where n = 45 and p = 60 and for each of them,
simulate data from diï¬€erent scenarios. Scenario A represents the case in which the original
MSFA model is correctly speciï¬ed.
Namely, we have S = 3 groups with sample sizes
n1 = n2 = n3 = n/3 and each of them has a single diï¬€erent study-speciï¬c factor. The total
number of shared factor is k = 3. The groups labels match exactly the three groups. To
test our modelâ€™s ability to detect the underlying structure, we also ï¬t the model without
providing the labelsâ€”meaning the model is unaware of the existence of three distinct
groups. We refer to this situation as Scenario Aâˆ—. In Scenario B there are S = 3 studies
that are perfectly homogeneous, i.e. ks = 0 for each s = {1, 2, 3} or equivalently Î¦ has
all entries equal to zero. Notably, we ï¬t each competing model providing the information
that S = 3 groups are present. Scenario C presents a complex situation in which there
exist S = 3 studies with partially shared latent factors. Speciï¬cally, study 1 and 2 present
their own factors (Ï•1, Ï•2) as in Scenario A, while a third factor is present for units of
the second and the third study.
Scenario D addresses a situation in which for each of
the three studies a subset of units has no speciï¬c factors. Notably, Scenario D does not
satisfy the identiï¬ability conditions of Theorem 2.5. The left panels of Figure 3 provide
a representation of the true speciï¬c latent factors sparsity pattern. Further details on the
data-generating process are reported in the Supplementary Materials.
Each combination of scenarios and dimensions is replicated independently R = 10 times.
For each of them all the competing methods have been ï¬t. Posterior estimates for all meth-
ods are obtained by running MCMC algorithms for each of the R replicates for 10,000 itera-
tions, discarding the ï¬rst 8,000, and then keeping the last 2,000 values. An implementation
of the Gibbs sampler of Section 2.3 is available at https://github.com/elenabortolato/APAFA.
19
For TETRIS we used the implementation available at https://github.com/igrabski/tetris.
To quantify the performance, we ï¬rst focus on quantities of interest that are invariant
from factor rotations. Speciï¬cally, we consider the posterior mean of the number of ac-
tive factors d and k, along with measures assessing the accuracy of estimating the total
variance matrix â„¦s = Î›Î›âŠ¤+ Î“sÎ“âŠ¤
s + Î£ and partial variance Î›Î›âŠ¤. We ï¬rst compute the
posterior means of these quantities and subsequently compare them to the corresponding
true matrices using RV coeï¬ƒcients [17], deï¬ned as
RVE,T =
Tr(EâŠ¤T)
p
Tr(EâŠ¤E)Tr(T âŠ¤T)
,
where E and T are both symmetric positive deï¬nite matrices representing the estimated
and true matrix, respectively. The RV coeï¬ƒcient takes values in (0,1) with values close to
0 for very dissimilar matrices, and values close to 1 for highly similar matrices.
Tables
1 and 2 report the results. Notably, Table 1 shows that both methods are comparable in
terms of posterior concentration around the true number of factors with a slightly better
performance by APAFA. Notable, TETRIS has not been ï¬tted in scenario Aâˆ—, since no
labels are assigned to units, and in scenario D since the method is not suited to ï¬nd sub-
groups of units with diï¬€erent factor within a given group. In terms of estimation of the
shared variance part, TETRIS has a slightly better or comparable performance in Scenario
A when it is correctly speciï¬ed but has an overall worse performance in all the other cases.
Qualitatively similar results are observed in the n < p case, reported in Table 2. The per-
formance of estimation of the shared variance, often the interest of the analysis, is reported
in Figure 4. In Scenario B, characterized by the absence of speciï¬c factors, TETRIS suc-
cessfully recognizes the shared structure. However, it faces challenges in distinguishing the
contributions to the variance components when group-speciï¬c characteristics are present.
To assess the sparsity of the speciï¬c factors and the induced adaptive partitioning of
20
Table 1: Monte Carlo average (and interquartile range) of the posterior mean number of
factors and RV coeï¬ƒcients for â„¦1, â„¦2, and â„¦3 on several simulation scenarios. Conï¬guration
n > p.
Scen
Method
d
k
â„¦1
â„¦2
â„¦3
A
APAFA
3.00 (1.00)
3.01 (0.23)
0.90 (0.07)
0.88 (0.07)
0.89 (0.05)
TETRIS
3.67 (1.26)
4.91 (0.89)
0.92 (0.07)
0.93 (0.09)
0.88 (0.08)
Aâˆ—
APAFA
4.00 (1.00)
3.00 (1.00)
0.69 (0.13)
0.78 (0.08)
0.75 (0.08)
B
APAFA
3.00 (0.00)
0.00 (0.00)
0.94 (0.04)
0.94 (0.04)
0.94 (0.04)
TETRIS
3.00 (0.00)
0.00 (0.00)
0.90 (0.12)
0.92 (0.05)
0.92 (0.09)
C
APAFA
3.00 (0.00)
3.00 (0.05)
0.89 (0.05)
0.78 (0.04)
0.92 (0.02)
TETRIS
3.00 (1.00)
2.00 (1.01)
0.72 (0.09)
0.75 (0.08)
0.79 (0.05)
D
APAFA
3.00 (0.00)
3.00 (0.12)
0.91 (0.03)
0.88 (0.03)
0.90 (0.04)
units into subgroups that do not deterministically align with the grouping variable x, we
compared the true zero-one pattern used to simulate the synthetic data reported in Fig-
ure 3 with the estimated Ïˆih(xi). To address the column-switch identiï¬ability issue, for
sake of this simulation evaluation when the true value of Î¦ is known, we post-processed the
order of the estimates of Î¦. Details are reported in the Appendix. Figure 5 displays the
ROC curves along with the corresponding area under the curve (AUC) for each replicated
dataset. The results indicate a strong ability of APAFA to detect the true partition induced
by the sparsity pattern of the speciï¬c factors in Î¦ in each scenario.
21
Table 2: Monte Carlo average (and interquartile range) of the posterior mean number of
factors and RV coeï¬ƒcients for â„¦1, â„¦2, and â„¦3 on several simulation scenarios. Conï¬guration
n < p.
Scen.
Method
d
k
â„¦1
â„¦2
â„¦3
A
APAFA
5.00 (0.00)
4.00 (0.00)
0.91 (0.10)
0.82 (0.1)
0.58 (0.19)
TETRIS
2.41 (0.99)
0.64 (0.86)
0.80 (0.09)
0.83 (0.15)
0.86 (0.18)
Aâˆ—
APAFA
5.99 (0.01)
5.97 (0.04)
0.87 (0.06)
0.88 (0.06)
0.86 (0.07)
B
APAFA
5.00 (0.00)
0.00 (1.00)
0.93 (0.06)
0.93 (0.06)
0.93 (0.06)
TETRIS
3.00 (0.00)
0.00 (0.00)
0.86 (0.08)
0.90 (0.05)
0.86 (0.12)
C
APAFA
3.00 (0.75)
3.00 (0.17)
0.87 (0.07)
0.78 (0.05)
0.92 (0.03)
TETRIS
5.00 (2.00)
1.00 (1.95)
0.71 (0.08)
0.72 (0.13)
0.76 (0.09)
D
APAFA
5.00 (0.00)
3.31 (1.00)
0.89 (0.02)
0.91 (0.03)
0.88 (0.04)
4
Real data illustrations
In this section, we analyze two datasets within the motivating contexts of animal co-
occurrence studies and genomics. Rather than replicating previous analyses, we emphasize
speciï¬c aspects allowed by the methodological advancements of the proposed model. In
particular, we focus on qualitative insights that APAFA is able to reveal, oï¬€ering valuable
perspectives that are beyond the reach of existing state-of-the-art methods.
4.1
Bird species occurence dataset
We ï¬rst examine the co-occurrence patterns of p = 50 bird species in Finland analyzing
a data set collected over nine years (2006-2014) across S = 200 locations [43]. Notably,
22
model
APAFA
TETRIS
0.00
0.25
0.50
0.75
1.00
A
A*
B
C
D
setting
RV(Î›Î›^)
n>p
0.00
0.25
0.50
0.75
1.00
A
A*
B
C
D
setting
RV(Î›Î›^)
n<p
Figure 4: Monte Carlo distribution of the RV coeï¬ƒcient for the shared variance component
under conï¬guration n < p (left panel) and n > p (right panel).
part of this datased has been already analyzed in [14]. The average number of sightings
per location in the entire period is of about 5, for a total of n = 914 sites examined during
the years. Shared factors may depend on ambient characteristics as temperature, latitude,
habitat type, and proximity to the ocean. To illustrate the performance of the proposed
model in this context we avoid a ï¬ne model speciï¬cation including these covariates and
rather see if a the estimated latent factors are able to reconstruct some of these information.
We model species presence or absence using the multivariate probit regression model,
yij = 1(zij > 0),
zi = Î›Î·i + Î“Ï•i + Ç«i,
Ç«i âˆ¼N(0, Î£).
We set the hyperparameters governing a priori the number of active common and speciï¬c
23
Specificity
Sensitivity
1.0
0.8
0.6
0.4
0.2
0.0
0.0
0.2
0.4
0.6
0.8
1.0
A
0.5
0.6
0.7
0.8
0.9
1.0
AUC
Specificity
Sensitivity
1.0
0.8
0.6
0.4
0.2
0.0
0.0
0.2
0.4
0.6
0.8
1.0
A*
0.5
0.6
0.7
0.8
0.9
1.0
Specificity
Sensitivity
1.0
0.8
0.6
0.4
0.2
0.0
0.0
0.2
0.4
0.6
0.8
1.0
C
0.5
0.6
0.7
0.8
0.9
1.0
AUC
Specificity
Sensitivity
1.0
0.8
0.6
0.4
0.2
0.0
0.0
0.2
0.4
0.6
0.8
1.0
D
0.5
0.6
0.7
0.8
0.9
1.0
AUC
Figure 5: ROC curves (left) and distribution of the AUC (right) computed over 10 exper-
iments for the posterior probability of assignment of the speciï¬c latent factors to units in
for each scenario (from top to bottom: A, Aâˆ—, C, D), with n > p.
24
factors to Î±Î· = 15 Î±Ï• = 15 respectively. We run the Gibbs sampler for 10,000 iterations
and use the last 5,000 to obtain posterior estimates.
A posteriori, the estimated number of active factors are Ë†d = 5 and Ë†k = 5. Among all
the possible aspects, it is interesting to focus on the interpretation of the main speciï¬c
factors Ï•i1. Notably, this factor takes non-zero values at a subset of locations, eï¬€ectively
grouping observations across multiple sites. This behavior aligns with the possibility of a
partially-shared latent factor similarly to [19]. Interestingly, a post-hoc analysis of these
locations, using a categorical covariate that describes habitat type (not used in the model
ï¬tting), revealed that these locations are predominantly urban. Consistently with this, it
is clear that the proposed method is able to ï¬nd subject-speciï¬c factors associated with
unobserved grouping variables. See the ï¬rst panel of Figure 6 where the speciï¬c factor
matrix Î¦ is reported with the columns ordered by habitat type.
Notably, this ï¬nding
aligns with recent ornithological literature examining the impact of urbanization on bird
interaction [44]. Similar qualitative insights can be appreciated in the second panel of the
same ï¬gure where the columns are ordered with the associated latitude value. Notably,
both the ï¬rst and fourth factors are strongly associated with latitude.
4.2
Immune dataset
As a second example, we analyze a publicly accessible dataset which contains transcriptomic
data for p = 63 genes linked to immune system function in female oncology patients with
ovarian cancer. This dataset can be accessed through the curatedOvarianData package in
Bioconductor [2]. Notably, this same dataset was utilized in the seminal MSFA paper by
[1]. This dataset comprises four studies: GSE9891 and GSE20565, which utilize the same
microarray platform for data acquisition, as well as TCGA and GSE26712, with respective
25
Î¦
environment
Broadleaved
Conifer
Open
Urban
Wetlands
1
2
3
4
5
Î¦
latitude
1
2
3
4
5
Figure 6: Posterior estimate of speciï¬c factors in the bird data example ordered by habitat
type (left) and by latitude (right). Colors range from blue for negative values to red for
positive values.
sample sizes of 285, 140, 578, and 195 units (n = 1198 observations in total).
We ï¬t
model (2) with hyperparameters governing a priori the number of active common and
speciï¬c factors to Î±Î· = 1 Î±Ï• = 4, respectively. The chain was initialized from d = k = 12
active factors. The shape and rate parameters chosen for the Inverse Gamma prior for the
elements of Î›, Î“ and diagonal elements of Î£ were all equal to 2. We run the Gibbs sampler
for 10,000 iterations and use the last 5,000 to obtain posterior estimates.
Posterior analysis identiï¬ed 5 shared factors and 8 speciï¬c factors. Notably, the num-
ber of shared factors is consistent with the ï¬ndings of [1] using the BIC criterion.
As
in the previous section, we compared the resulting matrix of activated speciï¬c factors
26
to covariate data not included in the initial analysis, which are also available in the
curatedOvarianData package. This comparison provides additional validation and insight
into the biological relevance of the factors identiï¬ed. Figure 7 shows that the grouping as-
sociated with the studies is captured within the structure of Î¦. Speciï¬cally, factors Ï•6 and
Ï•7 are clearly related to the group structure provided by x, in particular with the TCGA
and Gaus experiments, while individuals from the GSE9891 and GSE20565 studies, both
using the same microarray platform for data processing, do not present speciï¬c variance
adjustments. The remaining factors are linked to a small number of units each, resulting
in numerous conï¬gurations, far exceeding the initially presumed four. Upon examining
external covariates not included in the analysis, we found that Ï•1 is associated with 6 units
that are distinct from the rest, as they exhibit the clear cell histological subtype. Factor 3
distinguishes the gene expression of 7 patients with mucinous carcinoma from other types
of carcinoma and 8 patients with tumors in the fallopian tubes rather than in the ovary.
Factors 4 and 8 discriminate patients with a tumor classiï¬ed as other (sarcomatoid, adeno-
carcinoma, dysgerminoma) together with some units presenting clearcell and endometrial
tumor (endo, 26 units). See the boxplots in Figure 7 for details. Finally, Figure 8 illustrates
the contributions to the correlation matrix given by Î“hÎ“âŠ¤
h for h = 1, . . . , 8.
5
Discussion
Motivated by the strict grouping imposed by MSFA approaches we introduced a factor
analytic model where the partition induced by speciï¬c latent factors is inï¬‚uenced by ex-
ternal information but is not strictly determined. While the primary interest has been in
using a grouping categorical variable, consistently with the study structure of MSFA, the
proposed APAFA is much general and oï¬€ers the possibility to include also other categorical
27
Î¨
1
2
3
4
5
6
7
8
0
2
4
6
Ï•1
clearcell
âˆ’2
0
2
4
Ï•3
mucinous
âˆ’2
0
2
4
Ï•3
ft
0
1
2
3
4
5
Ï•4
other
âˆ’4
âˆ’2
0
1
Ï•8
endo
Figure 7: Posterior means of each local scale Ïˆih associated to speciï¬c factors (ï¬rst panel)
and distribution of speciï¬c factors (boxplots) compared to external covariates (indicated
with a cross).
or continuous covariates. These act in promoting sparsity in the latent factors as done by
[15] thereby providing a natural way to aid in the modelization of the covariance structure,
and connecting with modelling the heterogeneity in factor regression [21]. Importantly,
the heteroscedasticity driven by covariates is modeled in a rather ï¬‚exible manner, indeed
the link is neither linear nor log-linear, thanks to the local sparsity activation. The pro-
posed solutions proved to be beneï¬cial across many simulation settings, even in cases with
group misspeciï¬cation and complex heterogeneity both between among and within the ï¬xed
groups. Overlapping group structures are nonetheless detectable and may diï¬€er in terms
of ï¬‚exibility and granularity. For example, in Section 4.1, study units are grouped together
28
Î“1Î“1
T
Î“2Î“2
T
Î“3Î“3
T
Î“4Î“4
T
Î“5Î“5
T
Î“6Î“6
T
Î“7Î“7
T
Î“8Î“8
T
Figure 8: Estimated contribution to covariance matrix of speciï¬c factors in the gene ex-
pression case study.
in scenarios where the number of studies closely matches the number of units. Conversely,
in the example of Section 4.2 the model enables to detect higher granularity with respect
to that speciï¬ed a priori, identifying clusters of a dozen units out of a thousand.
SUPPLEMENTARY MATERIAL
Details on simulation study Details on the data generating mechanism, prior hyperpa-
rameters and initialization strategy used in the simulation experiments of Section 3
(pdf ï¬le).
Details on the post-processing step to obtain Figure 5 A short description of the
reordering criterion applied to the estimated factors to evaluate the quality of the
partitioning into subgroups in the experiments of Section 3 (pdf ï¬le).
29
References
[1] De Vito, Roberta, Bellio, Ruggero, Trippa, Lorenzo, and Parmigiani, Giovanni. (2019).
Multi-study factor analysis. Biometrics, 75(1), 337â€“346. Wiley Online Library.
[2] Ganzfried, Benjamin Frederick, Riester, Markus, Haibe-Kains, Benjamin, Risch,
Thomas, Tyekucheva, Svitlana, Jazic, Ina, Wang, Xin Victoria, Ahmadifar, Mahnaz,
Birrer, Michael J, Parmigiani, Giovanni, et al. (2013). curatedOvarianData: clinically
annotated data for the ovarian cancer transcriptome. Database, 2013. Oxford Academic.
[3] Chandra,
Noirrit Kiran,
Dunson,
David B.,
and Xu,
Jason. (2024). Inferring
Covariance Structure from Multiple Data Sources via Subspace Factor Analysis.
Journal of the American Statistical Association, 0(ja), 1â€“25. ASA Website. DOI:
10.1080/01621459.2024.2408777.
[4] Poworoznek, Evan, Ferrari, Federico, and Dunson, David. (2021). Eï¬ƒciently resolv-
ing rotational ambiguity in Bayesian matrix sampling with matching. arXiv preprint
arXiv:2107.13783.
[5] Xu, Maoran, Herring, Amy H., and Dunson, David B. (2023). Identiï¬able and inter-
pretable nonparametric factor analysis. arXiv preprint arXiv:2311.08254.
[6] FrÂ¨uhwirth-Schnatter, Sylvia, Hosszejni, Darjus, and Lopes, Hedibert Freitas. (2024).
Sparse Bayesian factor analysis when the number of factors is unknown. Bayesian Anal-
ysis, 1(1), 1â€“31. International Society for Bayesian Analysis.
[7] Papastamoulis, Panagiotis, and Ntzoufras, Ioannis. (2022). On the identiï¬ability of
Bayesian factor analytic models. Statistics and Computing, 32(2), 23. Springer.
30
[8] De Vito, Roberta, Bellio, Ruggero, Trippa, Lorenzo, and Parmigiani, Giovanni. (2021).
Bayesian multistudy factor analysis for high-throughput biological data. The Annals of
Applied Statistics, 15(4), 1723â€“1741. Institute of Mathematical Statistics.
[9] Wen, Wei, Wu, Chunpeng, Wang, Yandan, Chen, Yiran, and Li, Hai. (2016). Learning
structured sparsity in deep neural networks. Advances in Neural Information Processing
Systems, 29.
[10] Jantre, Sanket, Bhattacharya, Shrijita, and Maiti, Tapabrata. (2023). A comprehensive
study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks.
arXiv preprint arXiv:2308.09104.
[11] Fortuin, Vincent. (2022). Priors in Bayesian deep learning: a review. International
Statistical Review, 90(3), 563â€“591. Wiley Online Library.
[12] Cui, Tianyu, Havulinna, Aki, Marttinen, Pekka, and Kaski, Samuel. (2022). Informa-
tive Bayesian neural network priors for weak signals. Bayesian Analysis, 17(4), 1121â€“
1151. International Society for Bayesian Analysis.
[13] Sell, Torben, and Singh, Sumeetpal Sidhu. (2023). Trace-class Gaussian priors for
Bayesian learning of neural networks with MCMC. Journal of the Royal Statistical Society
Series B: Statistical Methodology, 85(1), 46â€“66. Oxford University Press US.
[14] Schiavon, Lorenzo, Canale, Antonio, and Dunson, David B. (2022). Generalized inï¬nite
factorization models. Biometrika, 109(3), 817â€“835. Oxford University Press.
[15] Schiavon, Lorenzo, Nipoti, Bernardo, and Canale, Antonio. (2024). Accelerated struc-
tured matrix factorization. Journal of Computational and Graphical Statistics.
31
[16] Legramanti, Sirio, Durante, Daniele, and Dunson, David B. (2020). Bayesian cumula-
tive shrinkage for inï¬nite factorizations. Biometrika, 107(3), 745â€“752. Oxford University
Press.
[17] Abdi, HervÂ´e. (2007). RV coeï¬ƒcient and congruence coeï¬ƒcient. Encyclopedia of Mea-
surement and Statistics, 849, 853. Sage Thousand Oaks, CA.
[18] Polson, Nicholas G., Scott, James G., and Windle, Jesse. (2013). Bayesian inference for
logistic models using PÂ´olyaâ€“Gamma latent variables. Journal of the American Statistical
Association, 108(504), 1339â€“1349. Taylor & Francis.
[19] Grabski, Isabella N., De Vito, Roberta, Trippa, Lorenzo, and Parmigiani, Giovanni.
(2023). Bayesian combinatorial MultiStudy factor analysis. The Annals of Applied Statis-
tics, 17(3), 2212. NIH Public Access.
[20] De Vito, Roberta, and Avalos-Pacheco, Alejandra. (2023). Multi-study factor regres-
sion model: an application in nutritional epidemiology. arXiv preprint arXiv:2304.13077.
[21] Avalos-Pacheco, Alejandra, Rossell, David, and Savage, Richard S. (2022). Hetero-
geneous large datasets integration using Bayesian factor regression. Bayesian Analysis,
17(1), 33â€“66. International Society for Bayesian Analysis.
[22] FrÂ¨uhwirth-Schnatter, Sylvia. (2023). Generalized cumulative shrinkage process priors
with applications to sparse Bayesian factor analysis. Philosophical Transactions of the
Royal Society A, 381(2247), 20220148. The Royal Society.
[23] Bhattacharya, Anirban, and Dunson, David B. (2011). Sparse Bayesian inï¬nite factor
models. Biometrika, 98(2), 291â€“306. Oxford University Press.
32
[24] Johnson, Valen E., and Rossell, David. (2010). On the use of non-local prior densities
in Bayesian hypothesis tests. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 72(2), 143â€“170. Oxford University Press.
[25] Rossell, David, and Telesca, Donatello. (2017). Nonlocal priors for high-dimensional
estimation. Journal of the American Statistical Association, 112(517), 254â€“265. Taylor
& Francis.
[26] National Academies of Sciences, Policy and Global Aï¬€airs, Board on Research Data
and Information, Division on Engineering and Physical Sciences, Committee on Applied
and Theoretical Statistics, and Board on Mathematical Sciences, et al. (2019). Repro-
ducibility and replicability in science. National Academies Press.
[27] Irizarry, Rafael A., Hobbs, Bridget, Collin, Francois, Beazer-Barclay, Yasmin D., An-
tonellis, Kristen J., Scherf, Uwe, and Speed, Terence P. (2003). Exploration, normaliza-
tion, and summaries of high density oligonucleotide array probe level data. Biostatistics,
4(2), 249â€“264. Oxford University Press.
[28] Shi, Leming, Reid, Laura H., Jones, Wendell D., Shippy, Richard, Warrington, Janet
A., Baker, Shawn C., Collins, Patrick J., De Longueville, Francoise, Kawasaki, Ernest
S., Lee, Kathleen Y., et al. (2006). The MicroArray Quality Control (MAQC) project
shows inter-and intraplatform reproducibility of gene expression measurements. Nature
Biotechnology, 24(9), 1151â€“1161. Nature Publishing Group.
[29] Ovaskainen, Otso, Tikhonov, Gleb, Norberg, Anna, Blanchet, F. Guillaume, Duan,
Leo, Dunson, David, Roslin, Tomas, and Abrego, Nerea. â€How to make more out of
community data? A conceptual framework and its implementation as models and soft-
ware.â€ Ecology letters, vol. 20, no. 5, pp. 561â€“576, 2017.
33
[30] Griï¬ƒths, Thomas L., and Ghahramani, Zoubin. â€The Indian Buï¬€et Process: an In-
troduction and Review.â€ Journal of Machine Learning Research, vol. 12, no. 4, 2011.
[31] Griï¬ƒn, Maryclare, and Hoï¬€, Peter D. â€Structured Shrinkage Priors.â€ Journal
of Computational and Graphical Statistics, vol. 33, no. 1, pp. 1â€“14, 2024. DOI:
10.1080/10618600.2023.2233577.
[32] Roy, Arkaprava, Lavine, Isaac, Herring, Amy H., and Dunson, David B. â€Perturbed
factor analysis: Accounting for group diï¬€erences in exposure proï¬les.â€ The Annals of
Applied Statistics, vol. 15, no. 3, pp. 1386, 2021.
[33] Chandra, Noirrit Kiran, Dunson, David B., and Xu, Jason. â€Inferring Covariance
Structure from Multiple Data Sources via Subspace Factor Analysis.â€ arXiv preprint
arXiv:2305.04113, 2023.
[34] Neal, Radford M. â€Learning stochastic feedforward networks.â€ Department of Com-
puter Science, University of Toronto, vol. 64, no. 1283, pp. 1577, 1990.
[35] Tang, Charlie, and Salakhutdinov, Russ R. â€Learning stochastic feedforward neural
networks.â€ Advances in Neural Information Processing Systems, vol. 26, 2013.
[36] Kingma, Diederik P., and Welling, Max. â€Stochastic gradient VB and the variational
auto-encoder.â€ In Second International Conference on Learning Representations, ICLR,
vol. 19, pp. 121, 2014.
[37] Sen, Deborshee, Papamarkou, Theodore, and Dunson, David. â€Bayesian neural net-
works and dimensionality reduction.â€ In Handbook of Bayesian, Fiducial, and Frequentist
Inference, pp. 188â€“209, 2024, Chapman and Hall/CRC.
34
[38] Goan, Ethan, and Fookes, Clinton. â€Bayesian neural networks: An introduction and
survey.â€ In Case Studies in Applied Bayesian Data Science: CIRM Jean-Morlet Chair,
Fall 2018, pp. 45â€“87, Springer, 2020.
[39] Arbel, Julyan, Pitas, Konstantinos, Vladimirova, Mariia, and Fortuin, Vincent.
â€A primer on Bayesian neural networks:
review and debates.â€ arXiv preprint
arXiv:2309.16314, 2023.
[40] Kingma, Diederik P., and Welling, Max. â€An introduction to variational autoen-
coders.â€ Foundations and Trends in Machine Learning, vol. 12, no. 4, pp. 307â€“392, 2019,
Now Publishers, Inc.
[41] Nalisnick, Eric, HernÂ´andez-Lobato, JosÂ´e Miguel, and Smyth, Padhraic. â€Dropout as
a structured shrinkage prior.â€ In International Conference on Machine Learning, pp.
4712â€“4722, 2019, PMLR.
[42] Harman, Harry H. Modern Factor Analysis, University of Chicago Press, 1976.
[43] LindstrÂ¨om, ËšAke, Green, Martin, Husby, Magne, KËšalËšas, John Atle, and Lehikoinen,
Aleksi. â€Large-scale monitoring of waders on their boreal and arctic breeding grounds in
northern Europe.â€ Ardea, vol. 103, no. 1, pp. 3â€“15, 2015, BioOne.
[44] Pena, JoËœao Carlos, Ovaskainen, Otso, MacGregor-Fors, Ian, Teixeira, Camila Palhares,
and Ribeiro, Milton Cezar. â€The relationships between urbanization and bird functional
traits across the streetscape.â€ Landscape and Urban Planning, vol. 232, pp. 104685, 2023,
Elsevier.
35
