A SPECTRAL METHOD FOR MULTI-VIEW SUBSPACE LEARNING USING THE
PRODUCT OF PROJECTIONS
RENAT SERGAZINOV
Department of Statistics, Texas A&M University, College Station, TX
ARMEEN TAEB
Department of Statistics, University of Washington, Seattle, WA
IRINA GAYNANOVA∗
Department of Biostatistics, University of Michigan, Ann Arbor, MI
Abstract. Multi-view data provides complementary information on the same set of observations, with
multi-omics and multimodal sensor data being common examples. Analyzing such data typically requires
distinguishing between shared (joint) and unique (individual) signal subspaces from noisy, high-dimensional
measurements. Despite many proposed methods, the conditions for reliably identifying joint and individual
subspaces remain unclear.
We rigorously quantify these conditions, which depend on the ratio of the
signal rank to the ambient dimension, principal angles between true subspaces, and noise levels.
Our
approach characterizes how spectrum perturbations of the product of projection matrices, derived from each
view’s estimated subspaces, affect subspace separation. Using these insights, we provide an easy-to-use and
scalable estimation algorithm. In particular, we employ rotational bootstrap and random matrix theory to
partition the observed spectrum into joint, individual, and noise subspaces. Diagnostic plots visualize this
partitioning, providing practical and interpretable insights into the estimation performance. In simulations,
our method estimates joint and individual subspaces more accurately than existing approaches. Applications
to multi-omics data from colorectal cancer patients and nutrigenomic study of mice demonstrate improved
performance in downstream predictive tasks.
1. Introduction
Multi-view data provides complementary information on the same set of observations, with multi-omics
and multimodal sensor data being common examples. Since each data source or view is potentially high-
dimensional, it is of interest to identify a low-dimensional representation of the signal and further distinguish
whether the signal is shared (joint) across views or view-specific (individual). For example, in § 5.1, we
consider colorectal cancer data with RNAseq and miRNA views. The goal is to identify joint signals from
these two views and validate whether these signals predict cancer subtypes (Guinney et al., 2015).
Canonical correlation analysis (Hotelling, 1936) is a time-tested approach for finding associated signals
across views, but it cannot distinguish individual signals. To address this limitation, many methods have
been developed to explicitly model joint and individual signals (Van Deun et al., 2009; Lock et al., 2013;
Zhou et al., 2015; Yang & Michailidis, 2016; Feng et al., 2018; Shu et al., 2019; Gaynanova & Li, 2019; Park
& Lock, 2020; Murden et al., 2022; Prothero et al., 2024).
Despite significant methodological advances, a crucial theoretical question remains: when can joint and
individual signals be reliably identified from multi-view data? To answer this question, we start with the
∗Corresponding author: irinagn@umich.edu.
1
arXiv:2410.19125v1  [stat.ML]  24 Oct 2024
0
10
20
30
40
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
Angle = 90, SNR = 7
0
10
20
30
40
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
Angle = 75, SNR = 3
0
10
20
30
40
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
Angle = 60, SNR = 2
0
10
20
30
40
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
Angle = 45, SNR = 7
0
10
20
30
40
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
Angle = 30, SNR = 3
0
10
20
30
40
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
Angle = 15, SNR = 2
Figure 1. The alignment between the observed spectrum of the product of projections
(grey histogram) and theoretical predictions from Theorem 3.1 (highlighted blocks) under
varying angles between individual subspaces and signal-to-noise ratios (SNR). The theoret-
ical intervals correspond to the joint, non-orthogonal individual, and orthogonal individual
and noise components. The vertical red lines show the singular values in the noiseless set-
ting.
two-view case and adopt the definition of joint and individual signal subspaces following Lock et al. (2013);
Zhou et al. (2015); Feng et al. (2018). In this framework, the joint signals correspond to the same subspace
and are orthogonal to individual signals, whereas the individual signals do not intersect (albeit may not be
orthogonal). Conceptually, it is clear that (1) separation of joint and individual signals becomes difficult
when the angle between individual subspaces gets small; (2) the total rank of the signal must be small to avoid
noisy directions being mistaken for the joint due to random overlaps. However, the precise quantification of
these concepts has been lacking.
1.1. Our contributions. Our main contribution is an explicit characterization of conditions under which
the joint and individual signals are identifiable, which depend on the signal rank, noise level, and principal
angles between individual subspaces. Our primary insight is that the spectrum perturbations of the product
of projection matrices, derived from each view’s estimated subspaces, affect subspace separation. Figure 1
provides a visual illustration of this theoretical result under varying subspace alignment and noise levels. The
observed spectrum of the product of projections, which corresponds to the cosine of principal angles between
the two subspaces, exhibits a clustering structure that corresponds exactly to joint and individual subspaces.
Specifically, the largest singular values close to 1 correspond to joint subspace, the middle singular values
correspond to non-orthogonal individual subspaces, and the smallest singular values correspond to orthogonal
individual and noise directions. The bars indicate the observed spectrum, whereas the colored areas highlight
the theoretically derived predictions on the spectrum perturbation in alignment with observed clustering. A
lack of interval overlap supports the idea that joint subspaces are distinguishable from individual subspaces
and noise.
Motivated by new theoretical insights on the perturbation of the spectrum of the product of projections,
we propose an estimation approach that quantifies these perturbations in practice with an easy-to-use and
scalable algorithm. First, we develop a novel rotational bootstrap technique to bound the perturbations of
the spectrum corresponding to joint and non-orthogonal individual subspaces (top two clusters). Second, we
utilize results from random matrix theory to quantify the maximal alignment possible by chance, providing
2
a bound on the bottom cluster (orthogonal individual and noisy subspace directions). We use these two
bounds to obtain estimates for the joint and individual subspaces. Further, we generate diagnostic plots
that visualize cluster partitioning with the two bounds overlaid (like the one in Figure 1), providing practical
and interpretable insights regarding the identifiability of joint and individual subspaces.
In simulations, our method is competitive compared to those of Lock et al. (2013); Feng et al. (2018);
Gaynanova & Li (2019); Shu et al. (2019); Park & Lock (2020), particularly in terms of true and false discovery
rates in estimating joint and individual subspaces. Additionally, we test our approach on colorectal cancer
data and nutrigenomic data. The colorectal cancer data includes tumor samples with RNASeq and miRNA
views, where we aim to identify joint components. We assess the quality of these components through their
predictive power for cancer subtype classification. The nutrigenomic data consists of mice with distinct
genotypes and diets, with RNASeq and liver lipid content as the two views. Here, we seek to identify joint
and individual components and link them to genotype and diet labels, respectively. In both datasets, our
approach achieves the best balance between parsimony and predictive accuracy.
1.2. Related works. Lock et al. (2013); Zhou et al. (2015); Gaynanova & Li (2019); Park & Lock (2020);
Yi et al. (2023); Prothero et al. (2024) consider optimization-based approaches for estimating joint and
individual subspaces. While effective, these methods can be computationally slow, particularly with high-
dimensional data, and often rely on ad-hoc techniques for rank selection (such as permutation or bi-cross-
validation). Feng et al. (2018) and Shu et al. (2019) are spectral methods, like ours, and utilize singular
value decomposition to derive joint and individual subspaces. Shu et al. (2019) extends canonical correlation
analysis to account for individual components. However, the individual components are restricted to be
orthogonal, leading to a different model from ours. Feng et al. (2018) uses the spectrum of the average of
projection matrices, which contains information about the principle angles between the two subspace views,
to estimate joint and individual components. The method allows for non-orthogonal individual signals and
is most similar to ours; however, it has certain limitations in comparison, as we discuss below.
Conceptually, Feng et al. (2018) considers the average of projection matrices, while our method is the first
to focus on the product of projection matrices. A key disadvantage of using the average of projection matrices
is that its spectrum exhibits a provably smaller gap (by a factor of less than one-half in noiseless settings)
between individual and joint components than the product. This results in a substantially worse clustering
of singular values in practice, especially in moderate to low signal-to-noise regimes (see e.g, Figure 4 in
Appendix A.4). The lack of clustering in the spectrum considered by Feng et al. (2018) makes separating
the joint directions from the remaining signals more challenging. See § 4 for more numerical comparison and
Appendix A.4 for more technical details.
2. Model
2.1. Notation. For a, b ∈R, let a ∧b = min(a, b), a ∨b = max(a, b).
For a matrix A ∈Rn×p, write
the compact singular value decomposition (SVD) as A = UΣV T , where Σ = diag(σ1(A), σ2(A), . . . ) with
the singular values σ1(A) ≥σ2(A) ≥· · · > 0 in descending order.
We denote σmin(A) = σrank(A)(A),
σmax(A) = σ1(A) as the smallest and largest non-trivial singular values. We denote σ(A) to be the collection
{σ1(A), σ2(A), . . . , σrank(A)(A)}. We use ∥A∥2 = σ1(A) to denote the spectral norm, ∥A∥F to denote the
Frobenius norm and col(A) to denote the column space of A. We write the subspaces in caligraphic, e.g., we
say A ⊂Rn to denote a subspace of Rn. For the projection onto a subspace A, we write PA ∈Rn×n. For
the projection onto the column space of A, we use Pcol(A), which can be defined as PA = UU ⊤. We denote
the direct sum of two subspaces A and B by A ⊕B. We adopt the convention to denote the submatrix:
A[a:b,c:d] represents the a-to-b-th row, c-to-d-th column of matrix A; we also use A[a:b,:] and A[:,c:d] to represent
a-to-b-th full rows of A and c-to-d-th full columns of A, respectively.
2.2. Setup. Let Y1 ∈Rn×p1, Y2 ∈Rn×p2 be observed data matrices corresponding to two views. The rows
of both matrices represent common n subjects, and the columns correspond to the different feature sets
collected for each view.
For example, in the COAD data (see § 5.1), the first view corresponds to the
RNASeq features and the second to the miRNA features. In the nutrigenomic study (see § 5.2), the first
view corresponds to the gene expressions and the second view corresponds to the lipid content of the liver.
We assume that Y1, Y2 are noisy observations of true X1, X2, and consider additive structured signal plus
3
noise decomposition:
Yk = Xk + Zk = Jk + Ik + Zk,
(1)
where the signal Xk is decomposed into joint part across views (Jk) and the view-specific individual part
(Ik), and Zk is the noise matrix with elements identically and independently distributed according to some
distribution.
Model (1) is adopted by many existing methods for multi-view data integration (Lock et al., 2013; Feng
et al., 2018; Gaynanova & Li, 2019; Shu et al., 2019; Park & Lock, 2020; Yi et al., 2023; Prothero et al.,
2024) with some variations in underlying definitions of joint and individual signals.
Here we adopt the
original definition as in Lock et al. (2013). To facilitate exposition, consider singular value decomposition
representation of each part of the signal:
Xk = Jk + Ik = UJkΣJkV ⊤
Jk + UIkΣIkV ⊤
Ik.
(2)
We refer to UJk as joint directions and to UIk as individual directions. Joint directions are joint due to the
equality of corresponding subspaces, that is they correspond to the joint subspace J := col(UJ1) = col(UJ2).
Thus, joint UJ1 can be reconstructed from joint UJ2 by a rotation by some orthogonal matrix. In contrast,
individual directions have non-intersecting subspaces Ik := col(UIk), that is I1∩I2 = {0}, which emphasizes
that they are indeed specific to each view. For identifiability, joint and individual subspaces are assumed to
be orthogonal, that is J ⊥Ik.
However, the individual I1 and I2 are not necessarily orthogonal and can be further decomposed into
orthogonal and non-orthogonal parts, leading to a three-part decomposition (joint, non-orthogonal individual,
orthogonal individual). We further provide explicit characterization and identifiability conditions of this
three-part decomposition of signal column spaces, which is based on Lemma 1 of Feng et al. (2018) and
Proposition 4 of Gaynanova & Li (2019).
Lemma 2.1 (Existence and Identifiability). Given a set of subspaces {col(X1), col(X2)}, there is a unique
set of subspaces {J , N1, N2, O1, O2} such that:
(1) col(Xk) = J ⊕Nk ⊕Ok with individual Ik := Nk ⊕Ok;
(2) J ⊥Ik, Nk ⊥Ol for k ̸= l;
(3) O1 ⊥O2;
(4) T Nk = {0} and maximal principle angle between N1 and N2 is < π/2.
Here, the individual subspace is given by Ik = Nk ⊕Ok, with Nk corresponding to non-orthogonal part
and Ok to orthogonal part. The three-part decomposition can be viewed from the perspective of ordering
the principal angles between subspaces col(X1) and col(X2) (§ 3.1). Specifically, it can be viewed as aligning
the basis of col(X1) and col(X1) into pairs, from perfectly aligned pairs (zero angles, joint subspace J ) to
somewhat aligned pairs (angles strictly between 0 and π/2, non-orthogonal N1 and N2) to orthogonal basis
that can not be paired (π/2 angles, orthogonal O1 and O2).
Our main goal is to distinguish joint subspace from individual based on the noisy observed Yk. Concep-
tually, it is clear that (1) separation of joint and individual signals becomes difficult when the angle between
individual subspaces gets small, i.e. the angle between N1 and N2; (2) the total rank of the signal must be
small to avoid noisy directions being mistaken for the joint signal due to random overlaps. However, the
precise quantification of these concepts has been lacking. Next, we illustrate how the spectrum of the product
of projection matrices allows us to distinguish joint subspaces from the aligned individual ones and provide
theoretical quantification of the spectrum perturbation as a result of noise. We then use these insights to
develop an effective and interpretable estimation approach for joint and individual subspaces.
3. Our method: Product of Projections Decomposition
Our theoretical insights, which subsequently motivate the proposed estimation approach, rely crucially
on the product of projection matrices associated with each view’s subspace. The spectrum of this product
corresponds to the cosines of principal angles between the pair of subspaces (Bjoerck & Golub, 1971). In
noiseless cases, this spectrum can be clustered to perfectly separate joint and individual subspaces (§ 3.1).
In the presence of noise, this spectrum is perturbed, and in § 3.2, we quantify these perturbations with
respect to each cluster, leading to explicit conditions for correct estimation of joint rank and quantification
4
of corresponding subspace estimation error. As theoretical perturbation bounds depend on unknown quan-
tities, in § 3.3 we estimate these bounds using rotational bootstrap and random matrix theory, leading to
a diagnostic plot that visualizes partitioning of the spectrum into clusters and provides insights into the
estimation performance. Throughout, we focus on the case of two views, with Appendix A.3 describing the
extension to the larger number of views.
3.1. Product of projections in the noiseless setting. Consider the noiseless case with Zk = 0n×pk
so that Yk = Xk. Let Xk = UkΣkV ⊤
k
be the compact singular value decomposition of signal Xk. Using
Lemma 2.1, we have:
Pcol(Xk) = UkU ⊤
k = PJ + PIk
⇒
Pcol(X1)Pcol(X2) = PJ + PI1PI2.
Consider the second term PI1PI2. Let UJ ∈Rn×dim(J ) be the matrix whose columns form orthonormal
basis of J , similarly let UN1, UN2, UO1, UO2 be the matrices of the orthonormal bases for N1, N2, O1, O2.
Then,
PI1PI2 = (PN1 + PO1)(PN2 + PO2) = PN1PN2 = UN1U ⊤
N1UN2 = (UN1H) Σ
 K⊤U ⊤
N2

.
Here, HΣKT is the singular value decomposition of U ⊤
N1UN2; diagonal entries of Σ equal cosines of the
non-zero principal angles between I1 and I2. Since I1 ∩I2 = {0}, σmax(Σ) < 1. Thus,
Pcol(X1)Pcol(X2) = [UJ
UN1H]
Idim J
0
0
Σ

[UJ
UN2K]⊤,
where the matrices [UJ
UN1H] and [UJ
UN2K] are orthonormal. Thus, the spectrum of Pcol(X1)Pcol(X2)
clusters into two components: a cluster with singular values equal to one and a cluster with singular values in
the range [σmin(Σ), σmax(Σ)]. Let rclust1 be the number of singular values in the first cluster (corresponding
to singular values one). Then, the joint and individual subspaces can be exactly identified via:
J = span of the first rclust1 left (or right) singular vectors of Pcol(X1)Pcol(X2),
Ik = span of the first rank(Xk) −rclust1 left (or right) singular vectors of Pcol(Xk)(I −PJ ).
Thus, in the noiseless setting, the spectrum of the product of projections Pcol(X1)Pcol(X2) (equivalently,
the set of cosines of principle angles between col(X1) and col(X2)) can be divided into two groups. The first
group contains values equal to one (equivalently, principle angles 0), whose corresponding singular vectors
form a basis for the joint subspace. The second group contains values strictly less than one (equivalently,
principle angles strictly greater than 0), reflecting the degree of alignment between the individual subspaces
I1 and I2. The gap between these two groups, formally defined as one minus the cosine of the smallest
principle angle between I1 and I2, narrows as the alignment between individual subspaces increases.
3.2. Product of projection matrices in a noisy setting. In practice, we do not have direct access
to the subspace col(Xk) because the signal Xk is unknown. Instead, we have to estimate b
Xk from noisy
observations, with a truncated singular value decomposition of Yk being a common approach (Feng et al.,
2018; Shu et al., 2019). However, the spectrum of the estimated product Pcol( b
X1)Pcol( b
X2) differs from the
spectrum of the true product Pcol(X1)Pcol(X2) in several key ways. First, even when a joint signal is present,
the largest singular values in the estimated product are generally below one and decrease further as the noise
level or the dimension of the individual views increases. Second, in the estimated product, the gap between
the singular values corresponding to the joint subspaces and those corresponding to the aligned individual
subspaces is typically smaller than in the true product. This gap also decreases as noise or dimensionality
increases, making it harder to separate joint signals from individual ones, especially when the individual
signals are closely aligned. Finally, even when the individual signals are orthogonal, the estimated product
may still have small but non-zero singular values, with their magnitudes increasing as the noise level rises.
Our main theoretical contribution lies in the precise characterization of perturbations in the spectrum
of c
M := Pcol( b
X1)Pcol( b
X2).
Our result provides general guarantees that hold regardless of the estimation
procedure used to approximate the signals ( b
X1, b
X2).
The quality of the chosen estimation procedure is
reflected in our analysis via deterministic quantities ε1 :=
Pcol(X1) (∆1 + ∆2 + ∆1∆2) Pcol(X2)

2 and ε2 :=
Pcol(X1)∆2 + ∆1Pcol(X2) + ∆1∆2

2 with ∆k := Pcol(Xk) −Pcol( b
Xk). The proof is in the Appendix A.1.
5
Theorem 3.1 (Spectrum of the product of projections). Let τmin := σmin (PN1PN2) and τmax := σmax (PN1PN2).
If rank(X1), rank(X2) > 0, then the spectrum of c
M consists of:
(1) a group of dim(J ) singular values in [1 −ε1 ∨0, 1] ;
(2) a group of rank (PN1PN2) singular values in [(τmin −ε1) ∨0, (τmax + ε2) ∧1] ;
(3) a remaining group of singular values in [0, ε2 ∧1].
This theorem states that in the presence of a signal, the spectrum of the product of projection matrices
c
M organizes into three groups: joint, non-orthogonal individual, and noise directions. The constraint ε1 <
max{1 −ε2, 1 −σmax(PN1PN2) −ε2} ensures that the singular values corresponding to the joint structure
are distinguishable from other clusters, that is the number of singular values above 1−ε1 correctly identifies
the joint rank. Similarly, the constraint ε1 + ε2 < σmin(PN1PN2) ensures that the group of singular values
corresponding to the non-orthogonal individual structure is distinguishable from the noise group. When
both constraints hold, the spectrum of c
M has three non-overlapping clusters corresponding to joint, non-
orthogonal individual, and noise directions. The gap between the clusters decreases with larger values of
ε1, ε2 and larger alignment of the non-orthogonal individual subspaces N1 and N2. Figure 1 illustrates these
theoretical intervals alongside observed singular values of c
M and true singular values of Pcol(X1)Pcol(X2).
The error terms ε1, ε2 depend on the noise level: higher noise leads to larger perturbations ∆k in estimating
each view, and thus larger values of ε1, ε2. Figure 1 shows how a smaller signal-to-noise ratio results in smaller
cluster gaps. The estimated subspace dimension col( b
Xk) also affects the size of ε1, ε2. Underestimating the
rank (e.g., rank( b
X1) < rank(X1)) can lead to large ε1 and ε2, especially if missing directions are aligned
with both col(X1) and col(X2). In fact, ε1 = 1 when a missing direction is part of the joint structure, in
which cases the effective number of large singular values corresponding to the joint structure gets smaller
than the true joint rank. Overestimating the rank (e.g., rank( b
X1) > rank(X1)) can also lead to large errors
due to increased dimensionality of ∆k, making the third component of ε2, ∥∆1∆2∥2, very large, albeit
slight overestimation would still keep those terms small.
Thus, overestimation is generally preferred to
underestimation. We use these insights in § 3.3, providing practical guidelines on marginal rank estimation.
The clustering of the singular values of c
M enables a natural estimator for the signal subspaces. The
joint subspace dimension is estimated as the number of singular values of c
M above 1 −ε1, denoted by brJ .
According to Theorem 3.1, this estimate is exact if ε1 < max{1 −ε2, 1 −σmax(PN1PN2) −ε2}. To estimate
the joint subspace itself, a candidate approach is to use the span of the first brJ left (or right) singular
vectors of c
M. However, this approach arbitrarily favors one view over the other, insisting that the joint
subspace is strictly contained in col( b
X1). To avoid this, we symmetrize the product of projection matrices
by bS := 1
2(c
M + c
M ⊤), which effectively averages the estimated joint directions from each view, yielding a
more accurate estimate. We estimate the joint and individual subspace structures as
b
J := span of the first brJ singular vectors of bS = 1
2

Pcol( b
X1)Pcol( b
X2) + Pcol( b
X2)Pcol( b
X1)

,
bIk := span of the first rank( b
Xk) −brJ left singular vectors of Pcol( b
Xk)(I −P b
J ),
(3)
and the following theorem characterizes their quality with the proof in Appendix A.1.
Theorem 3.2 (Estimation Error of the Joint and Individual Subspaces). Define RJ := Pcol(X1)∆2 +
∆1Pcol(X2) + ∆1∆2 and RIk := PJ ∆k + ∆J Pcol(Xk) + ∆J ∆k, where ∆J := P b
J −PJ and ∆k := Pcol(Xk) −
Pcol( b
Xk). Suppose ε1 < max{1 −ε2, 1 −σmax(PN1PN2) −ε2}. Then,
dim( b
J ) = dim(J )
and
∥P b
J −PJ ∥2 ≤dim(J )1/2∥RJ + R⊤
J ∥2
1 −σmax(PN1PN2)
.
Further, if the rank of each subspace view is correctly estimated, i.e. rank( b
Xk) = rank(Xk),
∥PbIk −PIk∥2 ≤2 dim(Ik)1/2∥RIk∥2.
This theorem quantifies the accuracy of estimated subspaces when the ranks are correctly specified. Since
the spectral norm of the difference between two projection matrices equals the sine of the largest principle
between the corresponding subspaces, our bounds measure the sine of the largest principle angles between
b
J and J , and between bIk and Ik, respectively. For joint subspace estimation, the bound depends on the
6
perturbation term RJ and the maximum alignment σmax(PN1PN2) between the individual subspaces. As
in Theorem 3.1, large perturbations and subspace alignment can lead to significant errors, even with the
correct rank.
3.3. Practical considerations and proposed algorithm. In practice, our primary goal is to use the
observed spectrum of Pcol( b
X1)Pcol( b
X2) to accurately determine the joint rank. Our new theoretical insights
characterize spectrum perturbation in terms of ε1 and ε2, both of which are unknown in practice. Further-
more, the magnitudes of ε1, ε2 depend on the quality of estimates b
Xk. Therefore, we focus on the following
practical questions: (i) how can we obtain estimates of col( b
Xk) for the signal Xk in each view k ∈{1, 2} to
minimize the magnitude of these perturbations? (ii) how can we estimate ε1 in practice to identify the top
cluster of potential joint directions? (iii) how can we bound ε2 to ensure the top cluster contains only joint
directions and avoid overlap from random noise alignment? Below, we outline answers to these practical
questions and summarize our proposed method.
(i) Estimation of col(Xk). To estimate the signal Xk, we construct b
Xk using truncated singular value
decomposition of observed Yk for each view k ∈{1, 2}. We choose the level of truncation based on the
criterion in Gavish & Donoho (2014). Let βk = n/pk be the aspect ratio, and ymed be the median singular
value of Yk.
Then we only keep those singular vectors whose corresponding singular values exceed the
threshold (0.56β3
k −0.95β2
k + 1.82βk + 1.43)bσk. This threshold is designed to obtain an optimal (in mean
squared error) reconstruction of the signal matrix. However, it tends to overestimate the true signal rank.
As we discuss after Theorem 3.1, overestimating the rank of each view is preferred over underestimation.
However, severe overestimation can result in high alignment between random noisy directions, leading to
cluster overlap. We will revisit this point in part (iii) and the diagnostic plot generated at the end of the
algorithm.
(ii) Estimation of ε1 via rotational bootstrap. From Theorem 3.1, the number of singular values of
c
M above 1 −ε1 can be used as an estimate of the joint subspace rank. To estimate ε1, we propose a novel
bootstrap procedure. We first review a rotational bootstrap approach in Prothero et al. (2024) and then
describe our proposal.
Prothero et al. (2024) adopt model (1), and estimate the maximal principle angle between the true
subspace col(Xk) and the subspace formed by truncated singular value decomposition of observed Yk, b
Xk =
bUk bΣk bV ⊤
k , k ∈{1, 2}, with brk := rank( b
Xk). For each bootstrap sample, the data replicate is generated as
Yk,(b) := Uk,(b)bΣkV ⊤
k,(b) + bEk, where bΣk is a matrix of singular values from b
Xk, Uk,(b) ∈Rn×brk, Vk,(b) ∈Rp×brk
are orthonormal, drawn uniformly and independently from the Haar measure, and bEk is the adjusted estimate
of the noise matrix (see Appendix C in Prothero et al. (2024)). Here Xk,(b) = Uk,(b)bΣkV ⊤
k,(b) represents the
signal replicate. Using data replicate Yk,(b), the corresponding truncated singular value decomposition of
rank brk is given by b
Xk,(b) := bUk,(b) bD(b) bV ⊤
k,(b). The resulting projection matrices Pcol(Xk),(b) := Uk,(b)U ⊤
k,(b)
and Pcol( b
Xk),(b) := bUk,(b) bU ⊤
k,(b) represent one bootstrap sample of the true and estimated signal subspaces,
respectively, allowing estimation of principal angles over replications.
In principle, the above procedure could be applied to estimate ε1. However, it leads to underestimation of
the true value, especially when the estimated ranks are small relative to the ambient dimension, as randomly
sampled subspaces are likely to be nearly orthogonal; see Appendix B.2 for numerical demonstrations. This
underestimation is because the procedure in Prothero et al. (2024) is performed separately for each view k.
Thus, the alignment across views is not preserved.
The main novelty of the proposed bootstrap procedure lies in preserving the spectrum of the product of
projections Pcol( b
X1)Pcol( b
X2) in each bootstrap replicate, in addition to preserving the individual spectrum of
b
Xk. Specifically, when generating bootstrap samples for each view, we preserve the alignment between the
two subspaces rather than sampling uniformly at random. Let Σc
M be a diagonal matrix containing singular
values of c
M = Pcol( b
X1)Pcol( b
X2). To generate U1,(b) and U2,(b) randomly but with pre-specified alignment, we
first generate an n×n matrix N with entries drawn independently from a standard normal distribution. We
let U1,(b) consist of the first br1 left singular vectors of N and U2,(b) consist of the next br2 left singular vectors.
We then align these matrices by updating U2,(b) to be U1,(b) cos(Σc
M) + U2,(b) sin(Σc
M). By construction, the
singular values of U ⊤
1,(b)U2,(b) are exactly cos(Σc
M). We then proceed as in Prothero et al. (2024), leading
7
to projection matrices Pcol(Xk),(b) := Uk,(b)U ⊤
k,(b) and Pcol( b
Xk),(b) := bUk,(b) bU ⊤
k,(b), k = 1, 2, for each replicate.
Given B bootstrap replicates, we estimate ε1 as:
bε1
:= 1
B
PB
b=1 ∥Pcol(X1),(b)(∆1,(b) + ∆2,(b) + ∆1,(b)∆2,(b))Pcol(X2),(b)∥2,
(4)
where ∆k,(b) = Pcol(Xk),(b) −Pcol( b
Xk),(b), k ∈{1, 2}. In Appendix B.3, we show numerically that bε1 tends to
overestimate ε1 and becomes more accurate with a larger signal-to-noise ratio.
(iii) Filtering out noise directions via bound on ε2. Due to a low signal-to-noise ratio or rank
over-specification, the estimated subspaces col( b
X1), col( b
X2) can include many noise directions. These noise
directions can exhibit alignment across the two views by random chance, with the degree of alignment
increasing as the number of noise directions grows (due to decreasing signal-to-noise ratio and/or increasing
rank over-specification). Such random alignments are reflected by the third cluster of singular values in
Theorem 3.1. As ε2 becomes large, this noise cluster will overlap with the top cluster corresponding to joint
directions, preventing accurate identification of the joint signal. To diagnose such cases in practice, we could
adapt a similar bootstrap estimation idea to ε2 as we did to ε1 in (ii). However, we found that a simpler
analytical bound can be obtained by utilizing results from random matrix theory.
To motivate the analytical bound, consider a simplified scenario with no individual signals, Pcol(XK) = PJ ,
with Pcol( b
Xk) = PJ + Pnoisek, that is the estimated signal is exactly the joint signal plus orthogonal noise
directions. Then ε2 =
Pcol(X1)∆2 + ∆1Pcol(X2) + ∆1∆2

2 = ∥Pnoise1Pnoise2∥2, representing the maximal
singular value of the product of random projections. While ε2 is generally more complex, the key intuition
remains: only singular values above the threshold expected from random noise alignment can be confidently
identified as the signal.
We explicitly characterize the distribution of the entire spectrum of the product of random projections
Pnoise1Pnoise2 by leveraging results from random matrix theory, which leads to the proposed analytical bound
on ∥Pnoise1Pnoise2∥2. Let Pnoise1 and Pnoise2 denote two independent projection matrices onto random sub-
spaces (drawn from a Haar measure) of Rn with ranks r1 and r2. Let qk := rk/n for k ∈{1, 2} be the aspect
ratio. Then the asymptotic distribution of λ, the squares of the singular values of Pnoise1Pnoise2, as n →∞
(Bouchaud et al., 2007) has a continuous part
(5)
f(λ) =
p
(λ+ −λ)(λ −λ−)
2πλ(1 −λ)
,
λ± = q1 + q2 −2q1q2 ± 2
p
q1q2(1 −q1)(1 −q2),
and two delta peaks A0δ(λ) and A1δ(λ −1) with A0 = 1 −q1 ∧q2 and A1 = (q1 + q2 −1) ∨0. We illustrate
this distribution in Appendix B.4 as a function of q1, q2. As the number of noisy directions increases, so does
the maximal singular value ∥Pnoise1Pnoise2∥2, reaching the value of one at q1 = q2 = 1/2. When q1, q2 < 1/2,
the maximal singular value is given by λ1/2
+
in (5).
To filter out the noise directions when determining the joint rank, we propose considering only singular
values with squares exceeding λ1/2
+ . Since, in practice, the true joint rank is unknown, we use marginal
rank( b
Xk) in place of rk to determine the aspect ratios qk in calculating λ+. This approach can be viewed
as conservative, as it effectively assumes that all the directions in estimated col( b
X1), col( b
X2) are potentially
noisy. Further, we illustrate how these bounds can be used to diagnose model performance in practice.
Full algorithm description and diagnostics. In summary, given the observed data matrices (Y1, Y2),
we obtain estimates of col( b
X1) and col( b
X2) as in (i) to construct c
M = Pcol( b
X1)Pcol( b
X2). We estimate the rank
of the joint subspace brJ as the number of singular values in the spectrum of c
M that are above 1 −bε1 ∨λ1/2
+
where bε1 and λ+ are given by (4) and (5), respectively. We estimate the joint and individual subspaces
according to (3). We summarize the steps in Appendix A.2.
We accompany our algorithm with a diagnostic plot (e.g., Figure 2 for COAD dataset), which illustrates
the spectrum of the product c
M together with the bounds 1−bε1 and λ1/2
+ . The best case scenario is observing
three clear clusters in the spectrum (see e.g., Figure 1 case of Angle = 50, SNR = 22), which provides
distinct separation of joint, non-orthogonal individual, and noise irrespective of threshold estimation. More
commonly, we expect to only see two clusters (noise and individual/joint). If the cluster of top values is
strictly above the threshold, we can be confident those are joint. In case the cluster is in the middle of the
threshold, it likely contains a combination of joint and individual. A lack of clustering structure would imply
8
a large magnitude of noise, preventing accurate separation. In the latter case, driven by our understanding
of the magnitude of ∥Pnoise1Pnoise2∥2 from Appendix B.4, we suggest further decreasing the initial marginal
rank estimates to reduce aspect ratios q1, q2, which subsequently reduce λ+. In § 5, we illustrate these ideas
when analyzing real data.
4. Simulations on Synthetic Data
We use simulated data to evaluate the performance of our proposed product of projections decomposition
method for joint and individual structure identification. For comparison, we consider methods of Lock et al.
(2013), Feng et al. (2018), Gaynanova & Li (2019), Shu et al. (2019) and Park & Lock (2020).
We simulate data from the model (1) with Yk = Jk + Ik + Zk, k = 1, 2, and Xk = Jk + Ik, all representing
n × pk matrices. We let n = 50, p1 = 80, p2 = 100, rank(Jk) = 4, rank(I1) = 5, and rank(I2) = 4. We
sample the singular values of Jk and Ik independently and identically from the uniform distribution. We
sample the row space of Jk and Ik uniformly at random according to the Haar measure. To obtain the
column spaces of Jk and Ik, we first generate an n × n matrix N where each entry is sampled independently
from a normal distribution and let UDV T be the corresponding singular value decomposition; we then set
the column space of Jk to be col(U[:,1:4]) and the column space of I1 to be col(U[:,5:9]); we then generate
the column space of I2 to be col(cos(ϕ)U[:,5:9] + sin(ϕ)U[:,10:14]), where ϕ encodes the largest principle angle
between the individual subspaces col(I1) and col(I2). We sample the entries of the noise matrix Zk ∈Rn×pk
independently and identically from the Gaussian distribution N(0, s2
k). From Theorem 5.32 of Vershynin
(2010), the largest singular value of an n × pk matrix whose entries are sampled from N(0, s2) is bounded
by s(√n + √pk). Therefore, to control the signal-to-noise ratio (SNR), we take the noise variance for each
view to be sk = ∥Xk∥2/(SNR(√n + √pk)).
The theoretical results of § 3 emphasize three main parameters affecting accurate identification of the
subspaces: maximum principle angle between the individual subspaces ϕ, noise level, and the marginal ranks
of estimated subspaces. To test the magnitude of each of these effects, we consider ϕ ∈{30◦, 90◦} and
SNR ∈{0.5, 2}. For the rank estimation, we consider three scenarios: (a) we let each method estimate
the ranks based on the default method’s implementation; (b) we supply marginal ranks that are smaller
than the truth (under-specified rank); (c) we supply marginal ranks that are larger than the truth (over-
specified rank). For (b) and (c), the miss-specification is achieved by sampling uniformly from {1, 2, 3} and
subtracting it from rank(Xk) in the under-specified setting or adding it to rank(Xk) in the over-specified
setting, respectively. Since the implementation of Lock et al. (2013), Gaynanova & Li (2019) and Park &
Lock (2020) does not allow specification of marginal ranks, we only implement (b) and (c) with methods of
Feng et al. (2018), Shu et al. (2019) and our approach.
Choosing the right accuracy measure for unsupervised multi-view decomposition is an open challenge, as
discussed in Gaynanova & Sergazinov (2024). Many approaches rely on the percent of variance explained or
the marginal view reconstruction. However, the percent of variance explained is maximized by a separate
singular value decomposition for each view, favoring a lack of joint structure and larger marginal ranks. On
the other hand, signal reconstruction error, though unbiased, does not account for structural assumptions or
misclassification of the joint directions as individual signals. We propose a new accuracy measure using the
subspace version of the false and true discovery metric from Taeb et al. (2020). For an estimated subspace bS
of a true subspace S, we evaluate the true positive proportion (TPP) and false discovery proportion (FDP).
These are then combined into a single metric, the F-score:
TPP = tr(P b
SPS)
dim(S) ,
FDP = tr
 (I −PS) P b
S

dim( bS)
,
F-score = 2 · (1 −FDP) · TPP
1 −FDP + TPP .
(6)
We compute the F-scores for joint and individual subspace estimates and take their average F-score as a
final accuracy measure, with larger values indicating better performance.
Table 1 displays mean F-score values across 50 replications for each combination of angle, signal-to-noise
ratio, and rank specification.
The results align with what is expected based on Theorem 3.1: for each
method, the performance deteriorates with larger alignment between individual subspaces (smaller angle),
and smaller signal-to-noise ratio. In the large signal-to-noise ratio case, rank under-specification is worse
than rank over-specification, which is also in agreement with the theory. In the low signal-to-noise ratio case,
the results for default rank estimation are worse than those with controlled rank miss-specification, which we
9
Table 1. Mean F-scores (6) on simulated data, larger values correspond to more accurate
estimation of joint and individual subspaces. All values are multiplied by 10. The results
are averaged across 50 iterations. The maximum standard error of the mean is 0.03. We
highlight the best result in bold and underline second best result. We denote by (∗) methods
that do not allow supplying marginal ranks for each view. We denote Lock et al. (2013) as
JIV, Feng et al. (2018) as AJI, Gaynanova & Li (2019) as SLI, Shu et al. (2019) as DCC,
Park & Lock (2020) as UNI, and our method as PPD.
Under-specified rank
Estimated rank
Over-specified rank
SNR = 2
SNR=0.5
SNR = 2
SNR=0.5
SNR = 2
SNR=0.5
90◦
30◦
90◦
30◦
90◦
30◦
90◦
30◦
90◦
30◦
90◦
30◦
JIV
–
–
–
–
8.27
7.88
4.74
4.33
–
–
–
–
AJI
8.54
7.65
5.07
4.56
9.81
8.55
3.85
3.67
9.17
7.70
5.01
4.70
SLI∗
–
–
–
–
9.61
7.84
1.49
1.57
–
–
–
–
DCC
8.61
7.59
5.10
4.52
9.80
8.28
3.78
3.60
9.14
7.53
4.93
4.46
UNI∗
–
–
–
–
6.73
6.05
4.34
4.05
–
–
–
–
PPD
8.44
7.86
5.12
4.61
9.91
9.75
3.86
3.67
9.19
8.01
5.07
4.67
suspect is due to significant challenges in estimating rank correctly in this setting, with default estimation
likely leading to even larger rank estimation error than controlled miss-specification. In terms of comparison
across methods, it is difficult to imagine a uniformly best-performing one since different approaches have
different strengths under different scenarios. However, our proposed method achieves the highest accuracy
in most settings, followed closely by Feng et al. (2018)’s method. A more detailed empirical and theoretical
comparison with Feng et al. (2018) is in Appendix A.4, where we show that our method can more effectively
distinguish between joint and individual signals.
5. Real Data Experiments
5.1. Colorectal cancer data. We analyze colorectal cancer data from the Cancer Genome Atlas with two
distinct data views: RNA sequencing (RNAseq) normalized counts and miRNA expression profiles (Guinney
et al., 2015). We process the data as in Zhang & Gaynanova (2022), leading to p1 = 1572 RNAseq and
p2 = 375 miRNA variables. Colorectal cancer has been classified into four consensus molecular subtypes
based on gene expression data (Guinney et al., 2015), and we obtain the labels from the Synapse platform
(Synapse ID syn2623706). After filtering for complete data, we arrive at n = 167 samples from two views
with cancer subtype information. Our goal is to extract joint and individual signals and determine which
signals are most predictive for cancer subtypes. We apply the proposed product of projections methods and
compare it with other methods from § 4. We also consider standard principal component analysis for each
view as the baseline.
First, we determine the ranks corresponding to joint and individual structures for each method. For the
proposed approach, we estimate the marginal ranks using the truncation method of Gavish & Donoho (2014).
The scree plot for each view is shown in Figure 2 (left), resulting in ranks of 40 and 42, respectively. We
then apply the proposed method, generating the diagnostic plot of the observed product of the projections
spectrum (Figure 2, middle). In the plot, the blue region represents the noise cutoff λ1/2
+ , and the green
region marks the top cluster based on the bootstrap estimate of ε1. Observe that the spectrum does not
show clear clustering, with the noise directions falling well within the region identified by the bootstrap.
This inconsistency is likely due to the high rank-to-dimension ratios, which inflate both bounds and the
low signal-to-noise ratio, which affects the bootstrap estimate. To address the inclusion of noise directions,
we re-examined the scree plot presented in Figure 2. The initial marginal ranks appeared too high, so we
identified the elbow point at rank 16 for each view based on the scree plot.
After adjusting the ranks, we re-applied the proposed approach with the updated diagnostic plot (Figure 2,
right). The updated plot shows more distinct clustering, with a clear separation between lower and higher
singular values. Both the bootstrap and noise bounds support this observation by identifying the cluster
10
0
50
100
150
200
0
50
100
150
Singular Value Index
Singular Value
View
mRNA
RNA
Scree plot
0
50
100
0.00
0.25
0.50
0.75
1.00
Singular Value
Frequency
Diagnostic plot, estimated ranks
0
50
100
150
0.00
0.25
0.50
0.75
1.00
Singular Value
Frequency
Diagnostic plot, manual ranks
Figure 2. Left: Scree plot for RNASeq and mRNAseq data. Blue point denotes the cutoff
suggested by Gavish & Donoho (2014). Red point indicates our suggested cutoff. Middle
/ Right: Spectrum of the product of projection matrices onto each view for two different
marginal ranks. The blue region indicates the random direction bound, [0, λ1/2
+ ], used to
filter out the noise directions. The green region is given by [1−bε1, 1] where bε1 is a bootstrap
estimate of ε1.
Table 2. Mean cancer subtype misclassification rates in percentages together with the
minimum principle angle between estimated individual subspaces.
We denote principle
components analysis by PCA, Lock et al. (2013) as JIV, Feng et al. (2018) as AJI, Gaynanova
& Li (2019) as SLI, Shu et al. (2019) as DCC, Park & Lock (2020) as UNI, and our method
as PPD.
Joint (J )
RNAseq (I1)
miRNA (I2)
Angle
Rank
Error
Rank
Error
Rank
Error
∠(I1, I2)
PCA
–
–
16
0
16
6.6
2
JIV
2
44.3
20
0
13
30.5
16
AJI
9
7.2
7
50.9
8
61.7
57
SLI
32
0
41
43.7
10
59.9
90
DCC
9
7.8
7
53.3
7
66.5
90
UNI
1
55.1
5
0
1
44.3
74
PPD
8
8.9
8
53.9
8
62.9
52
of high singular values as the joint structure.
This analysis highlights the effectiveness of the proposed
diagnostic plot, enabling practitioners to fine-tune parameters to obtain a more confident separation of joint
directions.
Next, we evaluate the predictive power of the estimated joint and individual components for cancer
subtype classification using multinominal regression. For a fair comparison, we use the same marginal ranks
as ours for Feng et al. (2018), Shu et al. (2019), and principal component analysis. For other methods, we
use their default rank estimation procedures, as custom ranks could not be specified. We assess classification
performance separately for the joint and the individual subspaces. We expect that the joint components
will be closely related to the cancer subtype, while the individual components would capture differences
between RNA and mRNA platforms unrelated to subtypes.
Table 2 shows the estimated ranks, miss-
classification error rates, and the smallest angle between individual subspaces for each method. As expected,
most methods perform better using joint components than individual ones. The exceptions are principal
components analysis and the methods of Lock et al. (2013) and Park & Lock (2020). For principle components
analysis and Lock et al. (2013), the angles between individual subspaces are small, suggesting that the joint
structure may be mistaken for the individual. For Park & Lock (2020), the estimated marginal rank of
miRNA view is much smaller compared to all other approaches, so it is possible that the joint structure was
11
0
10
20
30
40
0
10
20
30
40
Singular Value Index
Singular Value
colour
Gene
Lipid
Scree plot
0
10
20
30
0.00
0.25
0.50
0.75
1.00
Singular Value
Frequency
Diagnostic plot, estimated ranks
0
10
20
30
0.00
0.25
0.50
0.75
1.00
Singular Value
Frequency
Diagnostic plot, manual ranks
Figure 3. Left: Scree plot for gene expression and lipid content data. Blue point denotes
the cutoff suggested by Gavish & Donoho (2014). Red point indicates our suggested cutoff.
Middle / Right: Spectrum of the product of projection matrices onto each view for two
different marginal ranks. The blue region indicates the random direction bound, [0, λ1/2
+ ],
used to filter out the noise directions. The green region is given by [1 −bε1, 1] where bε1 is a
bootstrap estimate of ε1.
Table 3. Mean genotype and diet misclassification rates in percentages together with the
minimum principle angle between estimated individual subspaces.
We denote principle
components analysis by PCA, Lock et al. (2013) as JIV, Feng et al. (2018) as AJI, Gaynanova
& Li (2019) as SLI, Shu et al. (2019) as DCC, Park & Lock (2020) as UNI, and our method
as PPD.
Joint (J )
Gene (I1)
Lipid (I2)
Angles
Rank
Type
Diet
Rank
Type
Diet
Rank
Type
Diet
∠(I1, I2)
PCA
–
–
–
3
0
60
4
0
0
22
JIV
2
0
27.5
3
25
57.5
3
30
12.5
44
AJI
2
0
55
1
52.5
77.5
2
47.5
10
66
SLI
6
0
0
8
45
42.5
1
50
57.5
90
DCC
3
0
37.5
1
45
65
1
45
47.5
85
UNI
2
0
62.5
1
52.5
77.5
2
35
0
72
PPD
1
0
80
2
37.5
57.5
3
30
0
44
missed in that view. For joint subspace, Gaynanova & Li (2019) achieves the lowest misclassification rate;
however, the corresponding estimated ranks are much higher than for other approaches. In comparison, our
method offers a more parsimonious representation of the joint components while retaining strong predictive
power, effectively balancing simplicity and informativeness. Feng et al. (2018) and Shu et al. (2019) achieve
slightly better miss-classification rates (by 1%) but at the cost of including an additional joint component.
From the angles between the individual subspaces reported in Table 2, we see that this additional component
corresponds to aligned individual directions with an angle of 52 degrees between the views, which is excluded
from the joint subspace in our approach. Indeed, in Figure 2, this angle corresponds to the rotated individual
directions by falling exactly in-between our noise bound, λ1/2
+ , and bootstrap bound, 1 −ε1. This again
highlights the value of the diagnostic plot and underscores the clear subspace separation achieved by our
method.
5.2. Nutrigenomic mice data. The nutrigenomic mice data (Martin et al., 2007) contain measurements
from n = 40 mice from two views: gene expressions (p1 = 120) and lipid content of the liver (p2 = 21).
The data are available as part of the multi-omics R package (Rohart et al., 2017). We process the data as
12
in Yuan et al. (2022). The mice were selected from two genotypes (wild-type and PPAR-α mutant). They
were administered five distinct diets consisting of corn and colza oils (ref), a saturated fatty acid diet of
hydrogenated coconut oil (coc), an Omega6-rich diet of sunflower oil (sun), an Omega3-rich diet of linseed
oil (lin), and a diet with enriched fish oils (fish). Our goal is to identify the joint and individual components
across views. We expect that the joint components would be predictive of the mice genotype, while individual
components for the lipid view would be predictive of the diet.
As in § 5.1, we start by determining the ranks for each view. The truncation method of Gavish & Donoho
(2014) leads to ranks 11 and 7 for gene expressions and lipid concentrations, respectively. Figure 3, left,
shows the corresponding scree plot, which suggests that the selected ranks may be too large. Our proposed
diagnostic plot based on these ranks is displayed in Figure 3, middle. The noise bound is quite large, and
overlaps with the bootstrap bound, suggesting that the rank reduction would be beneficial in filtering the
noise directions. Previous analysis of these data has used ranks 3 for genes and 4 for lipids (Yuan et al.,
2022). These ranks appear to better align with an elbow on the scree plot and lead to a diagnostic plot with
a clear clustering structure and lack of overlap (Figure 3, right). We thus use these ranks for subsequent
analysis.
We evaluate the predictive power of the estimated joint and individual components separately for the
classification of genotype and diet based on multinomial linear regression. Table 3 shows that the identified
joint components for all methods perfectly separate the genotype, with our method providing the most
parsimonious representation of rank equal to one. Predicting the diet from the individual lipid features
turns out to be a harder task, with only our method and that of Park & Lock (2020) achieving the perfect
classification. While principle components analysis also achieves perfect diet classification, the angle between
individual components of 22 degrees is small and suggests a missing joint structure. A closer look at Figure 3
shows that the perfect classification of diet by our method is based on the rotated individual direction, which
is not part of our joint estimate. In contrast, Feng et al. (2018)’s method treats this direction as a joint
component, leading to worse diet classification accuracy. As in the example from Section 5.1, the diagnostic
plot proves valuable in evaluating the potential over-specification of ranks and providing confidence in the
separation of joint and individual components.
6. Conclusion
In this work, we describe a multi-view data decomposition method that naturally derives from the iden-
tifiability conditions for joint and individual subspaces. The advantages of our approach include subspace
estimation guarantees, scalability, and interpretable diagnostics.
Our work opens several avenues for future research. First, it would be valuable to rigorously extend our
method to the multi-view setting with partially shared joint signals. Our current extension in Appendix A.3
assumes fully shared joint components and selects the joint rank via pairwise testing.
Second, further
theoretical developments could elucidate the properties of our rotational bootstrap approach and refine
the asymptotic characterization of the noise bound to account for non-random subspaces. Finally, since
many practical applications emphasize predictive performance, extending our method to the semi-supervised
setting, as in Park et al. (2021); Palzer et al. (2022), would be beneficial.
7. Acknowledgements
Armeen Taeb is supported by NSF DMS-2413074 and by the Royalty Research Fund at the University of
Washington. Irina Gaynanova’s research was supported by NSF DMS-2422478.
References
Bjoerck, A. & Golub, G. H. (1971). Numerical methods for computing angles between linear subspaces.
In Milestones in Matrix Computation.
Bouchaud, J.-P., Laloux, L., Miceli, M. A. & Potters, M. (2007). Large dimension forecasting
models and random singular value spectra. The European Physical Journal B 55, 201–207.
Feng, Q., Jiang, M., Hannig, J. & Marron, J. (2018). Angle-based joint and individual variation
explained. Journal of multivariate analysis 166, 241–265.
13
Gavish, M. & Donoho, D. L. (2014). The optimal hard threshold for singular values is 4/sqrt(3). IEEE
Transactions on Information Theory 60, 5040–5053.
Gaynanova, I. & Li, G. (2019). Structural learning and integrative decomposition of multi-view data.
Biometrics 75, 1121–1132.
Gaynanova, I. & Sergazinov, R. (2024).
Comments on: Data integration via analysis of subspaces
(divas). TEST , 1–8.
Guinney, J., Dienstmann, R., Wang, X., De Reynies, A., Schlicker, A., Soneson, C., Marisa,
L., Roepman, P., Nyamundanda, G., Angelino, P. et al. (2015). The consensus molecular subtypes
of colorectal cancer. Nature medicine 21, 1350–1356.
Hotelling, H. (1936). Relations between two sets of variates. Biometrika 28(3-4), 321–377.
Lock, E. F., Hoadley, K. A., Marron, J. S. & Nobel, A. B. (2013). Joint and individual variation
explained (jive) for integrated analysis of multiple data types. The annals of applied statistics 7, 523.
Martin, P. G., Guillou, H., Lasserre, F., D´ejean, S., Lan, A., Pascussi, J.-M., SanCristobal,
M., Legrand, P., Besse, P. & Pineau, T. (2007). Novel aspects of pparα-mediated regulation of lipid
and xenobiotic metabolism revealed through a nutrigenomic study. Hepatology 45, 767–777.
Murden, R. J., Zhang, Z., Guo, Y. & Risk, B. B. (2022). Interpretive jive: Connections with cca and
an application to brain connectivity. Frontiers in Neuroscience 16, 969510.
Palzer, E. F., Wendt, C. H., Bowler, R. P., Hersh, C. P., Safo, S. E. & Lock, E. F. (2022).
sjive: Supervised joint and individual variation explained. Computational statistics & data analysis 175,
107547.
Park, J. Y. & Lock, E. F. (2020). Integrative factorization of bidimensionally linked matrices. Biometrics
76, 61–74.
Park, S., Ceulemans, E. & Van Deun, K. (2021). Sparse common and distinctive covariates regression.
Journal of Chemometrics 35, e3270.
Prothero, J., Jiang, M., Hannig, J., Tran-Dinh, Q., Ackerman, A. & Marron, J. (2024). Data
integration via analysis of subspaces (divas). TEST , 1–42.
Rohart, F., Gautier, B., Singh, A. & Lˆe Cao, K.-A. (2017). mixomics: An r package for ‘omics feature
selection and multiple data integration. PLoS computational biology 13, e1005752.
Shu, H., Wang, X. & Zhu, H. (2019). D-cca: A decomposition-based canonical correlation analysis for
high-dimensional datasets. Journal of the American Statistical Association .
Taeb, A., Shah, P. & Chandrasekaran, V. (2020). False discovery and its control in low rank estimation.
Journal of the Royal Statistical Society Series B: Statistical Methodology 82, 997–1027.
Van Deun, K., Smilde, A. K., Van Der Werf, M. J., Kiers, H. A. & Van Mechelen, I. (2009). A
structured overview of simultaneous component based data integration. BMC bioinformatics 10, 1–15.
Vershynin, R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027 .
Yang, Z. & Michailidis, G. (2016). A non-negative matrix factorization method for detecting modules in
heterogeneous omics multi-modal data. Bioinformatics 32, 1–8.
Yi, S., Wong, R. K. W. & Gaynanova, I. (2023). Hierarchical Nuclear Norm Penalization for Multi-View
Data Integration. Biometrics 79, 2933–2946.
Yu, Y., Wang, T. & Samworth, R. J. (2015). A useful variant of the davis–kahan theorem for statisticians.
Biometrika 102, 315–323.
Yuan, D., Zhang, Y., Guo, S., Wang, W. & Gaynanova, I. (2022). Exponential canonical correlation
analysis with orthogonal variation. arXiv preprint arXiv:2208.00048 .
Zhang, Y. & Gaynanova, I. (2022).
Joint association and classification analysis of multi-view data.
Biometrics 78, 1614–1625.
Zhou, G., Cichocki, A., Zhang, Y. & Mandic, D. P. (2015). Group component analysis for multiblock
data: Common and individual feature extraction. IEEE transactions on neural networks and learning
systems 27, 2426–2439.
14
Appendix A. Techinical details
A.1. Proofs of main results.
Lemma A.1. Let X, Y ∈Rn×n. If X⊤Y = 0 or XY ⊤= 0, then
(1) σr(X + Y ) ≥max(σr(X), σr(Y )) for all r ≥1 (assuming σ1 ≥σ2 ≥....0)
(2) σ2
1(X + Y ) ≤σ2
1(X) + σ2
1(Y )
Proof. Without loss of generality, consider X⊤Y = 0 (proof is analogous for XY ⊤= 0). Consider
(X + Y )⊤(X + Y ) = X⊤X + Y ⊤Y.
Thus, we have the eigenvalues satisfy
λr{(X + Y )⊤(X + Y )} = λr(X⊤X + Y ⊤Y ).
Weyl’s inequality states:
λn(Y ⊤Y ) + λr(X⊤X) ≤λr(X⊤X + Y ⊤Y ) ≤λ1(Y ⊤Y ) + λr(X⊤X)
λn(X⊤X) + λr(Y ⊤Y ) ≤λr(X⊤X + Y ⊤Y ) ≤λ1(X⊤X) + λr(Y ⊤Y )
λr(X⊤X + Y ⊤Y ) ≤min

λr(X⊤X) + λ1(Y ⊤Y ), λ1(X⊤X) + λr(Y ⊤Y )
	
Recall that σ2
r(A) = λr(A⊤A). Since λn(Y ⊤Y ) ≥0, λn(X⊤X) ≥0, we get (1). Similarly, since λ1(Y ⊤Y ) ≥
0, λ1(X⊤X) ≥0, taking r = 1 yields (2).
□
Theorem A.2 (Spectrum of the product of projection matrices). Let c
M := Pcol( b
X1)Pcol( b
X2). Suppose that
rank(X1), rank(X2) > 0. Define
ϵ1 :=
Pcol(X1) (∆1 + ∆2 + ∆1∆2) Pcol(X2)

2 ,
ϵ2 :=
Pcol(X1)∆2 + ∆1Pcol(X2) + ∆1∆2

2
with ∆k := Pcol(Xk) −Pcol( b
Xk). Then the spectrum of c
M consists of:
(1) a group of dim(J ) singular values in [1 −ϵ1 ∨0, 1] ;
(2) a group of rank (PN1PN2) singular values in [(σmin (PN1PN2) −ϵ1) ∨0, (σmax (PN1PN2) + ϵ2) ∧1];
(3) a remaining group of singular values in [0, ϵ2 ∧1].
Proof. First, we know from Section 3.1 that
(7)
Pcol(X1)Pcol(X2) = PJ + PI1PI2 = PJ + PN1PN2 = [UJ
UN1H]

Idim J
0
0
Σ

[UJ
UN2K]⊤,
where Σ is diagonal corresponding to the cosines of the non-zero principle angles between I1 and I2. Thus,
σr(Pcol(X1)Pcol(X2)) = 1 for 1 ≤r ≤dim(J ) and σr(Pcol(X1)Pcol(X2)) = σr−dim(J )+1 for dim(J ) + 1 ≤r ≤
dim(J ) + rank(PI1PI2).
The upper bound for each cluster follows directly from applying Weyl’s inequality as
σr(c
M) = σr(Pcol(X1)Pcol(X2) + Pcol(X1)∆2 + ∆1Pcol(X2) + ∆1∆2)
≤σr(Pcol(X1)Pcol(X2)) + ϵ2.
To get the lower bound, we rewrite c
M as:
c
M = Pcol(X1)Pcol(X2) + Pcol(X1)∆2 + ∆1Pcol(X2) + ∆1∆2
= Pcol(X1)(I + ∆1 + ∆2 + ∆1∆2)Pcol(X2)
|
{z
}
:=A1
+ Pcol(X1)(∆2 + ∆1∆2)(I −Pcol(X2))
|
{z
}
:=A2
+ (I −Pcol(X1))(∆1 + ∆1∆2)Pcol(X2)
|
{z
}
:=A3
+ (I −Pcol(X1))∆1∆2(I −Pcol(X2))
|
{z
}
:=A4
Note that by construction we have:
(1) (A1 + A2)⊤(A3 + A4) = 0,
(2) A1A⊤
2 = 0.
15
Then it holds that
σr(c
M) ≥σr(A1 + A2)
(Lemma A.1)
≥σr(A1)
(Lemma A.1)
≥σr(Pcol(X1)Pcol(X2)) −ϵ1.
(Weyl’s inequality)
□
Theorem A.3 (Estimation Error). Suppose ϵ1 < max{1 −ϵ2, 1 −σmax(PN1PN2) −ϵ2}. Define RJ :=
Pcol(X1)∆2 + ∆1Pcol(X2) + ∆1∆2 and RIk := PJ ∆k + ∆J Pcol(Xk) + ∆J ∆k, where ∆J := P b
J −PJ and
∆k := Pcol(Xk) −Pcol( b
Xk) . Then, dim( b
J ) = dim(J ) and the deviation of the estimated joint subspace from
the true joint subspace is bounded by
∥P b
J −PJ ∥2 ≤dim(J )1/2 RJ + R⊤
J

2
1 −σmax(PN1PN2)
.
Furthermore, if the rank of each subspace view is correctly estimated, i.e. rank( b
Xk) = rank(Xk), then, the
deviation of the estimated individual subspace from the true individual subspace is bounded by
∥PbIk −PIk∥2 ≤2 dim(Ik)1/2∥RIk∥F .
In the proof of Theorem 3.2, we use the adaptation of Davis-Kahan Theorem introduced in Yu et al.
(2015). The theorem is stated in terms of the sin Θ distance between the subspaces. To apply the theorem,
we state the definition of the sin Θ distance and principle angles here. Specifically, the sin Θ distance is used
to measure the difference between two p × r orthogonal columns bV and V . Suppose the singular values of
V ⊤bV are σ1 ≥σ2 ≥· · · ≥σr ≥0. Then we call
Θ(bV , V ) = diag(cos−1(σ1), cos−1(σ2), . . . , cos−1(σr))
as the principle angles. A quantitative measure of distance between the column spaces of bV and V is then
∥sin Θ(bV , V )∥2 or ∥sin Θ(bV , V )∥F . Note that the difference of projection matrices, (Pcol(bV −Pcol(V )), is
related to the sin Θ via the equality ∥Pcol(bV ) −Pcol(V )∥2 = ∥sin Θ(bV , V )∥2
Proof. We treat the joint and individual components separately.
Joint components. To apply Davis-Kahan theorem, we show that there exits an eigengap in the population-
level model S = 1
2
 Pcol(X1)Pcol(X2) + Pcol(X2)Pcol(X1)

.
From Section 3.1, we know that S = PJ + 1
2 (PN1PN2 + PN2PN1). Fix an orthonormal basis, UJ , for PJ .
Since J ⊥Nk, we have that SUJ = UJ . Hence, S has at least dim(J ) eigenvalues that are equal to 1 with
corresponding eigenvectors forming J . Now fix any orthonormal vector u ∈span(U ⊥
J ). Then we have:
∥Su∥2 =
PJ u + 1
2 (PN1PN2 + PN2PN1) u

2
≤∥PN1PN2∥2
(by triangle-inequality)
< 1
(since I1 ∩I2 = {0})
Therefore, the spectrum of S becomes σr(S) = 1 for 1 ≤r ≤dim(J ) and σr(S) ≤σmax(PN1PN2) < 1 for
r > dim(J ).
From here, applying Davis-Kahan, we have that:
∥sin Θ(U b
J , UJ )∥2 ≤2 dim(J )1/2∥bS −S∥2
1 −σmax(PN1PN2)
16
Algorithm 1: Algorithmic implementation of our method
Input: Data matrices from two views Y1 ∈Rn×p1 and Y2 ∈Rn×p2.
(1) Estimate marginal view signals b
Xk by truncated singular value decomposition of Yk with rank
as in Gavish & Donoho (2014)
(2) Compute perturbation bound bε1 by using rotational bootstrap in (4).
(3) Compute noise bound λ+ based on (5) with qk = rank( b
Xk)/n.
(4) Estimate joint rank brJ = |{σ(c
M) > λ1/2
+
∨(1 −bε1)}| with c
M = Pcol( b
X1)Pcol( b
X2)
(5) Estimate joint subspace b
J as the span of the first brJ singular vectors of bS = (c
M + c
M ⊤)/2
(6) Estimate individual subspaces bIk as span of the first left rank( b
Xk) −brJ singular vectors of
Pcol( b
Xk)(I −P b
J ).
Output: Joint subspace b
J and individual subspaces bI1 and bI2; a histogram of the spectrum of c
M
overlaid with λ1/2
+ , 1 −bε1 and the distribution f(λ) in (5).
Putting everything together, we have:
∥P b
J −PJ ∥2 = ∥sin Θ(U b
J , UJ )∥2
≤
2 dim(J )1/2 bS −S

2
1 −σmax(PN1PN2)
≤dim(J )1/2 Pcol(X1)∆2 + ∆1Pcol(X2) + ∆1∆2 + ∆2Pcol(X1) + Pcol(X2)∆1 + ∆2∆1

2
1 −σmax(PN1PN2)
= dim(J )1/2 RJ + R⊤
J

2
1 −σmax(PN1PN2)
.
Individual components. Since J ⊥I1 and J ⊥I2, we have that PJ ⊥Pcol(X1) = PI1 and PJ ⊥Pcol(X2) =
PI2. Therefore, we have that PJ ⊥Pcol(Xk) has exactly rIk eigenvalues equal to 1 and the rest equal to 0.
Therefore, applying Davis-Kahan, we get:
∥PbIk −PIk∥2 = ∥sin Θ(UbIk, UIk)∥2
≤2 dim(Ik)1/2 PJ ∆k + ∆J Pcol(Xk) + ∆J ∆k

2
= 2 dim(Ik)1/2∥RIk∥2.
□
A.2. Algorithm implementation. We provide the full implementation of our method in Algorithm 1.
Recall from Section 3.3, given the observed data matrices (Y1, Y2), we first obtain estimates of col( b
X1) and
col( b
X2) following Gavish & Donoho (2014). Using these estimates, we construct c
M = Pcol( b
X1)Pcol( b
X2). We
then estimate the rank of the joint subspace brJ by counting the number of singular values in the spectrum
of c
M that exceed (1 −bε1) ∨λ1/2
+ , where bε1 and λ+ are defined in equations (4) and (5), respectively. The
joint and individual subspaces are estimated according to equation (3).
A.3. Extension to multi-view case. For the case of k > 2, we establish the following lemma on the
identifiability, motivated by Feng et al. (2018). The main difference from the previous Lemma 2.1 is that for
the multi-view case, we further assume absence of the partially shared strctures. That is the joint subspace
is a shared between all the view.
Lemma A.4 (Identifiability). Given a set of subspaces {col(X1), . . . , col(XK)}, there is a unique set of
subspaces {J , I1, . . . , IK} such that:
(1) col(Xk) = J ⊕I with J ⊆col(Xk)
(2) J ⊥Ik
(3) T Ik = {0}.
17
With the above in mind, we propose to identify the joint rank by testing each pair of views separately
and setting the joint rank as the the minimum pairwise estimated joint rank. Specifically, we set dim b
J =
min1≤i<j≤k dim b
Jij, where b
Jij is the joint identified by considering views i and j respectively. To understand
why we take the minimum, we consider the following example. Suppose dim bJ12 > dim bJ13. The directions
that have been identified for each pair are above the strict noise bound and the bootstrap bound. This means
that we can rule out the possibility that some directions in b
J12 or b
J13 are the result of pure noise. This
means the estimate dim bJ12 includes highly aligned signal directions between the two views. In general, our
bootstrap bound always overestimates the true bound for the joint as we highlight in Section B. Therefore, we
are confident that this aligned directions in bJ12 must correspond to the highly rotated individual subspaces,
and not the corrupted joint signal. Therefore, by the above logic, we must exclude these directions from our
final estimate of the joint.
We estimate the joint subspace by taking the top dim b
J singular vectors of the following symmetric
product of projections
1
|P|
P
(i1,i2,...,ik)∈P Pcol( b
Xi1)Pcol( b
Xi2) . . . Pcol( b
Xik ), where P is the set of all permutations
of {1, 2, . . . , k}. In Theorem A.5, we show that a similar subspace estimation error bound holds for the multi-
view case as in the case of two views.
Theorem A.5 (Estimation Error for K > 2). Suppose our estimate of the joint rank is correct, i.e.,
dim b
J = dim J . Define Wij(S) = Pcol(Xij )I[ij ∈S]+∆ijI[ij ̸∈S] and RIk := PJ ∆k +∆J Pcol(Xk) +∆J ∆k,
where ∆J := P b
J −PJ and ∆k := Pcol(Xk) −Pcol( b
Xk) . Additionally, let P be the set of all permutations of
{1, , 2, . . . , K}. Then, the deviation of the estimated joint subspace from the true joint subspace is bounded
by:
∥P b
J −PJ ∥2 ≤
2 dim(J )1/2 
P
S⊂{1,2,...,K}
QK
j=1 Wij(S)

2
|P|

1 −
1
|P|
P
(i1,i2,...,ik)∈P
PIi1 PIi2 . . . PIik

2
.
Furthermore, if the rank of each subspace view is correctly estimated, i.e. rank( b
Xk) = rank(Xk)), then, the
deviation of the estimated individual subspace from the true individual subspace is bounded by:
∥PbIk −PIk∥2 ≤2 dim(Ik)1/2∥RIk∥2.
Proof. We adapt the proof from Theorem 3.2 to the multi-view case. Both proofs rely on the version of the
Davis-Kahan theorem of Yu et al. (2015).
Joint components. To apply Davis-Kahan theorem, we show that there exits an eigengap in the population-
level model S =
1
|P|
P
(i1,i2,...,ik)∈P Pcol(Xi1)Pcol(Xi2) . . . Pcol(Xik ) Similar to Section 3.1, we use the orthog-
onality assumption J ⊥Ik to obtain S = PJ +
1
|P|
P
(i1,i2,...,ik)∈P PIi1 PIi2 . . . PIik . Fix an orthonormal
basis, UJ , for PJ . Since J ⊥Ik, we have that SUJ = UJ . Hence, S has at least dim J eigenvalues that
are equal to 1 with corresponding eigenvectors forming J . Now fix any orthonormal vector u ∈span U ⊥
J .
Then we have:
∥Su∥2 =

PJ u + 1
|P|
X
(i1,i2,...,ik)∈P
PIi1 PIi2 . . . PIik u

2
≤
1
|P|
X
(i1,i2,...,ik)∈P
PIi1 PIi2 . . . PIik

2
(by triangle-inequality)
< 1.
(since
\
Ik = {0})
Therefore, the spectrum of S becomes σr(S) = 1 for 1 ≤r ≤dim J and σr(S) < 1 for r > dim J .
From here, applying Davis-Kahan, we have that:
∥sin Θ(U b
J , UJ )∥2 ≤
2 dim(J )1/2∥bS −S∥2
1 −
1
|P|
P
(i1,i2,...,ik)∈P
PIi1 PIi2 . . . PIik

2
18
0
1
2
3
4
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
AJIVE, SNR = 5
0
1
2
3
4
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
AJIVE, SNR = 3
0.0
0.5
1.0
1.5
2.0
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
AJIVE, SNR = 1.5
0
10
20
30
40
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
PPD, SNR = 5
0
10
20
30
40
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
PPD, SNR = 3
0
10
20
30
40
0.00
0.25
0.50
0.75
1.00
Singular values
Frequency
PPD, SNR = 1.5
Figure 4. Simulated data of § 4 with angle ϕ = 40◦and varying SNR. Top row: Spectrum
of bSsum analyzed by AJIVE (Feng et al., 2018). Blue region indicates the noise bound; green
region indicates the joint Wedin bound, both computed using bootstrap. Bottom row:
Spectrum of c
M analyzed in our approach. Blue region indicates the noise bound from (5);
green region indicates the joint bound computed using bootstrap from (4). For both rows,
red lines indicate the true spectrum of Ssum and M, respectively.
Putting everything together, we have:
∥P b
J −PJ ∥2 = ∥sin Θ(U b
J , UJ )∥2
≤
2 dim(J )1/2∥bS −S∥2
1 −
1
|P|
P
(i1,i2,...,ik)∈P
PIi1 PIi2 . . . PIik

2
≤
2 dim(J )1/2 P
S⊂{1,2,...,K}
QK
j=1 Wij(S)

2
|P|

1 −
1
|P|
P
(i1,i2,...,ik)∈P
PIi1 PIi2 . . . PIik

2
,
where Wij(S) = Pcol(Xij ) if ij ∈S and Wij(S) = ∆ij otherwise.
Individual components. The case for individual components is the same as in Theorem 3.2.
□
A.4. Detailed comparison with related methods. Our approach builds on the principal angle formu-
lation by analyzing the product of projections, which directly reveals subspace alignment. This formulation
was first explored for multi-view data in Feng et al. (2018), where the authors instead studied the sum of
the projection matrices Ssum = (Pcol(X1) + Pcol(X2))/2 rather than the product. Applying Lemma 2.1 to the
sum gives
Ssum = PJ + PO1/2 + PO2/2 + (PN1 + PN2)/2,
which reveals that its spectrum has dim(J ) eigenvalues equal to 1 with eigenvectors corresponding to joint
subspace, dim(O1) + dim(O2) eigenvalues equal to 1/2 with eigenvectors corresponding to orthogonal indi-
vidual subspaces, and dim(N1) + dim(N2) eigenvalues of the form {σi}2 = {1 ± cos(ϕi)}/2, where ϕi are
dim(N1) principal angles between non-orthogonal N1 and N2. The gap separating joint from individual
subspaces is of the size {1 −σmax(Σ)}/2. In contrast, Pcol(X1)Pcol(X2) has only dim(J ) + dim(N1) non-zero
singular values, and the gap separating joint from individual subspaces is twice larger, {1 −σmax(Σ)}.
19
The above comparison of the population quantities directly affects the resulting estimation.
Since in
practice only estimates col( b
X1) and col( b
X2) are available, the resulting bSsum and Pcol( b
X1)Pcol( b
X2) both have
perturbed spectrum. Since Ssum has a smaller spectral gap between joint and individual subspaces, and
a larger number of non-zero singular values unrelated to joint structure dim(O1) + dim(O2) + dim(N1) +
dim(N2), bSsum is more affected by noise perturbations than Pcol( b
X1)Pcol( b
X2). As shown in Figure 4, the
singular values from Feng et al. (2018) do not exhibit the clustering observed in our method.
Furthermore, estimating joint structure based on the product of projections has advantages beyond rank
estimation. To illustrate this, consider the symmetrized version
bS = (Pcol( b
X1)Pcol( b
X2) + Pcol( b
X2)Pcol( b
X1))/2.
The two objects are related as 2bS2
sum −bSsum = bS. Therefore, non-trivial eigenvectors of bS are also non-
trivial eigenvectors of bSsum. However, eigenvectors of bSsum with eigenvalue 1/2 are in the null space of bS.
In principle, these eigenvectors of bSsum correspond to the individual orthogonal directions in O1 and O2, or
specifically col( b
X1)∩col( b
X2)⊥or col( b
X1)⊥∩col( b
X2). Our proposed estimate of joint structure automatically
excludes such directions.
We further provide a more detailed comparison between the proposed approach and the method of Feng
et al. (2018) using angle ϕ = 40◦and varying SNR ∈{1, 3, 5}. One key advantage of the proposed algorithm
is the diagnostic plot, which could be used to assess the quality of joint rank estimation in practice. The
bottom row of Figure 4 illustrates these diagnostic plots by showing the spectrum of Pcol( b
X1)Pcol( b
X2) together
with the proposed bootstrap bound 1−bε1 (obtained in step 2 of Algorithm 1) and noise bound λ+ (obtained
in step 3 of Algorithm 1). Observe that singular values cluster into groups corresponding to joint, non-
orthogonal individual and remaining directions. In the high signal-to-noise ratio, our bootstrap bound for
joint directions effectively identifies the corresponding top cluster, leading to the correct estimate of the
joint rank as 4. Furthermore, the noise bound and the bootstrap bound do not overlap. In contrast, when
signal-to-noise is low, the bootstrap bound becomes loose due to the increased error in subspace estimation
(hence inflated ∆k in (4)). Here, the noise bound compensates by filtering out rotated directions since their
deteriorated alignment disqualifies them from being included as a joint signal. In comparison, the top row
of Figure 4 illustrates the spectrum of Pcol( b
X1) + Pcol( b
X2) used by AJIVE (Feng et al., 2018). The lack of
clustering in the spectrum makes the separation of joint directions more challenging. Like our approach,
Feng et al. (2018) uses the maximum of two bounds to determine joint rank: bootstrap estimate of the Wedin
bound to identify the joint structure and bootstrap bound to filter out the noise directions. Figure 4 shows
that our joint bound is tighter since the noise bound of Feng et al. (2018) almost always dominates the joint
bound. We believe that the tightness of our joint bound is due to taking explicit advantage of perturbation
ε1 in Theorem 3.1 when performing bootstrap.
In terms of noise bound, the two approaches agree in
values, differing by at most 2% on average across simulations, but the main difference is that our bound
λ+ is analytical based on random matrix theory whereas Feng et al. (2018) estimate it using the bootstrap.
Overall, Figure 4 suggests that improved performance of proposed method is due to a more clear separation
of joint directions in the spectrum of Pcol( b
X1)Pcol( b
X2) compared to the spectrum of Pcol( b
X1) + Pcol( b
X2), as
well as tighter bound that we use to identify this separation in practice.
Appendix B. Additional experiments
B.1. Simulations on synthetic data for more than two views. Similar to Section 4, we run simulations
for the case of 3 views under various rank corruption, signal-to-noise ratio, and angles. We set the joint
ranks to be rank(Jk) = 3, and the ranks of the individual subspaces to rank(I1), rank(I2), rank(I3). We take
the matrix dimensions to be n = 35 and p1 = 40, p2 = 45, p3 = 50. We report our results in Table 4. From
the table, we see that our method consistently performs in the top among other methods. In addition, we
note that the proposed approach offers strong performance even under the more difficult conditions such as
low signal-to-noise ratio and highly rotated angles.
B.2. Ablation study for bootstrap. We modify the bootstrap method proposed in Prothero et al. (2024)
to better align the resampled singular vector bases with the observed data.
Specifically, given orthog-
onal resampled bases U1,(b) and U2,(b), we adjust U2,(b) to align with U1,(b) using the formula:U2,(b) ←
20
Table 4. Quality of the joint and individual subspace estimates in terms of the metric
(6), where larger values indicate better performance. All values are multiplied by 10. The
results are averaged across 100 iterations. Maximum standard error of the mean is 0.03.
We highlight the best result in bold and underline second best result. We denote by (∗)
methods that do not allow supplying marginal ranks for each view. We denote Lock et al.
(2013) as JIV, Feng et al. (2018) as AJI, Gaynanova & Li (2019) as SLI, Park & Lock (2020)
as UNI, and our method as PPD.
Under-specified rank
Estimated rank
Over-specified rank
SNR = 2
SNR=0.5
SNR = 2
SNR=0.5
SNR = 2
SNR=0.5
90◦
30◦
90◦
30◦
90◦
30◦
90◦
30◦
90◦
30◦
90◦
30◦
JIV
–
–
–
–
9.08
7.49
4.81
4.55
–
–
–
–
AJI
8.67
7.77
6.04
5.69
9.79
8.45
3.81
3.63
9.08
7.63
6.21
5.91
SLI∗
–
–
–
–
9.47
7.43
1.17
1.09
–
–
–
–
UNI∗
–
–
–
–
9.04
8.08
4.32
3.92
–
–
–
–
PPD
8.03
7.41
5.43
5.44
9.83
9.75
3.70
3.64
9.12
9.07
5.67
5.57
0.0
0.2
0.4
0.6
1.0
1.5
2.0
2.5
3.0
SNR
Epsilon
Rank ratio = 8%
0.2
0.4
0.6
1.0
1.5
2.0
2.5
3.0
SNR
Epsilon
Rank ratio = 10%
0.2
0.4
0.6
1.0
1.5
2.0
2.5
3.0
SNR
Epsilon
Rank ratio = 12%
0.25
0.50
0.75
1.00
1.0
1.5
2.0
2.5
3.0
SNR
Epsilon
Rank ratio = 16%
0.25
0.50
0.75
1.00
1.0
1.5
2.0
2.5
3.0
SNR
Epsilon
Rank ratio = 20%
0.25
0.50
0.75
1.00
1.0
1.5
2.0
2.5
3.0
SNR
Epsilon
Rank ratio = 24%
Variable
Bootstrap epsilon
Bootstrap epsilon (No Correction)
True epsilon
Figure 5. Estimate of ε1 computed with and without our modification, aligning the view-
specific bases.
We simulate the data as in Section 4, fix the angle between individual
subspaces at 60◦, while varying the rank-to-dimension and signal-to-noise ratios. We produce
the confidence intervals based on 10 re-runs.
U1,(b) cos(Σc
M) + U2,(b) sin(Σc
M), where Σc
M is a diagonal matrix containing the principal angles between the
column spaces col( b
X1) and col( b
X2). This alignment ensures that the resampled bases reflect the geometric
relationship observed in the data, which is crucial for accurate estimation of ε1 and ε2. Without this cor-
rection, a straightforward approach would be to randomly sample U1,(b) and U2,(b) from the Haar measure.
However, as we argue in Section 3.3, such an approach tends to underestimate ε1 and ε2, particularly when
the rank-to-dimension ratio is low. In this regime, randomly sampled bases are likely to be nearly orthogonal,
which does not capture the true alignment between the column spaces of b
X1 and b
X2.
21
0.0
0.1
0.2
0.3
0.4
0.5
0
10
20
30
SNR
ε^ −ε
Angle =  90
0.0
0.1
0.2
0.3
0
10
20
30
SNR
ε^ −ε
Angle =  60
0.0
0.1
0.2
0.3
0.4
0
10
20
30
SNR
ε^ −ε
Angle =  30
0.0
0.1
0.2
0
10
20
30
SNR
s^ −s
0.0
0.1
0.2
0.3
0
10
20
30
SNR
s^ −s
0.0
0.1
0.2
0.3
0
10
20
30
SNR
s^ −s
Figure 6. Top row: Error in the bootstrap estimate of ϵ1. Bottom row: error in the
noise variance estimation by the robust estimator ymed/µβ.
We investigate the effect of our correction in Figure 5 for ε1 with similar conclusions holding for ε2.
To generate the figure, we simulate data following the procedure in Section 4, fixing the angle between
individuals at 60◦while varying the rank-to-dimension and signal-to-noise ratios.
The 95% confidence
intervals are computed using 10 re-runs.
From the figure, we observe that our corrected bound provides better control over the true ε1, offering
more accurate coverage in the low signal-to-noise ratio regime and tighter bounds in the high signal-to-noise
ratio regime. Recall that ε1 :=
Pcol(X1) (∆1 + ∆2 + ∆1∆2) Pcol(X2)

2. The projection operators Pcol(X1)
and Pcol(X2) reduce the estimate by filtering out components of the error terms that lie within the column
spaces of X1 and X2, respectively. Recall from Figure 7, for random projections Pcol(X1) and Pcol(X2), the
spectral density depends on the rank-to-dimension ratios: the bulk of singular values is around zero, with
the tail extending towards one as the rank-to-dimension ratio increases. Hence, the naive estimate tends
to underestimate ε1, especially when the rank-to-dimension ratio is low, because the resampled projections
Pcol(X1) and Pcol(X2) are nearly orthogonal, causing more directions to be filtered out. This effect is more
pronounced for low signal-to-noise ratios, as the error terms ∆k are larger. We note that our corrected
bound tends to be tighter in the high signal-to-noise ratio regime, and we leave a detailed investigation of
this phenomenon for future work.
B.3. Empirical error of the bootstrap. We simulate the data following Section 4, setting.
We run
our bootstrap model for a range of signal-to-noise ratios and angles, supplying the model with the true
marginal ranks. We report the difference ˆϵ1 −ϵ1 in Figure 6. From the figure, we see that our bootstrap
estimator tends to be accurate, except for the low signal-to-noise ratios for which it overestimates the true
parameter. We investigate this issue and find it related to the overestimation of the noise variance by the
robust estimator ymed/µβ with ymed, µβ denoting the median singular value of Y and the median of the
Marchenko-Pastur distribution with parameter β. We show the error in the variance estimation in Figure 6.
Recall that in our bootstrap as described in Section 3.3, we impute the signal directions back into the noise
estimate ˇEk, which are scaled by the noise variance. As our estimate of the noise variance overestimates the
22
0
2
4
6
0.00
0.25
0.50
0.75
1.00
Singular values
Density
Ratios q1 = q2 = 0.1
0
1
2
3
4
0.00
0.25
0.50
0.75
1.00
Singular values
Density
Ratios q1 = q2 = 0.2
0
1
2
3
0.00
0.25
0.50
0.75
1.00
Singular values
Density
Ratios q1 = q2 = 0.3
0.0
0.5
1.0
1.5
2.0
2.5
0.00
0.25
0.50
0.75
1.00
Singular values
Density
Ratios q1 = q2 = 0.4
0
1
2
3
0.00
0.25
0.50
0.75
1.00
Singular values
Density
Ratios q1 = q2 = 0.5
0
2
4
6
0.00
0.25
0.50
0.75
1.00
Singular values
Density
Ratios q1 = q2 = 0.6
Figure 7. Asymptotic distribution of the spectrum of the product of two random projec-
tions, parametrized by the subspace-to-ambient dimension ratios q1 = r1/n and q2 = r2/n.
truth, correspondingly our noise estimate becomes inflated, which finally leads to the inflated bε1 estimate.
We leave the improvement of our bootstrap estimator in the low signal-to-noise ratio regime as future work.
B.4. Asymptotic distribution of spectrum of random projections. To simulate the spectrum of two
random projection matrices, we sample orthonormal bases U1 and U2 of ranks r1 and r2 from the Haar
measure. We set the ambient dimension to n = 100. We then compute the squares of the singular values of
the matrix U ⊤
1 U2. In Figure 7, we present the histogram of these squares, λ, overlaid with the continuous
part of the asymptotic density provided in Equation (5).
23
