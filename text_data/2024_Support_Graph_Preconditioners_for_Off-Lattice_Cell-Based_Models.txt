Support Graph Preconditioners for Off-Lattice Cell-Based Models
Justin Steinman and Andreas Buttensch¨on
Department of Mathematics and Statistics, University of Massachusetts Amherst, Amherst 01003,
United States of America
October 8, 2024
Abstract
Off-lattice agent-based models (or cell-based models) of multicellular systems are increasingly used to create in-silico
models of in-vitro and in-vivo experimental setups of cells and tissues, such as cancer spheroids, neural crest cell migration,
and liver lobules. These applications, which simulate thousands to millions of cells, require robust and efficient numerical
methods. At their core, these models necessitate the solution of a large friction-dominated equation of motion, resulting
in a sparse, symmetric, and positive definite matrix equation. The conjugate gradient method is employed to solve this
problem, but this requires a good preconditioner for optimal performance.
In this study, we develop a graph-based preconditioning strategy that can be easily implemented in such agent-based
models.
Our approach centers on extending support graph preconditioners to block-structured matrices.
We prove
asymptotic bounds on the condition number of these preconditioned friction matrices. We then benchmark the conjugate
gradient method with our support graph preconditioners and compare its performance to other common preconditioning
strategies.
1
Introduction
Agent-based models that simulate individual entities such as humans, animals, or biological cells are an indispensable tool
for studying emergent behaviors in complex systems. Over the last few decades, biomedical research has adopted agent-
based models to develop digital-twins of in-vitro and in-vivo experiments on cell cultures and tissues [5]. To capture a
wide variety of applications and research questions, many different agent-based models have been developed. Broadly, we
categorize them into lattice-based (e.g. Cellular Automata [18] or Cellular Potts models [15]) and off-lattice models [25].
These different models each have advantages and disadvantages. For an overview, we refer the reader to the review by
[38]. Here, we focus on off-lattice models closely related to colloidal physics [10]. In these models, cells are approximated
by elastic spheroids [11, 14], capsules [6], ellipsoids [31], or surfaces of triangulated meshes [25]. The applications of these
models are highly varied, including slime-mold aggregation [31], cancer growth and migration [23, 27], cancer monolayers
and spheroids [11, 39], liver lobules [20], and neural crest cells [29].
A typical cell configuration of spherical cells with radii Ri is shown in Fig. 1. Advancing the simulation from t →t + ∆t
requires solving the overdamped equation of motion Γv = F, where Γ is the friction matrix, v the cells’ velocities, and F
the forces. The matrix Γ is block-structured, symmetric, and positive definite because the individual 3× 3 friction matrices
are symmetric and positive definite [25]. There is a nonzero off-diagonal block in Γ at position (i, j) when cells i and j are
in contact. We ultimately solve this large linear system using the conjugate gradient method.
Profiling our simulation software used in [6, 25, 39] shows that solving the equation of motion is often the most time-
intensive step. We hypothesize that this is because no good preconditioners are available. From an implementation point
of view, it is convenient to implement the linear algebra methods in a matrix-free manner.
This means that “off the
shelf” preconditioning techniques are difficult to use or adapt. Further, the condition number of the friction matrix Γ is
determined by the cells’ free surface area. This means that the condition number of the friction matrix varies during a
given simulation. Additionally, the sparsity structure of Γ is dynamic because it encodes interacting cell pairs. This makes
selecting a preconditioner difficult. Here, we solve this by using the collision graph constructed during the collision detection
phase as our central data structure instead of the usual sparse matrix implementations.
1
arXiv:2410.04512v1  [math.NA]  6 Oct 2024
1
2
3
4
1
2
3
4
Collision graph
Cell conﬁguration
Tree - Preconditioner
Figure 1: Graphical overview of our preconditioner construction. The figure illustrates the step-by-step construction of our proposed
preconditioner, proceeding from left to right. 1. Agent configuration. The initial setup showing individual agents (cells) in a spatial
arrangement. 2. The collision graph. Using collision detection algorithms, we construct a graph where nodes represent agents and
edges represent collisions or friction interactions between them. The friction matrix is the graph laplacian of the collision graph. 3.
The maximum spanning tree. Using Prim’s algorithm, we construct a maximum spanning tree from the collision graph. The graph
laplacian of this tree, is used as the preconditioner.
These observations, together with the increasing role agent-based models play in biomedical research, motivate our
work. Our goal is to develop and benchmark a preconditioning strategy for the friction matrix Γ that is easily implemented
in a matrix-free manner, and reduces the required computational time for solving the linear system.
1.1
Support Graph Preconditioners
The computational cost per iteration of the conjugate gradient method is dominated by the matrix-vector product. Thus,
we aim to reduce the number of required iterations. Let ek denote the true error i.e. the difference between the computed
approximation at the k-th iteration and the true solution. The well-known error estimate for the conjugate gradient method
is given by [34]:
∥ek∥Γ ≤2
 p
κ(Γ) −1
p
κ(Γ) + 1
!k
∥e0∥Γ,
where κ(Γ) is the spectral condition number of Γ, which is a function of the contact areas and the ratio of the friction
coefficients.
The error estimate suggests that a matrix with higher condition number requires more iterations. However, in practice,
conjugate gradient convergence is more complex and often faster than this estimate suggests [26]. While this estimate has
limited practical use, it does motivate the reduction of the condition number through preconditioning.
To precondition the system, we choose a symmetric matrix H = EET such that Γ−1 ∼H. We then solve the modified
system ET ΓEˆx = ET F and x = Eˆx.
This approach reduces the iteration count if κ(ET ΓE) < κ(Γ), and reduces
computational time if we choose H so that solving Hx = b is computationally inexpensive.
Preconditioning is crucial in many problems, particularly those arising from the discretization of partial differential
equations. While no unifying theory exists, it is a well-developed field [8, 16]. Our focus on a matrix-free implementation
limits the direct application of many existing preconditioning techniques to our problem.
In off-lattice agent-based models, we can interpret the off-diagonal sparsity pattern of Γ as a graph C (see Fig. 1). This
graph representation works as follows:
• Each cell at position ri is represented as the i-th vertex.
• We draw an edge e = (i, j) between vertices i and j when the cells’ contact area Aij is nonzero.
• The edges are weighted by the cell-cell friction matrices Γcc
ij .
• Cell-substrate matrices Γcs
i
are represented with weighted self-loops.
2
The result is an undirected, matrix-weighted, and labeled (by cell id) graph that represents the friction matrix.
The relationship between matrices and graphs is well-established [33] and underpins many algorithms for sparse matrices.
Typically, the matrix is constructed first, and its underlying graph is derived subsequently. Our approach reverses this
process: we start with the collision graph constructed during the collision detection phase, and derive the friction matrix
from it. Specifically, the friction matrix is the block Laplacian of the collision graph.
With employ a technique pioneered by Vaidya [37] that uses subgraphs of C as preconditioners. Subgraphs are advan-
tageous because they are sparse, yet capture much of the relevant information from C. This characteristic allows them to
effectively balance between approximating Γ and computational efficiency. In our implementation, we specifically use a
maximum spanning tree. This tree can be constructed in linearithmic time, and its associated matrix can be factored in
linear time. The broader study of using subgraphs as preconditioners is known as support graph theory.
Vaidya’s original manuscript lacked many proofs, which were later provided by [3]. In our work, we extend these proofs
to apply to block-structured matrices. This extension allows us to obtain estimates for the smallest and largest eigenvalues
of such preconditioned systems.
These eigenvalue bounds serve two important purposes: (1) They provide worst-case
convergence estimates; and (2) They are valuable in implementing the robust conjugate gradient stopping criteria, proposed
by [1, 30].
1.2
Outline
The remainder of this paper is organized as follows: In Section 2, we introduce the linear system arising from agent-based
models and its natural graph structure. This section provides the foundation for understanding the mathematical framework
of our approach. Section 3 focuses on Vaidya’s preconditioners. We explain how to construct these preconditioners for block-
structured matrices and solve the resulting systems in near-linear time. This section bridges the gap between graph theory
and numerical linear algebra. Section 4 presents our main theoretical contribution. Here, we extend support graph theory
to block-structured matrices and derive eigenvalue bounds for the preconditioned linear system Γ. This extension is crucial
for applying support graph theory to the matrices arising in agent-based models. In Section 5, we present our numerical
results. We demonstrate the effectiveness of our preconditioner using a series of numerical benchmarks. These experiments
validate our theoretical findings and showcase the practical benefits of our approach. Finally, Section 6 concludes the paper
with a discussion of our findings, and its implications for real-world use cases. We situate our contributions in the broader
theoretical landscape and discuss connections to the existing literature on this problem. Additionally, we provide potential
directions for future research in this area.
2
Preliminaries
This section introduces the basic graph and matrix structure of our problem. From the collision detection phase of an off-
lattice simulation, we derive a matrix-vector equation whose solution represents the velocities of all the cells. We then show
that the matrix we are solving is the block Laplacian of the collision graph, and we use this to prove positive definiteness.
To set the stage for our subsequent discussion, we briefly outline the steps of an off-lattice model, considering a simulation
of n spherical cells.
Step 1: Broad-phase collision detection. Identify possible cell contact pairs (i, j). Efficient divide-and-conquer algo-
rithms such as axis-aligned bounding boxes, are commonly used [36].
Step 2: Compute forces between cells. Examine the possible cell pairs identified in the previous step, and identify
interacting cells. Then, compute their contact area and contact force using a physical model. We denote the
contact area between cells i and j by Aij. For spherical cells, Hertz or JKR contact mechanics are commonly
used [25].
Step 3: Assemble the friction matrices. Construct the cell-cell and cell-substrate friction matrices. The cell-cell friction
matrix between cells i and j is given by
Γcc
ij = Aij
 γ∥uij ⊗uij + γ⊥(I −uij ⊗uij)

,
(2.1)
where γ∥and γ⊥are the parallel and perpendicular coefficients of friction, respectively (both of which are
positive), and uij is the unit contact vector [25]. If ri is the position of cell i, then
uij =
rj −ri
∥rj −ri∥.
3
The cell-substrate friction matrix has several possible forms depending on the cell shape. In the isotropic case
of spherical cells, we can write Γcs
i
= λmedI, where λmed is the coefficient of friction between the cell and the
medium. For ellipsoidal cells, the cell-substrate friction matrix takes on a similar form to the cell-cell friction
matrix in terms of directional friction coefficients and the unit direction vector. More complicated cell shapes
may not be as easily expressible, but we only require that all the friction matrices are symmetric positive
definite.
Step 4: Solve the equation of motion. The equation of motion for cell i is
Γcs
i vi +
X
Aij>0
Γcc
ij (vi −vj) = Fi,
(2.2)
where vi is the velocity vector and Fi is the total nonfrictional force acting on the cell. We interpret Equa-
tion (2.2) as one row of a large linear system Γv = F. We show that the friction matrix Γ is symmetric positive
definite. Hence, the conjugate gradient method is an efficient choice for obtaining an accurate solution.
Step 5: Update cell positions. Frequently, a forward Euler method is used:
ri(t + ∆t) = ri(t) + ∆t vi,
where the step-size ∆t is chosen according to the Euler’s method stability criterion. Higher-order integration
methods are rarely used. Two exceptions are PhysiCell [14], which employs a second-order Adam’s-Bashford
method with a fixed time-step, and [6], where an embedded Runga-Kutta-23 method with an adaptive time-step
is employed.
Since the friction matrix Γ is comprised of 3 × 3 blocks, we establish a few simple properties of these smaller friction
matrices. Note that each cell-cell friction matrix (and the cell-substrate friction matrix of an ellipsoidal cell) is the sum of
two orthogonal projectors. Let γmax and γmin be the maximum and minimum elements of {γ∥, γ⊥}, respectively.
Lemma 2.1 Let u ∈Rn be a unit vector, then the matrix
Υ = γ∥u ⊗u + γ⊥(I −u ⊗u) ,
1. is symmetric positive definite;
2. its eigenvalues are γ∥and γ⊥with multiplicities 1 and 2 respectively;
3. its eigenspaces are Eγ∥= span (u) and Eγ⊥= E⊥
γ∥;
4. its operator norm (with respect to the 2-norm) is ∥Υ∥= γmax; and
5. its condition number is
κ(Υ) = γmax
γmin
.
Proof: The projection matrices u ⊗u and I −u ⊗u are clearly symmetric. Note that (u ⊗u)u = u, so Υu = γ∥u. Since
the eigenvectors of a real symmetric matrix are orthogonal, take w such that wT u = 0. Then (u⊗u)w = 0, so Υw = γ⊥w.
The vector w lies in the orthogonal complement of u, which is two-dimensional. Both of the eigenvalues are positive, which
gives positive definiteness. The operator norm of a symmetric positive definite matrix is the maximum eigenvalue.
■
Observe that the off-diagonal sparsity pattern of Γ is determined by the interacting cell pairs. We interpret this as a
matrix-weighted graph.
Definition 2.2 A matrix-weighted graph G = (V, E, w) is an undirected graph with a matrix-valued weight function w: V ×
V →Rd×d. We require that w(e) is positive definite for all e ∈E, that w(v, v) is positive semidefinite for all v ∈V , and
that w(u, v) = 0 otherwise.
4
We allow nonzero weights on pairs (v, v) for convenience when representing cell-substrate friction. To refer to the number
of edges |E| and the number of vertices |V |, we use m and n respectively. We reserve the Fraktur font for objects relating
to the collision graph. Let D be the set of cells, E the set of interacting cell pairs, and w : D × D →R3×3 the weight
function with w(i, j) = Γcc
ij for all (i, j) ∈E and w(i, i) = Γcs
i
for all i ∈D. These form the collision graph C = (D, E, w).
This graph is typically very sparse with m = O(n).
Most of the matrices in this paper have a block structure. If A is an m × n block matrix, then we mean that A is m
blocks tall by n blocks wide. The precise size of the blocks is not important, but we assume they are square. To be explicit
when indexing block matrices, we use an underline to index over blocks. For example, define a 2 × 2 block matrix
A =

W
X
Y
Z

.
Then A11 = W11 and A11 = W. We also use this notation for vectors. If a =
x
y

, then a1 = x1 and a1 = x.
To formalize the relation between the friction matrix Γ and the collision graph C, we introduce the block Laplacian.
Definition 2.3 A block Laplacian is a symmetric block matrix whose off-diagonal blocks are either zero or negative definite,
and whose row and column sums are positive semidefinite. For a matrix-weighted graph G = (V, E, w), the block Laplacian
of G is the matrix L where
Lij =
(
−w(i, j)
i ̸= j,
w(i, i) + P
k̸=i w(i, k)
i = j.
It is easy to see that Γ is the block Laplacian of C. The following lemma establishes the definiteness needed to apply
the conjugate gradient method.
Lemma 2.4 All block Laplacians are positive semidefinite, and they are positive definite when their block row sums are
positive definite.
Proof: Let A be a block Laplacian. For all x,
xT Ax =
X
i
xT
i Aiixi +
X
i̸=j
xT
i Aijxj
≥
X
i̸=j
|xT
i Aijxi| +
X
i̸=j
xT
i Aijxj
=
X
i>j

|xT
i Aijxi| + |xT
j Aijxj| + 2(xT
i Aijxj)

.
This inequality is strict when the block row sums are positive definite. By the AM-GM and Cauchy-Schwarz inequalities
respectively,
|xT
i Aijxi| + |xT
j Aijxj| ≥2
q
|xT
i Aijxi| · |xT
j Aijxj| ≥2|xT
i Aijxj|.
This implies that each term in the summation above is nonnegative, which yields the desired result.
■
3
Support Graph Preconditioners
To effectively precondition Γ, we need to find an easily factorable matrix that closely approximates it.
This section
introduces support graph theory and Vaidya’s preconditioners as tools to do this, along with efficient graph algorithms for
their implementation. Typically, an underlying graph is derived from a given matrix. This graph is manipulated (e.g.,
by taking a subgraph) and its Laplacian is used as a preconditioner. However, since the friction matrix is derived from
the collision detection phase, it is more appropriate for us to view Γ as the underlying matrix of C, and we can derive
preconditioners from manipulations (e.g., subgraphs) of the collision graph. This is why support graph preconditioners are
a natural choice for simulations. In fact, we can entirely avoid assembling matrices by working with the collision graph.
All we need are the contact areas, normal vectors, and friction coefficients.
5
3.1
Vaidya’s Preconditioners
The first of Vaidya’s preconditioners is the maximum spanning tree (MST) preconditioner. The idea is to precondition the
Laplacian of a graph with the Laplacian of an MST. We work with trees because their Laplacians can be factored in linear
time, and MSTs in particular because they capture a lot relevant information about the graph. In other words, an MST
“supports” its parent graph well.
Since we are working with matrix-weighted graphs, we define an MST with respect to the minimum eigenvalues of the
weights, a choice that is justified in the next section. In the case of C, this is equivalent to weighting by contact area.
We also include the self-loops (i.e., the cell-substrate friction) in the MST. Let T be an MST of C and let P be its block
Laplacian. We call T a support graph of C and P an MST preconditioner of Γ.
Vaidya’s second class of preconditioners builds on the first by adding edges back to an MST. Given a parameter t, we
split T into t disjoint subtrees of roughly the same size where each subtree has at most m/t vertices. Then we add the
maximum weight edge in C between each pair of subtrees if they are connected in C but not in T. Let T′ be the augmented
support graph and P ′ its block Laplacian. We call P ′ an augmented MST preconditioner of Γ. The theoretically optimal
value of t is approximately n1/4 [7].
The only step left to define is how we generate the support graph. Prim’s algorithm is the best choice for finding an
MST because it tells us, at no extra cost, how to permute the rows and columns of the block Laplacian to generate zero
fill during factorization. We prove this in the next subsection. The time complexity of Prim’s algorithm is O(m log n).
Augmenting an MST is straightforward once it has been decomposed into subtrees. A simple partitioning algorithm is
presented in [7, TreePartition] and a more sophisticated one in [35].
In practice, it is often beneficial to give the preconditioner the same diagonal blocks as the original matrix. This has the
intuitive benefit of capturing all the information contained in a Jacobi preconditioner. Doing so invalidates our analysis in
the next section of the minimum eigenvalue of the preconditioned system, but it also decreases the maximum eigenvalue.
While this choice is theoretically unfounded, it can be helpful in the reality of finite precision arithmetic (see Section 5).
3.2
The Elimination Game
When factoring or performing Gaussian elimination on a matrix, new nonzero entries may be created, changing the sparsity
pattern of the matrix. These new entries, called fill, require more memory and slow down computations. However, permuting
the rows and columns of the matrix can change the amount of fill. There is a graphical interpretation of Gaussian elimination
that shows how fill is created, called the elimination game [32].
In the game, all the vertices of a graph are eliminated in some order (e.g., see Fig. 2 where the elimination order
is according to the vertex labels). When a vertex v is eliminated, fill edges are constructed so that every uneliminated
neighbor of v (connected by either an original or fill edge) becomes pairwise adjacent. In other words, the uneliminated
neighbors of v become a clique.
The set of fill edges is in bijection with the set of fill entries that would be created
during Gaussian elimination or decomposition. Note that the elimination game is never explicitly implemented but rather
implicitly performed. Using this game, we can easily analyze the amount of fill created by a given permutation. The general
problem of minimizing fill is NP-hard [40], but trees can easily be ordered to produce no fill.
1
3
2
4
5
3
2
4
1
5
No-ﬁll ordering
Ordering with ﬁll
Figure 2: An example of the elimination game being played on the same graph with different orderings. Graph edges and matrix
entries are in solid black and denoted by crosses respectively. Fill edges and entries are in dotted blue and blue circles respectively.
Lemma 3.1 An ordering of the vertices of a rooted tree where no vertex occurs before any of its children produces no fill
in the elimination game.
6
Proof: The first vertex eliminated must be a leaf. This produces no fill and yields another rooted tree. Inductively, the
hypothesis demands that each vertex must be a leaf when it is eliminated, otherwise, it would have children to eliminate
first. Since eliminating leaves produces no fill, we get the desired result.
■
Corollary 3.2 The reverse order in which vertices are added to the MST in Prim’s algorithm produces no fill in the
elimination game.
For augmented trees, reducing fill is far less simple. Reversing the traversal order of Prim’s algorithm can yield very
poor results. For example, if the added edges are between vertices that occur late in the traversal order, then lots of
unnecessary fill is created. Of course, vertices that are not the ancestor of (we say a vertex is its own ancestor) an endpoint
of an added edge can be eliminated as before without producing any fill. The rest of the vertices can be eliminated in the
order specified by a fill-reducing algorithm. Both GENMMD and METIS are tested in [7], but it is worth mentioning that
Chen and Toledo only work with matrices whose underlying graphs are regular meshes. Cell-based models tend to produce
more irregularities in their graph structure. A variety of algorithms like reverse Cuthill-McKee may also perform well [12].
3.3
Solving Preconditioners
Matrix inverses are almost never explicitly computed. Instead, large matrices are decomposed into the product of diagonal
and triangular matrices in which solving systems is easy.
For a symmetric positive definite matrix M, the Cholesky
decomposition finds a lower triangular matrix L such that M = LLT . We instead opt for the similar LDLT decomposition
in which D is a diagonal matrix and L has only ones on its diagonal. This is preferable for block matrices because it avoids
computing matrix square roots. This section provides a general decomposition algorithm for symmetric positive definite
block matrices and adapts it to the special case of nonsingular block Laplacians of trees.
Algorithm 1
It is worth restating this algorithm because we cannot take commutativity for granted as is often done.
1: function LDLT(A: n × n symmetric positive definite block matrix)
2:
L, D ←n × n block matrices
3:
for i ←1, n do
▷Current column
4:
X ←0
5:
for j ←1, . . . , i −1 do
6:
X ←X + LijDjjLij
7:
end for
8:
Dii ←Aii −X
9:
Lii ←I
10:
for j ←i + 1, . . . , n do
▷Current row
11:
Y ←0
12:
for k ←1, . . . , i −1 do
13:
Y ←Y + LikDkkLjk
▷Subtract previous outer products
14:
end for
15:
Lji ←(Aji −Y )D−1
ii
16:
end for
17:
end for
18:
return (L, D)
19: end function
The general LDLT algorithm takes O(n3) time, but we show that the preconditioners we want to factor are sparse and
only take linear or near-linear time. Another perk of this algorithm is that L can be calculated in-place because previous
entries in A are not reused.
Theorem 3.3 Algorithm 2 returns the LDLT factorization of the nonsingular block Laplacian of a matrix-weighted tree in
O(n) time. The sparsity pattern of L is the same as the lower triangle of the original matrix.
Proof: We assume the validity of Algorithm 1 and prove its equivalence to Algorithm 2 in this special case. Say that the
rows and columns of A are ordered as specified on line 3 of Algorithm 2, so i < j means that vertex i is at least as far
7
Algorithm 2
1: function TreeLDLT(A: nonsingular block Laplacian of a rooted tree)
2:
L, D ←n × n block matrices
3:
for all vertices i in decreasing order of distance to the root do
4:
X ←0
5:
for all children j of i do
6:
X ←X + LijDjjLij
7:
end for
8:
Dii ←Aii −X
9:
Lii ←1
10:
j ←parent of i
11:
Lji ←AjiD−1
ii
12:
end for
13:
return (L, D)
14: end function
from the root as vertex j. In this case, we say that i is younger than j and that j is older than i. Note that vertices i
and j being connected is equivalent to Aij being nonzero. Lines 5–7 in both algorithms behave identically because the only
vertices younger than i that are also connected to i are its children.
We want to show the equivalence of lines 10–16 in Algorithm 1 to lines 10–11 in Algorithm 2. We prove by induction
on i that, in Algorithm 1, Lji is nonzero only if i is connected to j for all j > i. The hypothesis holds for the first vertex
because Y is guaranteed to be zero as there are no younger vertices. Now assume that the hypothesis holds for all vertices
younger than some i. If Y is nonzero, then there exists an older vertex j and a younger vertex k such that k is connected
to both i and j. However, the only vertex older than k that it is connected to is its unique parent. This implies that i = j,
which contradicts line 10. Therefore, Y is zero and the hypothesis is true, meaning the only j we need to consider is the
parent of i.
■
When there is zero fill, no additional memory allocation is required for L even when using a matrix-free or sparse matrix
implementation. However, this is not the case for augmented MST preconditioners. Algorithm 2 is no longer valid and
extra fill entries need to be stored. Another benefit of the standard MST preconditioner is that solving systems in the
decomposed block Laplacian takes linear time. We provide algorithms for this in Section B.1.
This begs the question of whether there exists an effective augmentation strategy that does not create fill. This restricts
us to only add back edges between siblings in the MST. However, we suspect that this would not yield promising results
for reasons we formalize in the next section. Intuitively, we want to add back edges that drastically reduce the distance
between pairs of vertices, but sibling edges do a poor job of this.
Finally, it is known that block LU factorization for symmetric positive definite matrices is stable as long as the matrix
is well-conditioned [9, 19].
4
Block-Structured Support Graph Theory
The existing literature on support graphs focuses entirely on symmetric diagonally dominant matrices. Here, we generalize
a sequence of lemmas from [3] and [17] to work with block Laplacians as well, and we refer the reader to Appendix A for
further generalizations to matrices with positive definite off-diagonal blocks. We provide proofs where they differ from the
non-block case. The goal is to show that both of Vaidya’s preconditioners achieve a minimum eigenvalue of at least 1, and
that the condition number is O(κmn) with an MST preconditioner and O(κn2/t2) with an augmented MST preconditioner,
where κ is the maximum condition number of all the edge weights and we assume sufficient sparsity.
We use ⪰to represent the Loewner order. For matrices A and B, we say A ⪰B if and only if A −B is positive
semidefinite. A fact that we use without proof is that the Loewner order is a partial order on symmetric matrices. We
denote the set of finite generalized eigenvalues of a pair of matrices (A, B) by λ(A, B). That is to say, λ(A, B) is the set of
numbers λ such that there exists a vector x where Ax = λBx. If B is a preconditioner for A, then the condition number of
the preconditioned system B−1A is precisely the ratio of the extremal finite generalized eigenvalues λmax(A, B)/λmin(A, B).
We start by defining the support of a pair of matrices, which bounds the maximum eigenvalue of the pair.
8
Definition 4.1 The support of a pair of matrices (A, B) is
σ(A, B) = min{τ | τB ⪰A}.
If no such τ exists, then we say σ(A, B) = ∞.
Lemma 4.2 Suppose A and B are positive semidefinite matrices. Then
λmax(A, B) ≤σ(A, B),
and equality holds when the support is finite.
Proof: See [17, Lemma 4.4]
■
Since λmax(B, A) = λmin(A, B)−1, the supports σ(A, B) and σ(B, A) are all we need to bound the condition number
κ(B−1A). In fact, we already have the necessary tools to bound the minimum eigenvalue.
Theorem 4.3 Let G = (V, E, w) be a matrix-weighted graph with subgraph H = (V, F, w′) and block Laplacians LG and
LH. Then λmin(LG, LH) ≥1.
Proof:
Observe that LG = LH + LK where LK is the block Laplacian of the graph K = (V, E \ F, w −w′). Since
LG−LH = LK is a block Laplacian, it is positive semidefinite. This implies that σ(LH, LG) ≤1 and that λmin(LG, LH) ≥1.
■
Bounding the maximum eigenvalue requires more effort. To simplify computing the support of a matrix and precondi-
tioner, we use the following lemma to decompose the matrices into sums of positive semidefinite matrices.
Lemma 4.4 Let A = A1 + · · · + Ak and B = B1 + · · · + Bk where each Ai and Bi is positive semidefinite. Then
σ(A, B) ≤max
i {σ(Ai, Bi)}.
Proof: See [17, Lemma 4.7]
■
A further simplification we can make is to only consider block Laplacians with zero block row sums. This means that
we can ignore cell-substrate friction in the rest of the analysis using the following lemma.
Lemma 4.5 Let A be a block Laplacian and define A′ to be the matrix with the same off-diagonal blocks as A and zero
block row sums. Let B′ be a block matrix and B = B′ + A −A′. If βB′ ⪰A′ for some β ≥1, then βB ⪰A. Similarly, if
αA′ ⪰B′ for some α ≥1, then αA ⪰B.
Proof: See [3, Lemma 2.5]
■
The lower bound on α and β is not important because the pairs of matrices with which we are concerned have supports
of at least 1. Next, we prove a small lemma that is helpful in the rest of the section.
Lemma 4.6 Let A be a symmetric positive semidefinite matrix. Then
λmax(A)I ⪰A ⪰λmin(A)I.
Proof:
Since A is symmetric positive semidefinite, it is diagonalizable and we can write A = PDP −1 where D is the
diagonal matrix whose entries are the eigenvalues of A. For concision, let λ = λmax(A). Observe
λI −A = λI −PDP −1 = P(λI −D)P −1.
By the definition of λ, the middle factor of λI −D has all nonnegative entries, so the whole difference is positive semidefinite.
Similar logic applies for the minimum eigenvalue.
■
Now we are ready to prove the main result. We decompose a matrix and support graph preconditioner into sums of the
block Laplacians of individual edges and paths. Then we analyze their pairwise supports with the following three lemmas.
9
Lemma 4.7 Suppose A and B are symmetric positive definite matrices. Let
ˆA =







A
0
· · ·
0
−A
0
0
0
...
...
...
0
0
0
−A
0
· · ·
0
A







,
and
ˆB =







A
−A
−A
2A
−A
...
−A
2A
−A
−A
A







,
be (k + 1) × (k + 1) block matrices. Then k ˆB ⪰ˆA.
Proof:
Let C = k ˆB −A and define ˆC = diag(A−1)C which is a block-structured matrix in which the blocks are either
nonzero or a multiple of the identity matrix. Since diag(A−1) is positive definite, it follows that C is semi-positive definite
if ˆC is positive semidefinite. We prove that ˆC is positive semidefinite using induction as in the non-block-structured case
in [3, Lemma 2.7]. The only difference is that we use a block-structured symmetric Gaussian elimination, meaning the i-th
elimination step is
ˆCi = Ei ˆCi−1ET
i
where Ei is block-structured with identity blocks along its diagonal and two nonzero off-diagonal blocks:
E1i =
1
1 + iI, E(i+1)(i) =
1
i + 1I.
At completion of this process we obtain the matrix
ˆC = diag

0, 2kI, 3k
2 I, . . . ,
i + 1
i

kI, . . . , 0

.
Since the matrix ˆC has nonnegative values on its diagonal, this shows that ˆC is positive semidefinite.
■
Lemma 4.8 Suppose A and B are symmetric positive definite matrices. Let
ˆA =







A
0
· · ·
0
−A
0
0
0
...
...
...
0
0
0
−A
0
· · ·
0
A







,
and
ˆB =







B
−B
−B
2B
−B
...
−B
2B
−B
−B
B







,
be (k + 1) × (k + 1) block matrices. Then k · λmax(AB−1) ˆB ⪰ˆA.
Proof: Let λ = λmax(AB−1). When A = B, we have that λ = 1 and the statement is equivalent to Lemma 4.7. When
A ̸= B, we can reduce to the case of equality by multiplying ˆB by diag(AB−1). This yields
k · diag(AB−1) ˆB ⪰ˆA.
Next we show that k · λ ˆB ⪰k · diag(AB−1) ˆB. This is equivalent to showing that λI ⪰AB−1, which is true by Lemma 4.6.
The rest follows from the transitivity of the Loewner order.
■
Note that, up to permutation, the block Laplacian of a single edge looks like ˆA and that of a simple path with uniform
edge weights looks like ˆB. The following congestion-dilation lemma generalizes the previous one so that ˆB can have varied
edge weights.
Lemma 4.9 (congestion-dilation lemma) Let
ˆA =







A
0
· · ·
0
−A
0
0
0
...
...
...
0
0
0
−A
0
· · ·
0
A







10
and
ˆB =







C1
−B1
−B1
C2
−B2
...
−Bk−1
Ck
−Bk
−Bk
Ck+1







be (k + 1) × (k + 1) block matrices where A, Bi, and Ci are symmetric positive definite for all i and ˆB has zero block row
sums. Then
k · max
i {λmax(AB−1
i
)} · ˆB ⪰ˆA.
Proof: Let D = mini{λmin(Bi)} · I. Decompose
ˆB = ˆB1 + ˆB2 =







D
−D
−D
2D
−D
...
−D
2D
−D
−D
D







+







C1 −D
−B1 + D
−B1 + D
C2 −2D
−B2 + D
...
−Bk−1 + D
Ck −2D
−Bk + D
−Bk + D
Ck+1 −D







.
Let λ = maxi{λmax(AB−1
i
)} and write
k · λ ˆB −ˆA = (k · λ ˆB1 −ˆA) + (k · λ ˆB2).
The first summand is positive semidefinite by Lemma 4.8. The diagonal blocks of ˆ
B2 are positive semidefinite and the
nonzero off-diagonal blocks are negative semidefinite by Lemma 4.6. This and the fact that the block row sums are zero
means the second summand is a block Laplacian, so it is positive semidefinite.
■
Under the support of the path represented by ˆB, we call λ the congestion of the edge represented by ˆA, and k is its
dilation. A more concise statement of the lemma is that σ( ˆA, ˆB) is bounded above by the product of the congestion and
dilation. We use the this lemma to prove an upper bound on the maximum eigenvalue of a block Laplacian with an MST
preconditioner. The following proofs make use of the fact that
λmax(A)
mini{λmin(Bi)} ≥max
i {λmax(AB−1
i
)}.
Theorem 4.10 Let G = (V, E, w) be a matrix-weighted graph with MST T weighted by minimum eigenvalues. Let LG and
LT be their block Laplacians and let κ be the maximum condition number of all the edge weights in G. Then
λmax(LG, LT ) ≤κm(n −1).
Proof: For every edge e = (u, v), let p(e) be the path in T from u to v that uses at least 1/m fraction of each edge weight.
We can write
LG =
X
e∈E
Le
and
LT =
X
e∈E
Lp(e)
where Le and Lp(e) are the block Laplacians of the edges and paths respectively. By Lemma 4.4,
σ(LG, LT ) ≤max
e∈E {σ(Le, Lp(e))}.
The maximum possible length (edge count) of each p(e) is n −1. This is the dilation. Since T is an MST, the minimum
eigenvalue of each edge weight in p(e) is at least that of w(e).
This means that the congestion is at most κm.
The
congestion-dilation lemma yields the desired result.
■
Augmented MST preconditioners have a better upper bound that can be proven similarly.
11
Theorem 4.11 Let G = (V, E, w) be a matrix-weighted graph with augmented MST T ′ as described in Section 3.1. Let LG
and LT ′ be their block Laplacians and let κ be the maximum condition number of all the edge weights in G. If every vertex
has at most d neighbors, then
λmax(LG, LT ′) ≤2κd3n2
t2
.
Proof:
We perform a similar decomposition to the previous proof. Let S1, . . . , St be the subtrees of T ′. For each edge
e = (u, v), a path from u to v in T ′ is not necessarily unique. If both u and v are in the same Si, let p(e) be the unique
path from u to v contained in Si. If u is in Si and v is in Sj with i ̸= j, let p(e) be the concatenation of the following paths:
the unique path in Si from u to the endpoint of the edge that connects Si and Sj, that edge itself, and the unique path in
Sj from the other endpoint to v.
Now we must decide what fraction of each edge weight to use. Each edge e in Si can be in a support path from any of
the dn/t vertices in Si to any of their d neighbors. This is d2n/t total paths, so we use t/(d2n) fraction of each edge weight.
Following the same logic as in the previous proof, we get that the congestion is at most κ(d2n/t).
In the worst case, one of these paths may go across all dn/t −1 edges in one subtree, the edge connecting it to another
subtree, and all dn/t −1 edges in the other subtree. Therefore, the dilation is less than 2(dn/t) and the rest follows from
the congestion-dilation.
■
In the context of collision graphs, κ is simply maxi,j{Aij} · γmax/γmin. Since the minimum eigenvalue is at least 1, the
preceding bounds on the maximum eigenvalue are also bounds on the condition number of the preconditioned system.
5
Numerical Benchmarks
In the previous sections, we extended support graph theory to block-structured matrices, introduced the collision graph,
and argued for constructing preconditioners directly from the collision graph. Although we derived theoretical bounds on
the eigenvalues of the preconditioned system, past experience with using the conjugate gradient method in finite precision
arithmetic demonstrates that theoretical estimates often do not accurately predict performance in practice [26]. For this
reason, we conduct benchmarking experiments to compare five preconditioning strategies: (1) no preconditioner, (2) the
Jacobi preconditioner (i.e., the diagonal of Γ), (3) the block Jacobi preconditioner (i.e., the block diagonals of Γ), (4) the
support graph MST preconditioner (as introduced in Section 3), and (5) a modified support graph MST preconditioner
which uses the same block diagonals as the original matrix Γ.
5.1
Experimental Setup
To test the preconditioning strategies, we generate two types of simulation-like test cases. Our experiments simulate of
elastic spherical cells of radius r = 0.5 (units are in 10s of µm). The Hertz contact model is used to resolve cell-cell
contact areas and forces. We assume that the cell-substrate friction matrix is diagonal (i.e., Γcs = γmedI), and the cell-cell
friction matrix is given by equation (2.1). We consider both cases in which γ∥= γ⊥and γ∦= γ⊥. We find that cases in
which γ∦= γ⊥require more iterations for convergence. Typically the cell-cell friction coefficients are between 1–3 orders of
magnitude greater than the cell-substrate friction coefficients [? ]. We consider two types of spatial configurations for the
cells:
1. Random placement: Cells are randomly placed in a 3D-sphere to simulate noisy cell configurations.
2. Hexagonal close packing: Each cell is surrounded by 12 other cells. Additionally, we apply normally distributed noise
with mean zero and fixed standard deviation to vary the edge-vertex ratio.
The random configurations are created by randomly sampling locations in a 3D-sphere of given radius and only accepting
locations that are a minimum distance from previously placed cells.
5.2
Benchmarking Results
In Fig. 3, we report the observed mean convergence behavior of the conjugate gradient method for the friction matrix of
N = 10000 randomly placed cells in a 3D-sphere. The relative error is computed by comparing the conjugate gradient
solution to the solution obtained from a sparse direct method.
12
Figure 3: The average conjugate gradient convergence of 25 experiments, each of N = 10000 randomly placed cells in a large 3D-
sphere. The resulting block-structured linear systems are solved using the conjugate gradient method with five different preconditioning
strategies: (1) the identity; (2) Jacobi precondtioner; (3) block Jacobi precondtioner; (4) the support-graph preconditioner; and (5) the
row-support graph preconditioner. In the top row γmed = 3 × 105, γ∥= 2 × 106 and γ⊥= 8 × 106. In the middle row γmed = 3 × 104,
γ∥= 2 × 106 and γ⊥= 8 × 106. In the bottom row γmed = 3 × 104, γ∥= 2 × 107 and γ⊥= 8 × 107.
Rows:
The difference between the cell-substrate and cell-cell friction increases from single order of magnitude to three
orders of magnitude across rows. We observe that more iterations are required for larger order of magnitude differences.
Columns:
The ratio of edges to vertices in the collision graph is increasing from left to right. We observe that the
performance advantage of the support graph preconditioners becomes less pronounced as the edge-vertex ratio increases.
For low edge-vertex ratios, the support graph preconditioners result in approximately a 50% iteration count reduction.
To explore the degradation in convergence rate of the support graph preconditioner as the edge-vertex ratio increases, we
construct examples with high edge-vertex ratios. We generate cell configurations on three dimensional hexagonal lattices,
so that each cell has 12 neighbors (equivalent to an edge-vertex ratio of six). The convergence results for these cases are
reported in the last column of Fig. 4. In the first two rows of Fig. 4, there is a two order of magnitude difference between
the cell-substrate friction and the cell-cell friction, while in the last row there is a three order of magnitude difference.
We observe that the performance of the different preconditioners is comparable with the support graph preconditioner
performing the worst. In the last row, for larger order of magnitude differences between cell-substrate and cell-cell friction,
13
this difference is more pronounced.
To see whether we recover the support graph preconditioner performance as the edge-vertex ratio decreases, we add
a normally distributed random variable with mean zero and standard deviation σ to each cell location. As the standard
deviation increases, the edge-vertex ratio decreases, and the support graph preconditioners perform best once more. In the
cases with high edge-vertex ratios, the block Jacobi preconditioner outperforms the support graph preconditioner. This
observation originally motivated us to consider the row-support graph preconditioners in which we use the block diagonals
of the friction matrix together with the off-diagonals of the MST preconditioner.
Figure 4: Cells arranged on a three-dimensional hexagonal lattice with normal noise with mean zero and standard deviation σ applied
to each hexagonal cell position. In these examples we have N = 309 cells. For each configuration we report the number of edges per
vertex. As the edge to vertex ratio increases, the rate of convergence of both support graph preconditioners approaches that of the
block-Jacobi preconditioners. For the hexagonal configuration without noise, where each cell has 12 neighbours the rate of convergence
for all preconditioners is comparable. Here γmed = 3 × 104.
6
Discussion
We have proposed efficient preconditioners for linear systems arising from off-lattice agent-based models. Using the notions
of matrix-weighted graphs and block Laplacians, we extended support graph theory to this problem. By using maximum
spanning trees, we obtained preconditioners that are computationally efficient to compute and factor, while significantly
decreasing the condition number and iteration count in the conjugate gradient method. We proved bounds on the condition
number that, while overly pessimistic in practice, demonstrate asymptotic stability. Additionally, we proved a bound for
augmented spanning trees, but did not implement them because augmented trees lead to fill-in during factorization. In
such cases, it may be beneficial to use the augmented MST preconditioner with an incomplete LDLT decomposition that
completely ignores fill, however further research would be needed to develop a fill-reducing ordering. Finally, while we
have not proved any bounds that theoretically justify including the entire block diagonal of the original matrix in the
preconditioner, we provided numerical evidence of its potential helpfulness.
14
We benchmarked our proposed preconditioners and compared them to the identity, Jacobi, and block Jacobi precon-
ditioners. Support graph preconditioners showed significant advantages in scenarios with low edge-vertex ratios, but their
effectiveness decreased as this ratio increased. We observed that in situations where the support graph did not perform
well, the block Jacobi preconditioner performed well. These findings led to the development of row-support graph precon-
ditioners, combining block diagonal and support graph approaches. This combination resulted in a preconditioner which
shares properties with both the block Jacobi and MST preconditioner. Our benchmarking supports this observation, as
the row-support graph preconditioner performs well across various scenarios. Finally, our results highlight the importance
of benchmarking, as theoretical bounds frequently do not predict practical performance in finite precision arithmetic.
What are the key takeaways for real-world implementations? Since the collision graph is a byproduct of the collision
detection algorithm, it is logical to utilize it to define the system’s friction matrix. The collision graph allows for elegant
implementations of the required linear algebra operations and the construction of an MST preconditioner.
While our
theoretical results give us asymptotic bounds on the eigenvalues of the preconditioned system, and numerical exploration
indeed suggests a significant reduction in condition number, the observed number of required iterations in finite precision
arithmetic paints a more complicated picture. The choice of preconditioner is problem-specific. The key considerations are
the system’s edge-vertex ratio and magnitude difference between cell-substrate and cell-cell friction. The larger the difference
between cell-substrate and cell-cell friction, the more useful preconditioning becomes. Similarly, the lower the edge-vertex
ratio, the larger the reduction in required iterations when using the support graph preconditioner. Our results suggest that
in practice, the edge-vertex ratio could be used to switch between support graph and block Jacobi preconditioning, or we
could use the row-support graph preconditioner.
This paper generalizes and applies the earliest results in support graph theory. While Vaidya’s preconditioners are
very efficient to compute, more sophisticated preconditioners achieve much better condition numbers and near-linear time
convergence. The main tool they employ is the low-stretch spanning tree. These are the basis of Spielman’s groundbreaking
paper [35] that uses augmented low-stretch spanning trees as preconditioners. We can generalize the notion of stretch to
matrix-weighted graphs by examining the minimum eigenvalues of the edge weights, similar to our approach for MSTs.
Substantial progress has been made in solving symmetric diagonally dominant systems with preconditioners derived from
low-stretch spanning trees, most recently in [21] and [13]. Additionally, with a low-stretch spanning tree, we can implement
combinatorial algorithms like those described in [22] and [24] that do not use the conjugate gradient method at all. We
expect that all results from the existing literature can be generalized to block-structured matrices by adding a factor of κ
to the condition number bounds.
A
Positive Definite Off-Diagonal Blocks
An ostensible limitation of the theory presented in this paper is that it only works with block Laplacians, requiring off-
diagonal blocks to be zero or negative definite. This section describes methods of working with positive definite off-diagonal
blocks as well.
There are two established ways of handling positive off-diagonal entries in the non-block case. The first is a reduction
due to Gremban [17, Lemma 7.3] which is also described concisely in [35, Appendix A] and [28, Appendix A.2]. The validity
of this reduction immediately transfers to the block case. Let A be a block matrix whose off-diagonal blocks are either zero
or definite (positive or negative), and for every row i,
Aii ⪰
X
j̸=i
|Aij|
where | · | leaves positive semidefinite matrices unchanged and negates negative semidefinite ones. This matrix is positive
semidefinite by the proof from Lemma 2.4. We may call this a generalized block Laplacian, possibly originating from a
similarly defined generalized matrix-weighted graph. Then A = D+A(+)+A(−) where D contains the diagonal blocks, A(+)
the positive definite off-diagonal blocks, and A(−) the negative definite ones. To solve the system Ax = b, we construct a
2n × 2n block Laplacian system
A′
x1
x2

=
 b
−b

where
A′ =
D + A(−)
−A(+)
−A(+)
D + A(−)

.
The desired solution is then x = (x1 −x2)/2. Thanks to the simplicity of this reduction, the condition number analysis
from earlier still applies and ϵ-approximate solutions to the larger system produce ϵ-approximate solutions to the original
one.
15
The other method of solving a generalized Laplacian system is with a maximum weight basis preconditioner [4]. This
is a considerably more complicated technique that requires additional analysis of the condition number, but it does not
require a reduction to a larger problem. We have not proven any analogues for the block case.
B
Assorted Graph Algorithms
B.1
Directly Solving Systems from Trees
Once the LDLT decomposition of an MST preconditioner P is computed, we need to repeatedly solve systems Px = b.
This section describes how to do this in terms of the implicit tree structure of P (i.e., we do not distinguish between
the indices of rows and columns and the vertices they represent). We assume the rows and columns of P are ordered as
described in Lemma 3.1 so that L has the same sparsity pattern as the lower triangle of P. Let d be the order of the blocks
of P.
Step 1: Forward substitution. Define z = DLT x and solve Lz = b, and proceed as in algorithm 3. The cost is n −1
matrix-vector multiplications of d × d matrices.
Algorithm 3
function ForwardSolve(L, b)
for i ←1, . . . , n do
zi ←bi
for all children j of i do
zi ←zi −Ljizj
end for
end for
return z
end function
Step 2: Block diagonal solve. Define y = LT x and solve Dy = z. The cost is n solves of d × d matrices.
Algorithm 4
function DiagSolve(D, z)
for i ←1, . . . , n do
yi ←D−1
ii zi
end for
return y
end function
Step 3: Backward substitution. Solve LT x = y. The cost is n −1 matrix-vector multiplications of d × d matrices.
Algorithm 5
function BackwardSolve(L, y)
for i ←n, . . . , 1 do
j ←parent of i
xi ←yi −LT
ijxj
end for
return x
end function
16
B.2
Matrix-Vector Product of a graph
Lastly, we give an algorithm for computing matrix-vector products with the block Laplacian of a matrix-weighted graph.
This is needed for a matrix-free implementation of the conjugate gradient method.
Algorithm 6
function MatVec(G = (V, E, w), v)
for i ←1, . . . n do
xi ←w(i, i)vi
end for
for (i, j) ∈E do
xi ←xi + w(i, j)(vi −vj)
xj ←xj + w(i, j)(vj −vi)
end for
end function
References
[1] Owe Axelsson and Igor Kaporin. Error norm estimation and stopping criteria in preconditioned conjugate gradient
iterations. Numerical Linear Algebra with Applications, 8(4):265–286, 2001.
[2] Richard Barrett, Michael Berry, Tony F Chan, James Demmel, June Donato, Jack Dongarra, Victor Eijkhout, Roldan
Pozo, Charles Romine, and Henk Van der Vorst. Templates for the solution of linear systems: building blocks for
iterative methods. SIAM, 1994.
[3] Marshall Bern, John R Gilbert, Bruce Hendrickson, Nhat Nguyen, and Sivan Toledo. Support-graph preconditioners.
SIAM Journal on Matrix Analysis and Applications, 27(4):930–951, 2006.
[4] Erik Boman, Doron Chen, and Bruce Hendrickson. Maximum-weight-basis preconditioners. Numerical Linear Algebra
with Applications, 11:695 – 721, 10 2004.
[5] Andreas Buttensch¨on and Leah Edelstein-Keshet. Bridging from single to collective cell migration: A review of models
and links to experiments. PLoS computational biology, 16(12):e1008411, 2020.
[6] Andreas Buttensch¨on, Paul van Liedekerke, Margriet Palm, and Dirk Drasdo. Does single cell migration behavior
permit prediction of multi-cellular migration patterns: Lessons from a physics-based model., 2025.
[7] Doron Chen and Sivan Toledo.
Vaidya’s preconditioners:
Implementation and experimental study.
Electronic
Transactions on Numerical Analysis, 16(9):03, 2003.
[8] James W Demmel. Applied numerical linear algebra. SIAM, 1997.
[9] James W Demmel, Nicholas J Higham, and Robert S Schreiber. Stability of block lu factorization. Numerical linear
algebra with applications, 2(2):173–190, 1995.
[10] Dirk Drasdo. Coarse graining in simulated cell populations. Adv. Complex Syst., 08(02n03):319–363, 2005.
[11] Dirk Drasdo and Stefan H¨ohme. A single-cell-based model of tumor growth in vitro: monolayers and spheroids. Phys.
Biol., 2(3):133–147, 12 July 2005.
[12] Iain Duff and G´erard Meurant. The effect of ordering on preconditioned conjugate gradient. BIT, 29, 12 1989.
[13] Yuan Gao, Rasmus Kyng, and Daniel A Spielman. Robust and practical solution of laplacian equations by approximate
elimination. arXiv preprint arXiv:2303.00709, 2023.
[14] Ahmadreza Ghaffarizadeh, Randy Heiland, Samuel H Friedman, Shannon M Mumenthaler, and Paul Macklin.
Physicell: An open source physics-based cell simulator for 3-d multicellular systems. PLoS computational biology,
14(2):e1005991, 2018.
17
[15] Fran¸cois Graner and James A Glazier. Simulation of biological cell sorting using a two-dimensional extended potts
model. Physical review letters, 69(13):2013, 1992.
[16] Anne Greenbaum. Iterative methods for solving linear systems. SIAM, 1997.
[17] Keith D Gremban. Combinatorial preconditioners for sparse, symmetric, diagonally dominant linear systems. PhD
thesis, Carnegie Mellon University, 1996.
[18] Karl-Peter Hadeler and Johannes M¨uller. Cellular automata: analysis and applications. Springer, 2017.
[19] Nicholas J Higham. Accuracy and stability of numerical algorithms. SIAM, 2002.
[20] Stefan Hoehme, Marc Brulport, Alexander Bauer, Essam Bedawy, Wiebke Schormann, Matthias Hermes, Verena
Puppe, Rolf Gebhardt, Sebastian Zellmer, Michael Schwarz, et al. Prediction and validation of cell alignment along
microvessels as order principle to restore tissue architecture in liver regeneration. Proceedings of the National Academy
of Sciences, 107(23):10371–10376, 2010.
[21] Arun Jambulapati and Aaron Sidford.
Ultrasparse ultrasparsifiers and faster laplacian system solvers.
ACM
Transactions on Algorithms, 2021.
[22] Jonathan A Kelner, Lorenzo Orecchia, Aaron Sidford, and Zeyuan Allen Zhu. A simple, combinatorial algorithm for
solving sdd systems in nearly-linear time. In Proceedings of the forty-fifth annual ACM symposium on Theory of
computing, pages 911–920, 2013.
[23] Hildur Knutsdottir, John S Condeelis, and Eirikur Palsson. 3-d individual cell based computational modeling of tumor
cell–macrophage paracrine signaling mediated by egf and csf-1 gradients. Integrative Biology, 8(1):104–119, 2016.
[24] Yin Tat Lee and Aaron Sidford. Efficient accelerated coordinate descent methods and faster algorithms for solving
linear systems. In 2013 ieee 54th annual symposium on foundations of computer science, pages 147–156. IEEE, 2013.
[25] P. Van Liedekerke, A. Buttensch¨on, and D. Drasdo. Chapter 14 - off-lattice agent-based models for cell and tumor
growth: Numerical methods, implementation, and applications. In Miguel Cerrolaza, Sandra J. Shefelbine, and Diego
Garz´on-Alvarado, editors, Numerical Methods and Advanced Simulation in Biomechanics and Biological Processes,
pages 245 – 267. Academic Press, 2018.
[26] J¨org Liesen and Zdenek Strakos. Krylov subspace methods: principles and analysis. Numerical Mathematics and Scie,
2013.
[27] Paul Macklin, Mary E Edgerton, Alastair M Thompson, and Vittorio Cristini. Patient-calibrated agent-based modelling
of ductal carcinoma in situ (dcis): from microscopic measurements to macroscopic predictions of clinical progression.
Journal of theoretical biology, 301:122–140, 2012.
[28] Bruce M Maggs, Gary L Miller, Ojas Parekh, R Ravi, and Shan Leung Maverick Woo. Finding effective support-
tree preconditioners. In Proceedings of the seventeenth annual ACM symposium on Parallelism in algorithms and
architectures, pages 176–185, 2005.
[29] Rebecca McLennan, Linus J Schumacher, Jason A Morrison, Jessica M Teddy, Dennis A Ridenour, Andrew C Box,
Craig L Semerad, Hua Li, William McDowell, David Kay, et al. Neural crest migration is driven by a few trailblazer
cells with a unique molecular signature narrowly confined to the invasive front. Development, 142(11):2014–2025, 2015.
[30] G´erard Meurant and Petr Tich`y. Error Norm Estimation in the Conjugate Gradient Algorithm. SIAM, 2024.
[31] Eirikur Palsson. A 3-d model used to explore how cell adhesion and stiffness affect cell sorting and movement in
multicellular systems. Journal of Theoretical Biology, 254(1):1–13, 2008.
[32] Alex Pothen and Sivan Toledo. Elimination structures in scientific computing. Handbook of Data Structures and
Applications, pages 945–965, 2018.
[33] Jennifer Scott and Miroslav Tuma. Algorithms for sparse linear systems. Springer Nature, 2023.
18
[34] Jonathan Richard Shewchuk et al. An introduction to the conjugate gradient method without the agonizing pain.
Carnegie-Mellon University. Department of Computer Science Pittsburgh, 1994.
[35] Daniel A Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and solving symmetric,
diagonally dominant linear systems. SIAM Journal on Matrix Analysis and Applications, 35(3):835–885, 2014.
[36] Daniel J Tracy, Samuel R Buss, and Bryan M Woods.
Efficient large-scale sweep and prune methods with aabb
insertion and removal. In 2009 IEEE Virtual Reality Conference, pages 191–198. IEEE, 2009.
[37] PM Vaidya. Solving linear equations with symmetric diagonally dominant matrices by constructing good precondi-
tioners. unpublished manuscript uiuc 1990. In IMA Workshop on Graph Theory and Sparse Matrix Computation,
1991.
[38] P Van Liedekerke, M M Palm, N Jagiella, and D Drasdo.
Simulating tissue mechanics with agent-based models:
concepts, perspectives and some novel results. Comp. Part. Mech., 2(4):401–444, 26 November 2015.
[39] Paul Van Liedekerke, Johannes Neitsch, Tim Johann, K´evin Alessandri, Pierre Nassoy, and Dirk Drasdo. Quantitative
cell-based model predicts mechanical stress response of growing tumor spheroids over various growth conditions and
cell lines. PLoS computational biology, 15(3):e1006273, 2019.
[40] Mihalis Yannakakis. Computing the minimum fill-in is np-complete. SIAM Journal on Algebraic and Discrete Methods,
2, 03 1981.
19
