Adaptive Real-Time Multi-Loss Function Optimization Using
Dynamic Memory Fusion Framework: A Case Study on
Breast Cancer Segmentation
Amin Golnari
a,∗, Mostafa Diba
a
aFaculty of Electrical & Robotics, Shahrood University of Technology, Shahrood, Iran
Abstract
Deep learning has proven to be a highly effective tool for a wide range of appli-
cations, significantly when leveraging the power of multi-loss functions to optimize
performance on multiple criteria simultaneously.
However, optimal selection and
weighting loss functions in deep learning tasks can significantly influence model per-
formance, yet manual tuning of these functions is often inefficient and inflexible. We
propose a novel framework called dynamic memory fusion for adaptive multi-loss
function penalizing in real-time to address this. This framework leverages histori-
cal loss values data to dynamically adjust the weighting of multiple loss functions
throughout the training process. Additionally, this framework integrates an aux-
iliary loss function to enhance model performance in the early stages. To further
research horizons, we introduce the class-balanced dice loss function, designed to
address class imbalance by prioritizing underrepresented classes. Experiments on
breast ultrasound datasets demonstrate that the framework improves segmentation
performance across various metrics. These results demonstrate the effectiveness of
our proposed framework in ensuring that the model dynamically adjusts its focus
to prioritize the most relevant criteria, leading to improved performance in evolving
environments. The source code for our proposed methodology is publicly available
on GitHub.
Keywords:
Multi-Loss Optimization, Penalizing Loss Functions, Class-Balanced
Dice Loss, Medical Imaging, Breast Cancer Segmentation
1. Introduction
In deep learning, the strategic selection, combination, and dynamic weighting of
loss functions are crucial for optimizing performance across various tasks. This is
particularly important when dealing with challenges like class imbalance, boundary
∗Corresponding author
Email addresses: amingolnarii@gmail.com (Amin Golnari
), m.diba@shahroodut.ac.ir
(Mostafa Diba
)
Preprint submitted to Elsevier
October 29, 2024
arXiv:2410.19745v1  [cs.CV]  10 Oct 2024
precision, and feature learning. Deep learning models can achieve superior results
in various domains by carefully considering these factors and employing appropriate
loss functions. In GBE-Net [1], the combination of cross-entropy and dice loss is
shown to enhance segmentation accuracy, particularly in preserving the boundaries
of lesions. Similarly, MFMSNet integrates dice and binary cross-entropy losses to
handle class imbalance while refining tumor edges through multi-scale fusion [2].
In complex models, dynamically adjusting the weights of individual loss func-
tions during training is essential for effectively balancing them.
Approaches like
SoftAdapt dynamically adjust the weights of loss components, such as mean squared
error (MSE) and Kullback-Leibler (KL) divergence, to optimize tasks like autoen-
coders and variational autoencoders. This ensures a more balanced learning process,
preventing the model from overly focusing on any criterion. [3]. The dynamically
weighted balanced (DWB) loss proposed by Ruwani et al. [4] improves accuracy in
class-imbalanced datasets by adjusting weights based on class frequency and predic-
tion difficulty. These dynamic weighting strategies are crucial in applications where
class imbalance significantly impacts model performance.
Another example of an adaptive loss function application is found in unsuper-
vised image segmentation. Guermazi et al. [5] developed a dynamically weighted
loss function that balances feature similarity and spatial continuity during training.
The balance is automatically adapted based on cluster predictions, allowing more
accurate and semantically meaningful segmentations. Similar dynamic approaches
are applied in multiorgan segmentation, where Song et al. [6] introduced a dynamic
loss weighting algorithm that adjusts the weight assigned to each organ based on its
learning difficulty, ensuring more consistent performance across organs with varying
sizes and complexities.
The need for customized composite loss functions also extends to image fusion and
classification tasks. Research such as FuseFormer [7] and S2F-Net [8] have introduced
loss functions that balance pixel-level accuracy and structural similarity to ensure
quality fusion results in infrared and visible image fusion. Finger vein recognition
(FVR) also benefits from composite loss functions, where combining cosine softmax
and triplet loss improves feature learning, generalization, and robustness [9].
In
the context of ultrasound diagnosis systems, combining class activation mapping
(CAM)-based loss and quantitative ultrasound (QUS)-based loss using fixed weights
improves classification accuracy by enhancing lesion discrimination [10].
Meta-learning techniques have also been applied to dynamic loss weighting, as
shown by Jiang et al. [11], who proposed a meta-learning loss function that adapts
to label noise and class imbalance in biased datasets by dynamically adjusting loss
weights based on learned margins. These approaches demonstrate the effectiveness
of both composite and dynamic loss functions in enhancing performance across a
wide range of deep learning applications.
In this research, we introduce the dynamic memory fusion (DMF) framework,
which builds upon these advancements by dynamically adjusting the weights of mul-
tiple base loss functions during training. Unlike fixed-weight approaches, the DMF
2
framework leverages historical loss data to dynamically adjust the weighting of indi-
vidual loss functions in real time. This adaptive approach ensures a balanced focus
on different aspects of the learning task, preventing the model from becoming overly
biased towards any single component. Our framework draws inspiration from previ-
ous studies on composite loss functions and dynamic weighting, offering a solution
for multi-loss function optimization in deep learning.
2. Background
Deep learning tasks, especially in the domains of segmentation, classification,
and regression, have increasingly relied on composite and dynamically weighted loss
functions to address challenges such as class imbalance, boundary precision, and
feature learning. A growing body of research has explored how combining multiple
loss functions and dynamically adjusting their weights can lead to more effective
optimization, particularly for tasks requiring complex decisions at multiple scales
and resolutions.
In medical imaging, composite loss functions have become popular in improving
segmentation accuracy.
Jesson et al.
[12] introduced a novel approach to brain
tumor segmentation using a 3D fully connected network (FCN) with a multi-scale
loss function. This composite loss ensures learning at different spatial resolutions by
comparing predictions with down-sampled ground truth, addressing class imbalance
through a curriculum sample weighting strategy. Similarly, Sharifzadeh et al. [13]
developed an adaptive mixed loss function for ultrasound imaging, gradually shifting
from more straightforward B-mode data to more complex RF data. This progressive
approach improves reconstruction accuracy and avoids local minima during training,
resembling curriculum learning techniques.
Dynamic and composite loss functions have also been employed in diverse fields
beyond medical imaging. For example, Lv et al. [14] introduced SSDFusion, which
leverages a composite loss combining fusion and segmentation losses to fuse infrared
and visible images effectively while maintaining semantic accuracy. Similarly, Gao
et al. [15] enhanced face recognition performance by fusing multiple loss functions
automatically, learning the optimal balance between intra-class compactness and
inter-class separation.
Moreover, attention-based models, such as the attention-
gate medical transformer (AGMT) for ultrasound image segmentation, incorporate
specialized loss functions like the average radial derivative increment (∆ARD) to
improve shape feature detection, while other methods like δARD further enhance
the sensitivity to contours and regions in low-contrast medical images [16, 17].
Adaptive loss functions have proven effective in segmentation tasks where visual
detail is crucial. Chao et al. [18] introduced an adaptive composite loss function
for a multi-field-of-view (FoV) visual saliency model in 360° image processing. This
method dynamically adjusts the weights of Kullback-Leibler divergence (KLD), nor-
malized scanpath saliency (NSS), and linear correlation coefficient (CC) based on
their standard deviations during training. By doing so, the model optimizes perfor-
mance by balancing key saliency features. Similarly, B et al. [19] developed a deep
3
learning model for breast lesion segmentation in ultrasound images using a compos-
ite loss that combines binary cross-entropy (BCE) and boundary loss, demonstrating
the efficacy of composite loss functions in medical image analysis.
Dynamic loss weighting methods have also been introduced to tackle imbalanced
datasets and optimize neural network training.
Mang et al.
[20] presented DY-
NAWEIL, a dynamic weighted loss function for solving partial differential equations
(PDEs) on imbalanced data.
By adjusting the weights of the loss based on his-
torical errors during training, DYNAWEIL improves prediction accuracy in regions
with high errors. Similarly, Lu et al. [21] introduced dynamic weighted cross en-
tropy (DWCE) for semantic segmentation, where cross-entropy loss is dynamically
weighted based on class proportion to handle class imbalance effectively. Addition-
ally, Maldonado et al. [22] introduce an adaptive loss function based on ordered
weighted averaging (OWA) operators, dynamically adjusting class weights to han-
dle class imbalance and noise, improving classification performance over traditional
methods like focal loss and cross-entropy.
Dynamic loss weighting also finds applications in sentiment classification, as
demonstrated by Runda et al. [23], who introduced a model that adjusts the weights
of loss functions based on class prediction error rates, ensuring stable training even
in imbalanced datasets. A problem also addressed by Roy et al. [24] in their margin-
aware adaptive-weighted loss (MAAW), which enhances intraclass compactness and
interclass separability by dynamically adjusting weights for hard-to-train samples
using confidence scores.
In tasks requiring the balance of multiple cost functions, novel dynamic loss
weighting strategies have shown significant performance improvements. Groenendijk
et al. [25] introduced CoV-weighting, a multi-loss weighting strategy for single-task
learning problems such as depth estimation and semantic segmentation. By utilizing
the coefficient of variation, CoV-weighting dynamically adjusts the weights of loss
functions, eliminating the need for manual weight tuning and grid searches. Ocampo
et al. [26] extended this idea by introducing an adaptive loss weighting approach for
machine learning interatomic potentials (ML-IAPs). Using the SoftAdapt algorithm,
this method recalibrates the contribution of each loss component based on their loss
values, ensuring that no single component dominates the training process.
Xiang et al. [27] introduced lbPINNs, a self-adaptive loss-balanced method for
physics-informed neural networks (PINNs). This model adaptively assigns weights to
multiple loss terms during training, optimizing them at each epoch through Gaussian
probabilistic models. Barron [28] further extended this concept by proposing a gen-
eralized adaptive robust loss function that unifies several well-known loss functions,
such as Cauchy and Charbonnier, enabling automatic adjustment during training for
improved performance across a range of computer vision tasks.
These studies underscore the importance of combining loss functions and dy-
namically adjusting their weights to address diverse challenges across deep learning
applications. Whether handling class imbalance, ensuring boundary precision, or im-
proving segmentation accuracy, dynamic and composite loss functions provide robust
4
solutions for optimizing neural networks in various tasks.
3. Methodology
Manually selecting and weighting multiple loss functions in deep learning can be
a complex task, as the relative importance of each cost function may evolve through-
out the training process. A fixed weight assignment is often inadequate, as it fails to
account for the dynamic nature of learning, where different components may require
varying levels of focus at various stages. A more logical approach is to implement a
system that can dynamically adjust the weights of each loss function based on their
real-time contributions to the model’s overall performance. This framework adjusts
weights during the training process in real-time, running in parallel with model train-
ing. The computational overhead is minimal, requiring only the storage of a subset
of historical loss function values in small arrays. Consequently, memory consump-
tion remains low, ensuring computational efficiency even for complex tasks. This
adaptive weighting maintains a balanced and responsive training process, leading to
more effective optimization.
We introduce the DMF framework for adaptive loss optimization to address the
challenges inherent in deep learning tasks involving multiple loss functions. This
section details the loss function’s components, strategies for weight adjustment, the
integration of auxiliary losses, and the computational considerations involved.
3.1. Dynamic Memory Fusion Framework
The primary objective of the proposed framework is to minimize a DMF loss
function, denoted as L, which integrates multiple base loss functions with an auxiliary
loss that addresses specific task-related challenges, such as improving generalization
or mitigating bias. The total loss function is defined as:
L(y, ˆy) =
N
X
i=1
wiLi(y, ˆy) + γ(t)La(y, ˆy)
(1)
where y and ˆy refer to ground truth and predicted labels, respectively; N represents
the number of base loss functions, Li are the individual loss functions, wi are adaptive
weights dynamically adjust based on the historical data of each loss function, La
denotes the auxiliary loss, which can be any task-specific function critical in early
training stages, γ is a scaling factor controlling the contribution of the auxiliary loss,
and t indicates the current training step.
This formulation allows the model to adaptively prioritize different loss functions
throughout the training process, ensuring a balanced and optimized performance
while also incorporating essential task-specific considerations through the auxiliary
loss function.
5
3.2. Regularizing Loss Function
The auxiliary loss function, denoted as La, is integral to guiding the model dur-
ing the early stages of training by incorporating additional objectives crucial for the
learning process. The definition of the auxiliary loss is flexible and can be tailored
to the specific task and objectives at hand. The auxiliary loss is typically designed
to emphasize aspects of the model that require special attention during training,
particularly in the early phases when the model is still learning foundational pat-
terns in the data. To ensure that the influence of the auxiliary loss is most potent
during these initial stages and diminishes as training progresses, it is scaled by an
exponential decay factor γ, which is defined as:
γ(t) = γ0e−τt
(2)
where γ0 represents the initial weight of the auxiliary loss, and τ is the decay rate.
This scaling mechanism allows the auxiliary loss to influence the model during the
early training steps, guiding the learning process toward specific goals. As the train-
ing progresses and the model converges, the primary loss function gradually takes
precedence, allowing for a balanced and comprehensive optimization process.
4. Dynamic Weight Adjustment with Memory
The proposed framework’s dynamic adjustment of weights wi for each loss func-
tion is driven by historical loss-value data, referred to as memory. This section details
the role of memory in the weight adjustment process and its impact on optimization,
as well as the mechanism and strategies of the DMF for weight adjustment.
4.1. Memory Mechanism
In the DMF framework, memory plays a pivotal role in adapting the weights of
the loss functions over time. Each loss function maintains a record of its historical loss
values, which are utilized to compute statistical measures that guide the adjustment
of weights. The DMF framework employs three memory-based weighting strategies:
variance-based weighting, median absolute deviation (MAD)-based weighting, and
Bayesian-based weighting.
4.2. Variance-Based Weighting
The variance-based weighting approach adjusts the weight wi of each loss function
according to the variance of its loss history Hi. The variance is calculated as follows:
V ar(Hi) =
1
|Hi|
|Hi|
X
k=1

H(k)
i
−¯Hi
2
,
(3)
where |Hi| is the size of the loss history for the i-th loss function, H(k)
i
denotes the
k-th value in the loss history, and ¯Hi represents the mean of the loss history.
6
The weight wi with N base loss functions is then normalized to ensure a balanced
distribution:
wi =
V ar(Hi)
PN
j=1 V ar(Hj)
,
(4)
This normalization ensures that loss functions exhibiting higher variance are as-
signed greater weight, reflecting their increased contribution to the overall optimiza-
tion process.
4.3. MAD-Based Weighting
The MAD-based weighting approach offers robustness against outliers by utilizing
the median absolute deviation from the median of the loss history. The MAD for
the i-th loss function is computed as:
MAD(Hi) = median
H(k)
i
−median(Hi)


(5)
where median(·) represents the median operation. The weight wi for N base loss
functions is then determined by:
wi =
(MAD(Hi))−1
PN
j=1(MAD(Hj))−1
(6)
This approach favors loss functions with lower deviations from their median val-
ues, ensuring that more stable loss functions are weighted more heavily.
4.4. Bayesian-Based Weighting
The Bayesian-based weighting strategy updates weights based on likelihoods de-
rived from the MAD values. The likelihood for the i-th loss function is given by:
p(MADi|data) ∝
1
MAD(Hi)
(7)
The posterior distribution for the weight wi for N base loss functions is then
calculated as:
wi =
p(MADi|data) · pi
PN
j=1 p(MADj|data) · pj
(8)
where pi represents the prior probability for the i-th loss function. This approach
integrates prior knowledge with observed data, allowing for a more informed and
adaptive weighting process.
7
4.5. Comparison of Weighting Methods
The weighting methods offer distinct approaches to dynamically adjusting the
contributions of loss functions during model training. The variance-based weight-
ing method emphasizes loss functions exhibiting higher variability in performance
under the assumption that increased variance indicates a more dynamic or complex
contribution to the optimization process. Loss functions with greater variability are
considered to encapsulate more challenging aspects of the task, thus requiring ad-
ditional attention during training.
By adaptively increasing the weights of these
high-variance functions, the model shifts its focus toward aspects of the task that
may otherwise hinder convergence, thereby promoting a more balanced and efficient
optimization.
In contrast, the MAD-based weighting method prioritizes stability by assigning
higher weights to loss functions with smaller deviations from their median, signi-
fying more consistent performance over time. This method is designed to enhance
robustness to outliers, ensuring that stable loss functions are emphasized while mit-
igating the impact of abrupt fluctuations. By reducing the likelihood of the model
overreacting to transient changes, MAD-based weighting supports a more stable and
reliable training process, safeguarding against destabilization.
The Bayesian-based weighting method integrates both prior knowledge and real-
time data to update loss function weights dynamically. By assessing the likelihood of
each loss function’s contribution to the overall performance, this method allows the
model to combine predefined expectations, such as the relative importance of certain
tasks, with observed performance data. This results in a more informed and adaptive
training strategy, where the model balances accumulated learning with prior criteria,
leading to a more refined and targeted optimization process.
4.6. Normalization of Loss Histories
To ensure that the weighting mechanism is fair and unaffected by the scale of
individual loss values, we apply normalization techniques to the loss histories Hi
before computing statistical measures.
Two primary normalization methods are
used: min-max scaling and symmetric log scaling.
Min-Max Scaling. Min-max scaling normalizes the loss history Hi of each loss func-
tion to a fixed range, typically between 0 and 1. This scaling is crucial to prevent
loss functions with larger absolute values from dominating the weighting process.
The min-max scaling is defined as:
Hscaled
i
=
Hi −min(Hi)
max(Hi) −min(Hi)
(9)
where min(Hi) and max(Hi) are the minimum and maximum values of the loss his-
tory for the i-th loss function. This method ensures that all loss functions contribute
equally, regardless of their original scale.
8
Symmetric Log Scaling. While min-max scaling addresses the scale issue, it does not
handle cases where the distribution of loss values is skewed. Symmetric log scaling
is applied to better handle such skewness, particularly when loss values span several
orders of magnitude or contain both positive and negative values. The symmetric
log scaling function is defined as:
Hlog−scaled
i
=
(
log(1 + Hi),
if
Hi ≥0
−log(1 −Hi),
if
Hi < 0
(10)
By compressing the range of loss values, symmetric log scaling minimizes the im-
pact of outliers, ensuring that variance and MAD calculations become more resilient
and accurately reflect the overall behavior of the loss function.
Consequently, we first apply symmetric log normalization to stabilize the data
distribution and then use min-max normalization to rescale the values within the
range of 0 to 1. By first stabilizing the data with symmetric log scaling and then
normalizing the range with min-max scaling, the process ensures that both the distri-
bution and scale of the loss values are handled appropriately, leading to more robust
and effective weight adjustments during training.
4.7. Adaptive Weighting and the Challenges of Unbounded Weighting
In the DMF framework, the weights assigned to multiple loss functions are typ-
ically constrained to sum to 1.
This ensures that the contributions of each loss
function are proportionally balanced and normalized, preventing any single compo-
nent from disproportionately affecting the optimization process. This approach is
standard in multi-loss function optimization, as it promotes balanced optimization
and reduces the risk of overemphasizing any particular loss function.
Without this constraint, however, several challenges can arise. First, unbounded
growth in certain weights may occur, allowing specific components to dominate the
optimization and potentially leading to the neglect of other vital tasks. This im-
balance can distort the learning process, resulting in suboptimal performance across
the model’s objectives. Additionally, unconstrained weights may introduce instabil-
ity into the optimization process, as the overemphasis on certain loss functions can
cause erratic gradients, oscillating performance, and difficulties in reaching conver-
gence. Delayed convergence is another potential issue, as the model may frequently
shift focus between tasks, prolonging the training process and hindering the achieve-
ment of a stable solution. These risks emphasize the importance of maintaining the
weight sum constraint to ensure stable and efficient training.
5. Evaluation Metrics
To evaluate the performance of our method, we use a range of standard metrics,
including dice, intersection over union (IoU), precision, recall, and f1-score. These
metrics comprehensively assess the model’s ability to segment and classify pixels
correctly.
The dice score measures the similarity between predicted and ground
9
truth segments, emphasizing precision and recall. IoU evaluates the overlap between
the predicted and actual segments, providing insight into how well the prediction
captures the object. Precision quantifies how many of the predicted positives are
true, while accuracy measures the overall correctness of the predictions, considering
both positive and negative classifications. Together, these metrics offer a detailed
view of model performance across multiple aspects of segmentation and classification.
Dice =
2 × TP
2 × TP + FP + FN
(11)
IoU =
TP
TP + FP + FN
(12)
Precision =
TP
TP + FP
(13)
Recall =
TP
TP + FN
(14)
F1-score = 2 × Precision × Recall
Precision + Recall
(15)
where TP represents the number of true positives, correctly predicted pixels in the
positive class; TN is the number of true negatives, correctly predicted pixels in the
negative class; FP refers to false positives, or incorrectly predicted positive pixels;
and FN denotes false negatives, where pixels that should have been predicted as
positive were missed.
6. Loss Functions
In semantic segmentation tasks, loss functions are crucial in guiding the opti-
mization process. Different loss functions minimize the error between the predicted
segmentation and the ground truth. For our model, we use several loss functions,
including categorical cross-entropy, mean intersection over union (mean IoU), mean
dice loss, Focal loss, and Tversky loss, each designed to optimize different aspects of
segmentation performance.
6.1. Categorical cross-entropy
Categorical cross-entropy is a widely used loss function in multi-class segmenta-
tion tasks. It measures the dissimilarity between the predicted class probabilities
and the true class distribution. For a set of C classes, the categorical cross-entropy
loss is defined as:
LCE = −1
N
N
X
i=1
C
X
c=1
yic log(ˆyic)
(16)
10
where N is the number of pixels, C is the number of classes, yic is the ground truth
probability that pixel i belongs to class c, and ˆyic is the predicted probability for
pixel i in class c.
6.2. Mean Intersection over Union (Mean IoU)
Mean IoU is used to evaluate the overlap between the predicted segmentation
and the ground truth. It is also a loss function by optimizing the average IoU over
all classes. Mean IoU is computed as:
LIoU = 1 −1
C
C
X
c=1
TPc
TPc + FPc + FNc
(17)
where TPc, FPc, and FNc represent the true positives, false positives, and false
negatives for class c, respectively, and C is the number of classes.
6.3. Mean Dice Loss
The mean dice loss is derived from the dice similarity coefficient and is widely
used to measure overlap between two sets. For segmentation, it can be adapted as a
loss function to optimize the dice score for all classes:
LDice = 1 −1
C
C
X
c=1
2 × TPc
2 × TPc + FPc + FNc
(18)
where TPc, FPc, and FNc correspond to the true positives, false positives, and false
negatives for class c, and C represents the number of classes.
6.4. Focal Loss
Focal loss addresses class imbalance by assigning more weight to hard-to-classify
examples, reducing the contribution of easy examples. It modifies the cross-entropy
loss with a modulating factor:
LFocal = −1
N
N
X
i=1
C
X
c=1
αc(1 −ˆyic)γyic log(ˆyic)
(19)
where αc is a balancing factor for class c, γ, is a focusing parameter that adjusts the
rate at which easy examples are down-weighted, yic is the ground truth, and ˆyic is
the predicted probability.
6.5. Tversky Loss
Tversky loss is a generalization of dice loss that allows for a tunable trade-off
between false positives and false negatives. It is advantageous when there is a sig-
nificant class imbalance. The Tversky loss is defined as:
LTversky = 1 −1
C
C
X
c=1
TPc
TPc + α · FPc + β · FNc
(20)
11
where α and β are hyperparameters that control the penalty for false positives and
false negatives, respectively. This formulation allows the model to be more sensitive
to either false positives or false negatives based on the specific task requirements.
7. Class Balanced Dice Loss
Class imbalance is a common issue in semantic segmentation tasks.
Certain
classes (e.g., the background) may dominate the image, resulting in biased predic-
tions. To address this, we propose the class-balanced dice (CB-Dice) loss, an ex-
tension of the standard dice loss that incorporates class weights based on the pixel
distribution across classes.
The CB-Dice loss is motivated by the need to give more importance to minority
classes, often underrepresented in the data. In our implementation, we first calculate
the class weights by determining the proportion of pixels that belong to each class.
The intuition behind this is that the higher the proportion of a class in the image,
the lower its weight should be avoided by biasing the model towards the majority
class. These class weights are computed as follows:
class ratio = Pi
P
(21)
wi =
(class ratio)−1
P
i (class ratio)−1
(22)
where Pi represents the number of pixels belonging to class i, and P is the total
number of pixels in the image. By subtracting the class ratio from 1, we ensure that
classes with fewer pixels receive higher weights, balancing the impact of majority and
minority classes on the final loss value. The core of the CB-Dice loss is computed
similarly to the traditional dice score but includes class weights. The dice score for
each class is computed using the true positives (TP), false positives (FP), and false
negatives (FN), as follows:
Dicei =
2 · TPi
2 · TPi + FPi + FNi
(23)
The final CB-Dice score is the weighted sum of the individual dice score:
CB-Dice =
X
i
wi · Dicei
(24)
The corresponding CB-Dice loss is defined as:
LCB-Dice = 1 −CB-Dice
(25)
The CB-Dice loss ensures that the optimization process seeks to maximize the
CB-Dice score, thereby improving the segmentation performance across all classes,
especially the minority ones. This approach leads to a more balanced learning process
and helps to mitigate the effects of class imbalance in segmentation tasks.
12
8. Dataset and Materials
While deep learning classification tasks tend to be less complex and generally
involve a single loss function, more challenging tasks often require multiple loss func-
tions to handle fine-grained predictions, boundary accuracy, and class imbalance.
This makes them ideal for evaluating adaptive optimization frameworks like DMF.
Segmentation tasks, which often involve multiple loss functions, provide a valuable
context for testing the DMF framework’s effectiveness in addressing the complex-
ities of these tasks. Therefore, our focus on these tasks enables a comprehensive
assessment of DMF’s capabilities.
This research utilizes breast cancer segmentation as a case study to demonstrate
the compatibility and applicability of the DMF framework. By employing the DMF
framework, the model can dynamically adjust its focus between multiple cost func-
tions throughout the training process, ensuring that each receives appropriate atten-
tion at the right time. To support this investigation, three primary breast ultrasound
image segmentation datasets are utilized: the breast ultrasound images dataset, the
breast ultrasound cancer dataset, and a third dataset featuring synthetic images
generated from a generative model.
8.1. Breast Ultrasound Segmentation Images
The Breast Ultrasound Segmentation Images (BUSI) dataset was introduced by
Al-Dhabyani et al.
[29].
It was collected in 2018 at Baheya Hospital for Early
Detection & Treatment of Women’s Cancer in Cairo, Egypt.
It consists of data
from 600 female patients aged 25 to 75 years. The dataset comprises 780 images
categorized into normal, benign, and malignant groups. However, to evaluate the
segmentation model in this research, only the benign and malignant categories were
used, along with their corresponding ground truth masks.
8.2. Breast Ultrasound Cancer Dataset
Another dataset used in this research was introduced by Iqbal et al. [30], which
is an annotated version of the Mendeley dataset [31], referred to as the Breast Ultra-
sound Cancer (BUSC) dataset. The original Mendeley dataset, consisting of ultra-
sound images of benign and malignant breast tumors, lacked detailed annotations.
However, Iqbal et al. collaborated with experienced radiologists to manually anno-
tate the images, providing ground truth for the segmentation task. The transformed
version includes 100 benign and 150 malignant tumor images with annotations; both
images and masks have a resolution of 128×128 pixels.
8.3. Breast Ultrasound Synthetic Dataset
Additionally, this research incorporated 500 synthetic images of breast tumors
created by training a generative model on unannotated breast ultrasound images, re-
ferred to as the BUS Synthetic Dataset (BUS), as proposed by Iqbal et al. [30]. The
corresponding probability maps for these synthetic images were generated through
a separate model designed to produce segmentation probability outputs. Both the
13
images and masks have a resolution of 128×128 pixels, contributing to a more com-
prehensive and effective training dataset.
9. Preprocessing with Bilateral Filtering
In this research, bilateral filtering was applied during the preprocessing stage to
reduce speckle noise in breast ultrasound images. The bilateral filter is particularly
useful because it smooths the image while preserving edges, critical for distinguish-
ing between benign and malignant regions in the segmentation task. Studies like
[32] show that speckle-reducing bilateral filtering (SRBF) enhances boundaries while
preserving features, making it useful for ultrasound imaging and segmentation. The
bilateral filter combines both spatial proximity and pixel intensity in its weighting
function. The filtered image If(x) is computed as:
If(x) = 1
Wp
X
xi∈Ω
I(xi) · exp

−|x −xi|2
2σ2
s

· exp

−|I(x) −I(xi)|2
2σ2
r

(26)
where I(x) is the intensity of the pixel at location x, I(xi) is the intensity of the
neighboring pixel xi, and Ωrepresents the neighborhood around x. The parameter
σs controls the spatial proximity weight, determining how much influence nearby
pixels have, while σr controls the intensity difference weight, favoring pixels with
similar intensity values. Wp is the normalization factor, which is calculated as:
Wp =
X
xi∈Ω
exp

−|x −xi|2
2σ2
s

· exp

−|I(x) −I(xi)|2
2σ2
r

(27)
This filter effectively reduces speckle noise while preserving the boundaries in
ultrasound images.
10. Results
This section presents the results of applying the DMF framework with differ-
ent auxiliary loss functions. Each experiment involved training models using three
different weighting methods: variance-based, MAD-based, and Bayesian-based. To
maintain fairness in comparisons, we utilized a basic U-Net model across all ex-
periments to ensure that the methods, rather than model architectures, are being
compared. While more advanced models could enhance performance, our goal is to
isolate the benefits of the DMF framework itself. For every method, ten experiments
were conducted with the same weight initialization and data distribution for each ex-
periment, where 70% of the data was used for training, 15% for validation, and 15%
for testing, ensuring consistent comparison. The evaluation metrics include dice,
IoU, f1-score, precision, recall, and CB-Dice. The loss functions used in each ex-
periment comprised categorical cross-entropy, mean IoU, and mean Dice as primary
loss functions; additionally, Tversky, Focal, and CB-Dice were utilized as auxiliary
loss functions. The demo source code for implementing the proposed methodology
14
is openly available on GitHub at https://github.com/amingolnari/Demo-Dynamic-
Memory-Fusion-Framework. To facilitate usage, readers can run the code directly in
Google Colab.
Table 1 compares model performance, reported in percentages, with and without
the DMF framework, and various loss functions on the BUSI dataset. Similarly,
Table 2 compares model performance on the BUSC dataset using the same reporting
style. Additionally, Table 3 compares model performance on the BUS dataset.
Table 1: Performance metrics summary using the DMF framework with different auxiliary loss
functions, alongside a baseline model without DMF. All models were evaluated on the BUSI dataset.
Loss Function
Weighting Method
Dice
IoU
F1-score
Precision
Recall
CB-Dice
Tversky Loss
Variance
93.30 ±0.91
87.91 ±1.44
93.36 ±0.92
94.37 ±1.02
92.28 ±1.46
89.36 ±1.76
MAD
93.75 ±0.69
88.64 ±1.12
93.82 ±0.68
95.31 ±0.70
92.38 ±1.07
90.17 ±1.37
Bayesian
90.53 ±1.56
83.57 ±2.31
90.67 ±1.73
92.17 ±3.82
89.38 ±0.76
85.01 ±3.04
Focal Loss
Variance
94.35 ±1.01
89.67 ±1.70
94.43 ±0.97
96.12 ±0.61
92.84 ±1.71
91.13 ±1.68
MAD
92.67 ±1.02
86.88 ±1.63
92.98 ±0.91
96.95 ±0.37
89.41 ±1.52
88.41 ±1.72
Bayesian
93.19 ±1.17
87.75 ±1.86
93.22 ±1.18
94.13 ±1.12
92.42 ±1.44
89.25 ±2.27
CB-Dice Loss
Variance
95.22 ±0.53
91.34 ±0.43
95.25 ±0.55
95.58 ±0.89
94.93 ±0.87
92.55 ±0.97
MAD
95.02 ±0.52
90.75 ±0.49
94.98 ±0.49
95.14 ±0.39
94.82 ±0.51
91.98 ±0.76
Bayesian
92.60 ±0.46
87.42 ±0.30
93.31 ±0.53
91.56 ±0.18
95.12 ±0.87
88.94 ±0.32
No Auxiliary Loss
Variance
93.93 ±1.08
88.99 ±1.77
93.98 ±1.05
95.31 ±0.63
92.72 ±1.70
90.22 ±1.87
MAD
93.20 ±0.95
87.77 ±1.55
93.32 ±0.88
95.34 ±1.20
91.45 ±1.93
89.05 ±1.55
Bayesian
91.97 ±1.05
85.51 ±1.64
91.98 ±1.06
92.03 ±1.60
91.99 ±0.67
87.09 ±1.85
Using Fixed Weights
Not Used
91.84 ±0.73
85.19 ±0.97
91.81 ±0.86
92.78 ±0.96
90.85 ±0.63
87.05 ±1.41
The findings from Table 1 highlight that using the CB-Dice loss function with
the variance-based weighting method consistently produced the best results across
most metrics, including dice (95.22 ±0.53%), IoU (91.34 ±0.43%), f1-score (95.25
±0.55%), and CB-Dice (92.55 ±0.97%). This combination outperformed other loss
functions and weighting methods. Models utilizing the Focal and Tversky losses also
showed strong performance, particularly when combined with the variance-based
and MAD-based weighting methods. Focal loss achieved the highest precision (96.12
±0.61%) among all setups. In contrast, models without auxiliary loss or using fixed
weights performed noticeably worse, particularly with lower IoU and CB-Dice scores.
The comparison of model predictions with different auxiliary loss functions and
the ground truth masks is presented in Figure 1, showcasing the segmentation per-
formance across multiple samples from the BUSI dataset. Each model was selected
based on its highest score on the CB-Dice metric.
15
a
b
c
d
e
f
g
h
Figure 1: Each row presents a sample from the BUSI dataset, showing the input breast ultrasound
image and the corresponding ground truth mask alongside model predictions. (a) Original breast
ultrasound image, (b) Bilaterally filtered image, (c) Ground truth mask, (d) Predicted mask using
DMF with Tversky loss as the auxiliary loss function, (e) Predicted mask using DMF with Focal
loss as the auxiliary loss function, (f) Predicted mask using DMF with CB-Dice loss as the auxiliary
loss function, (g) Predicted mask using DMF without auxiliary loss function, (h) Predicted mask
using fixed weights for loss functions.
Table 2: Performance metrics summary using the DMF framework with different auxiliary loss
functions, alongside a baseline model without DMF. All models were evaluated on the BUSC
dataset.
Loss Function
Weighting Method
Dice
IoU
F1-score
Precision
Recall
CB-Dice
Tversky Loss
Variance
95.29 ±0.31
91.15 ±0.52
95.34 ±0.25
94.27 ±0.63
96.45 ±0.16
93.68 ±0.44
MAD
94.79 ±0.69
90.35 ±1.13
94.85 ±0.65
94.36 ±1.31
95.41 ±1.59
92.93 ±0.90
Bayesian
94.94 ±0.43
90.59 ±0.67
94.98 ±0.40
94.97 ±0.86
95.04 ±1.55
92.66 ±1.17
Focal Loss
Variance
94.67 ±0.23
90.07 ±0.37
94.80 ±0.21
92.90 ±0.54
96.78 ±0.21
92.70 ±0.66
MAD
94.80 ±0.41
90.21 ±0.71
94.87 ±0.43
93.64 ±0.66
96.17 ±1.21
92.67 ±0.73
Bayesian
94.75 ±0.62
90.24 ±1.09
94.76 ±0.61
95.07 ±0.56
94.45 ±0.72
92.63 ±0.82
CB-Dice Loss
Variance
95.28 ±0.53
91.75 ±0.32
95.67 ±0.22
94.44 ±0.41
96.94 ±0.85
93.28 ±0.71
MAD
94.49 ±0.25
89.75 ±0.41
94.53 ±0.30
94.28 ±0.64
94.80 ±1.21
92.51 ±0.68
Bayesian
95.24 ±0.24
91.07 ±0.39
95.27 ±0.22
94.43 ±0.68
96.14 ±0.27
93.55 ±0.63
No Auxiliary Loss
Variance
94.89 ±0.90
89.10 ±1.50
94.01 ±0.74
94.28 ±0.78
93.76 ±1.03
92.63 ±0.90
MAD
93.95 ±1.15
89.31 ±0.74
93.55 ±0.96
93.64 ±1.04
93.47 ±1.32
92.21 ±1.55
Bayesian
94.37 ±0.64
90.51 ±0.32
94.40 ±0.50
93.83 ±0.70
94.98 ±0.54
92.48 ±0.89
Using Fixed Weights
Not Used
93.40 ±0.81
89.77 ±1.14
94.06 ±0.89
93.76 ±1.41
94.36 ±0.78
92.39 ±1.08
16
Table 2 presents that the dice scores range from 93.95% to 95.29%, while the IoU
values vary between 89.10% and 91.75%, reflecting minimal variation between meth-
ods. Notably, the CB-Dice loss function with the Variance-based weighting method
achieved the highest mean IoU (91.75 ±0.32%) and nearly the highest dice score
(95.28 ±0.53%), demonstrating superior overall performance. Meanwhile, Tversky
loss with variance-based weighting performed similarly well, achieving the highest
dice score (95.29 ±0.31%) and a recall of (96.45 ±0.16%). The models without aux-
iliary losses showed slightly lower metrics, with the best dice score (94.89 ±0.90%)
using the variance-based weighting method. These results indicate that, while subtle,
the choice of auxiliary loss function and weighting method has only a minor impact
on overall model performance in this dataset.
a
b
c
d
e
f
g
h
Figure 2: Each row presents a sample from the BUSC dataset, showing the input breast ultrasound
image and the corresponding ground truth mask alongside model predictions. (a) Original breast
ultrasound image, (b) Bilaterally filtered image, (c) Ground truth mask, (d) Predicted mask using
DMF with Tversky loss as the auxiliary loss function, (e) Predicted mask using DMF with Focal
loss as the auxiliary loss function, (f) Predicted mask using DMF with CB-Dice loss as the auxiliary
loss function, (g) Predicted mask using DMF without auxiliary loss function, (h) Predicted mask
using fixed weights for loss functions.
The comparison of model predictions with different auxiliary loss functions and
the ground truth masks is presented in Figure 2, showcasing the segmentation per-
17
formance across multiple samples from the BUSC dataset. Each model was selected
based on its highest score on the CB-Dice metric.
Table 3: Performance metrics summary using the DMF framework with different auxiliary loss
functions, alongside a baseline model without DMF. All models were evaluated on the BUS dataset.
Loss Function
Weighting Method
Dice
IoU
F1-score
Precision
Recall
CB-Dice
Tversky Loss
Variance
93.35 ±0.22
91.28 ±0.37
95.42 ±0.21
97.21 ±0.42
93.69 ±0.49
93.20 ±0.30
MAD
92.34 ±0.45
86.27 ±0.27
92.78 ±0.39
97.33 ±0.27
88.86 ±0.64
88.72 ±0.65
Bayesian
94.15 ±0.32
89.24 ±0.59
94.20 ±0.34
95.53 ±0.58
93.14 ±0.68
91.47 ±0.49
Focal Loss
Variance
94.08 ±0.54
89.14 ±0.92
94.24 ±0.51
96.81 ±0.28
91.81 ±0.76
91.33 ±0.79
MAD
93.09 ±0.18
87.43 ±0.29
93.37 ±0.15
96.97 ±0.17
90.07 ±0.29
89.80 ±0.25
Bayesian
94.27 ±1.31
89.47 ±1.67
94.37 ±0.96
95.80 ±0.32
92.98 ±1.93
91.62 ±1.48
CB-Dice Loss
Variance
95.71 ±0.37
91.90 ±0.63
95.79 ±0.35
95.30 ±0.71
95.68 ±0.62
93.72 ±0.67
MAD
95.05 ±0.75
91.54 ±0.46
95.27 ±0.63
93.30 ±0.53
97.31 ±0.84
93.65 ±0.38
Bayesian
92.79 ±0.51
87.19 ±0.81
93.32 ±0.41
90.16 ±0.87
96.28 ±0.23
89.55 ±0.80
No Auxiliary Loss
Variance
93.99 ±0.36
88.96 ±0.62
94.11 ±0.35
96.24 ±0.30
92.06 ±0.48
91.20 ±0.55
MAD
93.26 ±1.58
87.81 ±2.58
93.57 ±1.34
96.94 ±0.12
90.47 ±2.58
90.08 ±2.40
Bayesian
93.84 ±0.11
88.70 ±0.19
93.93 ±0.12
95.80 ±0.63
92.16 ±0.50
90.99 ±0.15
Using Fixed Weights
Not Used
92.16 ±1.38
85.34 ±0.78
91.29 ±0.67
91.64 ±0.89
90.93 ±0.76
88.28 ±1.46
Notably, the Table 3 highlights that the CB-Dice loss with variance-based weight-
ing outperformed all other configurations, achieving the highest dice score (95.71
±0.37%), IoU (91.90 ±0.63%), f1-score (95.79 ±0.35%), and CB-Dice (93.72 ±0.67%),
making it the most effective combination. The MAD-based weighting method with
CB-Dice loss also showed strong results, particularly in precision (97.31 ±0.84%).
Among the models using Tversky loss, the MAD-based weighting method excelled
in precision (97.33 ±0.27%) and provided a balanced performance across metrics.
Meanwhile, models without auxiliary loss had consistently lower results, with the
highest dice score of (91.20 ±0.55%) and IoU of (88.96 ±0.62%), falling short com-
pared to models with auxiliary losses.
The comparison of model predictions with different auxiliary loss functions and
the ground truth masks is presented in Figure 3, showcasing the segmentation per-
formance across multiple samples from the BUS dataset. Each model was selected
based on its highest score on the CB-Dice metric.
18
a
b
c
d
e
f
g
h
Figure 3: Each row presents a sample from the BUS dataset, showing the input breast ultrasound
image and the corresponding ground truth mask alongside model predictions. (a) Original breast
ultrasound image, (b) Bilaterally filtered image, (c) Ground truth mask, (d) Predicted mask using
DMF with Tversky loss as the auxiliary loss function, (e) Predicted mask using DMF with Focal
loss as the auxiliary loss function, (f) Predicted mask using DMF with CB-Dice loss as the auxiliary
loss function, (g) Predicted mask using DMF without auxiliary loss function, (h) Predicted mask
using fixed weights for loss functions.
Overall, these results demonstrate that the choice of auxiliary loss function and
the weighting method play critical roles in optimizing segmentation performance on
each dataset.
Figure 4 shows the dynamic weighting behavior of W1, W2, and W3 during train-
ing for categorical cross-entropy, mean IoU, and mean dice loss functions in the DMF
framework. It employs three different weighting methods: variance-based, MAD-
based, and Bayesian-based. The variance-based method adjusts weights gradually in
the early stages, responding to fluctuations in task performance while maintaining
a balanced distribution throughout the training process.
In contrast, the MAD-
based method is more sensitive, resulting in sharper weight adjustments. Although
the Bayesian-based method exhibits smooth changes, it adopts a more aggressive
approach, focusing on a dominant task during training and maintaining this prefer-
ence. Each technique offers a unique strategy for handling multi-task loss functions,
with different levels of adaptability and task prioritization.
19
0
20
40
60
80
100
0.2
0.3
0.4
Weights Value
Variance-based weighting method
W1
W2
W3
0
20
40
60
80
100
0.2
0.3
0.4
Variance-based weighting method
W1
W2
W3
0
20
40
60
80
100
0.2
0.3
0.4
Variance-based weighting method
W1
W2
W3
0
20
40
60
80
100
0.2
0.3
0.4
Weights Value
MAD-based weighting method
W1
W2
W3
0
20
40
60
80
100
0.2
0.3
0.4
MAD-based weighting method
W1
W2
W3
0
20
40
60
80
100
0.2
0.3
0.4
MAD-based weighting method
W1
W2
W3
0
20
40
60
80
100
Epochs
0.00
0.25
0.50
0.75
1.00
Weights Value
Bayesian-based weighting method
W1
W2
W3
0
20
40
60
80
100
Epochs
0.00
0.25
0.50
0.75
1.00
Bayesian-based weighting method
W1
W2
W3
0
20
40
60
80
100
Epochs
0.00
0.25
0.50
0.75
1.00
Bayesian-based weighting method
W1
W2
W3
Figure 4: Weight dynamics of W1 in blue for categorical cross-entropy, W2 in red for mean IoU,
and W3 in green for mean dice during training in the DMF framework. The first row shows the
variance-based method, the second row shows the MAD-based method, and the third row shows
the Bayesian-based method. Models are trained on the BUSI dataset
Figure 5 illustrates the dynamic behavior of the loss functions L1, L2, and L3,
respectively, representing categorical cross-entropy, mean IoU, and mean Dice func-
tions within the DMF framework. The figure demonstrates the performance of three
different weighting methods: variance-based, MAD-based, and Bayesian-based. The
convergence of all loss functions is evident as each steadily decreases without signif-
icant fluctuations.
After a certain number of epochs, there are no considerable changes in the loss
values, indicating that they have reached a stable point. This confirms successful
optimization convergence, where further training brings minimal improvements. Al-
though the loss values remain stable, the weights continue to adjust (except for the
Bayesian-based weighting method, which is aggressive yet smooth in its changes),
according to Figure 4. This behavior suggests that the DMF framework avoids lo-
cal minima and maintains adaptability during training, ensuring robust performance
across tasks even in later stages.
20
0
20
40
60
80
100
0.2
0.4
0.6
0.8
Loss Value
Variance-based weighting method
L1
L2
L3
0
20
40
60
80
100
0.2
0.4
0.6
0.8
Variance-based weighting method
L1
L2
L3
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
Variance-based weighting method
L1
L2
L3
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
Loss Value
MAD-based weighting method
L1
L2
L3
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
MAD-based weighting method
L1
L2
L3
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
MAD-based weighting method
L1
L2
L3
0
20
40
60
80
100
Epochs
0.0
0.2
0.4
0.6
0.8
Loss Value
Bayesian-based weighting method
L1
L2
L3
0
20
40
60
80
100
Epochs
0.0
0.2
0.4
0.6
0.8
Bayesian-based weighting method
L1
L2
L3
0
20
40
60
80
100
Epochs
0.0
0.2
0.4
0.6
0.8
Bayesian-based weighting method
L1
L2
L3
Figure 5: Loss dynamics of L1 in blue for categorical cross-entropy, L2 in red for mean IoU,
and L3 in green for mean Dice during training in the DMF framework. The first row shows the
variance-based method, the second row shows the MAD-based method, and the third row shows
the Bayesian-based method. Models are trained on the BUSI dataset
11. Conclusion
In this study, we introduced the DMF framework for adaptive multi-loss function
optimization in deep learning, specifically applied to breast cancer segmentation.
The framework allows for real-time adjustment of loss function weights, effectively
addressing challenges such as class imbalance and varying task importance during
training.
Our experiments on the BUSI, BUSC, and BUS datasets demonstrated improve-
ments in segmentation performance, supported by various evaluation metrics, includ-
ing Dice, IoU, and Precision. The incorporation of CB-Dice loss has shown potential
in mitigating class imbalance issues. The DMF framework is not limited to medi-
cal imaging and can be applied to other deep learning tasks requiring multiple loss
functions. By providing open-source code, we aim to enhance reproducibility and
encourage further exploration of this approach.
While deep learning classification tasks are generally less complex and typically
involve only a single loss function, segmentation presents greater challenges.
In
segmentation, multiple loss functions are often necessary to address pixel-wise pre-
dictions, boundary accuracy, and class imbalance. This makes it a more suitable
domain for testing adaptive optimization frameworks like DMF. Therefore, our fo-
cus on segmentation tasks allows us to fully explore and demonstrate the effectiveness
21
of the DMF framework in managing these complex tasks.
For future work, we suggest investigating the DMF framework’s scalability and
its application in diverse machine learning scenarios. Previous observations may also
be beneficial in limiting weight adjustments and exploring additional methods, such
as exponential moving averages (EMA), for optimizing weight updates.
Data Availability Statement
The BUSI dataset can be accessed openly at https://www.kaggle.com/datasets/
sabahesaraki/breast-ultrasound-images-dataset.
The BUSC dataset is available
at https://data.mendeley.com/datasets/vckdnhtw26/1, while the BUS Synthetic
dataset can be found at https://data.mendeley.com/datasets/r4phtn49r7/1.
Declaration of Competing Interest
The authors declare that they have no known competing financial interests or
personal relationships that could have appeared to influence the work reported in
this paper.
Authorship Contribution Statement
Amin Golnari: Conceptualization, Software, Investigation, Methodology, Visu-
alization, Writing – Original Draft, Writing – Review & Editing. Mostafa Diba:
Investigation, Writing – Original Draft, Writing – Review & Editing.
Declaration of Generative AI in Scientific Writing
During the preparation of this work, the authors used OpenAI’s GPT-4o to en-
hance overall readability, improve language, and correct grammatical errors. After
using this service, the authors reviewed and edited the content as needed and take
full responsibility for the content of the publication.
References
[1] J. Feng, X. Dong, S. Chen, L. Zhou, X. Zheng, Gbe-net:
Global bound-
ary enhancement network for breast lesion segmentation in ultrasound images,
Biomedical Signal Processing and Control 96 (2024) 106644.
[2] R. Wu, X. Lu, Z. Yao, Y. Ma, Mfmsnet: A multi-frequency and multi-scale
interactive cnn-transformer hybrid network for breast ultrasound image seg-
mentation, Computers in Biology and Medicine 177 (2024) 108616.
[3] A. A. Heydari, C. A. Thompson, A. Mehmood, Softadapt: Techniques for adap-
tive loss weighting of neural networks with multi-part loss functions, arXiv
preprint arXiv:1912.12355 (2019).
22
[4] K. R. M. Fernando, C. P. Tsokos, Dynamically weighted balanced loss: class
imbalanced learning and confidence calibration of deep neural networks, IEEE
Transactions on Neural Networks and Learning Systems 33 (7) (2021) 2940–
2951.
[5] B. Guermazi, R. Ksantini, N. Khan, A dynamically weighted loss function for
unsupervised image segmentation, in: 2022 IEEE Eighth International Confer-
ence on Multimedia Big Data (BigMM), IEEE, 2022, pp. 73–78.
[6] Y. Song, J. Y.-C. Teoh, K.-S. Choi, J. Qin, Dynamic loss weighting for multi-
organ segmentation in medical images, IEEE transactions on neural networks
and learning systems (2023).
[7] A. Erdogan, E. Akagunduz, Fuseformer: A transformer for visual and thermal
image fusion, arXiv preprint arXiv:2402.00971 (2024).
[8] Y. Zhao, Y. Xia, Y. Ding, Y. Liu, S. Liu, H. Wang, S2f-net: Shared-specific
fusion network for infrared and visible image fusion, in: Proceedings of the 2024
International Conference on Multimedia Retrieval, 2024, pp. 497–505.
[9] W.-F. Ou, L.-M. Po, C. Zhou, Y. A. U. Rehman, P.-F. Xian, Y.-J. Zhang, Fusion
loss and inter-class data augmentation for deep finger vein feature learning,
Expert Systems with Applications 171 (2021) 114584.
[10] J. Tasnim, M. K. Hasan, Cam-qus guided self-tuning modular cnns with multi-
loss functions for fully automated breast lesion classification in ultrasound im-
ages, Physics in Medicine & Biology 69 (1) (2023) 015018.
[11] S. Jiang, J. Li, J. Zhang, Y. Wang, T. Xu, Dynamic loss for robust learning,
IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).
[12] A. Jesson, T. Arbel, Brain tumor segmentation using a 3d fcn with multi-scale
loss, in: International MICCAI Brainlesion Workshop, Springer, 2017, pp. 392–
402.
[13] M. Sharifzadeh, H. Benali, H. Rivaz, Phase aberration correction without ref-
erence data: An adaptive mixed loss deep learning approach, arXiv preprint
arXiv:2303.05747 (2023).
[14] Q. Lv, R. Yang, Y. Chen, Z. Zhou, C. Zhang, S. Liu, Ssdfusion: A semantic
segmentation driven framework for infrared and visible image fusion (2024).
[15] Q. Gao, Q. Peng, S. Xuan, K. Xiong, Multi-loss function fusion for face recog-
nition based on the convolutional neural network, in: 2019 12th International
Congress on Image and Signal Processing, BioMedical Engineering and Infor-
matics (CISP-BMEI), IEEE, 2019, pp. 1–6.
23
[16] Y. Zhao, X. Shen, J. Chen, W. Qian, L. Sang, H. Ma, Learning active con-
tour models based on self-attention for breast ultrasound image segmentation,
Biomedical Signal Processing and Control 89 (2024) 105816.
[17] Y. Zhao, X. Shen, J. Chen, W. Qian, H. Ma, L. Sang, loss for low-contrast
medical image segmentation, Machine Learning: Science and Technology 5 (1)
(2024) 015013.
[18] F.-Y. Chao, L. Zhang, W. Hamidouche, O. D´eforges, A multi-fov viewport-
based visual saliency model using adaptive weighting losses for 360◦images,
IEEE Transactions on Multimedia 23 (2020) 1811–1826.
[19] B. Sushma, A. Pulikala, Aapfc-busnet: Hierarchical encoder–decoder based cnn
with attention aggregation pyramid feature clustering for breast ultrasound im-
age lesion segmentation, Biomedical Signal Processing and Control 91 (2024)
105969.
[20] C. Mang, A. Tahmasebimoradi, D. Danan, M. Yagoubi, A dynamic weighted
loss function for enhancing the performance of neural networks, in: 16th World
Congress on Computational Mechanics (WCCM), 2024.
[21] S. Lu, F. Gao, C. Piao, Y. Ma, Dynamic weighted cross entropy for semantic
segmentation with extremely imbalanced data, in: 2019 International conference
on artificial intelligence and advanced manufacturing (AIAM), IEEE, 2019, pp.
230–233.
[22] S. Maldonado, C. Vairetti, K. Jara, M. Carrasco, J. L´opez, Owadapt: An adap-
tive loss function for deep learning using owa operators, Knowledge-Based Sys-
tems 280 (2023) 111022.
[23] Y. Runda, X. Yan, Z. Mingfang, Z. Li, W. Hongbin, Lstm sentiment classifi-
cation model based on one kind of dynamic loss weighting function, in: 2020
7th International Conference on Information Science and Control Engineering
(ICISCE), IEEE, 2020, pp. 819–824.
[24] D. Roy, R. Pramanik, R. Sarkar, Margin-aware adaptive-weighted-loss for deep
learning based imbalanced data classification, IEEE Transactions on Artificial
Intelligence 5 (2) (2023) 776–785.
[25] R. Groenendijk, S. Karaoglu, T. Gevers, T. Mensink, Multi-loss weighting with
coefficient of variations, in: Proceedings of the IEEE/CVF winter conference on
applications of computer vision, 2021, pp. 1469–1478.
[26] D. Ocampo, D. Posso, R. Namakian, W. Gao, Adaptive loss weighting for
machine learning interatomic potentials, Computational Materials Science 244
(2024) 113155.
24
[27] Z. Xiang, W. Peng, X. Liu, W. Yao, Self-adaptive loss balanced physics-informed
neural networks, Neurocomputing 496 (2022) 11–34.
[28] J. T. Barron, A general and adaptive robust loss function, in: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition, 2019,
pp. 4331–4339.
[29] W. Al-Dhabyani, M. Gomaa, H. Khaled, A. Fahmy, Dataset of breast ultrasound
images, Data in brief 28 (2020) 104863.
[30] A. Iqbal, M. Sharif, Unet: A semi-supervised method for segmentation of breast
tumor images using a u-shaped pyramid-dilated network, Expert Systems with
Applications 221 (2023) 119718.
[31] P. S. Rodrigues, Breast ultrasound image, Mendeley Data 1 (10) (2017).
[32] S. Balocco, C. Gatta, O. Pujol, J. Mauri, P. Radeva, Srbf: Speckle reducing
bilateral filtering, Ultrasound in medicine & biology 36 (8) (2010) 1353–1363.
25
