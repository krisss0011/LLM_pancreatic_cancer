AN EXPLAINABLE ATTENTION MODEL FOR CERVICAL
PRECANCER RISK CLASSIFICATION USING COLPOSCOPIC
IMAGES ∗
Smith K. Khare
1Applied AI and Data Science Unit, Mærsk Mc-Kinney Møller Institute
Faculty of Engineering
University of Southern Denmark
Odense, Denmark
2Centre for Clinical Artificial Intelligence
Odense University Hospital
Odense, Denmark
smkh@mmmi.sdu.dk
Berit Bargum Booth
Centre for Department of Gynecology and Obstetrics
Odense University Hospital
Odense, Denmark
Victoria Blanes-Vidal
Applied AI and Data Science Unit, Mærsk Mc-Kinney Møller Institute
Faculty of Engineering
University of Southern Denmark
Odense, Denmark
Lone Kjeld Petersen
Centre for Research Unit for Gynecology and Obstetrics (Odense)
Odense University Hospital
Odense, Denmark
Esmaeil S. Nadimi
1Applied AI and Data Science Unit, Mærsk Mc-Kinney Møller Institute
Faculty of Engineering
University of Southern Denmark
Odense, Denmark
2Centre for Clinical Artificial Intelligence
Odense University Hospital
Odense, Denmark
smkh@mmmi.sdu.dk
ABSTRACT
Cervical cancer remains a major worldwide health issue, with early identification and risk assessment
playing critical roles in effective preventive interventions. This paper presents the Cervix-AID-Net
∗Citation: Khare et al. An Explainable Attention Model for Cervical Precancer Risk Classification using Colposcopic
Images. Pages.... DOI:000000/11111.
arXiv:2411.09469v1  [eess.IV]  14 Nov 2024
Running Title for Header
model for cervical precancer risk classification. The study designs and evaluates the proposed Cervix-
AID-Net model based on patients colposcopy images. The model comprises a Convolutional Block
Attention Module (CBAM) and convolutional layers that extract interpretable and representative
features of colposcopic images to distinguish high-risk and low-risk cervical precancer. In addition,
the proposed Cervix-AID-Net model integrates four explainable techniques, namely gradient class
activation maps, Local Interpretable Model-agnostic Explanations, CartoonX, and pixel rate distortion
explanation based on output feature maps and input features. The evaluation using holdout and ten-
fold cross-validation techniques yielded a classification accuracy of 99.33% and 99.81%. The analysis
revealed that CartoonX provides meticulous explanations for the decision of the Cervix-AID-Net
model due to its ability to provide the relevant piece-wise smooth part of the image. The effect
of Gaussian noise and blur on the input shows that the performance remains unchanged up to
Gaussian noise of 3% and blur of 10%, while the performance reduces thereafter. A comparison study
of the proposed model’s performance compared to other deep learning approaches highlights the
Cervix-AID-Net model’s potential as a supplemental tool for increasing the effectiveness of cervical
precancer risk assessment. The proposed method, which incorporates the CBAM and explainable
artificial integration, has the potential to influence cervical cancer prevention and early detection,
improving patient outcomes and lowering the worldwide burden of this preventable disease.
Keywords Cervical cancer · colposcopy · deep learning · attention module · explainable artificial intelligence
1
Introduction
Cervical cancer is the fourth leading cause of death among female malignancies, with high morbidity and mortality
rates if diagnosed in late stages, mainly affecting sexually active adult women aged over 30 years [1, 2, 3]. The cervix
is part of the female reproductive system, located in the lowest fibromuscular section of the uterus and accessible for
inspection and sampling though the vagina. The cervix has different linings. The endocervical canal is lined with
glandular epithelium, and the ectocervix is lined with squamous epithelium. The squamous epithelium meets the
glandular epithelium at the squamocolumnar junction (SCJ). The SCJ moves during early adolescence and during a first
pregnancy. The original SCJ originates in the endocervical canal, but over time, the SCJ comes to lie on the ectocervix
and becomes the new SCJ [4]. In colposcopy terminology, the SCJ is this new SCJ. The epithelium between these two
SCJs is the transition zone (or transformation zone, TZ), and its position is variable, depending on factors such as age,
hormonal status, birth trauma, use of oral contraceptives, and pregnancy [5, 6]. Colposcopically the TZ is classified as:
Type 1, Type 2 and Type 3, depending on its visibility [7, 8]. Cervical cancer is preceded by Cervical Intraepithelial
Neoplasia (CIN) which arises in the TZ. In the year 2018, the World Health Organization (WHO) issued a worldwide
call to eliminate cervical cancer [9]. Cervical cancer develops from CIN lesions over years making time for screening,
diagnosis and preventive treatment. There is an urgent need for accurate and timely detection of cervical cancer. This is
particularly so in low- and middle-income nations, where severe poverty and gender discrimination significantly restrict
a woman’s ability to seek care, accounting for almost 88% of cervical cancer fatalities [10].
Many screening methods are available today to detect precancerous lesions and abnormal growth of epithelial cells on
the cervix. These methods include primary screening with cervical cytology (Pap tests), the human papillomavirus
(HPV) test or co-testing. Cervical cytology requires competent cytologists to perform microscopic analyses, often
unavailable in low-resource settings [11]. The HPV test is recommended by the WHO strategy due to the high sensitivity
of the test, but the associated lower specificity increases the number of screen positive females referred for secondary
screening by colposcopy and adds to the burden of specialists. Medical authorities recommend colposcopy as the gold
standard for assessing cervical cancer precursors. Examining colposcopy images is time-consuming and requires trained
medical specialists. Even skilled colposcopist miss cervical precancerous lesions in need for preventive treatment in
40% of examinations [12]. In recent years, automated decision-making using medical imaging techniques has increased
multi-fold due to the advancement of artificial intelligence (AI). Automated decision-making AI models has reflected
an immense potential for the detection of malignant tumors [13, 14, 15]. These advancements in AI have also attracted
colposcopists to get assistance from AI in clinical decision-making. Therefore, there is an increasing interest in using
AI to automate colposcopy image assessments, so that, identifying the presence of precancerous or cancerous cells in
the cervix, becomes feasible at a large scale, with higher speed and lower costs, than nowadays.
2
Literature review
The literature review discusses the recently developed automated decision-making AI models for assessment of cervical
precancer. Li et al. [16] implemented a graph convolutional network with edge features (E-GCN) for classifying
2
Running Title for Header
negative (Neg) and positive (Pos) classes. The positive class is comprised of low-grade squamous intraepithelial lesions
or worse (LSIL+), while the negative class includes non-cancerous cases. Elakkiya et al. [17] presented a hybrid
model called faster small-object detection neural networks (FSOD-GAN) combining faster region-based convolutional
neural network (FR-CNN) and generative adversarial network (GAN) to detect normal cervical images (categorized as
type 1, type 2 and type 3), and abnormal (AN) cervical images categorized based on cervical cancer severity, stage-I,
stage-II, and stage-III. Adweb et al. [18] utilized pre-existing 18-layer residual networks to classify healthy control
(HC) and cervical images from pre-cancerous lesions (PC). The authors tested their model by combining 4000 PC
images from one dataset and 800 from another, translated (augmented) to 1920 images. Kim et al. [19] developed
a convolutional neural network (CNN)-based architecture called AIDOT to classify normal, CIN1, CIN2/CIN3, and
cancer classes. Saini et al. [20] developed a CNN-based model called ColpoNet, which has been motivated by the
DenseNet model. The authors performed binary classification of colposcopic images in Type 1, which consists of
normal and CIN1, while Type 2 includes CIN2 or worse, respectively. Habtemariam et al. [21] used an existing
EfficientNetB0-based CNN model to perform cervix Type 1 TZ, Type 2 TZ, and Type 3 TZ classification. Ma et al. [22]
performed four class classifications to detect normal, LSIL, high-grade squamous intraepithelial lesion (HSIL), and
cancer. The authors combined CNN-based segmentation, following color and Haralick texture feature extraction, and a
multi-modal classifier model. Binhua et al. [23] developed colposcopy-based classification and diagnosis of cervical
lesions using a dense U-NET model to detect CIN1, CIN2, and CIN3. Kim et al. [24] developed a binary classification
model to classify normal/CIN1 and CIN2 or worse (CIN2+) using cervigram images (now discontinued). Cho et al.
[25] used Inception-Resnet-v2 to classify high-risk (CIN2, CIN3, and cancer) and low-risk (NC and CIN1) CINs;
while Resnet-152 to classify the low-risk squamous intraepithelial lesions (LSIL and NC) and high-risk squamous
intraepithelial lesions (cancer and HSIL). Liu et al. [26] developed a two binary classification model for classifying
NC versus LSIL+ and HSIL- versus HSIL+ using the residual network-50 model. Chandran et al. [27] developed
a colposcopy ensemble network (CYENET) to detect Type 1 TZ, Type 2 TZ, and Type 3 TZ cervical cancer in the
colposcopy images. Wu et al. [28] developed a colposcopic artificial intelligence auxiliary diagnostic system (CAIADS)
using the CNN model to detect CIN2+, CIN3+, and cancer. Sim˜oes et al. [29] used a hybrid neural network based
on Kohonen’s self-organizing maps and multi-layer perceptron (MLP) model to classify dot patterns in colposcopic
images. Asiedu et al. [30] developed an automated model using Gabor segmentation followed by Haralick’s texture,
color space transformation calculations, and lesion size estimation features. A support vector machine classifier was
used to classify these features into positive class (CIN1, CIN2, CIN3, and invasive cancer) and negative class (normal,
condiloma, and cervicitis). Miyagi et al. [31] proposed the classification of uterine cervical squamous epithelial lesions
from colposcopy images. Their method used a CNN model to classify low-grade (consisting of CIN1) and high-grade
(combining CIN2 and CIN3) classes. Song et al. [32] proposed a binary classification model for classifying negative
class (CIN1) and positive class (CIN2/3+). The authors used a multi-modal entity coreference CNN model to detect the
corresponding classes using cervicography images. Miyagi et al. [33] used the CNN model for the binary classification
of CIN1 and CIN2+ using colposcopic images.
The literature review reveals that the majority of existing methods used pre-existing models for the detection of cervical
lesions. Besides, most of existing automated decision-making models have limited performance concerning accuracy
and other evaluation matrices (refer Table 7). Also, most studies validate their model’s performance using holdout
(HO) validations, which are highly prone to over-fitting and biases. Additionally, only a few studies have explored
explainable artificial intelligence (XAI), which attempts to explain model decisions. This demands a need for an
accurate and explainable model for cervical precancer risk classification. Therefore, this study presents a novel and
explainable cervical precancer risk classification model using colposcopic images. The model uses a combination of
five convolutional layers and five Convolutional Block Attention Module (CBAM) to develop the proposed Cervix-
AID-Net model. Also, the proposed Cervix-AID-Net model integrates gradient class activation maps (Grad-CAM),
Local Interpretable Model-agnostic Explanations (LIME), CartoonX, and pixel rate distortion explanation (RDE). The
significant contributions of the paper are listed as follows:
• We propose a simple, lightweight, and effective cervical precancer risk classification model using CBAM that
can be widely applied to boost the representation power of CNNs.
• To the best of our knowledge, we are the first group to present the integration of Grad-CAM, LIME, CartoonX,
and pixel RDE for the explanations.
• In this paper we evaluate the amount of distortion required for each module to flip the decision from one class
to another.
The overall paper is organized as, Section 1 presents an introduction, literature review in Section 2, details about the
Cervix-AID-Net model in Section 3.2, experimental setup and results in Section 4, Discussion in Section 5, and finally,
Section 6 concludes the paper.
3
Running Title for Header
3
Methodology
The description of the proposed model comprises four subsections: dataset description, details of the proposed
Cervix-AID-Net model, performance evaluation, and explainable AI. Figure 1 shows an overview of the proposed
Cervix-AID-Net.
Figure 1: The proposed explainable Cervix-AID-Net model for high-risk and low-risk classification.
3.1
Dataset details
All data and images were collected from patients examined using a DYSIS colposcope Version 3 at the Department of
Gynecology and Obstetrics, Randers Regional Hospital, Denmark between 2017-2020. Colposcopy examinations were
included if the colposcopists had reported a full or partially visible transformation zone, and four cervical biopsies taken.
Images not suitable for annotation were excluded (blurry, too much light, mucous or blood covering visible changes).
Based on international guidelines for risk assessment and treatment of cervical cancer precursors histological diagnoses
of normal, inflammation and CIN1 were considered to represent low-grade disease. Other histological diagnoses present
were CIN grade 2, CIN3, carcinoma in situ (CIS), adenocarcinoma in situ (AIS) and squamous cell carcinoma (SCC)
were classified to represent high-grade disease. Images from women with a diagnosis of only ungradable CIN were
removed (n=1). Women who had one or more other diagnosis were still included and the worst of the other diagnoses
was considered to be the grade of dysplasia present. [34]. The details of the dataset is shown in Table 1.
3.2
Cervix-AID-Net
Attention has a significant influence on human perception [35]. The key feature of human visual systems is that they do
not process entire scenes at once. Instead, humans use a series of partial glimpses, deliberately focusing on crucial
regions, to better capture visual structure [36]. Inspired by this, we developed a lightweight CNN-based CBAM module
to classify high-risk and low-risk cervical precancer.
3.2.1
Convolutional Block Attention Module (CBAM)
Fig. 2 illustrates the overview of the CBAM. It comprises two successive sub-modules: channel-attention and spatial-
attention [37]. The CBAM adapts to enhance the intermediate feature map at each convolutional block of deep
networks. The CBAM consecutively evaluates 1D channel attention map Mch ∈RC×1×1 and a 2D spatial attention
maps Msp ∈R1×H×W for a given intermediate feature map S ∈RC×H×W as input. The complete attention process can
be described as:
S′ = Mch(S)⊙S
S′′ = Msp(S′)⊙S′
(1)
where Msp and Mch are spatial and channel attention map, C,H,and W are the channel, height, and width of input
feature map, and ⊙is element-wise multiplication (Hadamard product).
4
Running Title for Header
Table 1: Details of the dataset used to test the proposed model.
Number N(%)
Total (X+Y)
178
Age
median
(range)
30.4 (20.0-62.7)
BMI
median
(range)
22.8 (18.4-47.6)
Smoking
No
96 (53.9%)
Current
41 (23%)
Previous
40 (22.5%)
Unknown
1 (0.6%)
Contraception use
Oral
77 (43.2%)
IUD
28 (15.7%)
Condom
9 (5.1%)
Other
3 (1.7%)
None
61 (34.3%)
HPV vaccination
Not vaccinated
61 (34.3%)
Vaccinated
108 (60.7%)
On-going
8 (4.5%)
Unknown
1 (0.5%)
New Referral
(X=137)
Low-grade
(AS-
CUS/LSIL)
80 (58.4%)
High-grade
(ASC-
H/AGC/HSIL)
57 (41.6%)
HPV test with
referral (From
137)
Negative
2 (6.3%)
HPV 16 + other hr HPV
3 (9.4%)
Other hr HPV alone
26 (81.2%)
HPV 18 alone
1 (3.1%)
HPV
status
un-
known
146 (82.02%)
Follow-up due to
(Y=41)
CIN 1
3 (7.3%)
CIN 2
31 (75.6%)
Ungradable CIN
6 (14.6%)
Unknown
1 (2.4%)
BMI: Body Mass Index, IUD: intra-uterine device; ASCUS: atypical squamous cells of undetermined significance; ASC-H: atypical squamous cells favoring high grade; AGC: atypical glandular cells
Figure 2: Schematic of convolutional block attention module (CBAM).
5
Running Title for Header
3.2.2
Channel attention module (CAM)
Each channel in a feature map serves as a feature detector, with channel attention focusing on ‘what’ is significant to an
input image. Fig. 3 shows a graphical illustration of the steps involved in computing CAM. As shown in Fig 3, the
spatial size of the input feature is squeezed to optimize the channel attention. Therefore, CAM uses average-pooled and
max-pooled features simultaneously. The CAM module is evaluated as [37]:
Mch(S) = sigmoid(MLP(AVP(S))+MLP(MP(S)))
= sigmoid(W1(W0(Sch
avg))+W1(W0(Sch
max)))
(2)
where W0 and W1 are the weights of MLP, spatial context descriptors generated by average-pooling and max-pooling
are denoted by Sch
avg and Sch
max, Mch is channel attention map, AVP is average-pooling, MP is max-pooling, and MLP
denotes a shared multi-layer perceptron network with one hidden layer.
Figure 3: Schematic of channel attention module (CAM).
3.2.3
Spatial attention module (SAM)
CAM focuses on ‘what’ is significant to an input image, whereas SAM focuses on ‘where’ an informative part of
an image is located. The evaluation of spatial attention covers applying average-pooling and max-pooling along the
channel axis and concatenating them to extract representative features. After concatenation, a convolutional layer is
applied to generate SAM, Msp(S) ∈RH×W. Fig. 4 shows the steps involved in computing the SAM. The mathematically
spatial attention module can be evaluated as [37]:
Msp(S) = sigmoid( f 7×7([AVP(S);MP(S)]))
= sigmoid(f 7×7([Ssp
avg;Ssp
max]))
(3)
where Msp is spatial attention map, Ssp
max and Ssp
avg are max-pooled and average-pooled feature maps across the channel,
and f 7×7 represents convolutional operations using a filter size of 7. We developed a novel lightweight Cervix-AID-Net
Figure 4: Schematic of spatial attention module (SAM).
model using the CBAM block. Our Cervix-AID-Net model consists of five convolutional layers, five CBAM blocks,
and three dense layers. Each convolutional layer is followed by a CBAM block to extract relevant feature maps. Table 2
represents the detailed Cervix-AID-Net model with its tuning and learning parameters.
6
Running Title for Header
Table 2: Tuning and learning parameters of the proposed Cervix-AID-Net model.
Layer
Filters
KS
Padding
PS
ACT
PRM
IL (Size:) 224x224x3
–
–
–
–
–
CL
32
3x3
same
–
ReLU
896
CBAM
–
–
–
–
–
390
BN
–
–
–
–
–
128
MP
–
–
–
2x2
–
–
CL
64
3x3
same
–
ReLU
18496
CBAM
–
–
–
–
–
1194
BN
–
–
–
–
–
256
MP
–
–
–
2x2
–
–
CL
128
3x3
same
–
ReLU
73856
CBAM
–
–
–
–
–
4338
BN
–
–
–
–
–
512
MP
–
–
–
2x2
–
–
CL
384
3x3
same
–
ReLU
442752
CBAM
–
–
–
–
–
37394
BN
–
–
–
–
–
1536
MP
–
–
–
2x2
–
–
CL
256
3x3
same
–
ReLU
884992
CBAM
–
–
–
–
–
16770
BN
–
–
–
–
–
1024
MP
–
–
–
2x2
–
–
FL
–
–
–
–
–
–
DnL
Units: 256
–
–
–
ReLU
3211520
DnL
Units: 128
–
–
–
ReLU
32896
DpL
0.25
–
–
–
–
–
DnL
Units: 2
–
–
–
Softmax
258
Total
4729208
KS: Kernel size; PS: Pooling size; ACT: Activation; PRM: Parameters; ReLU: Rectified linear unit; IL: Input layer; CL: Convolutional layer; BN: Batch normalization; MP: Maxpooling layer, FL:
Flatten layer; DnL: Dense layer; DpL: Dropout
3.3
Performance evaluation
We used HO validation and ten-fold cross-validation (10-FCV) strategies to evaluate model performance. For HO
validation, training uses 80% data and the remaining for testing. Out of 20% testing data, 6% for validation and 94%
for testing. The model training, validation, and testing utilized 3153 images. Therefore, the final train, validation, and
test set contains 2524, 37, and 593 images, respectively. For 10F-CV, we randomly split the entire dataset into ten equal
parts, of which training uses nine parts while the tenth part is for testing. The process is repeated ten times to estimate
the average model performance. We have tested the Cervix-AID-Net model performance by evaluating accuracy
(ACC), precision (PRC), negative predicted value (NPV), false positive rate (FPR), F1 measure (F1), sensitivity (SEN),
specificity (SPF), balanced accuracy (B-ACC), and Mathew’s correlation coefficient (MCC), details in [38].
3.4
Explainable AI module
Explainable AI (XAI) are AI systems and models that can offer transparent and intelligible explanations for their
decision-making processes [39]. Traditional ML models, such as deep neural networks, are sometimes referred to
as “black boxes” since it can be hard to understand how they arrive at certain conclusions or predictions [40]. XAI
facilitates transparency, interpretability, trust, and human-readable explanations in the decisions yielded by ML or
DL models. We use four explainable modules: Grad-CAM, LIME, CartoonX, and pixel RDE to visualize and better
understand the decisions of our proposed model.
7
Running Title for Header
3.4.1
Grad-CAM
The Grad-CAM technique operates on output feature maps. It identifies crucial regions in the input image by computing
the gradients of the last convolutional layer, generating heat maps. These heat maps visually represent the significant
regions in the input image, which helped the network to arrive at certain decisions [41, 42]. The mathematical
formulation of grad-CAM technique is denoted by [41]
β m
k = 1
Z

∑
i ∑
j
∂ym
∂Fk
ij

(4)
where ∂ym
∂Fk represents the gradient score for class m with respect to feature maps Fk. β m
k denotes a partial linearization
of the deep network downstream from F, and captures the ‘importance’ of feature map k for a target class m. Finally,
grad-CAM is obtained by taking a ReLU of a weighted combination of forward activation maps, denoted as [41]:
Lm
Grad−CAM = ReLU

∑
k
β m
k Fk
(5)
3.4.2
LIME
LIME is model-agnostic, which means it may be used on any machine/deep learning model independent of architecture
or complexity [43]. The goal of LIME is to train surrogate models locally and explain a single prediction. It generates
a synthetic dataset by randomly permuting samples from a normal distribution and collects predictions based on the
opaque model to be explained. LIME uses the perturbed dataset to train an interpretable model. The mathematical
formulation for LIME is denoted by [44]
ζ(x) = arg min
m∈M L (g,m,πx)+Ω(m)
(6)
where M is a class of Cervix-AID-Net, L is a fidelity function, and Ω(m) measures complexity of the explanations
m ∈M .
3.4.3
Pixel RDE
Pixel RDE are model-independent explanations inspired by rate distortion theory, which examines lossy data compres-
sion [45]. In pixel RDE, explanations use a sparse mask to highlight relevant features from incoming data. The mask is
tailored to minimize distortion in model output after perturbing unselected input features while remaining sparse. RDE
tries to address the constrained optimization problem over a mask denoted by [45]:
min
sm∈{0,1}n:∥sm∥0≤ℓ
E
k∼K
h
d

Ψ(x),Ψ(x⊙sm +(1−sm)⊙k)
i
(7)
where Ψ : Rn →Rm is a pre-trained Cervix-AID-Net model with m(High−riskandLow−risk) classes, dimension of
model input is represented by n, x denotes relevant input features, sparse marks is represented by sm ∈{0,1}n, K is a
distribution over input perturbations k ∈Rn, ⊙is element-wise multiplication (Hadamard product), l ∈{1,2,...,n} is a
sparsity level for mask explanation sm, and d(Ψ(x)) is a measure of distortion. However, in practice, RDE optimization
problem is relaxed to continuous masks denoted by [45]:
min
sm∈[0,1]n E
k∼K
h
d

Ψ(x),Ψ(x⊙sm +(1−sm)⊙k)
i
+λ||sm||1
(8)
where λ takes values > 0, which is a hyper-parameter for the sparsity level.
3.4.4
CartoonX
CartoonX is a novel explanation technique that is a special case of RDE. CartoonX first executes RDE in the discrete
wavelet position-scale domain of an image x and then visualizes the wavelet mask sm as a pixel-wise smooth picture.
Wavelets efficiently represent 2D piece-wise smooth pictures, commonly known as cartoon-like images, along with
providing optimum representations for piece-wise smooth 1D functions [45]. Algorithm 1 illustrates the steps for
obtaining CartoonX explanations.
4
Experimental setup and Results
This section covers experimental setup, including evaluation criteria and evaluation metrics and results.
8
Running Title for Header
Algorithm 1 Algorithm for CartoonX
Data: Cervix-AID-Net (Ψ), image x ∈[0,1]n with channels c, and pixels k.
Parameters: Number of steps N, sparsity level (λ > 0), distortion d, number of noise samples L.
Initialization: sm := [1,...,1]T on DWT coefficients h = [h1,..,hk]T with x = f(h), where f is DWT −1.
for i ←1 to N do do
Sampling: Sample L adaptive Gaussian noise samples k(1),..,k(L) ∼N (µ,σ2);
Obfuscations: Evaluate obfuscations y(1),..,y(L) with y(i) := f(h⊙sm +(1−sm)⊙k(i);
Clipping: Clip obfuscations into [0,1]n;
Distortion: Approx. estimated distortion ˆED(x,sm,Ψ) := ∑L
l=1 d(Ψ(x),Ψ(y(i)))2/L;
Loss: Compute loss, l(sm) := ˆED(x,sm,Ψ)+λ||sm||1;
Updation: Update mask sm with gradient descent step using ∆sml(sm) and clip sm back to [0,1]k;
end for
DWT coefficients: Get DWT coefficients ˆh for greyscale image ˆx of x;
Set: ξ := f(ˆh⊙sm);
Clipping: Clip ξ to [0,1]k.
Table 3: Accuracy comparison of the proposed Cervix-AID-Net with benchmark CNN models.
Models
HO
10-FCV
AlexNet
95.45
98.07
G-Net
97.98
98.13
ECANet
98.82
98.73
Cervix-AID-Net
99.33
99.81
4.1
Experimental setup
The experiment was performed on an AMD Ryzen 7 PRO 6850U with Radeon Graphics with a frequency of 2.70
GHz. The experimental setup used a 64-bit operating system, x64-based processor, and 16 GB RAM. We used Jupyter
Notebook and Python version 3.11.4 to build the model. The high-risk class contains 1404 images, while the low-risk
class contains 1749 images. We reshaped all the images to 224 x 224, as the models processes an input image of 224 x
224. We used an “Adam” optimizer, loss function as “sparse categorical cross entropy”, and accuracy as an evaluation
matrix to compile the model. The model iterates for 25 epochs with a batch size of 32.
4.2
Results
The proposed Cervix-AID-Net model is trained and evaluated in multiple scenarios for comprehensive data analysis.
To have a faithful comparison concerning performance and tuning parameters, we compared the performance of the
proposed model with benchmark CNN models like AlexNet, GoogleNet (G-Net), and ECANet, respectively [46, 47, 48].
AlexNet is a simple eight-layered CNN model capable of performing 1000 class classifications, G-Net includes
inception layers, and ECANet is an efficient channel attention module for the CNN model. Table 3 shows the accuracy
of Cervix-AID-Net as compared with the benchmark models, using HO and 10-FCV techniques. The results show
that our proposed Cervix-AID-Net surpasses the performance obtained using benchmark CNN models for both, HO
and 10-FCV validation techniques. To further evaluate, validate, and compare the performance of the Cervix-AID-Net
model, we obtained evaluation matrices and compared them with the performance of benchmark CNN models. Table 4
indicates the performance report of various models using the HO validation technique. It is evident from Table 4 that
the proposed Cervix-AID-Net model surpasses most other models in terms of accuracy, sensitivity, F1 measure, MCC,
NPV, and balanced accuracy. However, ECANet and G-Net provide slightly better precision, specificity, and FPR than
the Cervix-AID-Net model. However, the main limitation of the HO validation technique is that model performance
depends significantly on the random split. Also, training uses only a portion of the data, which increases the probability
of over-fitting and bias. Therefore, to avoid instability of sampling (where different results are obtained when the
experiment is repeated with a new division), we validated our model performance using the 10-FCV technique. Table 4
provides the performance of the benchmark CNN models and the proposed Cervix-AID-Net model using the 10-FCV
technique. Table 4 confirms that our developed Cervix-AID-Net model is superior to other benchmark techniques. The
Cervix-AID-Net model yielded the highest performance of all the evaluation metrics. Therefore, analysis shows that
our Cervix-AID-Net model is robust and accurate over existing CNN models.
9
Running Title for Header
Table 4: Performance of the proposed Cervix-AID-Net obtained using HO and 10-FCV validation techniques.
HO
10-FCV
Models
AlexNet
G-Net
ECANet
Cervix-
AID-
Net
AlexNet
G-Net
ECANet
Cervix-
AID-
Net
ACC
95.45
97.98
98.82
99.33
98.07
98.13
98.73
99.81
SEN
99.16
100
98.47
98.85
97.06
98.63
99.21
99.86
SPF
92.98
96.52
99.10
99.70
98.90
97.74
98.36
99.77
PRC
90.38
95.38
98.85
99.62
98.65
97.15
97.93
99.72
F1
94.57
97.64
98.66
99.23
97.85
97.88
98.57
99.79
MCC
0.91
0.96
0.98
0.99
0.96
0.96
0.97
1
FPR
7.02
3.48
0.90
0.30
1.10
2.26
1.64
0.23
NPV
99.40
100
98.80
99.10
97.60
98.91
99.37
99.89
B-ACC
96.07
98.26
98.78
99.28
97.98
98.18
98.78
99.81
Table 5: Comparison of tuning parameters of the proposed Cervix-AID-Net with benchmark CNN models.
Models
TTP
TNP
NTNP
Size (MB)
AlexNet
25719386
25699786
19600
98.11
G-Net
7420982
7420470
512
28.31
ECANet
15464090
15463130
960
58.99
Cervix-AID-Net
4729208
4727480
1728
18.04
TTP: Total parameters; TNP: Trainable parameters; NTNP: Non trainable parameters
The effectiveness of the proposed Cervix-AID-Net model over existing benchmark CNN models was compared in
terms of network architecture. Table 5 represents a comparison of tuning parameters. It is evident from Table 5 that
our coined model has the least tuning parameters compared to the existing AlexNet, G-Net, and ECANet models. The
AlexNet, G-Net, and ECANet models require 98.11 MB, 28.31 MB, and 58.99 MB of space, respectively. The analysis
also reveals that the Cervix-AID-Net model required only 18.04 MB of storage, the least of all the models used for the
comparison. Therefore, our proposed model is lightweight and simple.
We also evaluated the confusion matrix to get insight into the proposed Cervix-AID-Net model. Figure 5 shows the
confusion matrix of the Cervix-AID-Net obtained using HO and 10-FCV techniques. As indicated in Figure 5 (a), one
image of low-risk has been classified as high-risk, while six images of high-risk class have been identified as low-risk in
HO validation. Similarly, for the 10-FCV technique, our developed Cervix-AID-Net model has correctly identified
1747 and 1400 images corresponding to low-risk and high-risk, respectively. However, out of 1749 low-risk instances,
two images were incorrectly identified as high-risk, and from 1404 instances of high-risk, four images were incorrectly
identified as low-risk. This indicates that our developed model provides an effective classification of targeted classes.
We further evaluated receiver operator characteristics (ROC) and the area under the curve (AUC) to investigate the
binary classification prediction performance of our proposed Cervix-AID-Net model. Figure 6 indicates the ROC and
the AUC curve obtained using HO and 10-FCV techniques. Our proposed model has obtained 99% and 100% of AUC
for HO and 10-FCV, which confirms the binary classification ability of the Cervix-AID-Net model.
5
Discussion
Our Cervix-AID-Net model aims to provide transparent and effective explanations for its decisions using four explainable
techniques. These XAI techniques require tuning of hyper-parameters to generate explanations for the model’s
decisions. Table 6 represents various tuning parameters used for LIME, pixel RDE, and CartoonX. Grad-CAM provides
explanations based on output feature maps that generate the gradients from the last convolutional layer. Fig. 7 indicates
the grad-CAM obtained from the last convolutional layer of our proposed Cervix-AID-NET model. Our analysis
confirms that the heat maps from grad-CAM outline the relevant regions around the cervix. However, there are some
instances where the network does not outline any regions around the cervix. It could be because of uneven light
distribution on the images, motion artifacts, or instances of outliers, shown in the second image in Fig. 7. On the other
hand, LIME explains specific instances using input features. LIME approximates the pre-trained model to generate a
surrogate model that explains local approximations around the input image. As evident from Fig. 7, LIME highlights
crucial regions in the neighborhood of the input image, which provides the most relevant input features for that particular
instance. Fig. 7 reveals that LIME maps the region around the cervix, marking relevant features to classify correctly in
10
Running Title for Header
(a) HO validation
(b) 10-FCV
Figure 5: Confusion matrix obtained for the proposed Cervix-AID-Net model (0-Low-risk and 1-High-risk).
11
Running Title for Header
(a) HO validation
(b) 10-FCV
Figure 6: ROC-AUC obtained for the Cervix-AID-Net model.
Table 6: Tuning parameters used for XAI methods.
Methods
LIME
Pixel RDE
CartoonX
Number of features
20
–
–
Number of samples
2000
–
–
Kernel size
1
–
–
Max distance
200
–
–
Ratio
0.2
–
–
Segmenter
Quick shift
–
–
Lambda
–
4
285
Step size
–
0.01
0.1
Number of steps
–
200
100
Batch size
–
–
16
Obfuscation
–
Gaussian
Gaussian
12
Running Title for Header
Table 7: Performance comparison of the Cervix-AID-Net with existing SOTA techniques employing binary classifica-
tion.
Author & Year
Images
Image-generating devices
Classes
Method
Validation
ACC
PRC
Recall
SPF
NPV
Li et al. [16] (2020)
7668
Traditional colposcopy
Pos vs Neg
E-GCN
HO (80:20)
81.95
81.97
81.78
–
–
Elakkiya et al. [17] (2022)
3105
Traditional colposcopy
Normal vs AN
FR-CNN-GAN
HO (80:20)
98.55
98.66
100
100
–
Adweb et al. [18] (2021)
7920
Traditional colposcopy
HC vs PC
ResNet
HO (60:40)
100
100
100
100
Kim et al. [19] (2022)
–
Digital colposcopy
–
AIDOTNet
–
–
81.13
74.14
83.05
–
Saini et al. [20] (2020)
800
Traditional colposcopy
Type 1 vs Type 2
ColpoNet
HO (70:15:15)
81.35
–
–
–
–
Kim et al. [24] (2013)
2000
Cervicography (discontinued)
Normal vs CIN2+
Texture features and SVM
10F-CV
–
–
75
76
–
Cho et al. [25] (2020)
791
Digital colposcopy
HighRisk vs LowRisk
Inception-ResNet
HO (85:15)
69.3
47.2
66.7
70.6
84
Liu et al. [26] (2021)
15276
Digital colposcopy
NC vs LSIL
ResNet-24
HO (70:10:20)
88.2
85.3
90.1
86.7
91
HSIL vs HSIL+
79.7
60.6
80.2
79.6
91.1
Sim˜oes et al. [29] (2014)
170
Digital colposcopy
–
MLP
–
72.15
–
69.78
68
–
Asiedu et al. [30] (2019)
–
Digital colposcopy
Pos vs Neg
Gabor segmentation-SVM
5F-CV
80
–
81.3
78.6
–
Miyagi et al. [31] (2020)
253
Traditional colposcopy
LSIL vs HSIL
CNN
HO (80:20)
94.1
97.7
95.6
83.3
71.4
Song et al. [32] (2015)
60000
Cervicography (discontinued)
Pos vs Neg
Multimodal CNN
10F-CV
89
–
83.21
94.79
–
Miyagi et al. [33] (2019)
310
Traditional colposcopy
CIN1 vs CIN2+
CNN
5F-CV
82.3
94.7
80
88.2
62.5
Our Cervix-AID-Net
3154
Digital colposcopy
HighRisk vs LowRisk
CNN with CBAM
HO (80:20)
99.33
99.62
98.85
99.70
99.10
10F-CV
99.81
99.72
99.86
99.77
99.89
high-risk or low-risk class, which are significant for that instance. However, LIME fails to identify all the features in the
image which are relevant for decision-making. Our observation with LIME was that though it does not highlight all the
regions accurately, LIME does not miss out on marking the crucial features around the cervix. LIME’s explanation
focuses primarily on the characteristics that contribute independently to the model output, and their contributions should
reflect their importance. However, such explanations are relevant and crucial in scenarios like segmentation, where
individual pixels carry relevant and interpretable meanings for the decision. However, for a classification problem, it is
desired to identify a group of features that are interpretable but not individual pixels that do not carry any interpretable
meaning. Therefore, we favour pixel RDE and CartoonX approaches, which seek a group of significant characteristics
rather than an estimate of individual relative contributions. Fig 8 provides an illustrative representation of pixel RDE
and CartoonX. The explanation reveals that the mask provided by pixel RDE is the explanation as it lies in pixel space.
Our analysis indicates that explanations of pixel RDE are highly non-stationary. At some instances, pixel RDE focuses
on the relevant regions as indicated by clinical experts, as highlighted in the images of the first, third, fifth, and sixth
rows. However, in some instances, the explanations yielded by pixel RDE were nowhere near the cervix region because
of light intensities or outliers, as indicated in the images of the second and fourth rows. Finally, Fig 8 shows that
explanations yielded by CartoonX methods are the most relevant as they map the crucial region in the image. Our
analysis shows that explanations given by CartoonX lie around the cervix region, which is the most crucial part of the
image. The reason for a more meticulous explanation of CartoonX is its ability to extract relevant piece-wise smooth
parts of an image instead of relevant pixel sparse regions. The sparsity in the wavelet domain captures interpretable
input features from the image compared to sparsity in the pixel domain, instance-based explanations, and output
neuron activations. Our analysis shows that CartoonX captures piece-wise smooth explanations that can reveal relevant
piece-wise smooth patterns that are not easily visible with existing grad-CAM that operates on the output feature maps
and pixel-based methods like LIME and pixel RDE.
Noise and blur are two prominent image classification issues that might impair the performance of ML/DL models.
Understanding and addressing these challenges is critical for enhancing image classifier robustness and accuracy.
Random noise defines the existence of random fluctuations in pixels that do not constitute part of the image’s actual
content. It may be introduced during acquisition, transmission, or processing and can influence image quality and
appearance. Blur occurs when an image’s features are not finely defined, commonly caused by motion during acquisition,
defocus, or other optical difficulties. To test these challenges, we added different levels of Gaussian noise to the test
images. We randomly selected eight images and added 3%, 5%, 10%, and 30% of Gaussian noise. We also added
a median blur of 5%, 10%, 20%, and 30% to these images. Analysis shows that the model performance remains
unchanged with Gaussian noise of 3%, and blur of 5%, and 10%. However, the accuracy drops by 25% with 5% of
Gaussian noise and 20% of blur. In addition, the performance of the developed model reduces by 50% for 10% and 30%
of Gaussian noise and 20% blur. The results indicate that the performance of our developed model does not change
until a random noise exceeds 3% and until a blur of 10%. The analysis shows that the proposed model can work with
low-resolution images with minimal loss.
Finally, the effectiveness of the developed Cervix-AID-Net model is test by comparing the performance with existing
state-of-the-art (SOTA) techniques. Table 7 represents the binary classification evaluation report of the SOTA techniques,
13
Running Title for Header
Figure 7: Example of grad-CAM and LIME explanations obtained from the last convolutional layer of the proposed
Cervix-AID-Net model. (0-Low-risk and 1-High-risk)
that is, the best-performing model or algorithm that achieves the highest accuracy or provides the most advanced
functionality. The comparison report of performance parameters shows that our developed model is superior. Hence,
our Cervix-AID-Net model is robust, accurate, and effective in classifying high-risk and low-risk cervical precancer
images. The key features of the proposed model are listed below:
• The dataset is unique due to the exact mapping of the cervix.
• The Cervix-AID-Net model is accurate and explainable for classifying high-risk and low-risk cervical precan-
cer.
• The model is lightweight due to fewer tuning and learnable parameters over benchmark CNN networks.
• The Cervix-AID-Net model generates highly discriminant features due to the CBAM module.
• The proposed model is robust due to validation using HO and 10-FCV techniques.
• The proposed Cervix-AID-Net is accurate as it has obtained 99.81% accuracy, outperforming SOTA techniques.
However, the proposed model has few short-comings, which are mentioned below:
• The model was tested on a relatively small dataset from a single hospital.
• The effects of domain shift are not verified due to the unavailability of external datasets.
• Only binary classification is targeted.
In the future, we will explore the following research directions to further improve our model, by addressing:
• Segmentation of low-risk and high-risk cervical precancer.
• Uncertainty quantification using different noise sources.
• Validation of model’s performance on an external dataset.
• Hyper-parameter tuning for XAI techniques.
14
Running Title for Header
Figure 8: Example of explanations obtained from CartoonX and pixel RDE
15
Running Title for Header
(a) Gaussian Noise
(b) Median blur noise
Figure 9: Example of different noise sources with various levels of noise disturbances.
16
Running Title for Header
6
Conclusion
The paper presents an effective AI framework for the classification of low-risk and high-risk cervical precancer
lesions from colposcopy images. The proposed Cervix-AID-Net model captures representation features from the
colposcopic images by maintaining attention over the channel and spatial domain. Our proposed model has effectively
identified what is the significant region in the image and where it is located within the image using CBAM. Therefore,
our Cervix-AID-Net model yielded the highest accuracy in classifying high-risk and low-risk cervical precancer
instances, compared to existing benchmark CNN models. In addition, our model facilitates visually readable transparent
explanations for decision-making, which makes it robust and effective. The proposed model explains relevant output
feature maps and important input features required for accurate decision-making. We show that piece-wise smooth
explanations extract representative and discriminant features of the input image. Thus, the explanations yielded by
CartoonX were the most interpretable and discriminant compared to pixel RDE, LIME, and grad-CAM techniques. The
experimental analysis demonstrated that the proposed model may yield effective decision-making on high-resolution
and low-resolution images. The Cervix-AID-Net model is lightweight, has been tested on multiple scenarios were it
yielded the best performance matrices compared to current models, and it has integrated XAI techniques. In conclusion,
the Cervix-AID-Net model, presented in this article, is an accurate, robust, transparent, effective, and simple, that could
be used in future for early identification and assessment of cervical precancer lesion during colposcopy.
7
Funding Information
For this study we got funding from the BetaHealth Program from NOVO Nordisk (ID-nummer 2023-1384) and an
Innovation Grant from Region Syddanmark.
References
[1] Freddie Bray, Jacques Ferlay, Isabelle Soerjomataram, Rebecca L. Siegel, Lindsey A. Torre, and Ahmedin Jemal.
Global cancer statistics 2018: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185
countries. CA: A Cancer Journal for Clinicians, 68(6):394–424, 2018.
[2] eClinicalMedicine. Global strategy to eliminate cervical cancer as a public health problem: are we on track?
eClinicalMedicine, 55:101842, 2023.
[3] Basic information about cervical cancer, centers for disease control and prevention date= Accessed: February, 20,
2024, doi = https://www.cdc.gov/cancer/cervical/basic info/index.htm,.
[4] Oluwole Fadare, Andres A Roma, Oluwole Fadare, and Andres A Roma. Normal anatomy of the uterine cervix.
Atlas of Uterine Pathology, pages 193–196, 2019.
[5] Sumita Mehta and Poonam Sachdeva. Colposcopy of female genital tract. Technical report, Springer, 2017.
[6] Walter Prendiville and Rengaswamy Sankaranarayanan. Chapter 2., Anatomy of the uterine cervix and the
transformation zone. Lyon (FR): International Agency for Research on Cancer, Lyon, France, 2017.
[7] Srikanta Dash, Prabira Kumar Sethy, and Santi Kumari Behera.
Cervical transformation zone segmenta-
tion and classification based on improved inception-resnet-V2 using colposcopy images. Cancer Informatics,
22:11769351231161477, 2023.
[8] Walter Prendiville and Rengaswamy Sankaranarayanan. Colposcopy and treatment of cervical precancer. Interna-
tional Agency for Research on Cancer, World Health Organization, 2017.
[9] World health organization, who director-general calls for all countries to take action to help end the suffering
caused by cervical cancer accessed= Accessed: February, 20, 2024, doi = https://www.who.int/news/item/18-05-
2018-who-dg-calls-for-all-countries-to-take-action-to-help-end-the-suffering-caused-by-cervical-cancer,.
[10] Marc Brisson and M´elanie Drolet. Global elimination of cervical cancer as a public health problem. The Lancet
Oncology, 20(3):319–321, 2019.
[11] Lynette Denny, Louise Kuhn, Michelle De Souza, Amy E. Pollack, William Dupree, and Thomas C. Wright.
Screen-and-Treat Approaches for Cervical Cancer Prevention in Low-Resource SettingsA Randomized Controlled
Trial. JAMA, 294(17):2173–2181, 11 2005.
[12] Mark H. Stoler, Michelle D. Vichnin, Alex Ferenczy, Daron G. Ferris, Gonzalo Perez, Jorma Paavonen, Elmar A.
Joura, Henning Djursing, Kristj´an Sigurdsson, Lucy Jefferson, Frances Alvarez, Heather L. Sings, Shuang Lu,
Margaret K. James, Alfred Saah, Richard M. Haupt, and for the FUTURE I, II and III Investigators . The accuracy
of colposcopic biopsy: Analyses from the placebo arm of the gardasil clinical trials. International Journal of
Cancer, 128(6):1354–1362, 2011.
17
Running Title for Header
[13] Karin Dembrower, Alessio Crippa, Eugenia Col´on, Martin Eklund, and Fredrik Strand. Artificial intelligence for
breast cancer detection in screening mammography in sweden: a prospective, population-based, paired-reader,
non-inferiority study. The Lancet Digital Health, 5(10):e703–e711, 2023.
[14] Yifeng Peng and Haijun Deng. Medical image fusion based on machine learning for health diagnosis and
monitoring of colorectal cancer. BMC Medical Imaging, 24(1):24, 2024.
[15] Maurizio C`e, Giovanni Irmici, Chiara Foschini, Giulia Maria Danesini, Lydia Viviana Falsitta, Maria Lina Serio,
Andrea Fontana, Carlo Martinenghi, Giancarlo Oliva, and Michaela Cellina. Artificial intelligence in brain tumor
imaging: A step toward personalized medicine. Current Oncology, 30(3):2673–2701, 2023.
[16] Yuexiang Li, Jiawei Chen, Peng Xue, Chao Tang, Jia Chang, Chunyan Chu, Kai Ma, Qing Li, Yefeng Zheng, and
Youlin Qiao. Computer-aided cervical cancer diagnosis using time-lapsed colposcopic images. IEEE Transactions
on Medical Imaging, 39(11):3403–3415, 2020.
[17] R. Elakkiya, V. Subramaniyaswamy, V. Vijayakumar, and Aniket Mahanti. Cervical cancer diagnostics healthcare
system using hybrid object detection adversarial networks. IEEE Journal of Biomedical and Health Informatics,
26(4):1464–1471, 2022.
[18] Khaled Mabrouk Amer Adweb, Nadire Cavus, and Boran Sekeroglu. Cervical cancer diagnosis using very deep
networks over different activation functions. IEEE Access, 9:46612–46625, 2021.
[19] Seongmin Kim, Hwajung Lee, Sanghoon Lee, Jae-Yun Song, Jae-Kwan Lee, and Nak-Woo Lee. Role of artificial
intelligence interpretation of colposcopic images in cervical cancer screening. Healthcare, 10(3), 2022.
[20] Sumindar Kaur Saini, Vasudha Bansal, Ravinder Kaur, and Mamta Juneja. Colponet for automated cervical cancer
screening using colposcopy images. Machine Vision and Applications, 31:1–15, 2020.
[21] Lidiya Wubshet Habtemariam, Elbetel Taye Zewde, and Gizeaddis Lamesgin Simegn. Cervix type and cervical
cancer classification system using deep learning techniques. Medical Devices: Evidence and Research, 15:163–
176, 2022. PMID: 35734419.
[22] Jing-Hang Ma, Shang-Feng You, Ji-Sen Xue, Xiao-Lin Li, Yi-Yao Chen, Yan Hu, and Zhen Feng. Computer-aided
diagnosis of cervical dysplasia using colposcopic images. Frontiers in Oncology, 12:905623, 2022.
[23] Binhua Dong, Huifeng Xue, Ye Li, Ping Li, Jiancui Chen, Tao Zhang, Lihua Chen, Diling Pan, Peizhong Liu,
and Pengming Sun. Classification and diagnosis of cervical lesions based on colposcopy images using deep fully
convolutional networks: A man-machine comparison cohort study. Fundamental Research, 2022.
[24] Edward Kim and Xiaolei Huang. A data driven approach to cervigram image analysis and classification. In Color
medical image analysis, pages 1–13. Springer, 2013.
[25] Bum-Joo Cho, Youn Jin Choi, Myung-Je Lee, Ju Han Kim, Ga-Hyun Son, Sung-Ho Park, Hong-Bae Kim, Yeon-Ji
Joo, Hye-Yon Cho, Min Sun Kyung, et al. Classification of cervical neoplasms on colposcopic photography using
deep learning. Scientific reports, 10(1):13652, 2020.
[26] Lu Liu, Ying Wang, Xiaoli Liu, Sai Han, Lin Jia, Lihua Meng, Ziyan Yang, Wei Chen, Youzhong Zhang, and
Xu Qiao. Computer-aided diagnostic system based on deep learning for classifying colposcopy images. Annals of
Translational Medicine, 9(13), 2021.
[27] Venkatesan Chandran, MG Sumithra, Alagar Karthick, Tony George, M Deivakani, Balan Elakkiya, Umashankar
Subramaniam, S Manoharan, et al. Diagnosis of cervical cancer based on ensemble deep learning network using
colposcopy images. BioMed Research International, 2021, 2021.
[28] Aiyuan Wu, Peng Xue, Guzhalinuer Abulizi, Dilinuer Tuerxun, Remila Rezhake, and Youlin Qiao. Artificial
intelligence in colposcopic examination: A promising tool to assist junior colposcopists. Frontiers in Medicine,
10:1060451, 2023.
[29] Priscyla W Sim˜oes, Narjara B Izumi, Ramon S Casagrande, Ramon Venson, Carlos D Veronezi, Gustavo P
Moretti, Edroaldo L da Rocha, Cristian Cechinel, Luciane B Ceretta, Eros Comunello, et al. Classification of
images acquired with colposcopy using artificial neural networks. Cancer informatics, 13:CIN–S17948, 2014.
[30] Mercy Nyamewaa Asiedu, Anish Simhal, Usamah Chaudhary, Jenna L. Mueller, Christopher T. Lam, John W.
Schmitt, Gino Venegas, Guillermo Sapiro, and Nimmi Ramanujam. Development of algorithms for automated
detection of cervical pre-cancers with a low-cost, point-of-care, pocket colposcope. IEEE Transactions on
Biomedical Engineering, 66(8):2306–2318, 2019.
[31] Yasunari Miyagi, Kazuhiro Takehara, Yoko Nagayasu, and Takahito Miyake. Application of deep learning to the
classification of uterine cervical squamous epithelial lesion from colposcopy images combined with hpv types.
Oncology letters, 19(2):1602–1610, 2020.
18
Running Title for Header
[32] Dezhao Song, Edward Kim, Xiaolei Huang, Joseph Patruno, H´ector Mu˜noz-Avila, Jeff Heflin, L. Rodney Long,
and Sameer Antani. Multimodal entity coreference for cervical dysplasia diagnosis. IEEE Transactions on
Medical Imaging, 34(1):229–245, 2015.
[33] Yasunari Miyagi, Kazuhiro Takehara, and Takahito Miyake. Application of deep learning to the classification of
uterine cervical squamous epithelial lesion from colposcopy images. Molecular and clinical oncology, 11(6):583–
589, 2019.
[34] Rebecca B Perkins, Richard S Guido, Philip E Castle, David Chelmow, Mark H Einstein, Francisco Garcia,
Warner K Huh, Jane J Kim, Anna-Barbara Moscicki, Ritu Nayar, et al. 2019 ASCCP risk-based management
consensus guidelines: Updates through 2023. Journal of lower genital tract disease, 28(1):3–6, 2024.
[35] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 20(11):1254–1259, 1998.
[36] Hugo Larochelle and Geoffrey E Hinton. Learning to combine foveal glimpses with a third-order boltzmann
machine. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural
Information Processing Systems, volume 23. Curran Associates, Inc., 2010.
[37] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. CBAM: Convolutional block attention
module. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018.
[38] G¨urol Canbek, Tugba Taskaya Temizel, and Seref Sagiroglu. PToPI: A comprehensive review, analysis, and
knowledge representation of binary classification performance measures/metrics. SN Computer Science, 4(1):13,
2022.
[39] Smith K. Khare, Victoria Blanes-Vidal, Esmaeil S. Nadimi, and U. Rajendra Acharya. Emotion recognition and
artificial intelligence: A systematic review (2014–2023) and research recommendations. Information Fusion,
102:102019, 2024.
[40] Smith K. Khare, Sonja March, Prabal Datta Barua, Vikram M. Gadre, and U. Rajendra Acharya. Application of
data fusion for automated detection of children with developmental and mental disorders: A systematic review of
the last decade. Information Fusion, 99:101898, 2023.
[41] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv
Batra. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In 2017 IEEE
International Conference on Computer Vision (ICCV), pages 618–626, 2017.
[42] El-Sayed A. El-Dahshan, Mahmoud M. Bassiouni, Smith K. Khare, Ru-San Tan, and U. Rajendra Acharya.
ExHyptNet: An explainable diagnosis of hypertension using efficientnet with PPG signals. Expert Systems with
Applications, 239:122388, 2024.
[43] Smith K. Khare and U. Rajendra Acharya. Adazd-Net: Automated adaptive and explainable alzheimer’s disease
detection system using EEG signals. Knowledge-Based Systems, 278:110858, 2023.
[44] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you? explaining the predictions of
any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and
data mining, pages 1135–1144, 2016.
[45] Stefan Kolek, Duc Anh Nguyen, Ron Levie, Joan Bruna, and Gitta Kutyniok. Cartoon explanations of image
classifiers. In European Conference on Computer Vision, pages 443–458. Springer, 2022.
[46] Md Zahangir Alom, Tarek M Taha, Christopher Yakopcic, Stefan Westberg, Paheding Sidike, Mst Shamima Nasrin,
Brian C Van Esesn, Abdul A S Awwal, and Vijayan K Asari. The history began from alexnet a comprehensive
survey on deep learning approaches. arXiv preprint arXiv1803.01164, 2018.
[47] R Anand, T Shanthi, MS Nithish, and S Lakshman. Face recognition and classification using googleNET
architecture. In Soft Computing for Problem Solving SocProS 2018, Volume 1, pages 261–269. Springer, 2020.
[48] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. ECA-Net efficient channel
attention for deep convolutional neural networks. In Proceedings of the IEEECVF conference on computer vision
and pattern recognition, pages 11534–11542, 2020.
19
